- Danh s√°ch ch∆∞a l√†m ƒë∆∞·ª£c:

	C·∫ßn ƒë·ªçc hi·ªÉu ph·∫ßn VPC trong AWS part 2

	AWS Part 1:
	
		EC2 Load Balancer
		
	AWS Cloud For Project Set Up | Lift & Shift:
	
		Ch∆∞a ƒëƒÉng nh·∫≠p web th√†nh c√¥ng v·ªõi user, password l√† admin_vp
		
	Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:
	
		Build & Deploy Artifact:
		
			Ph·∫ßn target group ki·ªÉm tra v·∫´n unhealthy
			
	Jenkins:
	
		Jenkins Master and Slave t·ª´ ch·ªó n√†y v·ªÅ sau, xem x√©t ch·ª•p ·∫£nh m·ªõi v√† l√†m l·∫°i
		
	Terraform:
	
		Ch∆∞a upload ƒë∆∞·ª£c file l√™n s3 ·ªü ph·∫ßn cu·ªëi
		
	Ansible:
	
		Ch·ªó roles c√≥ ph·∫ßn ch·∫°y b·ªã l·ªói
		
	AWS Part 2:
	
		B·ªã l·ªói t·∫°i Website in VPC
		
	Docker:
	
		Building images
		
			Ch∆∞a th·ªÉ show ƒë√∫ng trang web
			
	Containerization:
	
		Ph·∫ßn cu·ªëi ch·∫°y theo source git c·ªßa t√°c gi·∫£ ch∆∞a ƒë∆∞·ª£c
		
	App Deployment on Kubernetes Cluster:
	
		Deploy App on K8s Cluster:
		
			Ingress ch∆∞a c√≥ address
			
	GitOps Project:
	
		Deploy to EKS:
		
			Ch∆∞a ch·∫°y l√™n trang web ƒë∆∞·ª£c


- Iaas, Paas
- ORACLE VM VIRTUAL BOX
- AWS Command line Interface











- GIT BASH

	Git Bash l√† m·ªôt ·ª©ng d·ª•ng gi·∫£ l·∫≠p d√≤ng l·ªánh (terminal) d√†nh cho Windows.

	N√≥ cung c·∫•p:

		Git CLI (Command Line Interface) ‚Üí ƒë·ªÉ ch·∫°y c√°c l·ªánh Git (git init, git clone, git commit, ‚Ä¶).

		Bash shell ‚Üí m·ªôt m√¥i tr∆∞·ªùng gi·ªëng Linux/Unix ƒë·ªÉ ch·∫°y c√°c l·ªánh bash (ls, pwd, cd, rm, touch, ‚Ä¶).

	N√≥i ƒë∆°n gi·∫£n: Git Bash gi√∫p ng∆∞·ªùi d√πng Windows c√≥ tr·∫£i nghi·ªám gi·ªëng nh∆∞ ƒëang l√†m vi·ªác tr√™n Linux khi d√πng Git.
	
	T·∫°i sao c·∫ßn Git Bash?

		Tr√™n Linux/Mac, Git th∆∞·ªùng ƒëi k√®m v·ªõi Bash s·∫µn c√≥.

		Nh∆∞ng Windows kh√¥ng c√≥ Bash g·ªëc ‚Üí v√¨ v·∫≠y Git for Windows ra ƒë·ªùi, k√®m theo Git Bash ƒë·ªÉ:

			D√πng l·ªánh Git thu·∫≠n ti·ªán.

			D√πng c√°c l·ªánh bash c∆° b·∫£n (thay v√¨ ch·ªâ Command Prompt hay PowerShell).
			
	C√°c th√†nh ph·∫ßn ch√≠nh

		Git ‚Üí c√¥ng c·ª• qu·∫£n l√Ω phi√™n b·∫£n.

		Bash ‚Üí shell d·ª±a tr√™n MSYS2 (m·ªôt m√¥i tr∆∞·ªùng gi√∫p mang Linux tools sang Windows).

		Unix tools ‚Üí b·∫°n c√≥ th·ªÉ d√πng nhi·ªÅu l·ªánh quen thu·ªôc trong Linux (ssh, scp, cat, nano, vim, ‚Ä¶).

- VAGRANT

	Vagrant l√† m·ªôt c√¥ng c·ª• m√£ ngu·ªìn m·ªü (open-source) d√πng ƒë·ªÉ t·∫°o, qu·∫£n l√Ω v√† t·ª± ƒë·ªông h√≥a m√¥i tr∆∞·ªùng m√°y ·∫£o (VMs).

	N√≥ gi√∫p l·∫≠p tr√¨nh vi√™n, tester, DevOps‚Ä¶ c√≥ th·ªÉ d·ª±ng m√¥i tr∆∞·ªùng ph√°t tri·ªÉn gi·ªëng nhau ch·ªâ v·ªõi m·ªôt file c·∫•u
	h√¨nh (Vagrantfile).

	Vagrant th∆∞·ªùng ch·∫°y tr√™n VirtualBox, VMware, Hyper-V ho·∫∑c Docker.
	
	∆Øu ƒëi·ªÉm c·ªßa Vagrant

		T√°i t·∫°o m√¥i tr∆∞·ªùng d·ªÖ d√†ng

		Ch·ªâ c·∫ßn 1 file Vagrantfile, b·∫•t k·ª≥ ai c≈©ng c√≥ th·ªÉ t·∫°o ra m√¥i tr∆∞·ªùng y h·ªát.

		Tr√°nh l·ªói ki·ªÉu: "M√°y anh ch·∫°y ƒë∆∞·ª£c, m√°y em kh√¥ng ch·∫°y".

		T√≠ch h·ª£p v·ªõi nhi·ªÅu provider

		H·ªó tr·ª£ VirtualBox, VMware, Docker, AWS EC2...

		T·ª± ƒë·ªông h√≥a

		C√≥ th·ªÉ c√†i ƒë·∫∑t ph·∫ßn m·ªÅm, c·∫•u h√¨nh h·ªá ƒëi·ªÅu h√†nh th√¥ng qua provisioning tools (Shell script, Ansible, Puppet, Chef...).

		D·ªÖ d√πng

		L·ªánh ch√≠nh th∆∞·ªùng ch·ªâ c√≥ vagrant up, vagrant halt, vagrant destroy.
		
	C√°ch ho·∫°t ƒë·ªông

		B·∫°n c√†i Vagrant + m·ªôt provider (v√≠ d·ª• VirtualBox).

		T·∫°o file Vagrantfile m√¥ t·∫£ m√¥i tr∆∞·ªùng (OS, RAM, CPU, m·∫°ng, ph·∫ßn m·ªÅm c·∫ßn c√†i).
		
		Ch·∫°y l·ªánh:
		
			vagrant up
			
				Vagrant s·∫Ω t·∫£i box (gi·ªëng nh∆∞ 1 template OS) t·ª´ Vagrant Cloud, t·∫°o m√°y ·∫£o, c·∫•u h√¨nh m·∫°ng, c√†i ph·∫ßn m·ªÅm.

- CHOCOLATEY/BREW
- SUBLIME TEXT EDITOR
- AWS CLI

- IAM WITH MFA
- Homebrew
- SonarSource:

	SonarSource l√† m·ªôt c√¥ng ty c√¥ng ngh·ªá chuy√™n ph√°t tri·ªÉn c√°c c√¥ng c·ª• gi√∫p ph√¢n t√≠ch ch·∫•t
	l∆∞·ª£ng m√£ ngu·ªìn v√† ph√°t hi·ªán v·∫•n ƒë·ªÅ b·∫£o m·∫≠t trong ph·∫ßn m·ªÅm.
	
- SonarQube:

	C√¥ng c·ª• ph√¢n t√≠ch code ph·ªï bi·∫øn nh·∫•t c·ªßa SonarSource.

	H·ªó tr·ª£ h∆°n 25 ng√¥n ng·ªØ l·∫≠p tr√¨nh (Java, C#, Python, JavaScript, C/C++, Go‚Ä¶).

	T√≠ch h·ª£p v√†o CI/CD ƒë·ªÉ t·ª± ƒë·ªông ki·ªÉm tra code m·ªói khi build.

	Gi√∫p ph√°t hi·ªán:

		Bug (l·ªói c√≥ th·ªÉ g√¢y crash ho·∫∑c k·∫øt qu·∫£ sai).

		Code Smell (ƒëo·∫°n code kh√≥ b·∫£o tr√¨, kh√¥ng t·ªëi ∆∞u).

		Security Vulnerability (l·ªó h·ªïng b·∫£o m·∫≠t).

- SONARCLOUD:

	D·ªãch v·ª• SaaS tr√™n cloud (kh√¥ng c·∫ßn c√†i SonarQube server).

	D√πng ƒë·ªÉ ph√¢n t√≠ch code c·ªßa d·ª± √°n public ho·∫∑c private tr√™n GitHub, GitLab, Bitbucket, Azure DevOps.
	
	

	Homebrew l√† m·ªôt tr√¨nh qu·∫£n l√Ω g√≥i (package manager) ph·ªï bi·∫øn tr√™n macOS (v√† Linux).

	N√≥i ƒë∆°n gi·∫£n, Homebrew gi√∫p b·∫°n c√†i ƒë·∫∑t, qu·∫£n l√Ω v√† c·∫≠p nh·∫≠t ph·∫ßn m·ªÅm b·∫±ng c√°c
	l·ªánh trong terminal, thay v√¨ ph·∫£i t·∫£i file .dmg ho·∫∑c .pkg v√† c√†i th·ªß c√¥ng.
	
- choco
- MFA device

	MFA (Multi-Factor Authentication) = X√°c th·ª±c ƒëa y·∫øu t·ªë.

	Khi b·∫≠t MFA, ngo√†i username + password, b·∫°n c·∫ßn th√™m m√£ OTP t·ª´ thi·∫øt b·ªã MFA th√¨ m·ªõi ƒëƒÉng nh·∫≠p ƒë∆∞·ª£c.

	Gi√∫p tƒÉng c∆∞·ªùng b·∫£o m·∫≠t cho t√†i kho·∫£n AWS (nh·∫•t l√† v·ªõi root user ho·∫∑c IAM user c√≥ quy·ªÅn cao).
	
	MFA Device (thi·∫øt b·ªã MFA) trong AWS:
	
		MFA Device = ngu·ªìn ph√°t m√£ OTP ƒë·ªÉ ƒëƒÉng nh·∫≠p AWS.
		
		AWS h·ªó tr·ª£ nhi·ªÅu lo·∫°i thi·∫øt b·ªã MFA:

		Virtual MFA device (ph·ªï bi·∫øn nh·∫•t):

			D√πng app tr√™n ƒëi·ªán tho·∫°i, v√≠ d·ª•:

				Google Authenticator

				Authy

				Microsoft Authenticator

				LastPass Authenticator

			Khi ƒëƒÉng nh·∫≠p AWS, b·∫°n m·ªü app ‚Üí nh·∫≠p m√£ OTP hi·ªÉn th·ªã (thay ƒë·ªïi m·ªói 30 gi√¢y).

		Hardware MFA device (ph·∫ßn c·ª©ng):

			Thi·∫øt b·ªã v·∫≠t l√Ω nh·ªè (gi·ªëng USB token).

			V√≠ d·ª•: Gemalto, Yubikey.

			Ph√π h·ª£p cho m√¥i tr∆∞·ªùng c·∫ßn b·∫£o m·∫≠t cao.

		U2F security key (kh√≥a b·∫£o m·∫≠t chu·∫©n FIDO):

			D√πng USB/NFC key (nh∆∞ Yubico YubiKey).

			Ch·ªâ c·∫ßn c·∫Øm v√†o m√°y ho·∫∑c ch·∫°m v√†o khi AWS y√™u c·∫ßu x√°c th·ª±c.

		SMS MFA (√≠t d√πng):

			AWS g·ª≠i m√£ OTP qua tin nh·∫Øn SMS.

			Tuy nhi√™n k√©m an to√†n (d·ªÖ b·ªã hack SIM), n√™n √≠t ƒë∆∞·ª£c khuy√™n d√πng.
			
	C√°ch ho·∫°t ƒë·ªông:
	
		ƒêƒÉng nh·∫≠p v√†o AWS Console b·∫±ng username & password.

		AWS y√™u c·∫ßu MFA code.

		B·∫°n nh·∫≠p m√£ OTP t·ª´ MFA device (app ho·∫∑c hardware).

		N·∫øu ƒë√∫ng ‚Üí v√†o ƒë∆∞·ª£c AWS Console.
		
	T·∫°i sao n√™n d√πng MFA?
	
		NgƒÉn hacker truy c·∫≠p ngay c·∫£ khi m·∫≠t kh·∫©u b·ªã l·ªô.

		B·∫£o v·ªá root account (t√†i kho·∫£n qu·∫£n tr·ªã t·ªëi cao trong AWS).

		TƒÉng c∆∞·ªùng tu√¢n th·ªß b·∫£o m·∫≠t (PCI-DSS, ISO, SOC2‚Ä¶).
		
	AWS MFA Device = thi·∫øt b·ªã ph√°t m√£ OTP (virtual app, USB key, hardware token‚Ä¶) ƒë·ªÉ ƒëƒÉng nh·∫≠p AWS an to√†n h∆°n.

- .csv file AWS
- Cloudwatch AWS
- SNS topic AWS
- EC2 AWS

	EC2 (Elastic Compute Cloud) l√† d·ªãch v·ª• m√°y ch·ªß ·∫£o (virtual server) tr√™n n·ªÅn t·∫£ng AWS (Amazon Web Services).

	N√≥i n√¥m na: EC2 gi·ªëng nh∆∞ b·∫°n thu√™ m·ªôt c√°i m√°y t√≠nh trong ƒë√°m m√¢y ƒë·ªÉ c√†i ƒë·∫∑t h·ªá ƒëi·ªÅu h√†nh,
	ph·∫ßn m·ªÅm, web server, database‚Ä¶ v√† ch·∫°y ·ª©ng d·ª•ng c·ªßa b·∫°n.
	
	Khi t·∫°o m·ªôt EC2 Instance, b·∫°n c·∫ßn ch·ªçn:
	
		AMI (Amazon Machine Image) ‚Üí gi·ªëng nh∆∞ ch·ªçn h·ªá ƒëi·ªÅu h√†nh (Ubuntu, Amazon Linux, Windows‚Ä¶).

		Instance type ‚Üí c·∫•u h√¨nh ph·∫ßn c·ª©ng (t2.micro, m5.large, ‚Ä¶).

		Storage (EBS) ‚Üí dung l∆∞·ª£ng ·ªï c·ª©ng.

		Security Group ‚Üí gi·ªëng firewall, cho ph√©p m·ªü port (v√≠ d·ª•: 22 cho SSH, 80 cho HTTP, 443 cho HTTPS).

		Key Pair ‚Üí ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o server qua SSH.
		
	EC2 d√πng ƒë·ªÉ l√†m g√¨?
	
		Host website ho·∫∑c API.

		Ch·∫°y database (MySQL, PostgreSQL, MongoDB‚Ä¶).

		Ch·∫°y ·ª©ng d·ª•ng microservices, container (Docker, Kubernetes).

		L√†m m√°y ch·ªß dev/test cho l·∫≠p tr√¨nh vi√™n.

		X·ª≠ l√Ω big data, AI/ML, game server.
		
	EC2 = M√°y ch·ªß ·∫£o trong AWS ‚Üí b·∫°n mu·ªën ch·∫°y app/web g√¨ th√¨ ch·ªâ c·∫ßn t·∫°o m·ªôt EC2 instance r·ªìi c√†i ƒë·∫∑t nh∆∞ tr√™n m√°y t√≠nh th·∫≠t.

- AWS Certificate Manager:

	ACM l√† d·ªãch v·ª• c·ªßa AWS ƒë·ªÉ c·∫•p ph√°t, qu·∫£n l√Ω v√† tri·ªÉn khai ch·ª©ng ch·ªâ SSL/TLS cho ·ª©ng d·ª•ng, website v√† d·ªãch v·ª• AWS.

	N√≥ gi√∫p b·∫°n b·∫£o m·∫≠t k·∫øt n·ªëi HTTPS m√† kh√¥ng c·∫ßn t·ª± mua ho·∫∑c t·ª± qu·∫£n l√Ω certificate th·ªß c√¥ng.

	N√≥i ng·∫Øn g·ªçn: ACM = d·ªãch v·ª• qu·∫£n l√Ω ch·ª©ng ch·ªâ SSL/TLS trong AWS.
	
	T√≠nh nƒÉng ch√≠nh

		C·∫•p ph√°t (Provision)

			ACM c√≥ th·ªÉ c·∫•p ch·ª©ng ch·ªâ SSL/TLS mi·ªÖn ph√≠ cho domain c·ªßa b·∫°n.

			V√≠ d·ª•: www.myapp.com c√≥ th·ªÉ xin ch·ª©ng ch·ªâ t·ª´ ACM thay v√¨ mua ngo√†i.

		Tri·ªÉn khai (Deploy)

			D√πng tr·ª±c ti·∫øp v·ªõi c√°c d·ªãch v·ª• AWS:

				Elastic Load Balancer (ELB)

				CloudFront (CDN)

				API Gateway

				App Runner / Elastic Beanstalk

		Gia h·∫°n t·ª± ƒë·ªông (Auto-renewal)

			Ch·ª©ng ch·ªâ do ACM c·∫•p ƒë∆∞·ª£c t·ª± ƒë·ªông gia h·∫°n tr∆∞·ªõc khi h·∫øt h·∫°n ‚Üí kh√¥ng lo downtime v√¨ qu√™n renew SSL.

		Import Certificate (Nh·∫≠p ch·ª©ng ch·ªâ)

			N·∫øu b·∫°n ƒë√£ c√≥ ch·ª©ng ch·ªâ t·ª´ CA kh√°c (V√≠ d·ª•: DigiCert, GoDaddy, Let‚Äôs Encrypt), b·∫°n c√≥ th·ªÉ import v√†o ACM ƒë·ªÉ qu·∫£n l√Ω.

		Qu·∫£n l√Ω t·∫≠p trung

			L∆∞u tr·ªØ v√† ph√¢n ph·ªëi ch·ª©ng ch·ªâ an to√†n.

			Kh√¥ng c·∫ßn copy file .crt v√† .key th·ªß c√¥ng.

- RSA 2048 AWS
- VMware
- Host OS
- Guest OS
- VM
- Snapshot
- Hypervisor
- CentOS VM, Ubuntu VM

	CentOS VM

		CentOS (Community ENTerprise Operating System) l√† m·ªôt b·∫£n ph√¢n ph·ªëi Linux d·ª±a tr√™n Red Hat Enterprise Linux (RHEL).

		Th∆∞·ªùng d√πng trong doanh nghi·ªáp, server production v√¨:

			·ªîn ƒë·ªãnh, √≠t thay ƒë·ªïi.

			Chu k·ª≥ ph√°t h√†nh d√†i (7‚Äì10 nƒÉm).

			T∆∞∆°ng th√≠ch v·ªõi ph·∫ßn m·ªÅm vi·∫øt cho RHEL.

		Package Manager: yum (CentOS 7), dnf (CentOS 8+).

		Th√≠ch h·ª£p: Web server (Apache, Nginx), Database server (MySQL, PostgreSQL), m√¥i tr∆∞·ªùng Enterprise.

		Tuy nhi√™n: CentOS Linux ch√≠nh th·ª©c b·ªã ng·ª´ng ph√°t tri·ªÉn t·ª´ 2021, thay th·∫ø b·∫±ng CentOS Stream (rolling release, c·∫≠p nh·∫≠t nhanh h∆°n).
		
	Ubuntu VM

		Ubuntu l√† b·∫£n ph√¢n ph·ªëi Linux d·ª±a tr√™n Debian.

		Th√¢n thi·ªán h∆°n v·ªõi ng∆∞·ªùi d√πng, t√†i li·ªáu v√† c·ªông ƒë·ªìng h·ªó tr·ª£ c·ª±c nhi·ªÅu.

		Package Manager: apt (Advanced Package Tool).

		C√≥ nhi·ªÅu phi√™n b·∫£n:

			Ubuntu Server ‚Üí ch·∫°y tr√™n cloud, server.

			Ubuntu Desktop ‚Üí d√πng l√†m h·ªá ƒëi·ªÅu h√†nh c√° nh√¢n.

		Ph√π h·ª£p: Development, AI/ML, Cloud (AWS, Azure, GCP), container (Docker, Kubernetes).
		
- RHEL (Red Hat Enterprise Linux) v√† Debian:

	RHEL (Red Hat Enterprise Linux)

		Ngu·ªìn g·ªëc: D·ª±a tr√™n Linux kernel, ph√°t tri·ªÉn b·ªüi Red Hat.

		ƒê·ªãnh h∆∞·ªõng: H·ªá ƒëi·ªÅu h√†nh th∆∞∆°ng m·∫°i, enterprise, d√πng trong doanh nghi·ªáp v√† data center.

		H·ªó tr·ª£: C√≥ h·ª£p ƒë·ªìng support t·ª´ Red Hat (SLA, c·∫≠p nh·∫≠t b·∫£o m·∫≠t, bug fix).

		Package manager: rpm (Red Hat Package Manager) + yum/dnf.

		∆Øu ƒëi·ªÉm:

			·ªîn ƒë·ªãnh, chu k·ª≥ release l√¢u d√†i (7‚Äì10 nƒÉm).

			ƒê∆∞·ª£c ch·ª©ng nh·∫≠n ƒë·ªÉ ch·∫°y nhi·ªÅu ph·∫ßn m·ªÅm enterprise (Oracle DB, SAP, VMware...).

			H·ªó tr·ª£ t·ªët cho server v·∫≠t l√Ω v√† m√°y ·∫£o.

		Nh∆∞·ª£c ƒëi·ªÉm:

			Ph·∫£i tr·∫£ ph√≠ ƒë·ªÉ d√πng b·∫£n ch√≠nh th·ª©c v√† nh·∫≠n c·∫≠p nh·∫≠t.

			T√†i li·ªáu c·ªông ƒë·ªìng √≠t h∆°n so v·ªõi Ubuntu/Debian.

		B·∫£n mi·ªÖn ph√≠, c·ªông ƒë·ªìng h√≥a c·ªßa RHEL: CentOS, Rocky Linux, AlmaLinux.
		
	Debian

		Ngu·ªìn g·ªëc: M·ªôt trong nh·ªØng b·∫£n Linux l√¢u ƒë·ªùi nh·∫•t, ph√°t tri·ªÉn b·ªüi c·ªông ƒë·ªìng (ra ƒë·ªùi 1993).

		ƒê·ªãnh h∆∞·ªõng: H·ªá ƒëi·ªÅu h√†nh mi·ªÖn ph√≠, m√£ ngu·ªìn m·ªü, t·∫≠p trung v√†o s·ª± ·ªïn ƒë·ªãnh v√† t·ª± do ph·∫ßn m·ªÅm.

		Package manager: dpkg + apt.

		∆Øu ƒëi·ªÉm:

			R·∫•t ·ªïn ƒë·ªãnh, uy t√≠n l√¢u ƒë·ªùi.

			L√† n·ªÅn t·∫£ng cho nhi·ªÅu distro ph·ªï bi·∫øn (nh∆∞ Ubuntu, Linux Mint...).

			Ho√†n to√†n mi·ªÖn ph√≠.

			C·ªông ƒë·ªìng h·ªó tr·ª£ r·ªông r√£i.

		Nh∆∞·ª£c ƒëi·ªÉm:

			Kh√¥ng c√≥ h·ªó tr·ª£ th∆∞∆°ng m·∫°i ch√≠nh th·ª©c (tr·ª´ khi th√¥ng qua b√™n th·ª© 3).

			Chu k·ª≥ ph√°t h√†nh h∆°i ch·∫≠m ‚Üí √≠t c·∫≠p nh·∫≠t t√≠nh nƒÉng m·ªõi so v·ªõi Ubuntu.

- Linux distros

	Linux Distro l√† g√¨?

		Linux Distro (Linux Distribution) = b·∫£n ph√¢n ph·ªëi Linux.

		Linux ch·ªâ c√≥ kernel (nh√¢n h·ªá ƒëi·ªÅu h√†nh). Mu·ªën d√πng ƒë∆∞·ª£c, ng∆∞·ªùi ta ph·∫£i th√™m:

			Tr√¨nh qu·∫£n l√Ω g√≥i (package manager)

			Th∆∞ vi·ªán h·ªá th·ªëng

			C√¥ng c·ª• qu·∫£n tr·ªã

			M√¥i tr∆∞·ªùng desktop (GUI) (n·∫øu c·∫ßn)

			·ª®ng d·ª•ng m·∫∑c ƒë·ªãnh

		Khi k·∫øt h·ª£p t·∫•t c·∫£ nh·ªØng th√†nh ph·∫ßn n√†y l·∫°i, ta c√≥ m·ªôt Linux Distribution ‚Äì t·ª©c l√† m·ªôt "phi√™n b·∫£n Linux" ho√†n ch·ªânh.
		
	V√≠ d·ª• c√°c Linux Distro ph·ªï bi·∫øn:

		D·ª±a tr√™n Debian: Ubuntu, Linux Mint, Kali Linux.

		D·ª±a tr√™n RHEL: CentOS, Rocky Linux, AlmaLinux, Fedora.

		D√†nh cho b·∫£o m·∫≠t: Kali Linux, Parrot OS.

		D√†nh cho ng∆∞·ªùi m·ªõi: Ubuntu, Linux Mint, Zorin OS.

		D√†nh cho enterprise: RHEL, SUSE, Oracle Linux.

		D√†nh cho developer/hacker th√≠ch tu·ª≥ bi·∫øn: Arch Linux, Gentoo.

- Putty
- Oracle VM Virtualbox, Virtualbox
- ISO file
- Vagrant
- BIOS

	BIOS = Basic Input/Output System.

	L√† m·ªôt ph·∫ßn m·ªÅm h·ªá th·ªëng c·∫•p th·∫•p ƒë∆∞·ª£c l∆∞u trong chip nh·ªõ ROM/Flash tr√™n mainboard.

	Khi b·∫°n b·∫≠t m√°y t√≠nh, BIOS s·∫Ω l√† ch∆∞∆°ng tr√¨nh ch·∫°y ƒë·∫ßu ti√™n, tr∆∞·ªõc c·∫£ h·ªá ƒëi·ªÅu h√†nh (Windows, Linux, macOS‚Ä¶).
	
	Ch·ª©c nƒÉng ch√≠nh c·ªßa BIOS

		POST (Power-On Self-Test)

			Ki·ªÉm tra ph·∫ßn c·ª©ng: RAM, CPU, b√†n ph√≠m, ·ªï c·ª©ng‚Ä¶ c√≥ ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng kh√¥ng.

		Kh·ªüi t·∫°o ph·∫ßn c·ª©ng

			G·∫Øn driver c∆° b·∫£n cho b√†n ph√≠m, chu·ªôt, card m√†n h√¨nh‚Ä¶

		Ch·ªçn thi·∫øt b·ªã kh·ªüi ƒë·ªông (Bootloader)

			Quy·∫øt ƒë·ªãnh m√°y s·∫Ω boot t·ª´ HDD/SSD, USB, CD/DVD hay Network.

		Cung c·∫•p giao di·ªán c·∫•u h√¨nh (BIOS Setup Utility)

			Cho ph√©p b·∫°n ch·ªânh th·ªùi gian, th·ª© t·ª± boot, m·∫≠t kh·∫©u BIOS, √©p xung CPU/RAM, b·∫≠t/t·∫Øt thi·∫øt b·ªã‚Ä¶
			
	BIOS = ph·∫ßn m·ªÅm ch·∫°y ngay khi b·∫≠t m√°y, nhi·ªám v·ª• l√† ki·ªÉm tra ph·∫ßn c·ª©ng v√† n·∫°p h·ªá ƒëi·ªÅu h√†nh.

	Ng√†y nay, ph·∫ßn l·ªõn m√°y t√≠nh m·ªõi ƒë√£ d√πng UEFI, nh∆∞ng ng∆∞·ªùi ta v·∫´n hay g·ªçi chung l√† "BIOS".

- Vtx
- Reboot
- Ethernet (emp0s8), Ethernet (emp0s3)
- ACPI shutdown trong Oracle VM
- SSH
- NAT trong Oracle VM
- Vagrantfile
- Linux distros
- Desktop Linux OS

	Desktop Linux OS (hay c√≤n g·ªçi l√† Linux Desktop Operating System) l√† m·ªôt h·ªá ƒëi·ªÅu h√†nh Linux ƒë∆∞·ª£c thi·∫øt k·∫ø
	ƒë·ªÉ s·ª≠ d·ª•ng tr√™n m√°y t√≠nh c√° nh√¢n, laptop ho·∫∑c workstation v·ªõi giao di·ªán ƒë·ªì h·ªça (GUI) th√¢n thi·ªán, gi·ªëng
	nh∆∞ Windows hay macOS.
	
	Th√†nh ph·∫ßn ch√≠nh c·ªßa Desktop Linux OS

		Kernel (Linux kernel): l√µi c·ªßa h·ªá ƒëi·ªÅu h√†nh, qu·∫£n l√Ω ph·∫ßn c·ª©ng.

		Distro (b·∫£n ph√¢n ph·ªëi): v√≠ d·ª• Ubuntu, Fedora, Linux Mint, Manjaro‚Ä¶ ‚Üí cung c·∫•p b·ªô c√¥ng c·ª•, ·ª©ng d·ª•ng, package manager.

		Desktop Environment (DE): giao di·ªán ƒë·ªì h·ªça ng∆∞·ªùi d√πng, nh∆∞:

			GNOME (Ubuntu)

			KDE Plasma (Kubuntu, openSUSE)

			XFCE, LXQt (nh·∫π, ti·∫øt ki·ªám t√†i nguy√™n)

		·ª®ng d·ª•ng ng∆∞·ªùi d√πng: tr√¨nh duy·ªát (Firefox, Chrome), b·ªô office (LibreOffice), media player, email client...
		
	ƒê·∫∑c ƒëi·ªÉm c·ªßa Desktop Linux OS

		Mi·ªÖn ph√≠ & m√£ ngu·ªìn m·ªü.

		T√πy bi·∫øn cao (c√≥ th·ªÉ thay ƒë·ªïi giao di·ªán, DE, theme).

		·ªîn ƒë·ªãnh, b·∫£o m·∫≠t (√≠t virus h∆°n Windows).

		H·ªó tr·ª£ ph·∫ßn c·ª©ng kh√° r·ªông, nh∆∞ng v·∫´n c√≤n h·∫°n ch·∫ø v·ªõi m·ªôt s·ªë driver (v√≠ d·ª• card ƒë·ªì h·ªça, m√°y in ƒë·ªùi m·ªõi).

		Nhi·ªÅu b·∫£n ph√¢n ph·ªëi kh√°c nhau ph·ª•c v·ª• nhu c·∫ßu kh√°c nhau:

			Th√¢n thi·ªán cho ng∆∞·ªùi m·ªõi: Ubuntu, Linux Mint, Zorin OS

			D√†nh cho dev/pro: Fedora, openSUSE, Debian

			Si√™u nh·∫π: Lubuntu, Puppy Linux
			
	Desktop Linux OS l√† phi√™n b·∫£n Linux ƒë∆∞·ª£c ƒë√≥ng g√≥i s·∫µn v·ªõi giao di·ªán ƒë·ªì h·ªça v√† ·ª©ng d·ª•ng c·∫ßn thi·∫øt cho ng∆∞·ªùi
	d√πng c√° nh√¢n, gi·ªëng nh∆∞ Windows/macOS, nh∆∞ng mi·ªÖn ph√≠ v√† t√πy bi·∫øn m·∫°nh m·∫Ω.

- Server Linux OS

	Server Linux OS (Linux Server Operating System) l√† m·ªôt h·ªá ƒëi·ªÅu h√†nh Linux ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ ch·∫°y tr√™n m√°y
	ch·ªß (server) thay v√¨ m√°y t√≠nh c√° nh√¢n. N√≥ cung c·∫•p m√¥i tr∆∞·ªùng ·ªïn ƒë·ªãnh, b·∫£o m·∫≠t, v√† hi·ªáu su·∫•t cao ƒë·ªÉ ch·∫°y
	c√°c d·ªãch v·ª• m·∫°ng (web, database, mail, file server, container...).
	
	ƒê·∫∑c ƒëi·ªÉm c·ªßa Server Linux OS

		Kh√¥ng c·∫ßn giao di·ªán ƒë·ªì h·ªça (GUI) ‚Üí th∆∞·ªùng ch·ªâ ch·∫°y ·ªü ch·∫ø ƒë·ªô command line (CLI) ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n.

		T·ªëi ∆∞u cho hi·ªáu su·∫•t v√† b·∫£o m·∫≠t ‚Üí ch·∫°y l√¢u d√†i, √≠t c·∫ßn reboot.

		H·ªó tr·ª£ nhi·ªÅu d·ªãch v·ª• server: web server (Apache, Nginx), database (MySQL, PostgreSQL), container (Docker, Kubernetes)...

		C·∫≠p nh·∫≠t ·ªïn ƒë·ªãnh, v√≤ng ƒë·ªùi d√†i (LTS ‚Äì Long Term Support).

		Kh·∫£ nƒÉng m·ªü r·ªông (scalability) ƒë·ªÉ ph·ª•c v·ª• nhi·ªÅu ng∆∞·ªùi d√πng c√πng l√∫c.
		
	C√°c b·∫£n ph√¢n ph·ªëi (distro) Server Linux ph·ªï bi·∫øn

		Ubuntu Server (d·ªÖ d√πng, c·ªông ƒë·ªìng l·ªõn, h·ªó tr·ª£ LTS).

		CentOS Stream (ti·∫øp n·ªëi CentOS, g·∫ßn v·ªõi Red Hat Enterprise Linux).

		Red Hat Enterprise Linux (RHEL) (b·∫£n th∆∞∆°ng m·∫°i, h·ªó tr·ª£ doanh nghi·ªáp).

		Debian Server (·ªïn ƒë·ªãnh, √≠t thay ƒë·ªïi, r·∫•t ph·ªï bi·∫øn).

		SUSE Linux Enterprise Server (SLES) (doanh nghi·ªáp, h·ªó tr·ª£ m·∫°nh m·∫Ω).
		
	Server Linux OS l√† Linux ƒë∆∞·ª£c tinh ch·ªânh cho m√¥i tr∆∞·ªùng server, ch√∫ tr·ªçng hi·ªáu su·∫•t, b·∫£o m·∫≠t, ƒë·ªô ·ªïn ƒë·ªãnh.

	Desktop Linux OS th√¨ thi√™n v·ªÅ tr·∫£i nghi·ªám ng∆∞·ªùi d√πng cu·ªëi v·ªõi GUI v√† ·ª©ng d·ª•ng vƒÉn ph√≤ng/gi·∫£i tr√≠.
	
	Khi n√†o c·∫ßn d√πng Linux Server?

		B·∫°n s·∫Ω d√πng Linux Server khi:

			C·∫ßn m·ªôt m√°y ch·∫°y d·ªãch v·ª• cho nhi·ªÅu ng∆∞·ªùi kh√°c truy c·∫≠p (web server, database server, mail server, file server...).

			C·∫ßn h·ªá th·ªëng ·ªïn ƒë·ªãnh, ch·∫°y 24/7 (kh√¥ng t·∫Øt m√°y nh∆∞ PC c√° nh√¢n).

			C·∫ßn m·ªôt m√¥i tr∆∞·ªùng b·∫£o m·∫≠t, nh·∫π, √≠t t·ªën t√†i nguy√™n (th∆∞·ªùng kh√¥ng c√≥ GUI, ch·ªâ d√πng d√≤ng l·ªánh).

			Tri·ªÉn khai trong data center, cloud (AWS, Azure, GCP) ho·∫∑c tr√™n m√°y ch·ªß v·∫≠t l√Ω.

		Tr∆∞·ªùng h·ª£p deploy web b·∫±ng AWS th√¨ sao?

			Khi b·∫°n deploy website l√™n AWS, b·∫°n th∆∞·ªùng s·∫Ω ch·∫°y tr√™n m·ªôt d·ªãch v·ª• nh∆∞ EC2 (m√°y ·∫£o trong AWS).

			M√°y EC2 n√†y th∆∞·ªùng ƒë∆∞·ª£c c√†i m·ªôt distro Linux Server (v√≠ d·ª•: Ubuntu Server, Amazon Linux, CentOS Stream, Debian Server...).

			Sau ƒë√≥ b·∫°n s·∫Ω c√†i nginx, Apache (httpd), Node.js, Java, MySQL, v.v. tr√™n ƒë√≥ ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng.

- RPM based, Debian based trong Linux

	Trong th·∫ø gi·ªõi Linux, khi ng∆∞·ªùi ta n√≥i ‚ÄúRPM-based‚Äù v√† ‚ÄúDebian-based‚Äù, h·ªç ƒëang n√≥i v·ªÅ h·ªá qu·∫£n l√Ω
	g√≥i (package management system) m√† b·∫£n ph√¢n ph·ªëi (distro) ƒë√≥ s·ª≠ d·ª•ng.
	
	Debian-based:
	
		Ngu·ªìn g·ªëc: t·ª´ Debian.

		Tr√¨nh qu·∫£n l√Ω g√≥i: dpkg (Debian Package).

		ƒê·ªãnh d·∫°ng g√≥i: .deb.

		C√¥ng c·ª• cao h∆°n: apt-get, apt, aptitude (d√πng ƒë·ªÉ c√†i ƒë·∫∑t, update t·ª´ repository).

		ƒê·∫∑c ƒëi·ªÉm:

			Kho ph·∫ßn m·ªÅm r·∫•t l·ªõn, c·ªông ƒë·ªìng m·∫°nh.

			Th√¢n thi·ªán cho ng∆∞·ªùi m·ªõi (v√≠ d·ª• Ubuntu).

		V√≠ d·ª• distro Debian-based:

			Debian

			Ubuntu (v√† c√°c bi·∫øn th·ªÉ: Kubuntu, Xubuntu, ‚Ä¶)

			Linux Mint

			Kali Linux

			Pop!_OS
			
	RPM-based:

		Ngu·ªìn g·ªëc: t·ª´ Red Hat.

		Tr√¨nh qu·∫£n l√Ω g√≥i: rpm (Red Hat Package Manager).

		ƒê·ªãnh d·∫°ng g√≥i: .rpm.

		C√¥ng c·ª• cao h∆°n:

			yum (Yellowdog Updater, Modified ‚Äì d√πng trong CentOS, RHEL c≈©).

			dnf (Dandified YUM ‚Äì thay th·∫ø yum trong Fedora, RHEL 8+).

		ƒê·∫∑c ƒëi·ªÉm:

			·ªîn ƒë·ªãnh, nhi·ªÅu b·∫£n th∆∞∆°ng m·∫°i (RHEL, SUSE).

			Th∆∞·ªùng d√πng trong m√¥i tr∆∞·ªùng doanh nghi·ªáp.

		V√≠ d·ª• distro RPM-based:

			Red Hat Enterprise Linux (RHEL)

			CentOS, CentOS Stream

			Fedora

			openSUSE, SUSE Linux Enterprise

- ftp
- telnet
- jenkin
- tar
- bitbucket
- codecommit
- yum
- vagrant cloud

	Vagrant Cloud l√† m·ªôt d·ªãch v·ª• online do HashiCorp cung c·∫•p, ƒë√≥ng vai tr√≤ nh∆∞
	m·ªôt ‚Äúkho l∆∞u tr·ªØ box‚Äù (Vagrant boxes) cho c·ªông ƒë·ªìng v√† doanh nghi·ªáp.

	B·∫°n c√≥ th·ªÉ xem n√≥ gi·ªëng nh∆∞ Docker Hub nh∆∞ng d√†nh cho Vagrant.
	
	C√°c ch·ª©c nƒÉng ch√≠nh c·ªßa Vagrant Cloud

		Chia s·∫ª box (Vagrant Boxes)

			T·∫£i v·ªÅ c√°c box c√≥ s·∫µn (Ubuntu, CentOS, Debian, Windows, ‚Ä¶).

			V√≠ d·ª•:	

				vagrant init hashicorp/bionic64
				vagrant up

		ƒêƒÉng t·∫£i box c·ªßa b·∫°n

			N·∫øu b·∫°n t·∫°o m·ªôt m√°y ·∫£o Vagrant v·ªõi c·∫•u h√¨nh ƒë·∫∑c bi·ªát, b·∫°n c√≥ th·ªÉ upload l√™n
			Vagrant Cloud ƒë·ªÉ chia s·∫ª cho ƒë·ªìng ƒë·ªôi ho·∫∑c c·ªông ƒë·ªìng.

		Qu·∫£n l√Ω box private/public

			Public box: ai c≈©ng c√≥ th·ªÉ t·∫£i.

			Private box: ch·ªâ nh√≥m/b·∫°n b√® ho·∫∑c c√¥ng ty c·ªßa b·∫°n c√≥ quy·ªÅn truy c·∫≠p.
			
		T√≠ch h·ª£p v·ªõi Vagrant CLI

			B·∫°n c√≥ th·ªÉ login b·∫±ng CLI:
			
				vagrant login
				
		Team & Enterprise

			Cho ph√©p t·∫°o t·ªï ch·ª©c, nh√≥m, ph√¢n quy·ªÅn ƒë·ªÉ qu·∫£n l√Ω box trong n·ªôi b·ªô c√¥ng ty.

- daemon
- hostmanager
- memcache
- apache tomcat
- nginx
- EPEL repository
- systemctl command
- OSI
- DNS & DHCP
- .ppk va .pem trong key pair aws
- AWS EC2
- AWS AMI
- tag trong EC2
- security group trong AMI
- AWS IAM

	IAM (Identity and Access Management) l√† d·ªãch v·ª• c·ªßa AWS d√πng ƒë·ªÉ qu·∫£n l√Ω ng∆∞·ªùi d√πng, nh√≥m v√† quy·ªÅn truy c·∫≠p ƒë·∫øn c√°c t√†i nguy√™n AWS.

	N√≥ gi√∫p b·∫°n ki·ªÉm so√°t ai c√≥ th·ªÉ truy c·∫≠p v√†o d·ªãch v·ª• n√†o v√† ƒë∆∞·ª£c l√†m g√¨ trong AWS.

	N√≥i ng·∫Øn g·ªçn: IAM = H·ªá th·ªëng ph√¢n quy·ªÅn & b·∫£o m·∫≠t trong AWS.
	
	Th√†nh ph·∫ßn ch√≠nh trong IAM:
	
		User (Ng∆∞·ªùi d√πng)

			ƒê·∫°i di·ªán cho m·ªôt c√° nh√¢n (developer, admin‚Ä¶) ho·∫∑c m·ªôt ·ª©ng d·ª•ng.

			M·ªói user c√≥ credentials (username/password ho·∫∑c access key/secret key).

		Group (Nh√≥m)

			T·∫≠p h·ª£p nhi·ªÅu user.

			Quy·ªÅn ƒë∆∞·ª£c g√°n cho group ‚Üí t·∫•t c·∫£ user trong group ƒë·ªÅu c√≥ quy·ªÅn ƒë√≥.

		Role (Vai tr√≤)

			D√πng cho d·ªãch v·ª• AWS ho·∫∑c ·ª©ng d·ª•ng ƒë·ªÉ c√≥ quy·ªÅn truy c·∫≠p t√†i nguy√™n.

			V√≠ d·ª•: EC2 c√≥ th·ªÉ assume m·ªôt role ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu t·ª´ S3.

		Policy (Ch√≠nh s√°ch)

			T√†i li·ªáu JSON ƒë·ªãnh nghƒ©a quy·ªÅn truy c·∫≠p (cho ph√©p hay t·ª´ ch·ªëi).

			V√≠ d·ª•: policy cho ph√©p user ƒë·ªçc file trong S3 bucket.
			
	Ch·ª©c nƒÉng ch√≠nh c·ªßa IAM:
	
		Qu·∫£n l√Ω ng∆∞·ªùi d√πng & nh√≥m (user, group).

		C·∫•p quy·ªÅn chi ti·∫øt b·∫±ng policy (theo nguy√™n t·∫Øc least privilege ‚Äì ch·ªâ c·∫•p quy·ªÅn t·ªëi thi·ªÉu c·∫ßn thi·∫øt).

		H·ªó tr·ª£ MFA (Multi-Factor Authentication) ƒë·ªÉ tƒÉng b·∫£o m·∫≠t.

		T√≠ch h·ª£p v·ªõi d·ªãch v·ª• kh√°c nh∆∞ EC2, Lambda, S3, RDS‚Ä¶
		
	V√≠ d·ª• th·ª±c t·∫ø:
	
		Dev A ch·ªâ c·∫ßn ƒë·ªçc d·ªØ li·ªáu trong S3 ‚Üí t·∫°o IAM user + policy s3:GetObject.

		Dev B c·∫ßn quy·ªÅn qu·∫£n l√Ω EC2 ‚Üí g√°n policy AmazonEC2FullAccess.

		EC2 server c·∫ßn quy·ªÅn truy c·∫≠p DynamoDB ‚Üí t·∫°o IAM Role cho EC2.
		
	AWS IAM = Qu·∫£n l√Ω danh t√≠nh & quy·ªÅn truy c·∫≠p trong AWS, gi√∫p ƒë·∫£m b·∫£o an to√†n v√† ki·ªÉm so√°t ch√≠nh x√°c ai ƒë∆∞·ª£c
	l√†m g√¨ tr√™n t√†i nguy√™n AWS.
	
- AWS IAM security credentials

	Trong AWS IAM, security credentials l√† c√°c th√¥ng tin x√°c th·ª±c m√† m·ªôt IAM user ho·∫∑c role d√πng ƒë·ªÉ truy c·∫≠p t√†i nguy√™n AWS.

	C√≥ 2 c√°ch ch√≠nh ƒë·ªÉ truy c·∫≠p AWS:

		AWS Management Console (giao di·ªán web) ‚Üí d√πng username + password (+ MFA n·∫øu b·∫≠t).

		AWS CLI / SDK / API ‚Üí d√πng Access keys (Access key ID + Secret access key).
		
	C√°c lo·∫°i Security Credentials trong IAM:
	
		Password (Console access)

			D√πng ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o AWS Management Console.

			C√≥ th·ªÉ y√™u c·∫ßu user ƒë·ªïi m·∫≠t kh·∫©u khi l·∫ßn ƒë·∫ßu ƒëƒÉng nh·∫≠p.
			
		Access Keys (Programmatic access)

			G·ªìm 2 ph·∫ßn:

				Access key ID ‚Üí gi·ªëng username.

				Secret access key ‚Üí gi·ªëng m·∫≠t kh·∫©u.

			D√πng ƒë·ªÉ truy c·∫≠p AWS qua CLI, SDK, API.

			V√≠ d·ª• khi c·∫•u h√¨nh AWS CLI:
			
				aws configure
				AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
				AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
				Default region name [None]: ap-southeast-1
				Default output format [None]: json
				
		SSH Keys (cho EC2 Instance Connect)

			D√πng ƒë·ªÉ k·∫øt n·ªëi SSH v√†o c√°c m√°y EC2.	

		Server Certificates

			D√πng cho AWS services nh∆∞ Elastic Load Balancing (ELB) ho·∫∑c CloudFront ƒë·ªÉ h·ªó tr·ª£ HTTPS (SSL/TLS).
			
		MFA Devices

			Virtual (Google Authenticator, Authy), Hardware (YubiKey‚Ä¶), SMS.

			D√πng ƒë·ªÉ tƒÉng b·∫£o m·∫≠t cho login.
			
		X.509 Certificates (√≠t d√πng hi·ªán nay)

			D√πng cho c√°c d·ªãch v·ª• c≈© ho·∫∑c t√≠ch h·ª£p ƒë·∫∑c bi·ªát.
			
	Best Practices (khuy·∫øn ngh·ªã b·∫£o m·∫≠t)

		Kh√¥ng bao gi·ªù d√πng Access Key c·ªßa root user üö´.

		T·∫°o IAM user/role ri√™ng cho t·ª´ng ·ª©ng d·ª•ng.

		B·∫≠t MFA cho root user v√† user quan tr·ªçng.

		D√πng IAM Role thay v√¨ access key n·∫øu app ch·∫°y tr√™n AWS (VD: EC2 ‚Üí S3).

		Rotate access key th∆∞·ªùng xuy√™n (x√≥a key c≈©, t·∫°o key m·ªõi).

		Ch·ªâ c·∫•p quy·ªÅn theo nguy√™n t·∫Øc least privilege (t·ªëi thi·ªÉu c·∫ßn thi·∫øt).
		
	Security Credentials trong AWS IAM = t·∫≠p h·ª£p th√¥ng tin x√°c th·ª±c (password, access keys,
	SSH keys, MFA‚Ä¶) m√† user/role d√πng ƒë·ªÉ truy c·∫≠p AWS m·ªôt c√°ch an to√†n.
	
- Console Access:

	Console Access nghƒ©a l√† m·ªôt IAM User (ho·∫∑c root user) c√≥ th·ªÉ ƒëƒÉng nh·∫≠p v√†o AWS Management Console (giao di·ªán web c·ªßa AWS) b·∫±ng:

		Username/Password

		(MFA code n·∫øu ƒë√£ b·∫≠t Multi-Factor Authentication

	N√≥i ng·∫Øn g·ªçn: Console Access = quy·ªÅn truy c·∫≠p AWS qua giao di·ªán web (console.aws.amazon.com).
	
- AWS Billing:

	AWS Billing l√† h·ªá th·ªëng qu·∫£n l√Ω chi ph√≠, h√≥a ƒë∆°n v√† thanh to√°n trong t√†i kho·∫£n AWS.
	
	N√≥ cho ph√©p b·∫°n:

		Xem chi ph√≠ s·ª≠ d·ª•ng d·ªãch v·ª• AWS (theo ng√†y, theo d·ªãch v·ª•, theo region).

		Qu·∫£n l√Ω h√≥a ƒë∆°n (invoice) v√† ph∆∞∆°ng th·ª©c thanh to√°n.

		Thi·∫øt l·∫≠p c·∫£nh b√°o chi ph√≠ (budget alerts) ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° ng√¢n s√°ch.

		Ph√¢n t√≠ch chi ph√≠ theo t·ª´ng d·ª± √°n, team, ho·∫∑c m√¥i tr∆∞·ªùng (prod/dev/test).
		
	C√°c th√†nh ph·∫ßn ch√≠nh trong AWS Billing

		Bills (H√≥a ƒë∆°n)

			Hi·ªÉn th·ªã chi ti·∫øt chi ph√≠ theo t·ª´ng d·ªãch v·ª• (EC2, S3, RDS, Lambda‚Ä¶).

			C√≥ th·ªÉ t·∫£i v·ªÅ d·∫°ng PDF ho·∫∑c CSV.

		Payment Methods (Ph∆∞∆°ng th·ª©c thanh to√°n)

			AWS ch·∫•p nh·∫≠n th·∫ª t√≠n d·ª•ng/ghi n·ª£ (Visa, Mastercard, AmEx‚Ä¶) v√† m·ªôt s·ªë ph∆∞∆°ng th·ª©c kh√°c.

		Budgets (Ng√¢n s√°ch)

			ƒê·∫∑t ng√¢n s√°ch chi ph√≠ (v√≠ d·ª• $50/th√°ng).

			Nh·∫≠n c·∫£nh b√°o qua email ho·∫∑c SNS n·∫øu v∆∞·ª£t m·ª©c.

		Cost Explorer

			C√¥ng c·ª• tr·ª±c quan h√≥a chi ph√≠ (bi·ªÉu ƒë·ªì, b√°o c√°o).

			Cho ph√©p ph√¢n t√≠ch xu h∆∞·ªõng chi ti√™u, d·ª± ƒëo√°n chi ph√≠ trong t∆∞∆°ng lai.

		Free Tier Usage

			Hi·ªÉn th·ªã m·ª©c s·ª≠ d·ª•ng AWS Free Tier (mi·ªÖn ph√≠ 12 th√°ng ho·∫∑c vƒ©nh vi·ªÖn cho m·ªôt s·ªë d·ªãch v·ª•).

			Gi√∫p tr√°nh v∆∞·ª£t gi·ªõi h·∫°n free m√† ph√°t sinh ph√≠ ngo√†i √Ω mu·ªën.

		Consolidated Billing (cho AWS Organizations)

			N·∫øu b·∫°n c√≥ nhi·ªÅu t√†i kho·∫£n AWS trong m·ªôt Organization, c√≥ th·ªÉ g·ªôp h√≥a ƒë∆°n ‚Üí nh·∫≠n gi·∫£m gi√° volume.
			
	Best Practices v·ªÅ Billing

		Lu√¥n b·∫≠t Billing Alerts ƒë·ªÉ theo d√µi chi ph√≠.

		D√πng AWS Budgets ƒë·ªÉ c·∫£nh b√°o v∆∞·ª£t m·ª©c ng√¢n s√°ch.

		G·∫Øn Tags cho t√†i nguy√™n (v√≠ d·ª•: Project: Ecommerce, Env: Dev) ƒë·ªÉ ph√¢n t√≠ch chi ph√≠ theo d·ª± √°n.

		X√≥a t√†i nguy√™n kh√¥ng d√πng (EC2, EBS, Elastic IP, Load Balancer‚Ä¶) v√¨ AWS v·∫´n t√≠nh ph√≠ n·∫øu c√≤n t·ªìn t·∫°i.
		
	AWS Billing = h·ªá th·ªëng qu·∫£n l√Ω chi ph√≠, h√≥a ƒë∆°n v√† thanh to√°n trong AWS, gi√∫p b·∫°n ki·ªÉm so√°t v√† t·ªëi
	∆∞u chi ti√™u khi d√πng d·ªãch v·ª• AWS.
	
- AWS CloudWatch:

	Amazon CloudWatch l√† d·ªãch v·ª• gi√°m s√°t (monitoring) v√† quan s√°t (observability) c·ªßa AWS.
	
	N√≥ gi√∫p b·∫°n theo d√µi:

		Metrics (ch·ªâ s·ªë: CPU, RAM, Network, Disk I/O‚Ä¶).

		Logs (nh·∫≠t k√Ω ·ª©ng d·ª•ng, h·ªá ƒëi·ªÅu h√†nh, d·ªãch v·ª• AWS).

		Events (s·ª± ki·ªán h·ªá th·ªëng, thay ƒë·ªïi tr·∫°ng th√°i).

		Alarms (c·∫£nh b√°o khi v∆∞·ª£t ng∆∞·ª°ng).
		
	Th√†nh ph·∫ßn ch√≠nh c·ªßa CloudWatch

		Metrics:

			Thu th·∫≠p s·ªë li·ªáu v·ªÅ t√†i nguy√™n AWS.

			V√≠ d·ª•:

			EC2: CPUUtilization, NetworkIn/Out.

			RDS: FreeStorageSpace, DatabaseConnections.

			Lambda: Invocations, Duration, Errors.

		Alarms:

			ƒê·∫∑t ng∆∞·ª°ng c·∫£nh b√°o d·ª±a tr√™n metrics.

			V√≠ d·ª•: CPU EC2 > 80% trong 5 ph√∫t ‚Üí g·ª≠i c·∫£nh b√°o qua SNS (email/SMS) ho·∫∑c t·ª± ƒë·ªông scale up.

		Logs:

			L∆∞u v√† ph√¢n t√≠ch log t·ª´:

			·ª®ng d·ª•ng (Java, Python, NodeJS‚Ä¶).

			H·ªá ƒëi·ªÅu h√†nh (EC2).

			Lambda (log t·ª± ƒë·ªông v√†o CloudWatch Logs).

		Events (ho·∫∑c EventBridge):

			Theo d√µi s·ª± ki·ªán h·ªá th·ªëng AWS (v√≠ d·ª•: EC2 stop/start).

			C√≥ th·ªÉ t·ª± ƒë·ªông k√≠ch ho·∫°t Lambda function, SNS notification, ho·∫∑c Step Functions.

		Dashboards:

			T·∫°o b·∫£ng ƒëi·ªÅu khi·ªÉn t√πy ch·ªânh v·ªõi bi·ªÉu ƒë·ªì metrics, logs.

			V√≠ d·ª•: gi√°m s√°t to√†n b·ªô h·ªá th·ªëng microservices tr√™n 1 dashboard.
			
	V√≠ d·ª• th·ª±c t·∫ø

		B·∫°n c√≥ m·ªôt EC2 ch·∫°y Spring Boot:

		CloudWatch Metrics theo d√µi CPU, RAM, Disk.

		CloudWatch Logs l∆∞u l·∫°i log ·ª©ng d·ª•ng (/var/log/...).

		CloudWatch Alarm c·∫£nh b√°o khi CPU > 70%.

		CloudWatch Event k√≠ch ho·∫°t Auto Scaling ƒë·ªÉ th√™m EC2 m·ªõi khi t·∫£i cao.
		
	L·ª£i √≠ch

		Gi√∫p t·ª± ƒë·ªông gi√°m s√°t h·ªá th·ªëng ‚Üí kh√¥ng c·∫ßn ki·ªÉm tra th·ªß c√¥ng.

		D·ªÖ d√†ng t√¨m l·ªói nh·ªù log t·∫≠p trung.

		H·ªó tr·ª£ t·ªëi ∆∞u chi ph√≠ (theo d√µi s·ª≠ d·ª•ng t√†i nguy√™n).

		K·∫øt h·ª£p v·ªõi Auto Scaling, Lambda, SNS ƒë·ªÉ ph·∫£n ·ª©ng t·ª± ƒë·ªông.
		
	AWS CloudWatch = c√¥ng c·ª• gi√°m s√°t metrics, logs, events v√† t·∫°o c·∫£nh b√°o cho h·ªá th·ªëng AWS.

	N√≥ gi·ªëng nh∆∞ Prometheus + Grafana + ELK, nh∆∞ng ƒë∆∞·ª£c t√≠ch h·ª£p s·∫µn trong AWS.

- AWS IAM access key
- AWS Elastic Block Storage
- Volumn in configure storage of IAM S3
- AWS EC2 Image Builder
- AWS EC2 launch templates
- AWS Cloud Watch
- AWS Route53
- AWS RDS
- AWS cloudfront
- AWS Amazon S3
- AWS Amazon SNS
- Amazon Elastic File System
- AWS EFS
- AWS NFS
- AWS Auto Scaling
- AWS RDS
- public access trong RDS
- AWS S3
- Elastic Beanstalk
- Amazon ElastiCache
- Amazon Elastic Container Registry
- Poll SCM
- Webhooks
- CRUMB
- event-driven
- boto3 python
- python fabric
- Terraform
- Ansible
- ad hoc command
- ansible palybook
- Amazon VPC
- AWS CloudWatch Logs
- Bitbucket
- AWS CodeBuild
- AWS CodePipeline
- docker pull nginx:mainline-alpine-perl
- docker engine
- minikube
- Kops
- key pair va security group trong ec2 instance
- curl
- kubectl binary
- kubernetes cli
- kubernetes namespace
- kubernetes taints and tolerations
- Amazon Elastic Kubernetes Service
- kubernetes ingress
- git fork
- SSH key
- AWS Elastic Container Registry
- kubernetes cluster
- QA, BA trong doanh nghiep
- Nesus
- file TAR
- helm chart
- Server v·∫≠t l√Ω (physical server) v√† Server ·∫£o (virtual server / VM):

	Server v·∫≠t l√Ω (physical server)

		L√† m√°y ch·ªß th·∫≠t ƒë·∫∑t trong data center (c√≥ CPU, RAM, ·ªï c·ª©ng, card m·∫°ng).

		B·∫°n c√≥ th·ªÉ c√†i h·ªá ƒëi·ªÅu h√†nh l√™n n√≥: Linux Server (Ubuntu Server, CentOS, Debian Server, ‚Ä¶)
		ho·∫∑c Windows Server.

		Sau khi c√†i xong, b·∫°n thao t√°c qu·∫£n l√Ω b·∫±ng SSH (n·∫øu Linux) ho·∫∑c RDP (n·∫øu Windows).

	Server ·∫£o (virtual server / VM)

		L√† m√°y ch·ªß ·∫£o h√≥a t·∫°o ra t·ª´ ph·∫ßn m·ªÅm nh∆∞ VMware, VirtualBox, KVM, ho·∫∑c d·ªãch v·ª• Cloud
		nh∆∞ AWS EC2, Azure VM, Google Compute Engine.

		B√™n trong server ·∫£o ƒë√≥, b·∫°n c≈©ng c√†i Linux Server (ho·∫∑c Windows Server).

		Khi truy c·∫≠p v√† thao t√°c, tr·∫£i nghi·ªám kh√¥ng kh√°c g√¨ server v·∫≠t l√Ω.

	Nghƒ©a l√†:

		D√π b·∫°n d√πng server v·∫≠t l√Ω hay server ·∫£o, n·∫øu h·ªá ƒëi·ªÅu h√†nh c√†i b√™n trong l√† Linux Server, th√¨ b·∫°n v·∫´n
		s·∫Ω thao t√°c b·∫±ng d√≤ng l·ªánh Linux Server (CLI qua SSH).



Missing in Jenkin lession:
	- Agent/Node/Slave in Jenkins
	- Using Agent/Note/Slave
Missing Build tools lession


--- V√≤ng ƒë·ªùi DevOps:

	1. CODE

	L·∫≠p tr√¨nh vi√™n vi·∫øt code (Java, HTML, CSS, JS, ‚Ä¶).

	ƒê√¢y l√† ngu·ªìn g·ªëc c·ªßa m·ªçi th·ª© trong pipeline.

	2. CODE BUILD

	Code ƒë∆∞·ª£c build th√†nh g√≥i ch·∫°y ƒë∆∞·ª£c (JAR, WAR, Docker image, ‚Ä¶).

	C√¥ng c·ª• th∆∞·ªùng d√πng: Maven, Gradle, Jenkins, GitLab CI, GitHub Actions.

	3. CODE TEST

	Ch·∫°y test t·ª± ƒë·ªông (unit test, integration test).

	ƒê·∫£m b·∫£o code kh√¥ng b·ªã l·ªói logic.

	4. CODE ANALYSIS

	Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng code (coding style, security scan, bug detection).

	C√¥ng c·ª•: SonarQube, PMD, Checkstyle.

	5. DELIVERY

	Chu·∫©n b·ªã ph·∫ßn m·ªÅm ƒë·ªÉ ƒë∆∞a qua m√¥i tr∆∞·ªùng staging ho·∫∑c production.

	Bao g·ªìm artifact repository (Nexus, Artifactory).

	6. DB / Security / OS CHANGES

	C√°c thay ƒë·ªïi li√™n quan t·ªõi database migration, c·∫•u h√¨nh h·ªá ƒëi·ªÅu h√†nh, b·∫£o m·∫≠t.

	Th∆∞·ªùng ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a b·∫±ng Ansible, Terraform, Liquibase, Flyway.

	7. SOFTWARE TESTING

	Test ·ªü m·ª©c h·ªá th·ªëng (system testing, performance testing, user acceptance testing).

	M·ª•c ti√™u: ki·ªÉm th·ª≠ to√†n b·ªô ·ª©ng d·ª•ng trong m√¥i tr∆∞·ªùng staging.

	8. DEPLOY TO PROD

	Deploy ·ª©ng d·ª•ng sang production environment.

	C√¥ng c·ª•: Docker, Kubernetes, Jenkins, ArgoCD.

	9. GO LIVE

	·ª®ng d·ª•ng b·∫Øt ƒë·∫ßu ch·∫°y cho ng∆∞·ªùi d√πng th·∫≠t.

	ƒê√≤i h·ªèi monitoring (Prometheus, Grafana, ELK stack) ƒë·ªÉ theo d√µi ho·∫°t ƒë·ªông.

	10. USER APPROVAL

	Ng∆∞·ªùi d√πng/kh√°ch h√†ng ki·ªÉm tra v√† x√°c nh·∫≠n ·ª©ng d·ª•ng ƒë√°p ·ª©ng y√™u c·∫ßu.

	Feedback ƒë∆∞·ª£c ƒë∆∞a l·∫°i cho team Dev, t·∫°o v√≤ng l·∫∑p m·ªõi.


--- C√°c l·ªánh cmd:

	- cmd: choco list
	- cmd: choco install virtualbox --version=7.0.8 -y
	- cmd: choco install vagrant --version=2.3.7 -y
	- cmd: choco install corretto17jdk -y
	- cmd: choco install maven -y
	- cmd: choco install awscli -y
	- cmd: choco install intellijidea-community -y
	- cmd: choco install vscode -y
	- cmd: choco install sublimetext3 -y
	
	
--- VM setup:

	- cmd: ip addr show
	
		ƒê√¢y l√† l·ªánh trong Linux d√πng ƒë·ªÉ hi·ªÉn th·ªã th√¥ng tin c√°c ƒë·ªãa ch·ªâ IP tr√™n m√°y. N√≥ thu·ªôc b·ªô c√¥ng
		c·ª• iproute2 (thay th·∫ø d·∫ßn ifconfig).
		
		K·∫øt qu·∫£ khi ch·∫°y:
		
			[root@localhost apache-tomcat-10.1.46]# ip addr show
			1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
				link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
				inet 127.0.0.1/8 scope host lo
				   valid_lft forever preferred_lft forever
				inet6 ::1/128 scope host
				   valid_lft forever preferred_lft forever
			2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:e4:ec:85 brd ff:ff:ff:ff:ff:ff
				inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute enp0s3
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fee4:ec85/64 scope link noprefixroute
				   valid_lft forever preferred_lft forever
			3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:00:b4:c3 brd ff:ff:ff:ff:ff:ff
				inet 192.168.56.10/24 brd 192.168.56.255 scope global noprefixroute enp0s8
				   valid_lft forever preferred_lft forever
				inet6 fe80::a00:27ff:fe00:b4c3/64 scope link
				   valid_lft forever preferred_lft forever
			4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:3c:20:80 brd ff:ff:ff:ff:ff:ff
				inet 192.168.1.25/24 brd 192.168.1.255 scope global dynamic noprefixroute enp0s9
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fe3c:2080/64 scope link
				   valid_lft forever preferred_lft forever
				   
			Loopback (lo):
			
				lo: card loopback, d√πng ƒë·ªÉ giao ti·∫øp n·ªôi b·ªô trong m√°y (localhost).

				127.0.0.1: ƒë·ªãa ch·ªâ IP ƒë·∫∑c bi·ªát ƒë·ªÉ tham chi·∫øu ch√≠nh m√°y ƒë√≥.

				mtu 65536: k√≠ch th∆∞·ªõc g√≥i tin t·ªëi ƒëa.

				IPv6 t∆∞∆°ng ·ª©ng: ::1.
				
			enp0s3 (NAT interface):
			
				ƒê√¢y l√† card m·∫°ng NAT do VirtualBox t·∫°o.

				10.0.2.15/24: IP g√°n t·ª± ƒë·ªông trong m·∫°ng NAT c·ªßa VirtualBox.

				dynamic: IP ƒë∆∞·ª£c c·∫•p t·ª´ DHCP.

				D√πng ƒë·ªÉ m√°y ·∫£o c√≥ th·ªÉ truy c·∫≠p internet th√¥ng qua host.

			enp0s8 (Host-Only Adapter):
			
				ƒê√¢y l√† card Host-Only Network.

				IP 192.168.56.10: cho ph√©p giao ti·∫øp gi·ªØa m√°y ·∫£o v√† host, nh∆∞ng kh√¥ng ra internet.

				Th∆∞·ªùng d√πng khi b·∫°n c·∫•u h√¨nh private_network trong Vagrant (192.168.56.x).
				
			enp0s9 (Bridge Adapter):
			
				ƒê√¢y l√† card Bridge (n·ªëi th·∫≥ng v√†o m·∫°ng LAN/WiFi th·ª±c t·∫ø c·ªßa b·∫°n).

				IP 192.168.1.25: ƒë∆∞·ª£c c·∫•p t·ª´ DHCP c·ªßa router WiFi/LAN.

				Cho ph√©p m√°y ·∫£o ho·∫°t ƒë·ªông nh∆∞ m·ªôt m√°y ri√™ng trong m·∫°ng th·∫≠t, c√°c m√°y kh√°c trong m·∫°ng LAN c√≥ th·ªÉ
				truy c·∫≠p tr·ª±c ti·∫øp.


	- cmd: ssh ngoctuanqng1@192.168.1.17
	
		ƒê√¢y l√† l·ªánh k·∫øt n·ªëi ƒë·∫øn m·ªôt m√°y t√≠nh kh√°c qua giao th·ª©c SSH (Secure Shell).	

		Ph√¢n t√≠ch t·ª´ng ph·∫ßn:

			ssh: ch∆∞∆°ng tr√¨nh d√πng ƒë·ªÉ ƒëƒÉng nh·∫≠p t·ª´ xa an to√†n, cho ph√©p b·∫°n ch·∫°y l·ªánh tr√™n m·ªôt m√°y kh√°c qua m·∫°ng.

			ngoctuanqng1: t√™n user m√† b·∫°n mu·ªën ƒëƒÉng nh·∫≠p v√†o m√°y ƒë√≠ch.

			@: ph√¢n t√°ch gi·ªØa user v√† ƒë·ªãa ch·ªâ m√°y.

			192.168.1.17: ƒë·ªãa ch·ªâ IP c·ªßa m√°y ƒë√≠ch trong m·∫°ng LAN (·ªü ƒë√¢y l√† m√°y b·∫°n mu·ªën SSH v√†o).
			
		Quy tr√¨nh khi ch·∫°y l·ªánh:

			M√°y b·∫°n m·ªü k·∫øt n·ªëi SSH ƒë·∫øn ƒë·ªãa ch·ªâ 192.168.1.17 qua c·ªïng 22 (m·∫∑c ƒë·ªãnh).

			Server SSH tr√™n m√°y 192.168.1.17 ph·∫£n h·ªìi, y√™u c·∫ßu x√°c th·ª±c.

			B·∫°n nh·∫≠p password (ho·∫∑c d√πng SSH key) cho user ngoctuanqng1.

			N·∫øu ƒëƒÉng nh·∫≠p th√†nh c√¥ng, b·∫°n s·∫Ω c√≥ m·ªôt terminal shell c·ªßa m√°y 192.168.1.17 v√† c√≥ th·ªÉ ch·∫°y l·ªánh t·ª´ xa.
			
		B√¢y gi·ªù b·∫°n ƒëang ·ªü trong m√°y 192.168.1.17 v·ªõi quy·ªÅn user ngoctuanqng1.

		M·ªôt s·ªë tu·ª≥ ch·ªçn hay d√πng:

			Ch·ªâ ƒë·ªãnh c·ªïng kh√°c (n·∫øu SSH server kh√¥ng ch·∫°y tr√™n port 22):

				ssh -p 2222 ngoctuanqng1@192.168.1.17
				
			Ch·∫°y 1 l·ªánh t·ª´ xa m√† kh√¥ng c·∫ßn login interactive:
			
				ssh ngoctuanqng1@192.168.1.17 "hostname"
				
				S·∫Ω in ra hostname c·ªßa m√°y ƒë√≠ch r·ªìi tho√°t.
	
	- cmd: hostname
	
		ƒê√¢y l√† l·ªánh trong Linux/Unix d√πng ƒë·ªÉ hi·ªÉn th·ªã ho·∫∑c ƒë·∫∑t t√™n c·ªßa m√°y t√≠nh (hostname).	
			
		Khi ch·∫°y ƒë∆°n gi·∫£n hostname
			
			N√≥ s·∫Ω in ra t√™n m√°y (hostname) hi·ªán t·∫°i.
			
				$ hostname
				localhost.localdomain
				
				localhost ‚Üí t√™n ng·∫Øn (short hostname).

				localhost.localdomain ‚Üí t√™n ƒë·∫ßy ƒë·ªß (FQDN ‚Äì Fully Qualified Domain Name).
				
			ƒê·∫∑t l·∫°i hostname (c·∫ßn quy·ªÅn sudo)
			
				sudo hostname myserver
				
				hostname t·∫°m th·ªùi ƒë·ªïi th√†nh myserver (ch·ªâ c√≥ hi·ªáu l·ª±c ƒë·∫øn khi reboot).
				
		Ki·ªÉm tra c√°c t√πy ch·ªçn kh√°c:

			hostname -i ‚Üí in ra ƒë·ªãa ch·ªâ IP g·∫Øn v·ªõi hostname.

			hostname -f ‚Üí in ra FQDN (t√™n ƒë·∫ßy ƒë·ªß v·ªõi domain).

			hostname -s ‚Üí in ra t√™n ng·∫Øn (short hostname).
			
		Li√™n h·ªá th·ª±c t·∫ø:

			Hostname gi√∫p ph√¢n bi·ªát c√°c m√°y trong m·∫°ng n·ªôi b·ªô ho·∫∑c h·ªá th·ªëng server.

			Khi b·∫°n SSH v√†o m√°y kh√°c, d√≤ng prompt th∆∞·ªùng hi·ªÉn th·ªã user@hostname ƒë·ªÉ bi·∫øt m√¨nh ƒëang ·ªü ƒë√¢u.
	
	- cmd: pwd
	
		ƒê√¢y l√† l·ªánh trong Linux/Unix d√πng ƒë·ªÉ hi·ªÉn th·ªã ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c hi·ªán t·∫°i (th∆∞ m·ª•c m√† b·∫°n ƒëang ƒë·ª©ng trong terminal).

		pwd = print working directory.	
	
	- cmd: mkdir /f/vagrant-vms
	
		mkdir (make directory): l·ªánh d√πng ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi trong Linux/Unix.

		/f/vagrant-vms: ƒë∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c mu·ªën t·∫°o.

		Nghƒ©a l√† b·∫°n ƒëang mu·ªën t·∫°o m·ªôt th∆∞ m·ª•c c√≥ t√™n vagrant-vms b√™n trong th∆∞ m·ª•c /f.

		M·ªôt v√†i l∆∞u √Ω quan tr·ªçng

			Trong Linux, ƒë∆∞·ªùng d·∫´n /f/... c√≥ nghƒ©a l√† trong th∆∞ m·ª•c g·ªëc / c√≥ m·ªôt th∆∞ m·ª•c con t√™n l√† f.
			
			Trong Windows + Git Bash / WSL, c√∫ ph√°p /f/... th∆∞·ªùng √°m ch·ªâ ·ªï ƒëƒ©a F: ƒë∆∞·ª£c mount v√†o Linux environment.

				V√≠ d·ª•:

					/c/... ‚Üí ·ªï ƒëƒ©a C:

					/d/... ‚Üí ·ªï ƒëƒ©a D:

					/f/... ‚Üí ·ªï ƒëƒ©a F:
					
					=> Nghƒ©a l√† b·∫°n ƒëang t·∫°o th∆∞ m·ª•c vagrant-vms trong ·ªï ƒëƒ©a F:.
	
	- cmd: vagrant init eurolinux-vagrant/centos-stream-9
	
		ƒê√¢y l√† l·ªánh c·ªßa Vagrant d√πng ƒë·ªÉ kh·ªüi t·∫°o (init) m·ªôt m√¥i tr∆∞·ªùng m√°y ·∫£o m·ªõi d·ª±a tr√™n box c√≥
		t√™n eurolinux-vagrant/centos-stream-9.
		
		Gi·∫£i th√≠ch chi ti·∫øt t·ª´ng ph·∫ßn:

			vagrant: c√¥ng c·ª• qu·∫£n l√Ω m√¥i tr∆∞·ªùng ·∫£o (VM) d√πng chung v·ªõi VirtualBox, VMware, Hyper-V, Docker...

			init: kh·ªüi t·∫°o project Vagrant m·ªõi. L·ªánh n√†y s·∫Ω t·∫°o ra m·ªôt file Vagrantfile trong th∆∞ m·ª•c hi·ªán t·∫°i.

			eurolinux-vagrant/centos-stream-9: t√™n c·ªßa box (h√¨nh ·∫£nh h·ªá ƒëi·ªÅu h√†nh template) ƒë∆∞·ª£c l·∫•y t·ª´ Vagrant Cloud.

				eurolinux-vagrant ‚Üí publisher/organization ƒë√£ t·∫°o box.

				centos-stream-9 ‚Üí t√™n box (·ªü ƒë√¢y l√† CentOS Stream 9).
				
		K·∫øt qu·∫£ khi ch·∫°y l·ªánh:

			T·∫°o file Vagrantfile trong th∆∞ m·ª•c hi·ªán t·∫°i (n·∫øu ch∆∞a c√≥).
			
			V√≠ d·ª• n·ªôi dung c∆° b·∫£n:

				Vagrant.configure("2") do |config|
				  config.vm.box = "eurolinux-vagrant/centos-stream-9"
				end

			File n√†y ƒë·ªãnh nghƒ©a m√°y ·∫£o m√† Vagrant s·∫Ω qu·∫£n l√Ω.
	
	
	- cmd: vagrant init ubuntu/jammy64
	- cmd: cat Vagrantfile
	- cmd: vagrant up
	- cmd: vagrant box list
	- cmd: vagrant status
	- cmd: vagrant ssh
	- cmd: whoami
	- cmd: exit
	- cmd: vagrant halt
	- cmd: vagrant reload
	- cmd: vagrant destroy
	- cmd: vagrant global-status
	- cmd: vagrant global-status --prune
	- cmd: sudo -i
	- cmd: history

	-- Manual:
	
		Oracle VM Virtualbox (Hypervisor)
		
		ISO file (CentOS & Ubuntu)
		
		Login tool (Git Bash & Putty)
		
		T·∫£i file iso c·ªßa centos v√† ubuntu ƒë·ªÉ import v√†o Oracle VM Virtualbox
		
	-- Automated:
	
		VirtualBox (Hypervisor)
		
		Vagrant (Creates vms with Vagrantfile)
		
		commands (vagrant up)
		
		V√†o vagrant cloud ƒë·ªÉ t√¨m l·ªánh c·ªßa vagrant ƒë·ªÉ k√©o file v·ªÅ centos 9
		(eurolinux-vagrant/centos-stream-9) v√† ubuntu jammy (ubuntu/jammy64)
		
	T·∫£i c√°c file iso r·ªìi import v√†o storage c·ªßa t·ª´ng vm
	
	-- Oracle VM VirtualBox l√† g√¨?

		L√† ph·∫ßn m·ªÅm ·∫£o h√≥a (virtualization), cho ph√©p b·∫°n ch·∫°y nhi·ªÅu h·ªá ƒëi·ªÅu h√†nh (OS)
		kh√°c nhau tr√™n c√πng m·ªôt m√°y t√≠nh th·∫≠t.

		V√≠ d·ª•: m√°y th·∫≠t c·ªßa b·∫°n ch·∫°y Windows, nh∆∞ng b·∫°n c√≥ th·ªÉ c√†i th√™m m·ªôt m√°y ·∫£o Ubuntu
		Linux trong VirtualBox ƒë·ªÉ h·ªçc, test m√† kh√¥ng ·∫£nh h∆∞·ªüng Windows.
		
		T·∫°i sao c·∫ßn d√πng VirtualBox?
		
			H·ªçc t·∫≠p & Th·ª≠ nghi·ªám

				B·∫°n c√≥ th·ªÉ c√†i nhi·ªÅu h·ªá ƒëi·ªÅu h√†nh (Ubuntu, CentOS, Kali, Windows Server, ‚Ä¶)
				ƒë·ªÉ h·ªçc IT, m·∫°ng, b·∫£o m·∫≠t, DevOps, l·∫≠p tr√¨nh.

				Th·ª≠ nghi·ªám ph·∫ßn m·ªÅm, c·∫•u h√¨nh h·ªá th·ªëng, c√†i package m√† kh√¥ng s·ª£ l√†m h·ªèng m√°y th·∫≠t.

			M·∫°ng & Server

				Gi·∫£ l·∫≠p m√¥i tr∆∞·ªùng server: c√†i Apache/Nginx, MySQL, Kafka, RabbitMQ, Docker‚Ä¶

				D·ªÖ d√†ng c·∫•u h√¨nh m·∫°ng ·∫£o (NAT, Bridged, Host-only) ‚Üí h·ªçc c√°ch c√°c server giao
				ti·∫øp v·ªõi nhau.

			Ph√°t tri·ªÉn ph·∫ßn m·ªÅm

				T·∫°o m√¥i tr∆∞·ªùng ri√™ng bi·ªát ƒë·ªÉ test code, deploy ·ª©ng d·ª•ng.

				C√≥ th·ªÉ snapshot (l∆∞u tr·∫°ng th√°i m√°y ·∫£o) ‚Üí n·∫øu h·ªèng th√¨ rollback l·∫°i ngay.

			An to√†n & B·∫£o m·∫≠t

				Ch·∫°y th·ª≠ ph·∫ßn m·ªÅm l·∫°, file nghi ng·ªù virus trong m√°y ·∫£o.

				M√°y ·∫£o c√°ch ly v·ªõi m√°y th·∫≠t, h·∫°n ch·∫ø r·ªßi ro.

			Ph·ªï bi·∫øn trong DevOps / Cloud

				Tr∆∞·ªõc khi tri·ªÉn khai th·∫≠t tr√™n server (AWS, GCP, Azure), b·∫°n c√≥ th·ªÉ m√¥ ph·ªèng m√¥i
				tr∆∞·ªùng cloud ngay tr√™n m√°y t√≠nh c√° nh√¢n b·∫±ng VirtualBox.

				K·∫øt h·ª£p v·ªõi Vagrant ƒë·ªÉ t·ª± ƒë·ªông t·∫°o m√¥i tr∆∞·ªùng dev/test.
				
		Khi n√†o kh√¥ng c·∫ßn VirtualBox?

			N·∫øu b·∫°n ch·ªâ mu·ªën ch·∫°y Linux th∆∞·ªùng xuy√™n ‚Üí c√≥ th·ªÉ dual
			boot (c√†i song song Windows + Linux).

			N·∫øu b·∫°n ch·ªâ c·∫ßn m√¥i tr∆∞·ªùng dev nhanh ‚Üí c√≥ th·ªÉ d√πng WSL
			(Windows Subsystem for Linux) thay cho VM.
	
	-- File ISO l√† g√¨?

		ISO file = m·ªôt b·∫£n sao to√†n b·ªô (image) c·ªßa ƒëƒ©a CD/DVD.

		N√≥ ch·ª©a ƒë·∫ßy ƒë·ªß h·ªá ƒëi·ªÅu h√†nh ho·∫∑c ph·∫ßn m·ªÅm, gi·ªëng nh∆∞ b·∫°n c·∫ßm m·ªôt c√°i ƒëƒ©a c√†i ƒë·∫∑t.

		Trong VirtualBox, file ISO ƒë∆∞·ª£c d√πng nh∆∞ ƒëƒ©a c√†i ƒë·∫∑t h·ªá ƒëi·ªÅu h√†nh cho m√°y ·∫£o (VM).

		V√≠ d·ª•:

		N·∫øu b·∫°n mu·ªën c√†i Ubuntu tr√™n VirtualBox ‚Üí b·∫°n c·∫ßn t·∫£i file ubuntu-22.04.iso.

		N·∫øu b·∫°n mu·ªën c√†i Windows 10 ‚Üí b·∫°n c·∫ßn file Win10.iso.

		N·∫øu b·∫°n mu·ªën c√†i CentOS/RHEL ‚Üí t·∫£i file CentOS-Stream-9.iso ho·∫∑c rhel.iso.
		
	-- Setting trong Oracle VM Virtualbox:
	
		General: Th√¥ng tin chung c·ªßa VM (t√™n, phi√™n b·∫£n OS, m√¥ t·∫£).

		System: CPU, RAM, Boot order.

		Display: c·∫•u h√¨nh card m√†n h√¨nh, ƒë·ªô ph√¢n gi·∫£i.

		Storage: g·∫Øn ·ªï c·ª©ng ·∫£o (VDI, VMDK) ho·∫∑c file ISO c√†i h·ªá ƒëi·ªÅu h√†nh.

		Audio: b·∫≠t/t·∫Øt √¢m thanh.

		Network: c·∫•u h√¨nh m·∫°ng cho m√°y ·∫£o.

		USB, Shared Folders, User Interface: c·∫•u h√¨nh USB, th∆∞ m·ª•c chia s·∫ª, giao di·ªán ng∆∞·ªùi d√πng.
		
	-- Controller trong VirtualBox l√† g√¨?
	
		Controller l√† b·ªô ƒëi·ªÅu khi·ªÉn (controller) gi·∫£ l·∫≠p ƒë·ªÉ qu·∫£n l√Ω c√°c thi·∫øt b·ªã l∆∞u tr·ªØ (Storage Devices) g·∫Øn v√†o m√°y ·∫£o.

		N√≥ ho·∫°t ƒë·ªông gi·ªëng nh∆∞ card ƒëi·ªÅu khi·ªÉn ·ªï c·ª©ng/·ªï ƒëƒ©a trong m√°y th·∫≠t.

		C√°c controller ph·ªï bi·∫øn: IDE, SATA, SCSI, SAS, NVMe, Floppy.
		
		Controller: IDE

			IDE (Integrated Drive Electronics) l√† chu·∫©n c≈©, th∆∞·ªùng d√πng cho ·ªï ƒëƒ©a CD/DVD v√† ·ªï c·ª©ng th·∫ø h·ªá c≈©.

			Trong VirtualBox:

			IDE th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ mount file ISO (nh∆∞ ƒëƒ©a c√†i CentOS, Ubuntu).

			·ªû ·∫£nh c·ªßa b·∫°n:

				Controller: IDE c√≥ 1 thi·∫øt b·ªã g·∫Øn l√† Empty (·ªï CD/DVD tr·ªëng).

				Khi b·∫°n ch·ªçn v√† g·∫Øn file ISO ‚Üí VirtualBox coi nh∆∞ b·∫°n b·ªè "ƒëƒ©a CD ·∫£o" v√†o ·ªï IDE n√†y ƒë·ªÉ boot v√† c√†i OS.

		Controller: SATA

			SATA (Serial ATA) l√† chu·∫©n m·ªõi h∆°n, nhanh h∆°n IDE, th∆∞·ªùng d√πng cho ·ªï c·ª©ng hi·ªán ƒë·∫°i.

			Trong VirtualBox:

				Controller: SATA ƒëang ch·ª©a centosvm.vdi ‚Üí ƒë√¢y ch√≠nh l√† ·ªï c·ª©ng ·∫£o (Virtual Disk Image) c·ªßa m√°y CentOS.

				H·ªá ƒëi·ªÅu h√†nh (CentOS) sau khi c√†i s·∫Ω n·∫±m trong ·ªï n√†y.
		
	ACPI l√† g√¨?

		ACPI (Advanced Configuration and Power Interface) l√† m·ªôt chu·∫©n do Intel, Microsoft v√† Toshiba ph√°t tri·ªÉn.

		N√≥ cho ph√©p h·ªá ƒëi·ªÅu h√†nh qu·∫£n l√Ω ngu·ªìn ƒëi·ªán c·ªßa ph·∫ßn c·ª©ng (power management).

		Nh·ªù ACPI, OS c√≥ th·ªÉ:

			T·∫Øt/m·ªü m√°y b·∫±ng ph·∫ßn m·ªÅm.

			ƒê∆∞a m√°y v√†o ch·∫ø ƒë·ªô Sleep/Hibernate.

			Gi·∫£m t·ªëc ƒë·ªô CPU khi kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám ƒëi·ªán.

		ACPI Shutdown trong VirtualBox l√† g√¨?

			Khi b·∫°n ch·ªçn ACPI Shutdown trong VirtualBox (ho·∫∑c trong c√°c hypervisor kh√°c), n√≥ s·∫Ω g·ª≠i t√≠n hi·ªáu
			ACPI ƒë·∫øn h·ªá ƒëi·ªÅu h√†nh trong m√°y ·∫£o.

			H√†nh ƒë·ªông n√†y t∆∞∆°ng t·ª± nh∆∞ nh·∫•n n√∫t Power tr√™n m√°y t√≠nh th·∫≠t (nh∆∞ng kh√¥ng ph·∫£i "t·∫Øt c·ª©ng").

			H·ªá ƒëi·ªÅu h√†nh trong VM s·∫Ω nh·∫≠n t√≠n hi·ªáu v√† t·ª± th·ª±c hi·ªán qu√° tr√¨nh shutdown an to√†n:

				ƒê√≥ng ·ª©ng d·ª•ng.

				Ghi d·ªØ li·ªáu c√≤n trong RAM xu·ªëng ƒëƒ©a.

				Sau ƒë√≥ t·∫Øt m√°y.

			Kh√°c v·ªõi Power Off

				ACPI Shutdown: T·∫Øt an to√†n, gi·ªëng nh∆∞ b·∫°n b·∫•m n√∫t Shutdown trong Windows/Linux.

				Power Off (Hard Power Off): T·∫Øt c·ª©ng, gi·ªëng nh∆∞ r√∫t d√¢y ngu·ªìn ‚Üí c√≥ th·ªÉ g√¢y m·∫•t d·ªØ li·ªáu ho·∫∑c h·ªèng
				file h·ªá th·ªëng.
				
				
--- Linux:

	Command line:

		- cmd: cat /etc/os-release
		- cmd: cd
		
			cd (kh√¥ng tham s·ªë) ‚Üí quay v·ªÅ th∆∞ m·ª•c home c·ªßa user.
			
		- cmd: cd /tmp/
		
			/tmp/ c√≥ d·∫•u / ·ªü ƒë·∫ßu ‚Üí ƒë√¢y l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi.

			Nghƒ©a l√†: ƒëi th·∫≥ng v√†o th∆∞ m·ª•c /tmp n·∫±m ·ªü g·ªëc c·ªßa h·ªá th·ªëng (/).

			D√π b·∫°n ƒëang ƒë·ª©ng ·ªü ƒë√¢u th√¨ k·∫øt qu·∫£ v·∫´n gi·ªëng nhau.		
		
		- cmd: uptime
		- cmd: free -m
		- cmd: mkdir ops bakupdir
		
			mkdir = make directory, d√πng ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi.

				ops = t√™n th∆∞ m·ª•c th·ª© nh·∫•t b·∫°n mu·ªën t·∫°o.

				bakupdir = t√™n th∆∞ m·ª•c th·ª© hai b·∫°n mu·ªën t·∫°o.
				
			Khi vi·∫øt hai t√™n th∆∞ m·ª•c nh∆∞ v·∫≠y, mkdir s·∫Ω t·∫°o c√πng l√∫c 2 th∆∞ m·ª•c trong th∆∞ m·ª•c hi·ªán t·∫°i	
		
		- cmd: touch devopsfile{1..10}.txt
		- cmd: cp devopsfile1.txt dev/
		- cmd: ls dev/
		- cmd: ls /home/vagrant/dev/
		- cmd: cp --help
		- cmd: mv devopsfile3.txt ops/
		- cmd: mv testfile1.txt testfile22.txt
		
			ƒê√¢y l√† l·ªánh di chuy·ªÉn (move) ho·∫∑c ƒë·ªïi t√™n (rename) file/th∆∞ m·ª•c trong Linux/Unix.
			
			V√¨ c·∫£ hai t√™n ƒë·ªÅu ·ªü c√πng m·ªôt th∆∞ m·ª•c ‚Üí l·ªánh n√†y ƒë·ªïi t√™n file t·ª´ testfile1.txt th√†nh testfile22.txt.
			
		- cmd: touch testfile1.txt
		- cmd: mv *.txt textdir/
		- cmd: rm devopsfile10.txt
		- cmd: rm -r mobile
		- cmd: mkdir testdir{1..5}
		- cmd: rm -rf *
		- cmd: history
		- cmd: cat /etc/os-release
		
			ƒê√¢y l√† l·ªánh ƒë·ªÉ xem th√¥ng tin v·ªÅ h·ªá ƒëi·ªÅu h√†nh Linux ƒëang ch·∫°y.
			
		- cmd: sudo yum install vim -y
		- cmd: vim firstfile.txt
		- cmd: ls -l
		- cmd: file anaconda-ks.cfg
		- cmd: file yum
		- cmd: file /bin/pwd
		- cmd: mkdir -p /opt/dev/ops/devops/test
		- cmd: ln -s /opt/dev/ops/devops/test/command.txt cmds
		- cmd: unlink cmds
		- cmd: ls -lt
		- cmd: ls -ltr
		- cmd: ls -ltr /etc/
		- cmd: hostname centos7.devops. in
		- cmd: ls -ltr /etc/
		- cmd: grep firewall anaconda-ks.cfg
		- cmd: grep Firewall anaconda-ks.cfg
		- cmd: grep -i Firewall anaconda-ks.cfg
		- cmd: grep -i firewall < anaconda-ks.cfg
		- cmd: grep -i firewall *
		- cmd: grep -iR firewall *
		- cmd: grep -R SELINUX /etc/*
		- cmd: grep -vi firewall anaconda-ks.cfg
		- cmd: less anaconda-ks.cfg
		- cmd: more anaconda-ks.cfg
		- cmd: head anaconda-ks.cfg
		- cmd: head -20 anaconda-ks.cfg
		- cmd: tail anaconda-ks.cfg
		- cmd: tail -2 anaconda-ks.cfg
		- cmd: tail -f anaconda-ks.cfg
		- cmd: tail -f yum.log
		- cmd: cut -d: -f1 /etc/passwd
		- cmd: cut -d: -f3 /etc/passwd
		- cmd: awk -F': ' ' {print $1}' /etc/passwd
		- cmd: sed 's/coronavirus/covid19/g' samplefile.txt
		- cmd: sed -i 's/covid19/nothing/g' samplefile.txt
		- cmd: sed -i 's/coronavirus/covid19/g' samplefile.txt
		- cmd: uptime > /tmp/sysinfo.txt
		- cmd: ls > /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m
		- cmd: df -h
		- cmd: echo "Good Morning"
		- cmd: echo "#############################" > /tmp/sysinfo.txt
		- cmd: date > /tmp/sysinfo.txt
		- cmd: echo "#############################" >> /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m >> /tmp/sysinfo.txt
		- cmd: df -h >> /tmp/sysinfo.txt
		- cmd: yum install vim -y > /dev/null
		- cmd: cat /dev/null > /tmp/sysinfo.txt
		- cmd: free -m > /dev/null
		- cmd: freeeee -m 2>> /tmp//error.log
		- cmd: free -m 1>> /tmp//error.log
		- cmd: free -m &>> /tmp//error.log
		- cmd: freesdsd -m &>> /tmp//error.log
		- cmd: wc -l /etc/passwd
		- cmd: wc -l < /etc/passwd
		- cmd: ls | wc -l
		- cmd: ls | grep host*
		- cmd: ls | grep host
		- cmd: tail /var/log/messages | grep -i vagrant
		- cmd: tail -20 /var/log/messages | grep -i vagrant
		- cmd: free -m | grep Mem
		- cmd: ls -l | tail
		- cmd: ls -l | head
		- cmd: find /etc -name host*
		- cmd: find / -name host*
		- cmd: yum install mlocate -y
		- cmd: updatedb
		- cmd: locate host
		- cmd: head -1 /etc/passwd
		- cmd: grep vagrant /etc/passwd
		- cmd: id vagrant
		- cmd: useradd ansible
		- cmd: useradd jenkins
		- cmd: useradd aws
		- cmd: tail -4 /etc/passwd
		- cmd: tail -4 /etc/group
		- cmd: id ansible
		- cmd: groupadd devops
		- cmd: ls -l
		- cmd: ls -ld /opt/devopsdir
		- cmd: chown -R ansible:devops /opt/devopsdir
		- cmd: ls -ld /opt/devopsdir
		- cmd: chmod o-x /opt/devopsdir
		- cmd: chmod g+w /opt/devopsdir
		- cmd: su - miles
		- cmd: su - aws
		- cmd: chown aws.devops /opt/webdata
		- cmd: chmod -R 770 /opt/webdata
		- cmd: chmod -R 754 /opt/webdata/
		- cmd: sudo yum install git -y
		- cmd: sudo useradd test
		- cmd: passwd ansible
		- cmd: su - ansible
		- cmd: sudo useradd test12
		- cmd: visudo
		- cmd: ls -l /etc/sudoers
		- cmd: cd /etc/sudoers.d/
		- cmd: cat vagrant
		- cmd: cat *
		- cmd: rpm -qa
		- cmd: telnet
		- cmd: arch
		- cmd: uname -m
		- cmd: curl https://rpmfind.net/linux/RPM/centos-stream/9/appstream/x86_64/telnet-0.17-85.el9.x86_64.html
		- cmd: curl https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/telnet-0.17-85.el9.x86_64.rpm -o telnet-0.17-85.el9.x86_64.rpm
		- cmd: rpm -ivh telnet-0.17-85.el9.x86_64.rpm
		- cmd: telnet
		- cmd: rpm -qa | grep telnet
		- cmd: rpm -e telnet-0.17-85.el9.x86_64
		- cmd: wget https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/httpd-2.4.57-8.el9.x86_64.rpm
		- cmd: cd /etc/yum.repos.d/
		- cmd: yum search httpd
		- cmd: yum install httpd
		- cmd: dnf remove httpd
		- cmd: dnf install httpd -y
		- cmd: dnf --help
		- cmd: yum upgrade
		- cmd: yum install jenkins
		- cmd: sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
		- cmd: sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
		- cmd: sudo yum upgrade
		- cmd: dnf repolist
		- cmd: dnf install epel-release -y
		- cmd: dnf history
		- cmd: systemctl status httpd
		- cmd: systemctl start httpd
		- cmd: sudo reboot
		- cmd: systemctl enable httpd
		- cmd: systemctl status sshd
		- cmd: systemctl is-active httpd
		- cmd: systemctl is-enabled httpd
		- cmd: cat /etc/systemd/system/multi-user.target.wants/httpd.service
		- cmd: top
		- cmd: ps aux
		- cmd: ps -ef
		- cmd: ps -ef | grep httpd | grep -v 'grep'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}' | xargs kill -9
		- cmd: file jenkins_06122020.tar.gz
		- cmd: tar -czvf kenkins_06122020.tar.gz jenkins
		- cmd: tar -xzvf jenkins_06122020.tar.gz
		- cmd: tar -xzvf jenkins_06122020.tar.gz -C/pop/
		- cmd: tar --help
		- cmd: yum install zip unzip -y
		- cmd: zip -r jenkins_06122020.zip jenkins
		- cmd: ls -ltr jenkins*
		- cmd: rm -rf jenkins
		- cmd: unzip jenkins_06122020.zip
		- cmd: userdel -r devops
		- cmd: export EDITOR=vim
		- cmd: wget http://archive.ubuntu.com/ubuntu/pool/universe/t/tree/tree_2.0.2-1_amd64.deb
		- cmd: dpkg -i tree_2.0.2-1_amd64.deb
		- cmd: dpkg -l
		- cmd: dpkg -l | grep tree
		- cmd: dpkg -r tree
		- cmd: apt update
		- cmd: apt search tree
		- cmd: apt install tree
		- cmd: apt install apache2
		- cmd: systemctl status apache2
		- cmd: apt upgrade
		- cmd: apt remove apache2
		- cmd: apt purge apache2
		
	
	Tr√™n host: b·∫°n kh√¥ng c√≥ sudo ‚Üí l·ªói.

	Sau khi vagrant ssh: b·∫°n ƒëang ·ªü trong m√°y ·∫£o, n∆°i sudo ƒë√£ ƒë∆∞·ª£c c√†i v√† c·∫•u h√¨nh s·∫µn ‚Üí ch·∫°y ƒë∆∞·ª£c sudo -i.
	
	$ vagrant ssh
	
		A Vagrant environment or target machine is required to run this
		command. Run `vagrant init` to create a new Vagrant environment. Or,
		get an ID of a target machine from `vagrant global-status` to run
		this command on. A final option is to change to a directory with a
		Vagrantfile and to try again.
		
		C√≥ nghƒ©a l√† b·∫°n ƒëang ch·∫°y vagrant ssh nh∆∞ng Vagrant kh√¥ng bi·∫øt b·∫°n mu·ªën SSH v√†o m√°y ·∫£o n√†o.
		
		B·∫°n ch∆∞a t·∫°o m√°y ·∫£o b·∫±ng vagrant init + vagrant up.

		Ho·∫∑c b·∫°n ƒëang kh√¥ng ƒë·ª©ng trong th∆∞ m·ª•c c√≥ ch·ª©a file Vagrantfile.

		Ho·∫∑c ch∆∞a c√≥ m√°y ·∫£o n√†o ƒëang ch·∫°y.
		
	C√°ch tho√°t ra kh·ªèi vim:

		Nh·∫•n ph√≠m Esc (ƒë·ªÉ ch·∫Øc ch·∫Øn kh√¥ng c√≤n ·ªü ch·∫ø ƒë·ªô nh·∫≠p vƒÉn b·∫£n n·ªØa).

		Sau ƒë√≥ g√µ m·ªôt trong c√°c l·ªánh sau r·ªìi b·∫•m Enter:

		:q ‚Üí tho√°t n·∫øu ch∆∞a ch·ªânh s·ª≠a g√¨.

		:q! ‚Üí tho√°t b·ªè qua thay ƒë·ªïi (kh√¥ng l∆∞u).

		:wq ‚Üí l∆∞u r·ªìi tho√°t.

		:x ‚Üí t∆∞∆°ng t·ª± :wq (l∆∞u r·ªìi tho√°t).

	Vim:
	
		L·ªánh Shift + g
		
			Di chuy·ªÉn con tr·ªè xu·ªëng d√≤ng cu·ªëi c√πng c·ªßa file.
			
		L·ªánh yy

			yy = yank line (sao ch√©p 1 d√≤ng hi·ªán t·∫°i).

			Khi b·∫°n ƒëang ·ªü ch·∫ø ƒë·ªô Normal mode (b·∫•m Esc ƒë·ªÉ ch·∫Øc ch·∫Øn):

			yy s·∫Ω copy nguy√™n c·∫£ d√≤ng n∆°i con tr·ªè ƒëang ƒë·ª©ng v√†o b·ªô nh·ªõ t·∫°m (clipboard c·ªßa Vim).

			V√≠ d·ª•:
			
				Con tr·ªè ƒëang ·ªü d√≤ng s·ªë 5, g√µ yy ‚Üí d√≤ng s·ªë 5 ƒë∆∞·ª£c copy.
				
		L·ªánh yyyy

		L·ªánh p

			p = put (d√°n sau con tr·ªè).

			Sau khi copy b·∫±ng yy, b·∫°n c√≥ th·ªÉ d√°n d√≤ng ƒë√≥ xu·ªëng ngay d∆∞·ªõi con tr·ªè b·∫±ng p.

			V√≠ d·ª•:

				B·∫°n ƒë·ª©ng ·ªü d√≤ng s·ªë 5, g√µ yy.

				Sau ƒë√≥ xu·ªëng d√≤ng s·ªë 10, g√µ p.

				D√≤ng s·ªë 5 s·∫Ω ƒë∆∞·ª£c ch√®n v√†o sau d√≤ng s·ªë 10.
				
		L·ªánh dd
		
		L·ªánh ddddd
		
		L·ªánh u
		
		L·ªánh /
		
		L·ªánh :%s/coronavirus/covid19/g
		
			Thay to√†n b·ªô t·∫•t c·∫£ c√°c ch·ªØ "coronavirus" trong file th√†nh "covid19".
			
			Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:
		
				: ‚Üí v√†o command mode trong Vim (g√µ l·ªánh).

				% ‚Üí ph·∫°m vi √°p d·ª•ng: to√†n b·ªô file.

				N·∫øu ch·ªâ vi·∫øt :s/.../.../ th√¨ n√≥ ch·ªâ thay trong d√≤ng hi·ªán t·∫°i.

				% nghƒ©a l√† t·ª´ d√≤ng 1 ƒë·∫øn d√≤ng cu·ªëi.

				s ‚Üí vi·∫øt t·∫Øt c·ªßa substitute (thay th·∫ø).

				coronavirus ‚Üí l√† chu·ªói t√¨m ki·∫øm (c√°i c·∫ßn thay).

				covid19 ‚Üí l√† chu·ªói thay th·∫ø.

				g ‚Üí global trong d√≤ng ‚Üí thay t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán trong m·ªói d√≤ng.

				N·∫øu b·ªè g, th√¨ tr√™n m·ªói d√≤ng ch·ªâ thay l·∫ßn xu·∫•t hi·ªán ƒë·∫ßu ti√™n.
		
		L·ªánh :%s/covid19//g
		
			X√≥a to√†n b·ªô t·∫•t c·∫£ c√°c ch·ªØ covid19 trong to√†n b·ªô file.
			
			Gi·∫£i th√≠ch ng·∫Øn g·ªçn:

				:% ‚Üí √°p d·ª•ng cho to√†n b·ªô file.

				s ‚Üí substitute (thay th·∫ø).

				covid19 ‚Üí chu·ªói c·∫ßn t√¨m.

				// ‚Üí thay th·∫ø b·∫±ng chu·ªói r·ªóng (nghƒ©a l√† x√≥a ƒëi).

				g ‚Üí thay t·∫•t c·∫£ c√°c l·∫ßn xu·∫•t hi·ªán tr√™n m·ªói d√≤ng.
		
	Grep:
	
		grep trong Linux l√† m·ªôt l·ªánh d√πng ƒë·ªÉ t√¨m ki·∫øm chu·ªói (string/pattern) trong file ho·∫∑c
		trong output c·ªßa l·ªánh kh√°c. N√≥ l√† vi·∫øt t·∫Øt c·ªßa Global Regular Expression Print.
		
		C√°ch ho·∫°t ƒë·ªông

			grep s·∫Ω qu√©t qua t·ª´ng d√≤ng d·ªØ li·ªáu.

			N·∫øu d√≤ng n√†o kh·ªõp v·ªõi chu·ªói ho·∫∑c bi·ªÉu th·ª©c ch√≠nh quy (regex) m√† b·∫°n ch·ªâ ƒë·ªãnh ‚Üí n√≥ s·∫Ω in d√≤ng ƒë√≥ ra m√†n h√¨nh.
			
	dnf:
	
		Trong Linux (ƒë·∫∑c bi·ªát l√† CentOS, RHEL, Fedora), dnf l√† m·ªôt tr√¨nh qu·∫£n l√Ω g√≥i (package manager).
		
		Gi·∫£i th√≠ch ng·∫Øn g·ªçn

			dnf vi·∫øt t·∫Øt c·ªßa Dandified YUM.

			N√≥ l√† phi√™n b·∫£n m·ªõi thay th·∫ø cho yum t·ª´ RHEL/CentOS 8 tr·ªü ƒëi.

			D√πng ƒë·ªÉ:

			C√†i ƒë·∫∑t ph·∫ßn m·ªÅm.

			G·ª° b·ªè ph·∫ßn m·ªÅm.

			C·∫≠p nh·∫≠t h·ªá th·ªëng.

			Qu·∫£n l√Ω kho (repository).
			
	yum:
	
		Trong Linux (ƒë·∫∑c bi·ªát l√† c√°c b·∫£n CentOS, RHEL, Fedora c≈©), yum l√† m·ªôt tr√¨nh qu·∫£n l√Ω g√≥i (package manager).
		
		Gi·∫£i th√≠ch ng·∫Øn g·ªçn

			yum vi·∫øt t·∫Øt c·ªßa Yellowdog Updater, Modified.

			N√≥ gi√∫p b·∫°n c√†i ƒë·∫∑t, c·∫≠p nh·∫≠t, g·ª° b·ªè v√† qu·∫£n l√Ω ph·∫ßn m·ªÅm tr√™n h·ªá th·ªëng Linux d·ª±a tr√™n g√≥i RPM.

			yum l√†m vi·ªác v·ªõi c√°c repository (kho ph·∫ßn m·ªÅm) ƒë·ªÉ t·ª± ƒë·ªông t·∫£i v·ªÅ v√† x·ª≠ l√Ω ph·ª• thu·ªôc gi·ªØa c√°c g√≥i (dependencies).

		Quan h·ªá gi·ªØa yum v√† dnf

			yum l√† c√¥ng c·ª• c≈© (d√πng tr√™n CentOS 7, RHEL 7 tr·ªü xu·ªëng).

			T·ª´ CentOS/RHEL 8 tr·ªü ƒëi, yum ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·ªüi dnf.

			Tuy nhi√™n, ƒë·ªÉ tr√°nh g√¢y r·ªëi cho ng∆∞·ªùi d√πng quen yum, nhi·ªÅu h·ªá th·ªëng m·ªõi v·∫´n ƒë·ªÉ l·ªánh yum t·ªìn t·∫°i, nh∆∞ng th·ª±c
			ch·∫•t n√≥ ch·ªâ l√† symlink (li√™n k·∫øt) tr·ªè t·ªõi dnf.
			
	apt:
	
		apt (Advanced Package Tool) l√† tr√¨nh qu·∫£n l√Ω g√≥i (package manager) d√πng trong c√°c h·ªá ƒëi·ªÅu h√†nh Linux
		thu·ªôc h·ªç Debian (nh∆∞ Ubuntu, Debian, Linux Mint).

		N√≥ gi√∫p b·∫°n c√†i ƒë·∫∑t, n√¢ng c·∫•p, g·ª° b·ªè v√† qu·∫£n l√Ω ph·∫ßn m·ªÅm m·ªôt c√°ch d·ªÖ d√†ng, thay v√¨ ph·∫£i t·∫£i th·ªß c√¥ng t·ª´ng file .deb.
		
		
		
--- GIT:

	- Commands:
	
		- cmd: git init
		- cmd: ls -a
		- cmd: touch saturn{1..10}.py
		- cmd: git status
		- cmd: git add .
		- cmd: git commit -m "new files commited"
		- cmd: git config --global user.email "ngoctuanqng1@gmail.com"
		- cmd: git config --global user.name  "ngoctuanqng1"
		- cmd: git remote add origin https://github.com/ngoctuanqng/gitpractice.git
		- cmd: cat .git/config
		- cmd: git branch -m main
		- cmd: git push -u origin main
		- cmd: git add satural.py
		- cmd: git push origin main
		- cmd: git log
		- cmd: git log --oneline
		- cmd: git show 625b034
		- cmd: git pull
		- cmd: git branch -c sprintl
		- cmd: git branch -a
		- cmd: git checkout sprintl
		- cmd: git rm saturn6.py saturn7.py saturn8.py
		- cmd: git mv saturn1.py saturn11.py
		- cmd: touch jupiter{1..4}.rb
		- cmd: git push origin sprintl
		- cmd: touch sun earth venus mercury
		- cmd: git switch sprint1
		- cmd: git merge sprintl
		- cmd: git clone https://github.com/ngoctuanqng/gitpractice.git
		- cmd: git checkout jupiter1.rb
		- cmd: git diff
		- cmd: git diff --cached
		- cmd: git restore --staged jupiter1.rb
		- cmd: git diff 8dff644..cbb01a8
		- cmd: git revert HEAD
		- cmd: git reset --hard 8dff644
		- cmd: cat .git/config
		- cmd: rm -rf .ssh/*
		- cmd: cat .ssh/id_rsa.pub
		- cmd: git tag
		- cmd: git show v2.0.0
		- cmd: git tag -a v3.5.3 -m "Release 3.5.3"
		- cmd: systemctl restart httpd
		
		
		
		
		
	Ta c√≥ th·ªÉ t·∫°o ·ªü local tr∆∞·ªõc, sau ƒë√≥ t·∫°o ·ªü remote, d√πng l·ªánh git remote add origin ƒë·ªÉ ƒë·∫©y local v√†o remote
	
	touch a v√† touch a/ l√† kh√°c nhau, touch a s·∫Ω t·∫°o file a, c√≤n touch a/ s·∫Ω t·∫°o folder t√™n l√† a
	

--- Vagrant and Linux Servers:

	- Command:
	
		- cmd: vagrant destroy --force
		- cmd: ls ~/.vagrant.d/
		- cmd: cat /proc/cpuinfo
		- cmd: vagrant reload --provision
		- cmd: vi /etc/hostname
		- cmd: hostname finance
		- cmd: yum install httpd wget vim unzip zip -y
		- cmd: unzip 2138_aqua_nova.zip
		- cmd: rm -r 2138_aqua_nova
		- cmd: cp -r * /var/www/html/
		- cmd: systemctl status firewalld
		- cmd: systemctl stop firewalld
		- cmd: systemctl disable firewalld
		- cmd: vagrant init ubuntu/focal64
		- cmd: sudo apt update
		
		- cmd: sudo apt install apache2 \
                 ghostscript \
                 libapache2-mod-php \
                 mysql-server \
                 php \
                 php-bcmath \
                 php-curl \
                 php-imagick \
                 php-intl \
                 php-json \
                 php-mbstring \
                 php-mysql \
                 php-xml \
                 php-zip
				 
		- cmd: sudo mkdir -p /srv/www
		- cmd: sudo chown www-data: /srv/www
		- cmd: curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www
		- cmd: vim /etc/apache2/sites-available/wordpress.conf
		- cmd: sudo a2ensite wordpress
		- cmd: sudo a2enmod rewrite
		- cmd: sudo a2dissite 000-default
		- cmd: sudo service apache2 reload
		- cmd: sudo mysql -u root
		- cmd: CREATE DATABASE wordpress;
		- cmd: show databases;
		- cmd: CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
		- cmd: GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
		- cmd: FLUSH PRIVILEGES;
		- cmd: sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php
		- cmd: vim /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data vim /srv/www/wordpress/wp-config.php
		- cmd: vagrant ssh web02
		- cmd: vagrant destroy web01
		- cmd: vagrant up web03
		- cmd: ls /usr/lib/systemd/system/
		- cmd: cat /usr/lib/systemd/system/httpd.service
		- cmd: ls /etc/httpd/
		- cmd: ls /var/log/httpd/
		- cmd: wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.46/bin/apache-tomcat-10.1.46.tar.gz
		- cmd: tar xzvf apache-tomcat-10.1.46.tar.gz
		- cmd: dnf install java-17-openjdk -y
		- cmd: [root@localhost apache-tomcat-10.1.46]# bin/startup.sh
		- cmd: ps -ef | grep tomcat
		- cmd: kill 9886
		- cmd: [root@localhost ~]# useradd --home-dir /opt/tomcat --shell /sbin/nologin tomcat
		- cmd: [root@localhost ~]# cp -r apache-tomcat-10.1.46/* /opt/tomcat
		- cmd: [root@localhost ~]# chown -R tomcat.tomcat /opt/tomcat/
		- cmd: vim /etc/systemd/system/tomcat.service
		
			[Unit]
			Description=Tomcat
			After=network.target

			[Service]
			Type=forking

			User=tomcat
			Group=tomcat

			workingDirectory=/opt/tomcat

			Environment=JAVA_HOME=/usr/lib/jvm/jre

			Environment=CATALINA_HOME=/opt/tomcat
			Environment=CATALINE_BASE=/opt/tomcat

			ExecStart=/opt/tomcat/bin/startup.sh
			ExecStop=/opt/tomcat/bin/shutdown.sh

			[Install]
			WantedBy=multi-user.target

		- cmd: systemctl daemon-reload
		- cmd: [root@localhost ~]# systemctl start tomcat
		- cmd: [root@localhost ~]# systemctl status tomcat
		- cmd: [root@localhost ~]# systemctl enable tomcat
		
		
		
		
		
	
		
		
		
	G√°n c√°c n·ªôi dung c·ªßa trang web v√†o /var/www/html/ ƒë·ªÉ c√≥ th·ªÉ ch·∫°y l√™n trang web mong mu·ªën
	
	Download tomcat 10
	
	Truy c·∫≠p Tomcat:
	
		Tomcat theo m·∫∑c ƒë·ªãnh s·∫Ω nghe tr√™n port 8080, tr·ª´ khi b·∫°n ƒë√£ s·ª≠a file server.xml.
		
		Trong output ip addr show, card enp0s8 c√≥ ƒë·ªãa ch·ªâ: inet 192.168.1.11/24 brd 192.168.1.255
		
		ƒê√¢y l√† IP ƒë·ªÉ b·∫°n truy c·∫≠p t·ª´ host ho·∫∑c c√°c m√°y kh√°c trong c√πng m·∫°ng 192.168.1.x.
		
		B·∫°n c·∫ßn m·ªü tr√¨nh duy·ªát v√† nh·∫≠p:
		
			http://192.168.1.11:8080/
	
	Intall and configure WordPress:
	
		https://ubuntu.com/tutorials/install-and-configure-wordpress#7-configure-wordpress
		
			WordPress l√† g√¨?

			WordPress l√† m·ªôt CMS (Content Management System) ‚Äì h·ªá th·ªëng qu·∫£n tr·ªã n·ªôi dung.

			N√≥ gi√∫p b·∫°n t·∫°o website/blog m√† kh√¥ng c·∫ßn vi·∫øt code nhi·ªÅu.

			R·∫•t ph·ªï bi·∫øn ƒë·ªÉ l√†m: blog, website tin t·ª©c, website c√¥ng ty, th·∫≠m ch√≠ th∆∞∆°ng m·∫°i ƒëi·ªán
			t·ª≠ (v·ªõi plugin nh∆∞ WooCommerce).

		‚ÄúInstall and Configure WordPress‚Äù (c√†i ƒë·∫∑t v√† c·∫•u h√¨nh WordPress) ƒë·ªÉ l√†m g√¨?

			Install (c√†i ƒë·∫∑t):

				T·∫£i m√£ ngu·ªìn WordPress v·ªÅ m√°y ch·ªß (server Linux).

				ƒê·∫∑t n√≥ v√†o th∆∞ m·ª•c web server (Apache/nginx).

				K·∫øt n·ªëi v·ªõi c∆° s·ªü d·ªØ li·ªáu (MySQL/MariaDB).

				Sau b∆∞·ªõc n√†y, server c·ªßa b·∫°n c√≥ WordPress, nh∆∞ng n√≥ ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ ch·∫°y.

			Configure (c·∫•u h√¨nh):

				T·∫°o file wp-config.php ƒë·ªÉ WordPress bi·∫øt ph·∫£i k·∫øt n·ªëi t·ªõi database n√†o, username/password l√† g√¨.

				Thi·∫øt l·∫≠p c√°c th√¥ng s·ªë nh∆∞ ng√¥n ng·ªØ, timezone, theme, plugin.

				Truy c·∫≠p qua tr√¨nh duy·ªát ƒë·ªÉ ho√†n t·∫•t wizard (b·∫°n ƒë·∫∑t t√™n site, t√†i kho·∫£n admin, m·∫≠t kh·∫©u‚Ä¶).

				Sau b∆∞·ªõc n√†y, b·∫°n c√≥ m·ªôt website WordPress ch·∫°y ƒë∆∞·ª£c, s·∫µn s√†ng d√πng.

		T√≥m l·∫°i

			Install WordPress = ƒë∆∞a WordPress v√†o server.

			Configure WordPress = k·∫øt n·ªëi WordPress v·ªõi database + thi·∫øt l·∫≠p ƒë·ªÉ n√≥ ch·∫°y th√†nh m·ªôt website ho√†n ch·ªânh.
			
	Promt ƒë·ªÉ t·∫°o multivm:
	
		Multivm vagrantfile web01 ubuntu20, web02 with ubuntu20 & db01 with centos 7. Private IP for all the
		vms. Provisioning for db01. Set hostname also
		
		
		
		
		
	N·ªôi dung file Vagrantfile khi m·ªü m·ªôt s·ªë comment:
	
		Case 1 (ubuntu):

			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end
			
		Case 2 (ubuntu):
		
			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.synced_folder "E:\\devops_practice\\scripts\\shellscripts", "/opt/scripts"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end			

		Case 3 (centos):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.10"
			  config.vm.network "public_network"
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip git -y
				mkdir /opt/devopsdir
				free -m
				uptime
			  SHELL
			end
			
		Case 4 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.33.10"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end

		Case 5 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.56.14"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end
			
			Ch√∫ th·ªÉ truy c·∫≠p v√†o link 192.168.56.14
			
		Case 6 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.22"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			end
			
		Case 7 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.26"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			end
			
		Case 8 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.28"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip vim -y
				systemctl start httpd
				systemctl enabled httpd
				mkdir -p /tmp/finance
				cd /tmp/finance
				wget https://www.tooplate.com/zip-templates/2138_aqua_nova.zip
				unzip -o 2138_aqua_nova.zip
				cp -r 2138_aqua_nova/* /var/www/html/
				systemctl restart httpd
				cd /tmp/
				rm -rf /tmp/finance
			  SHELL
			end
			
		Case 9 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.30"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
			  sudo apt update -y
			  sudo apt install -y apache2 \
							  ghostscript \
							  libapache2-mod-php \
							  mysql-server \
							  php \
							  php-bcmath \
							  php-curl \
							  php-imagick \
							  php-intl \
							  php-json \
							  php-mbstring \
							  php-mysql \
							  php-xml \
							  php-zip

			  sudo mkdir -p /srv/www
			  sudo chown www-data: /srv/www
			  curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www

			  cat > /etc/apache2/sites-available/wordpress.conf <<EOF
			<VirtualHost *:80>
				DocumentRoot /srv/www/wordpress
				<Directory /srv/www/wordpress>
					Options FollowSymLinks
					AllowOverride Limit Options FileInfo
					DirectoryIndex index.php
					Require all granted
				</Directory>
				<Directory /srv/www/wordpress/wp-content>
					Options FollowSymLinks
					Require all granted
				</Directory>
			</VirtualHost>
			EOF

			  sudo a2ensite wordpress
			  sudo a2enmod rewrite
			  sudo a2dissite 000-default

			  mysql -u root -e CREATE DATABASE wordpress;
			  mysql -u root -e CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
			  mysql -u root -e GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
			  mysql -u root -e FLUSH PRIVILEGES;

			  sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php

			  systemctl restart mysql
			  systemctl restart apache2
			  SHELL
			end
			
			L√∫c ƒë·∫ßu l√† sudo apt update v√† sudo apt install apache2 nh∆∞ng sau ƒë√≥ th√™m -y ƒë·ªÉ n√≥ t·∫°o ƒë·ªìng √Ω, n·∫øu
			kh√¥ng c√≥ n√≥ s·∫Ω absort r·ªìi g√¢y ra l·ªói

		Case 10 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  # ---------- Web01 ----------
			  config.vm.define "web01" do |web01|
				web01.vm.box = "ubuntu/focal64"
				web01.vm.hostname = "web01"
				web01.vm.network "private_network", ip: "192.168.56.41"
			  end

			  # ---------- Web02 ----------
			  config.vm.define "web02" do |web02|
				web02.vm.box = "ubuntu/focal64"
				web02.vm.hostname = "web02"
				web02.vm.network "private_network", ip: "192.168.56.42"
			  end

			  # ---------- DB01 ----------
			  config.vm.define "db01" do |db01|
				db01.vm.box = "centos/7"
				db01.vm.hostname = "db01"
				db01.vm.network "private_network", ip: "192.168.56.43"
				# Provisioning script for DB01
				db01.vm.provision "shell", inline: <<-SHELL
				  yum install -y wget unzip mariadb-server -y
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL
			  end
			end

			C√°i n√†y s·∫Ω kh·ªüi t·∫°o nhi·ªÅu vm 1 l√∫c
			
			C√≥ th·ªÉ thao t√°c v·ªõi t·ª´ng vm theo c√°c l·ªánh sau:
			
				vagrant ssh web02
				
				vagrant destroy web01
				
				vagrant destroy -f web01
				
				vagrant up web03
				
				vagrant destroy --force
				
					X√≥a t·∫•t c·∫£ c√°c vm
				
			Code tr√™n centos 7 c≈© n√™n ƒë√£ b·ªã EOL n√™n s·∫Ω ƒë∆∞·ª£c th√™m v·ªõi d√≤ng code sau ƒë·ªÉ fix:

				db01.vm.provision "shell", inline: <<-SHELL
				  # --- Fix CentOS 7 EOL repo ---
				  sed -i 's|mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-Base.repo
				  sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-Base.repo
				  yum clean all
				  yum makecache

				  # --- Install MariaDB ---
				  yum install -y wget unzip mariadb-server
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL

	httpd:
	
		httpd trong Linux th∆∞·ªùng d√πng ƒë·ªÉ ch·ªâ Apache HTTP Server ‚Äì m·ªôt trong nh·ªØng web server ph·ªï
		bi·∫øn v√† l√¢u ƒë·ªùi nh·∫•t tr√™n th·∫ø gi·ªõi.
		
		Gi·∫£i th√≠ch

		httpd = HTTP Daemon (daemon nghƒ©a l√† d·ªãch v·ª• ch·∫°y ng·∫ßm tr√™n h·ªá th·ªëng).

		Khi c√†i Apache tr√™n Linux (v√≠ d·ª• CentOS, RHEL, Fedora), g√≥i d·ªãch v·ª• s·∫Ω ƒë∆∞·ª£c ƒë·∫∑t t√™n l√† httpd.

		Nhi·ªám v·ª• ch√≠nh:

			Nh·∫≠n request HTTP/HTTPS t·ª´ client (tr√¨nh duy·ªát, curl, API‚Ä¶).

			X·ª≠ l√Ω v√† tr·∫£ v·ªÅ response (HTML, CSS, JS, JSON, file tƒ©nh, d·ªØ li·ªáu ƒë·ªông qua PHP/Python/Java‚Ä¶).

		httpd c√≥ th·ªÉ m·ªü r·ªông b·∫±ng module (v√≠ d·ª•: mod_ssl cho HTTPS, mod_rewrite cho rewrite URL, mod_php ƒë·ªÉ ch·∫°y PHP).
		
	L·ªánh ƒë·ªÉ in n·ªôi dung c·ªßa file html ra m√†n h√¨nh:

		[root@Finance ~]# cd /var/www/html/
		[root@Finance html]# ls
		[root@Finance html]# vim index.html
		index.html
		[root@Finance html]# systemctl restart httpd

	Ki·ªÉm tra xem c√≥ t·∫•t c·∫£ c√°i n√†o ƒëang ch·∫°y:
	
		vagrant global-status
		
		
--- Variables, JSON and YAML:

	- Commands:
	
		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ skill="DevOps"

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo $skill
		DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo skill
		skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo "I am learning $skill"
		I am learning DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo 'I am learning $skill'
		I am learning $skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ Num=123
		
	Python edittors online
	
	Json editor
	
	Yaml editor
	
--- Networking:

	- Commands:
	
		- cmd: sudo apt update
		- cmd: sudo apt install -y net-tools
		- cmd: ifconfig
		- cmd:
		
				vagrant@web01:~$ ping 192.168.56.41
				PING 192.168.56.41 (192.168.56.41) 56(84) bytes of data.
				64 bytes from 192.168.56.41: icmp_seq=1 ttl=64 time=0.021 ms
				64 bytes from 192.168.56.41: icmp_seq=2 ttl=64 time=0.037 ms
				64 bytes from 192.168.56.41: icmp_seq=3 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=4 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=5 ttl=64 time=0.042 ms

		- cmd: root@web01:~# vi /etc/hosts
		
				127.0.0.1       localhost

				# The following lines are desirable for IPv6 capable hosts
				::1     ip6-localhost   ip6-loopback
				fe00::0 ip6-localnet
				ff00::0 ip6-mcastprefix
				ff02::1 ip6-allnodes
				ff02::2 ip6-allrouters
				ff02::3 ip6-allhosts
				127.0.1.1       ubuntu-focal    ubuntu-focal

				127.0.2.1 web01 web01

				192.168.56.41 web01
				
		- cmd: ping web01
		- cmd: logout
		- cmd: $ tracert www.google.in

			   Tracing route to www.google.in [2404:6800:4003:c11::5e]
			   over a maximum of 30 hops:
			   
			     1     1 ms     9 ms     1 ms  2405:4803:d75f:ab0:5e3a:45ff:fe2e:8f78
			     2     *        *        *     Request timed out.
			     3     *        *        *     Request timed out.
			     4    73 ms     3 ms     3 ms  2405:4802:f500::37d
			     5    71 ms     *        *     2405:4800::1117:5118:a
			     6     *        *        *     Request timed out.
			     7    49 ms    48 ms    47 ms  2405:4800::101:1112:1
			     8    48 ms    45 ms    46 ms  2001:4860:0:1::5bc2
			     9    49 ms    51 ms    51 ms  2001:4860::c:4000:db82
			    10   157 ms    82 ms    82 ms  2001:4860::c:4003:1c94
			    11     *        *        *     Request timed out.
			    12     *        *        *     Request timed out.
			    13     *        *        *     Request timed out.
			    14     *        *        *     Request timed out.
			    15     *        *        *     Request timed out.
			    16     *        *        *     Request timed out.
			    17     *        *        *     Request timed out.
			    18     *        *        *     Request timed out.
			    19     *        *        *     Request timed out.
			    20    82 ms   166 ms    82 ms  se-in-f94.1e100.net [2404:6800:4003:c11::5e]
			   
			   Trace complete.
		
		- cmd: vagrant@web01:~$ netstat -antp
		
			   (Not all processes could be identified, non-owned process info
			    will not be shown, you would have to be root to see it all.)
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED -
			   tcp6       0      0 :::22                   :::*                    LISTEN      -
			   
		- cmd: root@web01:~# netstat -antp
		
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      598/systemd-resolve
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      856/sshd: /usr/sbin
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED 3245/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED 3358/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED 2250/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED 3146/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED 3492/sshd: vagrant
			   tcp6       0      0 :::22                   :::*                    LISTEN      856/sshd: /usr/sbin


		- cmd:  root@web01:~# ps -ef | grep apache2
				
				root        3602    3589  0 05:53 pts/4    00:00:00 grep --color=auto apache2
				
		- cmd:  root@web01:~# ss -tunlp
		
				Netid          State           Recv-Q          Send-Q                      Local Address:Port                   Peer Address:Port          Process
				udp            UNCONN          0               0                           127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=12))
				udp            UNCONN          0               0                        10.0.2.15%enp0s3:68                          0.0.0.0:*              users:(("systemd-network",pid=1932,fd=20))
				tcp            LISTEN          0               4096                        127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=13))
				tcp            LISTEN          0               128                               0.0.0.0:22                          0.0.0.0:*              users:(("sshd",pid=856,fd=3))
				tcp            LISTEN          0               128                                  [::]:22                             [::]:*              users:(("sshd",pid=856,fd=4))
				
		- cmd: apt install nmap -y
		- cmd: nmap
		- cmd:  root@web01:~# nmap localhost
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 05:56 UTC
				Nmap scan report for localhost (127.0.0.1)
				Host is up (0.0000030s latency).
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.44 seconds
				
		- cmd:  root@web01:~# nmap web01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 06:47 UTC
				Nmap scan report for web01 (127.0.2.1)
				Host is up (0.0000040s latency).
				Other addresses for web01 (not scanned): 192.168.56.41
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.12 seconds

		- cmd:  root@web01:~# dig www.google.com

				; <<>> DiG 9.18.30-0ubuntu0.20.04.2-Ubuntu <<>> www.google.com
				;; global options: +cmd
				;; Got answer:
				;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54039
				;; flags: qr rd ra; QUERY: 1, ANSWER: 6, AUTHORITY: 0, ADDITIONAL: 1

				;; OPT PSEUDOSECTION:
				; EDNS: version: 0, flags:; udp: 65494
				;; QUESTION SECTION:
				;www.google.com.                        IN      A

				;; ANSWER SECTION:
				www.google.com.         285     IN      A       142.251.12.99
				www.google.com.         285     IN      A       142.251.12.147
				www.google.com.         285     IN      A       142.251.12.104
				www.google.com.         285     IN      A       142.251.12.106
				www.google.com.         285     IN      A       142.251.12.105
				www.google.com.         285     IN      A       142.251.12.103

				;; Query time: 36 msec
				;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)
				;; WHEN: Sun Sep 21 06:49:44 UTC 2025
				;; MSG SIZE  rcvd: 139
				
		- cmd:  root@web01:~# nslookup www.google.com
		
				Server:         127.0.0.53
				Address:        127.0.0.53#53

				Non-authoritative answer:
				Name:   www.google.com
				Address: 142.251.12.103
				Name:   www.google.com
				Address: 142.251.12.105
				Name:   www.google.com
				Address: 142.251.12.106
				Name:   www.google.com
				Address: 142.251.12.104
				Name:   www.google.com
				Address: 142.251.12.147
				Name:   www.google.com
				Address: 142.251.12.99
				Name:   www.google.com
				Address: 2404:6800:4003:c11::63
				Name:   www.google.com
				Address: 2404:6800:4003:c11::67
				Name:   www.google.com
				Address: 2404:6800:4003:c11::69
				Name:   www.google.com
				Address: 2404:6800:4003:c11::68
				
		- cmd:  root@web01:~# route -n
		
				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				10.0.2.2        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# route

				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				default         _gateway        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				_gateway        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# arp
		
				Address                  HWtype  HWaddress           Flags Mask            Iface
				_gateway                 ether   52:54:00:12:35:02   C                     enp0s3
				10.0.2.3                 ether   52:54:00:12:35:03   C                     enp0s3
				192.168.56.42                    (incomplete)                              enp0s8
				
		- cmd: mtr
		- cmd: mtr www.google.com
		
		- cmd:  root@web01:~# nmap db01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 07:06 UTC
				Failed to resolve "db01".
				WARNING: No targets were specified, so 0 hosts scanned.
				Nmap done: 0 IP addresses (0 hosts up) scanned in 0.06 seconds
				
		- cmd:  root@web01:~# telnet 192.168.56.43 3306
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				HHost '192.168.56.41' is not allowed to connect to this MariaDB serverConnection closed by foreign host.
				
		- cmd: sudo yum install -y net-tools
		
		- cmd:  root@web01:~# telnet 192.168.56.43 22
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				SSH-2.0-OpenSSH_7.4



	OSI model
	
	Switch
	
	Router
	
	Firewall
	
	Gateway IP
	
	Wireless Access Point
	
	Port
	
	TCP, UDP
	
--- Introducing Containers:

	- Commands:
	
		- cmd: sudo install -m 0755 -d /etc/apt/keyrings
		- cmd: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
		- cmd: echo \
			  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
		- cmd: sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
		
		- cmd:  root@ubuntu-focal:~# systemctl status docker
		
				‚óè docker.service - Docker Application Container Engine
					 Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
					 Active: active (running) since Sun 2025-09-21 07:47:33 UTC; 1min 12s ago
				TriggeredBy: ‚óè docker.socket
					   Docs: https://docs.docker.com
				   Main PID: 3868 (dockerd)
					  Tasks: 9
					 Memory: 21.3M
					 CGroup: /system.slice/docker.service
							 ‚îî‚îÄ3868 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

				Sep 21 07:47:33 ubuntu-focal systemd[1]: Started Docker Application Container Engine.
				
		- cmd: docker run hello-world
		- cmd:  root@ubuntu-focal:~# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
				hello-world   latest    1b44b5a3e06a   6 weeks ago   10.1kB
				
		- cmd:  root@ubuntu-focal:~# docker ps
		
				CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
				
		- cmd:  root@ubuntu-focal:~# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND    CREATED              STATUS                          PORTS     NAMES
				00001e808420   hello-world   "/hello"   About a minute ago   Exited (0) About a minute ago             cool_agnesi
				
		- cmd: docker run --name web01 -d -p 9080:80 nginx
		- cmd: docker inspect web01
		- cmd:  root@ubuntu-focal:~# curl http://172.17.0.2:80
		
				<!DOCTYPE html>
				<html>
				<head>
				<title>Welcome to nginx!</title>
				<style>
				html { color-scheme: light dark; }
				body { width: 35em; margin: 0 auto;
				font-family: Tahoma, Verdana, Arial, sans-serif; }
				</style>
				</head>
				<body>
				<h1>Welcome to nginx!</h1>
				<p>If you see this page, the nginx web server is successfully installed and
				working. Further configuration is required.</p>

				<p>For online documentation and support please refer to
				<a href="http://nginx.org/">nginx.org</a>.<br/>
				Commercial support is available at
				<a href="http://nginx.com/">nginx.com</a>.</p>

				<p><em>Thank you for using nginx.</em></p>
				</body>
				</html>
				
		- cmd: docker build -t tesimg .
		- cmd: docker run -d -P tesimg
		- cmd:  root@ubuntu-focal:~/images# docker ps
		
				CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                       NAMES
				f41b4289fc35   tesimg    "/usr/sbin/apache2ct‚Ä¶"   2 minutes ago    Up 2 minutes    0.0.0.0:32768->80/tcp, [::]:32768->80/tcp   elegant_moore
				44fde054c460   nginx     "/docker-entrypoint.‚Ä¶"   25 minutes ago   Up 25 minutes   0.0.0.0:9080->80/tcp, [::]:9080->80/tcp     web01
				
				Con s·ªë 32768 trong output docker ps n√†y ch√≠nh l√† c·ªïng ng·∫´u nhi√™n tr√™n m√°y host m√† Docker √°nh
				x·∫° ƒë·∫øn c·ªïng 80 trong container.

		- cmd: docker stop web01 elegant_moore
		
		- cmd:  root@ubuntu-focal:~/images# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS                        PORTS     NAMES
				f41b4289fc35   tesimg        "/usr/sbin/apache2ct‚Ä¶"   6 minutes ago    Exited (137) 28 seconds ago             elegant_moore
				44fde054c460   nginx         "/docker-entrypoint.‚Ä¶"   29 minutes ago   Exited (0) 38 seconds ago               web01
				00001e808420   hello-world   "/hello"                 32 minutes ago   Exited (0) 32 minutes ago               cool_agnesi
				
		- cmd: docker rm elegant_moore web01 cool_agnesi
		
		- cmd:  root@ubuntu-focal:~/images# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED          SIZE
				tesimg        latest    c197410a4d04   13 minutes ago   271MB
				nginx         latest    41f689c20910   5 weeks ago      192MB
				hello-world   latest    1b44b5a3e06a   6 weeks ago      10.1kB
				
		- cmd: docker rmi c197410a4d04 41f689c20910 1b44b5a3e06a
		- cmd: wget https://raw.githubusercontent.com/devopshydclub/vprofile-project/refs/heads/docker/compose/docker-compose.yml
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose down
		- cmd: docker system prune -a
		- cmd: git clone https://github.com/devopshydclub/emartapp.git
		
		
		
		
		
	Khi b·∫°n copy‚Äìpaste v√†o vim th√¨ l·ªói format YAML th∆∞·ªùng x·∫£y ra do:

		D√≠nh tab thay v√¨ space.

		Paste v√†o vim ·ªü ch·∫ø ƒë·ªô autoindent g√¢y l·ªách d√≤ng.

		Ho·∫∑c paste t·ª´ Windows ‚Üí Linux th√¨ d√≠nh k√Ω t·ª± ·∫©n (CRLF).

		Kh·∫Øc ph·ª•c:
		
			Tr∆∞·ªõc khi paste, g√µ: :set paste
			
	Docker compose l√† ƒë·ªÉ run c√°c image ch·ª© kh√¥ng ph·∫£i ƒë·ªÉ build c√°c image
				

		



		

	N·ªôi dung file Vagrantfile khi m·ªü m·ªôt s·ªë comment:
	
		Case 1:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
			   sudo apt-get update
			   sudo apt-get install \
				ca-certificates \
				curl \
				gnupg -y

			   sudo install -m 0755 -d /etc/apt/keyrings
			   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
			   sudo chmod a+r /etc/apt/keyrings/docker.gpg
			   echo \
			     "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			     "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			     sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
			   sudo apt-get update
			   sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
			  SHELL
			end
			
		Case 2:
		
			FROM ubuntu:latest AS BUILD_IMAGE
			RUN apt update && apt install wget unzip -y
			RUN wget https://www.tooplate.com/zip-templates/2128_tween_agency.zip
			RUN unzip 2128_tween_agency.zip && cd 2128_tween_agency && tar -czf tween.tgz * && mv tween.tgz /root/tween.tgz

			FROM ubuntu:latest
			LABEL "project"="Marketing"
			ENV DEBIAN_FRONTEND=noninteractive

			RUN apt update && apt install apache2 git wget -y
			COPY --from=BUILD_IMAGE /root/tween.tgz /var/www/html/
			RUN cd /var/www/html/ && tar xzf tween.tgz

			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			VOLUME /var/log/apache2
			WORKDIR /var/www/html
			EXPOSE 80

		Case 3:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
				sudo apt-get update
				sudo apt-get install \
					ca-certificates \
					curl \
					gnupg -y

				sudo install -m 0755 -d /etc/apt/keyrings
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
				sudo chmod a+r /etc/apt/keyrings/docker.gpg
				echo \
				  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
				  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
				sudo curl -L "https://github.com/docker/compose/releases/download/v2.1.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

				chmod +x /usr/local/bin/docker-compose
			   SHELL
			end
			
	N·ªôi dung file docker-compose.yml:
	
		Case 1:
		
			version: '3.8'
			services:
			  vprodb:
				image: vprocontainers/vprofiledb
				ports:
				  - "3306:3306"
				volumes:
				  - vprodbdata:/var/lib/mysql
				environment:
				  - MYSQL_ROOT_PASSWORD=vprodbpass

			  vprocache01:
				image: memcached
				ports:
				  - "11211:11211"

			  vpromq01:
				image: rabbitmq
				ports:
				  - "15672:15672"
				environment:
				  - RABBITMQ_DEFAULT_USER=guest
				  - RABBITMQ_DEFAULT_PASS=guest

			  vproapp:
				image: vprocontainers/vprofileapp
				ports:
				  - "8080:8080"
				volumes: 
				  - vproappdata:/usr/local/tomcat/webapps

			  vproweb:
				image: vprocontainers/vprofileweb
				ports:
				  - "80:80"
			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books
		  
--- Bash Scripting:

	- Commands:
		
		- cmd: vim /etc/hostname
		
			File n√†y ƒë·ªÉ chuy·ªÉn ƒë·ªïi t√™n hostname
			
		- cmd: hostname scriptbox
		- cmd: hostname
		- cmd: yum install vim -y
		- cmd: chmod +x firstscript.sh
		- cmd: chmod +x dismantle.sh
		- cmd: w
		- cmd: who
		- cmd: free -m | grep Mem
		- cmd: free -m | grep Mem | awk '{print $4}'
		- cmd: source .bashrc
		- cmd: ip addr show | grep -v LOOPBACK | grep -ic mtu
		- cmd: cat /var/run/httpd/httpd.pid
		- cmd: ssh-copy-id vagrant@192.168.10.13
		- cmd: ssh vagrant@web01
		- cmd: passwd devops
		- cmd: vim /etc/ssh/sshd_config
		- cmd: ssh devops@web01 uptime
		- cmd: ssh-keygen
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i .ssh/id_rsa devops@web01 uptime
		- cmd: cat .ssh/id_rsa
		- cmd: cat .ssh/id_rsa.pub
		- cmd: for host in `cat remhosts`; do echo $host;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host hostname;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host uptime;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host sudo yum install git -y;done
		- cmd: echo "testfile" >testfile.txt
		- cmd: scp testfile.txt devops@web01:/tmp/
		- cmd: scp testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/home/devops/
		
		
		
		
	Varantfile:

		Vagrant.configure("2") do |config|

		  config.vm.define "scriptbox" do |scriptbox|
			scriptbox.vm.box = "eurolinux-vagrant/centos-stream-9"
				scriptbox.vm.network "private_network", ip: "192.168.10.12"
				scriptbox.vm.provider "virtualbox" do |vb|
			 vb.memory = "1024"
		   end
		  end

		  config.vm.define "web01" do |web01|
			web01.vm.box = "eurolinux-vagrant/centos-stream-9"
				web01.vm.network "private_network", ip: "192.168.10.13"
		  end

		  config.vm.define "web02" do |web02|
			web02.vm.box = "eurolinux-vagrant/centos-stream-9"
				web02.vm.network "private_network", ip: "192.168.10.14"
		  end

		   config.vm.define "web03" do |web03|
			web03.vm.box = "ubuntu/bionic64"
				web03.vm.network "private_network", ip: "192.168.10.15"
		  end
		end
	
		
		
		
	N·ªôi dung file script:
	
		Case 1:
		
			#!/bin/bash

			echo "Welcome to bash script."
			echo

			echo "The uptime of the system is: "
			uptime

			echo "Memory Utilization"
			free -m

			echo "Disk Utilization"
			df -h
			
		Case 2:
		
			#!/bin/bash

			### This script prints system info ###

			echo "Welcome to bash script."
			echo

			#checking system uptime
			echo "#################################"
			echo "The uptime of the system is: "
			uptime

			# Memory Utilization
			echo "#################################"
			echo "Memory Utilization"
			free -m

			# Disk Utilization
			echo "#################################"
			echo "Disk Utilization"
			df -h
			
		Case 3:
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip
			unzip 2098_health.zip
			sudo cp -r 2098_health.zip/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo
			
		Case 4:
		
			C√°i n√†y c√≥ th·ªÉ ch·∫°y l√™n trang web
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip > /dev/null
			unzip 2098_health.zip > /dev/null
			sudo cp -r 2098_health/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo

			sudo systemctl status httpd
			ls /var/www/html/
			
		Case 5:
		
			[root@scriptbox ~]# SKILL="DevOps"
			[root@scriptbox ~]# echo $SKILL
			DevOps
			[root@scriptbox ~]# echo SKILL
			SKILL
			[root@scriptbox ~]# PACKAGE="httpd wget unzip"
			[root@scriptbox ~]# yum install $PACKAGE -y
			Last metadata expiration check: 0:12:59 ago on Sun 21 Sep 2025 11:19:55 AM UTC.
			Package httpd-2.4.62-7.el9.x86_64 is already installed.
			Package wget-1.21.1-8.el9.x86_64 is already installed.
			Package unzip-6.0-59.el9.x86_64 is already installed.
			Dependencies resolved.
			Nothing to do.
			Complete!
			
		Case 6:
		
			[root@scriptbox scripts]# ls
			firstscript.sh  websetup.sh
			[root@scriptbox scripts]# mv firstscript.sh 1_firstscript.sh
			[root@scriptbox scripts]# mv websetup.sh 2_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh
			[root@scriptbox scripts]# cp 2_websetup.sh 3_vars_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh
		
		Case 7:
		
			#!/bin/bash
			ART_NAME="2098_health"
			SVC="httpd"
			URL="https://www.tooplate.com/zip-templates/2098_health.zip"
			ART_NAME="2098_health"
			TEMPDIR="/tmp/webfiles"

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install $PACKAGE -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start $SVC
			sudo systemctl enable $SVC
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p $TEMPDIR
			cd $TEMPDIR
			echo

			wget $URL > /dev/null
			unzip $ART_NAME.zip > /dev/null
			sudo cp -r $ART_NAME/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart $SVC
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf $TEMPDIR
			echo

			sudo systemctl status $SVC
			ls /var/www/html/
			
		Case 8:
		
			#!/bin/bash
			sudo systemctl stop httpd
			sudo rm -rf /var/www/html/*
			sudo yum remove httpd wget unzip -y
			
		Case 9:
		
			#!/bin/bash

			echo "Value of 0 is "
			echo $0

			echo "Value of 1"
			echo $1

			echo "Value of 2"
			echo $2

			echo "Value of 3"
			echo $3
			
				[root@scriptbox scripts]# ./4_args.sh Linux AWS Ansible Jenkins
				Value of 0 is
				./4_args.sh
				Value of 1
				Linux
				Value of 2
				AWS
				Value of 3
				Ansible
				
		Case 10:
		
			[root@scriptbox scripts]# echo $USER
			root
			[root@scriptbox scripts]# echo $HOSTNAME
			scriptbox
			[root@scriptbox scripts]# echo $RANDOM
			11889
			[root@scriptbox scripts]# SKILL="DevOps"
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# SKILL='DevOps'
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# echo "Ihave got $SKILL skill."
			Ihave got DevOps skill.
			[root@scriptbox scripts]# echo 'Ihave got $SKILL skill.'
			Ihave got $SKILL skill.
			[root@scriptbox scripts]# echo "Ihave got \$SKILL skill."
			Ihave got $SKILL skill.

			[root@scriptbox scripts]# uptime
			 13:25:12 up  3:03,  1 user,  load average: 0.02, 0.04, 0.03
			[root@scriptbox scripts]# UP="uptime"
			[root@scriptbox scripts]# echo $UP
			uptime
			[root@scriptbox scripts]# UP=`uptime`
			[root@scriptbox scripts]# echo $UP
			13:25:42 up 3:03, 1 user, load average: 0.01, 0.03, 0.03
			
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-21 10:44 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         330         155           3         414         436
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         330         155           3         414         436
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			155
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 155 mb.
			
		Case 11:
		
			#!/bin/bash

			echo "Welcome $USER on $HOSTNAME."
			echo "##############################################"

			FREERAM=$(free -m | grep Mem | awk '{print $4}')
			LOAD=`uptime | awk '{print $9}'`
			ROOTFREE=$(df -h | grep '/dev/sdal' | awk '{print $4}')

			echo "##############################################"
			echo "Available free RAM is $FREERAM MB"
			echo "##############################################"
			echo "Current Load Average $LOAD"
			echo "##############################################"
			echo "Free ROOT partition size is $ROOTFREE"
			
		Case 12:
		
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo CURRENT_USER
			CURRENT_USER
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-22 13:05 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         282         383           3         229         485
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         282         383           3         229         485
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			382
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 382 mb.
			[root@scriptbox scripts]# ./6_command_subs.sh
			Welcome root on scriptbox.
			##############################################
			##############################################
			Available free RAM is 382 MB
			##############################################
			Current Load Average 0.04,
			##############################################
			Free ROOT partition size is
			
		Case 13:
		
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo SEASON
			SEASON
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON

			[root@scriptbox ~]# cd /opt/scripts/

		Case 14:
		
			#!/bin/bash
			echo "The $SEASON season is more than expected, this time."
			
			[root@scriptbox scripts]# chmod +x testvars.sh
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# ./testvars.sh
			The  season is more than expected, this time.
			[root@scriptbox scripts]# export SEASON
			[root@scriptbox scripts]# ./testvars.sh
			The Monsoon season is more than expected, this time.
			
		Case 15:
		
			# .bashrc
			# User specific aliases and functions
			alias rm='rm -i'
			alias cp='cp -i'
			alias mv='mv -i'
			# Source global definitions
			if [ -f /etc/bashrc ]; then
					. /etc/bashrc
			fi
			export SEASON="Monsoon"


			[root@scriptbox ~]# echo $SEASON
			[root@scriptbox ~]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# vi .bashrc
			[root@scriptbox ~]# sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
		
		Case 16:
			
			#!/bin/bash
			export SEASON='Winter'
			
			[vagrant@scriptbox ~]$ echo $SEASON
			Winter
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
			
		Case 17:
		
			#!/bin/bash
			echo "Enter youe skills:"
			read SKILL
			echo "Your $SKILL skill is in high Demand in the IT Industry."
			read -p 'Username: ' USR
			read -sp 'Password: ' pass
			echo
			echo "Login Successfull: Welcome USER $USR,"

			[root@scriptbox scripts]# chmod +x 7_userInput.sh
			[root@scriptbox scripts]# ./7_userInput.sh
			Enter youe skills:
			CloudComputing
			Your CloudComputing skill is in high Demand in the IT Industry.
			Username: ngoctuanqng1
			Password:
			Login Successfull: Welcome USER ngoctuanqng1,

		Case 18:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# chmod +x 8if1.sh
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 120
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:40:30 PM UTC 2025
			Script execution completed successfully.
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 50
			Script execution completed successfully.
		
		Case 19:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			else
					echo "You have entered number less than 100."
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 60
			You have entered number less than 100.
			Script execution completed successfully.
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 110
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:43:41 PM UTC 2025
			Script execution completed successfully.
			
		Case 20:
		
			#!/bin/bash
			value=$(ip addr show | grep -v LOOPBACK | grep -ic mtu)
			if [ $value -eq 1 ]
			then
					echo "1 Active Network Interface found."
			elif [ $value -gt 1 ]
			then
					echo "Found Multiple active Interface."
			else
					echo "No Active interface found."
			fi
			
			[root@scriptbox ~]# ./9_ifelif.sh
			Found Multiple active Interface.
			[root@scriptbox ~]# ls
			9_ifelif.sh  anaconda-ks.cfg  original-ks.cfg
			[root@scriptbox ~]# ls /opt/scripts/
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  dismantle.sh  testvars.sh
			[root@scriptbox ~]# mv ./9_ifelif.sh /opt/scripts/
			[root@scriptbox ~]# cd /opt/scripts/
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  9_ifelif.sh  dismantle.sh  testvars.sh
			
		Case 21:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null
			if [ $? -eq 0 ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
			[root@scriptbox scripts]# ./11_monit.sh
			#######################################################
			Mon Sep 22 04:12:59 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 22:
		
			#!/bin/bash
			# mm hh dom MM DOW COMMAND
			# mm hh dom MM DOW COMMAND
			# 30 20 * * 1-5 COMMAND
			* * * * * /opt/scripts/11_monit.sh &>> /var/log/monit_httpd.log

			[root@scriptbox scripts]# crontab -e
			no crontab for root - using an empty one
			crontab: installing new crontab
			[root@scriptbox scripts]# systemctl stop httpd
			[root@scriptbox scripts]# ls /var/log/monit_httpd.log
			/var/log/monit_httpd.log
			[root@scriptbox scripts]# cat /var/log/monit_httpd.log
			#######################################################
			Mon Sep 22 04:20:08 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################

			#######################################################
			Mon Sep 22 04:21:03 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 23:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null

			if [ -f /var/run/httpd/httpd.pid ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
		Case 24:
		
			#!/bin/bash
			for VAR1 in java .net python ruby php
			do
					echo "Looping...."
					sleep 1
					echo "#####################################"
					echo "Value of VAR1 is $VAR1."
					echo "#####################################"
					date
			done
			
			[root@scriptbox scripts]# chmod +x 13_for.sh
			[root@scriptbox scripts]# ./13_for.sh
			Looping....
			#####################################
			Value of VAR1 is java.
			#####################################
			Mon Sep 22 04:28:16 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is .net.
			#####################################
			Mon Sep 22 04:28:18 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is python.
			#####################################
			Mon Sep 22 04:28:19 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is ruby.
			#####################################
			Mon Sep 22 04:28:20 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is php.
			#####################################
			Mon Sep 22 04:28:21 PM UTC 2025
			
		Case 25:
		
			#!/bin/bash
			MYUSERS="alpha beta gamma"
			for usr in $MYUSERS
			do
					echo "Adding user $usr."
					useradd $usr
					id $usr
					echo "##############################"
			done
			
			[root@scriptbox scripts]# chmod +x 14_for.sh
			[root@scriptbox scripts]# ./14_for.sh
			Adding user alpha.
			uid=1001(alpha) gid=1001(alpha) groups=1001(alpha)
			##############################
			Adding user beta.
			uid=1002(beta) gid=1002(beta) groups=1002(beta)
			##############################
			Adding user gamma.
			uid=1003(gamma) gid=1003(gamma) groups=1003(gamma)
			##############################
	
		Case 26:
		
			#!/bin/bash
			counter=0
			while [ $counter -lt 5 ]
			do
					echo "Looping...."
					echo "Value of counter is $counter."
					counter=$(( $counter + 1))
					sleep 1
			done
			echo "Out of the loop"
			
			[root@scriptbox ~]# chmod +x 15_while.sh
			[root@scriptbox ~]# ./15_while.sh
			Looping....
			Value of counter is 0.
			Looping....
			Value of counter is 1.
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 3.
			Looping....
			Value of counter is 4.
			Out of the loop
			
		Case 27:
		
            #!/bin/bash
            counter=2
            while true
            do
                echo "Looping...."
                echo "Value of counter is $counter."
                counter=$(( $counter * 2))
                sleep 1
            done
            echo "Out of the loop"
			
			[root@scriptbox ~]# ./16_while.sh
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 4.
			Looping....
			Value of counter is 8.
			Looping....
			Value of counter is 16.
			Looping....
			Value of counter is 32.
			Looping....
			Value of counter is 64.
			Looping....
			Value of counter is 128.
			...
			
		Case 28:
		
			127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
			::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
			192.168.10.13 web01
			192.168.10.14 web02
			192.168.10.15 web03
			
			[root@localhost ~]# ping web01
			PING web01 (192.168.10.13) 56(84) bytes of data.
			64 bytes from web01 (192.168.10.13): icmp_seq=1 ttl=64 time=3.83 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=2 ttl=64 time=2.57 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=3 ttl=64 time=1.44 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=4 ttl=64 time=3.46 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=5 ttl=64 time=2.18 ms

		Case 29:
		
			web01: visudo
		
				## Allow root to run any commands anywhere
				root    ALL=(ALL)       ALL
				devops ALL=(ALL)        NOPASSWD: ALL
				
			web03: /etc/ssh/sshd_config

				PasswordAuthentication yes
				ChallengeResponseAuthentication no
			
		Case 30:
		
			multios_websetup.sh:
		
				#!/bin/bash

				# Variable Declaration
				#PACKAGE="httpd wget unzip"
				#SVC="httpd"
				URL='https://www.tooplate.com/zip-templates/2098_health.zip'
				ART_NAME='2098_health'
				TEMPDIR="/tmp/webfiles"

				yum --help &> /dev/null

				if [ $? -eq 0 ]
				then
				   # Set Variables for CentOS
				   PACKAGE="httpd wget unzip"
				   SVC="httpd"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo yum install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/

				else
					# Set Variables for Ubuntu
				   PACKAGE="apache2 wget unzip"
				   SVC="apache2"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo apt update
				   sudo apt install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/
				fi 

		Case 31:
		
			#!/bin/bash
			USR='devops'
			for host in `cat remhosts`
			do
					echo
					echo "#######################################"
					echo "Connecting to $host"
					echo "Pushing Script to $host"
					scp multios_websetup.sh $USR@$host:/tmp/
					echo "Executing Script on $host"
					ssh $USR@$host sudo /tmp/multios_websetup.sh
					ssh $USR@$host sudo rm -rf /tmp/multios_websetup.sh
					echo "#######################################"
					echo
			done

--- AWS Part-1:

	Setting EC2
	
	Khi t·∫°o inbound rules trong security group ch√∫ √Ω ƒë·∫øn port, ph·∫£i ƒë√∫ng port m·ªõi ch·∫°y ra ƒë∆∞·ª£c trang web
	
	Ch√∫ √Ω khi truy c·∫≠p ec2 v·ªõi ssh key pair th√¨ c·∫ßn import ƒë√∫ng file key pair, n·∫øu kh√¥ng s·∫Ω b·ªã denied
	
	My IP m√† b·∫°n th∆∞·ªùng th·∫•y khi c·∫•u h√¨nh Inbound rule trong Security Group c·ªßa AWS nghƒ©a l√†:

		AWS t·ª± ƒë·ªông l·∫•y ƒë·ªãa ch·ªâ IP public hi·ªán t·∫°i c·ªßa m√°y t√≠nh b·∫°n ƒëang d√πng
		(m√°y laptop/PC/ƒëi·ªán tho·∫°i ƒëang truy c·∫≠p v√†o AWS Console).

		Khi b·∫°n ch·ªçn My IP, rule s·∫Ω ch·ªâ cho ph√©p ƒë·ªãa ch·ªâ IP ƒë√≥ ƒë∆∞·ª£c truy c·∫≠p v√†o EC2 qua c·ªïng
		b·∫°n ch·ªçn (v√≠ d·ª• SSH port 22, ho·∫∑c HTTP port 80).

	Ch∆∞a ch·∫°y ƒë∆∞·ª£c mkfs

		Tuy nhi√™n l·∫°i ch·∫°y ƒë∆∞·ª£c l·ªánh mkfs.t4 /dev/nvme1n1p1
	
	
	
	
	
	
	
	
	

	AWS Part-1 - Command:
	
		- cmd: ssh -i "web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ssh -i "E:/devops learning download/AWS_Part-1/web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ss -tunlp | grep 80
		- cmd: curl http://localhost
		- cmd: apt update && apt install apache2 -y
		- cmd: apt install unzip -y
		- cmd: systemctl restart apache2
		- cmd: ps -ef | grep apache2
		- cmd: ss -tunlp | grep 2668
		- cmd: choco install awscli -y
		- cmd: aws --version
		- cmd: aws configure
		- cmd: ls ~/.aws/
		- cmd: cat ~/.aws/config
		- cmd: cat ~/.aws/credentials
		- cmd: aws sts get-caller-identity
		- cmd: aws ec2 describe-instances

		- cmd: sudo amazon-linux-extras install epel -y
		- cmd: sudo yum install stress -y
		- cmd: nohup stress -c 4 -t 300 &
		- cmd: top
		
			L·ªánh top trong Linux l√† m·ªôt c√¥ng c·ª• theo d√µi ti·∫øn tr√¨nh (process monitoring) ch·∫°y trong real-time.		
		
			C√¥ng d·ª•ng ch√≠nh

				Hi·ªÉn th·ªã t√¨nh tr·∫°ng h·ªá th·ªëng (CPU, RAM, uptime, load average).

				Li·ªát k√™ danh s√°ch ti·∫øn tr√¨nh (process) ƒëang ch·∫°y, s·∫Øp x·∫øp theo m·ª©c ƒë·ªô ti√™u th·ª• t√†i nguy√™n.

				Gi√∫p b·∫°n nhanh ch√≥ng bi·∫øt ti·∫øn tr√¨nh n√†o ƒëang chi·∫øm CPU ho·∫∑c RAM nhi·ªÅu nh·∫•t.

			M·ªôt s·ªë ph√≠m t·∫Øt h·ªØu √≠ch trong top

				P ‚Üí s·∫Øp x·∫øp theo CPU usage.

				M ‚Üí s·∫Øp x·∫øp theo RAM usage.

				k ‚Üí kill process (nh·∫≠p PID).

				q ‚Üí tho√°t kh·ªèi top.
				
			top = Task Manager trong Linux, gi√∫p b·∫°n gi√°m s√°t CPU, RAM, v√† ti·∫øn tr√¨nh theo th·ªùi gian th·ª±c.
		
		- cmd: fdisk -l	
		
			L·ªánh fdisk -l trong Linux ƒë∆∞·ª£c d√πng ƒë·ªÉ li·ªát k√™ (list) t·∫•t c·∫£ c√°c ph√¢n v√πng (partitions)
			v√† ·ªï ƒëƒ©a hi·ªán c√≥ tr√™n h·ªá th·ªëng.
			
			fdisk: c√¥ng c·ª• qu·∫£n l√Ω ph√¢n v√πng tr√™n Linux (t·∫°o, x√≥a, thay ƒë·ªïi partition).

			-l (list): hi·ªÉn th·ªã danh s√°ch c√°c thi·∫øt b·ªã l∆∞u tr·ªØ v√† th√¥ng tin chi ti·∫øt.
		
		- cmd: df -h
		
			L·ªánh df -h trong Linux d√πng ƒë·ªÉ hi·ªÉn th·ªã dung l∆∞·ª£ng ·ªï ƒëƒ©a v√† c√°c ph√¢n v√πng ƒëang ƒë∆∞·ª£c
			mount theo c√°ch d·ªÖ ƒë·ªçc (human-readable).
			
			df = disk filesystem (th√¥ng tin h·ªá th·ªëng t·ªáp).

			-h = human readable ‚Üí hi·ªÉn th·ªã dung l∆∞·ª£ng b·∫±ng ƒë∆°n v·ªã KB, MB, GB, TB thay v√¨ block.	

			Kh√°c v·ªõi fdisk -l:

				df -h ‚Üí xem dung l∆∞·ª£ng ƒëang s·ª≠ d·ª•ng c·ªßa ph√¢n v√πng ƒë√£ mount.

				fdisk -l ‚Üí xem c·∫•u tr√∫c ph√¢n v√πng c·ªßa ·ªï ƒëƒ©a (d√π ch∆∞a mount).			
		
		- cmd: mount /dev/nvme1n1p1 /var/www/html/images
		
			√ù nghƒ©a:

				mount: g·∫Øn (mount) m·ªôt ph√¢n v√πng/·ªï ƒëƒ©a v√†o m·ªôt th∆∞ m·ª•c tr√™n h·ªá th·ªëng.

				/dev/nvme1n1p1: thi·∫øt b·ªã v·∫≠t l√Ω/ph√¢n v√πng (·ªü ƒë√¢y l√† ·ªï NVMe, partition s·ªë 1).

				/var/www/html/images: th∆∞ m·ª•c mount point (n∆°i n·ªôi dung c·ªßa ph√¢n v√πng s·∫Ω xu·∫•t hi·ªán).
				
			Quy tr√¨nh ho·∫°t ƒë·ªông:

				Sau khi ch·∫°y l·ªánh, to√†n b·ªô n·ªôi dung g·ªëc trong /var/www/html/images (n·∫øu c√≥) s·∫Ω b·ªã ·∫©n
				ƒëi (kh√¥ng m·∫•t, ch·ªâ b·ªã che khu·∫•t) v√† ƒë∆∞·ª£c thay th·∫ø b·ªüi n·ªôi dung c·ªßa ph√¢n v√πng /dev/nvme1n1p1.

				Khi unmount (umount /var/www/html/images), d·ªØ li·ªáu g·ªëc trong th∆∞ m·ª•c l·∫°i hi·ªán ra.
				
			C√°c l∆∞u √Ω:
			
				Th∆∞ m·ª•c mount point ph·∫£i t·ªìn t·∫°i tr∆∞·ªõc

				Thi·∫øt b·ªã ph·∫£i c√≥ h·ªá th·ªëng t·ªáp (filesystem) h·ª£p l·ªá (ext4, xfs, vfat...).

				N·∫øu mu·ªën mount t·ª± ƒë·ªông khi kh·ªüi ƒë·ªông l·∫°i, c·∫ßn ch·ªânh file /etc/fstab.
				
		- cmd: mount -a
		
			√ù nghƒ©a:

				mount: g·∫Øn (mount) ph√¢n v√πng/thi·∫øt b·ªã v√†o h·ªá th·ªëng.

				-a (all): mount t·∫•t c·∫£ c√°c m·ª•c ƒë∆∞·ª£c khai b√°o trong file c·∫•u
				h√¨nh /etc/fstab (ngo·∫°i tr·ª´ nh·ªØng d√≤ng c√≥ option noauto).
				
			Khi n√†o d√πng:

				Sau khi b·∫°n ch·ªânh s·ª≠a file /etc/fstab ƒë·ªÉ th√™m m·ªõi ph√¢n v√πng c·∫ßn mount t·ª±
				ƒë·ªông khi kh·ªüi ƒë·ªông l·∫°i.

				Thay v√¨ ph·∫£i reboot, b·∫°n c√≥ th·ªÉ ch·∫°y mount -a ƒë·ªÉ ki·ªÉm tra ngay.

		- cmd: umount /var/www/html/images
		
			√ù nghƒ©a:

				umount (kh√¥ng c√≥ ch·ªØ n ·ªü ƒë·∫ßu) d√πng ƒë·ªÉ th√°o (unmount) m·ªôt ph√¢n v√πng kh·ªèi h·ªá th·ªëng.

				/var/www/html/images l√† mount point m√† tr∆∞·ªõc ƒë√≥ b·∫°n ƒë√£ mount thi·∫øt b·ªã v√†o.
				
			K·∫øt qu·∫£ khi ch·∫°y:

				Sau khi umount, d·ªØ li·ªáu tr√™n ph√¢n v√πng /dev/nvme1n1p1 s·∫Ω kh√¥ng c√≤n xu·∫•t
				hi·ªán ·ªü /var/www/html/images.

				N·ªôi dung g·ªëc c·ªßa th∆∞ m·ª•c /var/www/html/images (n·∫øu c√≥ tr∆∞·ªõc ƒë√≥) s·∫Ω hi·ªán ra l·∫°i.
		
		- cmd: vi /etc/fstab
		- cmd: systemctl daemon-reload
		
		
		- cmd: fdisk /dev/nvme1n1
		
			Th∆∞ m·ª•c s·∫Ω ƒë∆∞·ª£c l·∫•y theo l·ªánh gi√° tr·ªã c·ªßa Disk theo l·ªánh sau:
			
				[ec2-user@ip-172-31-16-206 ~]$ fdisk -l
				fdisk: cannot open /dev/nvme0n1: Permission denied
				fdisk: cannot open /dev/nvme1n1: Permission denied
				[ec2-user@ip-172-31-16-206 ~]$ sudo -i
				[root@ip-172-31-16-206 ~]# fdisk -l
				Disk /dev/nvme0n1: 8 GiB, 8589934592 bytes, 16777216 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				Disklabel type: gpt
				Disk identifier: 7129DF59-1D55-477D-A6E9-19E29F26FE4F

				Device           Start      End  Sectors  Size Type
				/dev/nvme0n1p1    2048     6143     4096    2M BIOS boot
				/dev/nvme0n1p2    6144  1030143  1024000  500M EFI System
				/dev/nvme0n1p3 1030144  3078143  2048000 1000M Linux extended boot
				/dev/nvme0n1p4 3078144 16777182 13699039  6.5G Linux root (x86-64)


				Disk /dev/nvme1n1: 5 GiB, 5368709120 bytes, 10485760 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				
			√ù nghƒ©a

				fdisk: c√¥ng c·ª• qu·∫£n l√Ω ph√¢n v√πng (partition) tr√™n ·ªï ƒëƒ©a.

				/dev/nvme1n1: thi·∫øt b·ªã NVMe (·ªï ƒëƒ©a v·∫≠t l√Ω, ch∆∞a ch·ªçn ph√¢n v√πng c·ª• th·ªÉ).

				Khi ch·∫°y l·ªánh n√†y, b·∫°n s·∫Ω m·ªü tr√¨nh t∆∞∆°ng t√°c (interactive mode) ƒë·ªÉ thao t√°c v·ªõi b·∫£ng ph√¢n
				v√πng c·ªßa ·ªï ƒëƒ©a ƒë√≥.
				
			L·ªánh con	Ch·ª©c nƒÉng
			m			Hi·ªÉn th·ªã menu tr·ª£ gi√∫p (help).
			p			In (print) b·∫£ng ph√¢n v√πng hi·ªán t·∫°i.
			n			T·∫°o (new) ph√¢n v√πng m·ªõi.
			d			X√≥a (delete) m·ªôt ph√¢n v√πng.
			t			Thay ƒë·ªïi lo·∫°i ph√¢n v√πng (partition type).
			w			L∆∞u thay ƒë·ªïi (write) xu·ªëng ƒëƒ©a v√† tho√°t.
			q			Tho√°t m√† kh√¥ng l∆∞u g√¨ (quit).
				
			L∆∞u √Ω quan tr·ªçng

				R·∫•t nguy hi·ªÉm: n·∫øu b·∫°n d (x√≥a) r·ªìi w, d·ªØ li·ªáu s·∫Ω m·∫•t.
				
		- cmd: lsof /var/www/html/images
		
			√ù nghƒ©a

				lsof = list open files ‚Üí li·ªát k√™ t·∫•t c·∫£ file ƒëang ƒë∆∞·ª£c m·ªü tr√™n h·ªá th·ªëng.

				/var/www/html/images = ƒë∆∞·ªùng d·∫´n b·∫°n mu·ªën ki·ªÉm tra.

			Khi ch·∫°y, n√≥ s·∫Ω hi·ªÉn th·ªã c√°c process (ti·∫øn tr√¨nh) n√†o ƒëang d√πng th∆∞ m·ª•c ho·∫∑c file trong th∆∞ m·ª•c n√†y.		
		
		- cmd: yum install lsof -y
		- cmd: yum install mariadb-server -y
		- cmd: systemctl start mariadb
		- cmd: systemctl status mariadb
		- cmd: systemctl stop mariadb
		- cmd: stress-ng --cpu 2 --timeout 60s
		- cmd: stress-ng --cpu $(nproc) --timeout 300s
		- cmd: stress-ng --cpu $(nproc) --cpu-load 60 --timeout 600s
		- cmd: sudo yum install -y amazon-efs-utils
		- cmd: mount -fav
		- cmd: apt install mysql-client -y
		
		
	AWS Part-1 - Amazon EFS (Elastic File System):	

		Amazon EFS (Elastic File System) l√† d·ªãch v·ª• l∆∞u tr·ªØ file ƒë∆∞·ª£c AWS cung c·∫•p, d√πng ƒë·ªÉ chia s·∫ª d·ªØ li·ªáu gi·ªØa
		nhi·ªÅu EC2 instance ho·∫∑c container m·ªôt c√°ch d·ªÖ d√†ng.
		
		ƒê·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa EFS:

			L∆∞u tr·ªØ d·∫°ng file (NFS ‚Äì Network File System), kh√¥ng ph·∫£i block nh∆∞ EBS v√† kh√¥ng ph·∫£i object nh∆∞ S3.

			T·ª± ƒë·ªông m·ªü r·ªông: dung l∆∞·ª£ng tƒÉng/gi·∫£m theo nhu c·∫ßu, b·∫°n ch·ªâ tr·∫£ ti·ªÅn cho dung l∆∞·ª£ng d√πng th·ª±c t·∫ø.

			Chia s·∫ª gi·ªØa nhi·ªÅu EC2: nhi·ªÅu m√°y ·∫£o (EC2) c√≥ th·ªÉ mount v√†o c√πng m·ªôt EFS v√† ƒë·ªçc/ghi d·ªØ li·ªáu chung.

			High availability: d·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u tr·ªØ t·ª± ƒë·ªông trong nhi·ªÅu AZ (Availability Zone) trong c√πng 1 region.

			Hi·ªáu nƒÉng cao, c√≥ th·ªÉ d√πng cho web server, big data, container, CI/CD.
		
	AWS Part-1 - Security group:	
		
		0.0.0.0/0 ‚Üí to√†n b·ªô IPv4 address space (m·ªçi ƒë·ªãa ch·ªâ IPv4 ƒë·ªÅu truy c·∫≠p ƒë∆∞·ª£c).

		::/0 ‚Üí to√†n b·ªô IPv6 address space (m·ªçi ƒë·ªãa ch·ªâ IPv6 ƒë·ªÅu truy c·∫≠p ƒë∆∞·ª£c).
		
		Ch√∫ √Ω khi t·∫°o load balancer th√¨ trong n√≥ s·∫Ω c√≥ 1 security group v√≠ d·ª• nh∆∞ sg-b, trong instance s·∫Ω c√≥ 1 security group v√≠
		d·ª• nh∆∞ a-sg, th√¨ trong a-sg ph·∫£i add th√™m b-sg ƒë·ªÉ n√≥ ch·∫•p nh·∫≠n truy c·∫≠p t·ª´ load balancer
		
		Ch√∫ √Ω khi g√°n public IP c·ªßa m√°y v√†o security group, ch√∫ng ta t·∫Øt m√°y r·ªìi m·ªü l·∫°i th√¨ s·∫Ω c√≥ public API m·ªõi,
		l√∫c n√†y c·∫ßn c·∫≠p nh·∫≠t n√≥ trong security group ƒë·ªÉ c√≥ public IP m·ªõi nh·∫•t
		
	AWS Part-1 - Security group NFS:
	
		Trong Security Group c·ªßa AWS, khi b·∫°n th·∫•y NFS th√¨ n√≥ l√† vi·∫øt t·∫Øt c·ªßa:

		NFS (Network File System): giao th·ª©c chia s·∫ª file qua m·∫°ng, th∆∞·ªùng d√πng b·ªüi Amazon EFS (Elastic File System).
		
		Th√¥ng tin c∆° b·∫£n v·ªÅ NFS trong Security Group:

			Protocol: TCP

			Port: 2049 (c·ªïng m·∫∑c ƒë·ªãnh c·ªßa NFS)

			√ù nghƒ©a: Cho ph√©p EC2 (ho·∫∑c c√°c m√°y kh√°c trong VPC) k·∫øt n·ªëi ƒë·∫øn EFS ƒë·ªÉ ƒë·ªçc/ghi file.
			
		V√≠ d·ª• Security Group cho EFS:

			ƒê·ªÉ EC2 c√≥ th·ªÉ mount EFS:

				Security Group c·ªßa EFS ph·∫£i m·ªü inbound rule:

				Type: NFS

				Protocol: TCP

				Port Range: 2049

				Source: Security Group c·ªßa EC2 (ho·∫∑c CIDR c·ªßa VPC, v√≠ d·ª• 10.0.0.0/16)
		
	AWS Part-1 - EBS:
	
		EBS trong AWS l√† vi·∫øt t·∫Øt c·ªßa Amazon Elastic Block Store. ƒê√¢y l√† d·ªãch v·ª• cung c·∫•p l∆∞u
		tr·ªØ d·∫°ng block (block storage) cho c√°c EC2 instance.		
		
		ƒê·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa Amazon EBS:

			Block storage:

				D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u th√†nh c√°c kh·ªëi (block), t∆∞∆°ng t·ª± nh∆∞ ·ªï c·ª©ng g·∫Øn v√†o m√°y t√≠nh.

				C√≥ th·ªÉ format th√†nh h·ªá th·ªëng file (ext4, xfs, NTFS, ‚Ä¶) ho·∫∑c d√πng tr·ª±c ti·∫øp cho
				database, ·ª©ng d·ª•ng c·∫ßn truy c·∫≠p I/O nhanh.

			G·∫Øn v·ªõi EC2 Instance:

				M·ªôt volume EBS gi·ªëng nh∆∞ ·ªï c·ª©ng ·∫£o g·∫Øn v√†o EC2.

				C√≥ th·ªÉ g·∫Øn m·ªôt volume cho nhi·ªÅu instance (ch·∫ø ƒë·ªô read-only) ho·∫∑c cho 1 instance (read/write).

			T√≠nh ƒë√†n h·ªìi (Elastic):

				C√≥ th·ªÉ tƒÉng/gi·∫£m dung l∆∞·ª£ng m√† kh√¥ng c·∫ßn d·ª´ng instance.

				H·ªó tr·ª£ thay ƒë·ªïi lo·∫°i volume (v√≠ d·ª• t·ª´ HDD sang SSD).

			T√≠nh b·ªÅn v·ªØng (Durability):

				EBS ƒë∆∞·ª£c l∆∞u tr·ªØ trong Availability Zone (AZ), c√≥ kh·∫£ nƒÉng replicate trong AZ ƒë·ªÉ tr√°nh
				m·∫•t d·ªØ li·ªáu khi c√≥ l·ªói ph·∫ßn c·ª©ng.

			Snapshot:

				C√≥ th·ªÉ t·∫°o b·∫£n sao (snapshot) c·ªßa volume v√† l∆∞u tr√™n Amazon S3.

				Snapshot d√πng ƒë·ªÉ backup ho·∫∑c t·∫°o volume m·ªõi t·ª´ b·∫£n copy.

	AWS Part-1 - EBS Snapshot:
	
		Trong Amazon EBS, snapshot ch√≠nh l√† c∆° ch·∫ø l∆∞u l·∫°i d·ªØ li·ªáu c·ªßa m·ªôt volume t·∫°i m·ªôt
		th·ªùi ƒëi·ªÉm c·ª• th·ªÉ (point-in-time).
					
		C·ª• th·ªÉ h∆°n:

			M·ªôt EBS volume gi·ªëng nh∆∞ ·ªï c·ª©ng g·∫Øn v√†o EC2 instance.

			Khi b·∫°n t·∫°o snapshot:

				AWS s·∫Ω "ch·ª•p l·∫°i" to√†n b·ªô d·ªØ li·ªáu c·ªßa volume t·∫°i th·ªùi ƒëi·ªÉm ƒë√≥.

				Snapshot n√†y ƒë∆∞·ª£c l∆∞u trong S3 n·ªôi b·ªô c·ªßa AWS (b·∫°n kh√¥ng tr·ª±c ti·∫øp th·∫•y trong
				bucket S3, m√† qu·∫£n l√Ω qua EC2 console/CLI).
				
		M·ª•c ƒë√≠ch ch√≠nh c·ªßa snapshot:

			Sao l∆∞u d·ªØ li·ªáu (backup)

				ƒê·ªÉ ph√≤ng s·ª± c·ªë m·∫•t d·ªØ li·ªáu, h·ªèng volume, ho·∫∑c EC2 b·ªã terminate.

			Ph·ª•c h·ªìi (restore)

				B·∫°n c√≥ th·ªÉ t·∫°o l·∫°i m·ªôt volume m·ªõi t·ª´ snapshot ‚Üí c√≥ d·ªØ li·ªáu gi·ªëng h·ªát volume ban ƒë·∫ßu t·∫°i th·ªùi ƒëi·ªÉm ch·ª•p.

			T·∫°o b·∫£n copy (clone)

				D√πng snapshot ƒë·ªÉ t·∫°o volume m·ªõi cho m√¥i tr∆∞·ªùng test/dev m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn volume g·ªëc.

			Di chuy·ªÉn d·ªØ li·ªáu gi·ªØa region

				Snapshot c√≥ th·ªÉ copy sang region kh√°c, sau ƒë√≥ t·∫°o volume ·ªü ƒë√≥.
			
		L∆∞u √Ω quan tr·ªçng:

			Snapshot EBS l√† incremental:

				Snapshot ƒë·∫ßu ti√™n = to√†n b·ªô d·ªØ li·ªáu.

				Snapshot ti·∫øp theo = ch·ªâ l∆∞u ph·∫ßn d·ªØ li·ªáu thay ƒë·ªïi t·ª´ l·∫ßn tr∆∞·ªõc ‚Üí ti·∫øt ki·ªám chi ph√≠.

			N·∫øu x√≥a 1 snapshot, d·ªØ li·ªáu c·ªßa snapshot ƒë√≥ ƒë∆∞·ª£c gi·ªØ l·∫°i trong c√°c snapshot
			sau (AWS ƒë·∫£m b·∫£o kh√¥ng m·∫•t d·ªØ li·ªáu mi·ªÖn l√† c√≤n √≠t nh·∫•t 1 snapshot trong chu·ªói).
			
	WS Part-1 - EC2 Launch Template:
	
		Launch Template l√† m·ªôt ‚Äúm·∫´u c·∫•u h√¨nh‚Äù (template) ch·ª©a s·∫µn th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o EC2 instance.

		N√≥ gi√∫p b·∫°n chu·∫©n h√≥a v√† t√°i s·ª≠ d·ª•ng c·∫•u h√¨nh khi kh·ªüi t·∫°o nhi·ªÅu EC2.

		C√≥ th·ªÉ coi nh∆∞ l√† m·ªôt "blueprint" cho instance.

		Th√¥ng tin l∆∞u trong Launch Template:

			Khi t·∫°o Launch Template, b·∫°n c√≥ th·ªÉ ƒë·ªãnh nghƒ©a s·∫µn:

				AMI (Amazon Machine Image) ‚Üí h·ªá ƒëi·ªÅu h√†nh, ph·∫ßn m·ªÅm c√†i s·∫µn.

				Instance type (t2.micro, t3.small, v.v.).

				Key pair (d√πng ƒë·ªÉ SSH).

				Security group.

				VPC + Subnet.

				EBS volume mapping (·ªï c·ª©ng g·∫Øn v√†o instance).

				IAM role cho EC2.

				User data (script ch·∫°y khi instance kh·ªüi ƒë·ªông).

				Network settings (private/public IP).
				
		T√≠nh nƒÉng ch√≠nh:

			T√°i s·ª≠ d·ª•ng c·∫•u h√¨nh:

				Kh√¥ng ph·∫£i nh·∫≠p tay l·∫°i t·ª´ng th√¥ng tin khi launch EC2 m·ªõi.

			Versioning (phi√™n b·∫£n h√≥a):

				M·ªói khi b·∫°n thay ƒë·ªïi Launch Template ‚Üí s·∫Ω t·∫°o ra m·ªôt version m·ªõi.

				C√≥ th·ªÉ quay l·∫°i version c≈© ho·∫∑c ch·ªçn version m·∫∑c ƒë·ªãnh.

			T√≠ch h·ª£p v·ªõi Auto Scaling v√† Spot Fleet:

				Khi b·∫°n t·∫°o Auto Scaling Group ho·∫∑c EC2 Spot Fleet, thay v√¨ c·∫•u h√¨nh ph·ª©c t·∫°p ‚Üí ch·ªâ c·∫ßn tr·ªè ƒë·∫øn Launch Template.

			So v·ªõi Launch Configuration (LC):

				Launch Template l√† phi√™n b·∫£n c·∫£i ti·∫øn c·ªßa Launch Configuration.

				H·ªó tr·ª£ nhi·ªÅu t√≠nh nƒÉng h∆°n (versioning, multiple instance types, T2/T3 Unlimited, placement groups...).

				AWS khuy√™n d√πng Launch Template thay v√¨ Launch Configuration.
				
		V√≠ d·ª• th·ª±c t·∫ø

			B·∫°n mu·ªën tri·ªÉn khai Auto Scaling Group cho web server:

			T·∫°o Launch Template v·ªõi c·∫•u h√¨nh:

				AMI Ubuntu 22.04

				Instance type: t3.micro

				Security Group m·ªü c·ªïng 80/443

				User Data: c√†i Nginx

			Khi Auto Scaling c·∫ßn t·∫°o th√™m EC2 ‚Üí n√≥ s·∫Ω d·ª±a v√†o Launch Template n√†y, ƒë·∫£m b·∫£o t·∫•t c·∫£ instance ƒë·ªÅu gi·ªëng nhau.
			
	AWS Part-1 - Load Balancing:
	
		Load Balancing = k·ªπ thu·∫≠t ph√¢n ph·ªëi l∆∞u l∆∞·ª£ng (traffic) ƒë·∫øn nhi·ªÅu server (targets) thay v√¨ d·ªìn h·∫øt v√†o m·ªôt server.

		M·ª•c ti√™u:

			TƒÉng t√≠nh s·∫µn s√†ng (High Availability) ‚Üí n·∫øu 1 server h·ªèng, traffic t·ª± ƒë·ªông chuy·ªÉn sang server kh√°c.

			TƒÉng kh·∫£ nƒÉng m·ªü r·ªông (Scalability) ‚Üí th√™m server m·ªõi ƒë·ªÉ g√°nh th√™m t·∫£i.

			C√¢n b·∫±ng hi·ªáu nƒÉng (Performance) ‚Üí tr√°nh 1 server b·ªã qu√° t·∫£i trong khi server kh√°c r·∫£nh.

			Trong AWS, Elastic Load Balancer (ELB) ch√≠nh l√† d·ªãch v·ª• th·ª±c hi·ªán load balancing cho EC2 v√†
			c√°c d·ªãch v·ª• backend kh√°c.
			
		Th√†nh ph·∫ßn ch√≠nh trong Load Balancing (EC2 + ELB):

			Load Balancer (ALB, NLB, GWLB, CLB):

				Th√†nh ph·∫ßn ƒë·ª©ng tr∆∞·ªõc client.

				Nh·∫≠n request t·ª´ user ‚Üí ph√¢n ph·ªëi ƒë·∫øn backend.

			Listener:

				Quy ƒë·ªãnh protocol & port (VD: HTTP:80, HTTPS:443, TCP:3306).

				C√≥ th·ªÉ c√≥ nhi·ªÅu listener trong m·ªôt Load Balancer.

			Target Group:

				T·∫≠p h·ª£p backend (EC2 instances, IPs, Lambda, ECS tasks).

				Load Balancer s·∫Ω g·ª≠i request ƒë·∫øn Target Group theo rule.

			Health Check:

				Load Balancer ki·ªÉm tra tr·∫°ng th√°i (healthy/unhealthy) c·ªßa t·ª´ng target.

				Ch·ªâ g·ª≠i request ƒë·∫øn target c√≤n "healthy".
				
		C√°c lo·∫°i Load Balancer trong AWS

			Application Load Balancer (ALB)

				D√πng cho HTTP/HTTPS.

				C√≥ t√≠nh nƒÉng routing theo path, host, header (Layer 7).

				V√≠ d·ª•: /api/* v√†o backend API, /img/* v√†o backend static server.

			Network Load Balancer (NLB)

				D√πng cho TCP/UDP (Layer 4).

				Hi·ªáu nƒÉng c·ª±c cao, ƒë·ªô tr·ªÖ th·∫•p, x·ª≠ l√Ω h√†ng tri·ªáu request/s.

			Gateway Load Balancer (GWLB)

				D√πng ƒë·ªÉ ph√¢n ph·ªëi traffic qua firewall/appliance (Layer 3).

				Classic Load Balancer (CLB)

				D·ªãch v·ª• ƒë·ªùi c≈©, √≠t d√πng cho h·ªá th·ªëng m·ªõi.
				
		V√≠ d·ª• th·ª±c t·∫ø:

			B·∫°n c√≥ 3 EC2 ch·∫°y web server:

			C·∫•u h√¨nh 1 Application Load Balancer (ALB) v·ªõi listener HTTP:80.

			G·∫Øn 3 EC2 n√†y v√†o Target Group.

			Khi ng∆∞·ªùi d√πng truy c·∫≠p website:

				Request ƒë·∫øn ALB.

				ALB ch·ªçn 1 EC2 trong target group (theo Round Robin ho·∫∑c rule kh√°c).

				N·∫øu 1 EC2 b·ªã down ‚Üí ALB ng·ª´ng g·ª≠i traffic ƒë·∫øn n√≥.
			
	AWS Part-1 - Target Group c·ªßa Load Balancing:
	
		Target Group l√† t·∫≠p h·ª£p c√°c backend (targets) ‚Äì t·ª©c l√† n∆°i nh·∫≠n request t·ª´ Load Balancer.

		C√°c target c√≥ th·ªÉ l√†:

			EC2 instances

			IP addresses (private IP)

			Lambda functions

			ECS tasks (trong case d√πng v·ªõi container)

		N√≥i ƒë∆°n gi·∫£n: Target Group = danh s√°ch c√°c server/·ª©ng d·ª•ng m√† Load Balancer s·∫Ω ph√¢n ph·ªëi traffic ƒë·∫øn.
		
		C√°ch ho·∫°t ƒë·ªông

			Ng∆∞·ªùi d√πng g·ª≠i request ‚Üí Load Balancer nh·∫≠n request.

			Load Balancer s·∫Ω chuy·ªÉn request ƒë·∫øn m·ªôt Target Group ƒë√£ g·∫Øn v·ªõi n√≥.

			Trong Target Group, LB ph√¢n ph·ªëi request ƒë·∫øn c√°c target c·ª• th·ªÉ (EC2, IP, ECS task...) d·ª±a
			theo policy (Round Robin, Least Outstanding Request, ‚Ä¶).

			Target Group c√≥ health check ƒë·ªÉ x√°c ƒë·ªãnh target n√†o c√≤n ‚Äús·ªëng‚Äù (healthy). LB ch·ªâ g·ª≠i traffic ƒë·∫øn c√°c target healthy.
			
		C√°c lo·∫°i Target Group (t√πy lo·∫°i Load Balancer):

			Application Load Balancer (ALB): target group c√≥ th·ªÉ ch·ª©a EC2, IP, ECS tasks, Lambda.

			Network Load Balancer (NLB): target group th∆∞·ªùng ch·ª©a EC2 ho·∫∑c IP.

			Gateway Load Balancer (GWLB): target group ch·ª©a appliances (v√≠ d·ª• firewall ·∫£o).
			
		Health Check:

			M·ªói Target Group c√≥ th·ªÉ c·∫•u h√¨nh health check ri√™ng (HTTP, HTTPS, TCP).

			N·∫øu target fail health check ‚Üí LB t·∫°m d·ª´ng g·ª≠i request ƒë·∫øn ƒë√≥.
			
		V√≠ d·ª• th·ª±c t·∫ø:

			B·∫°n c√≥ 2 EC2 web server ch·∫°y Nginx:

				T·∫°o Target Group web-servers.

				Add 2 EC2 instance (10.0.1.10, 10.0.1.11) v√†o Target Group.

				C·∫•u h√¨nh ALB rule: n·∫øu request ƒë·∫øn / ‚Üí forward ƒë·∫øn Target Group web-servers.

				Khi ALB nh·∫≠n request, n√≥ s·∫Ω forward sang 1 trong 2 EC2 ƒë√≥ (theo Round Robin).
				
	AWS Part-1 - EC2 Auto Scaling:
	
		Trong AWS, Auto Scaling (th∆∞·ªùng g·ªçi l√† EC2 Auto Scaling) l√† d·ªãch v·ª• gi√∫p b·∫°n t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh
		s·ªë l∆∞·ª£ng t√†i nguy√™n (EC2 instances) ƒë·ªÉ ƒë√°p ·ª©ng nhu c·∫ßu t·∫£i, m√† kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng.
		
		C√°c th√†nh ph·∫ßn ch√≠nh trong AWS Auto Scaling

		Launch Template / Launch Configuration:

			ƒê·ªãnh nghƒ©a c√°ch EC2 s·∫Ω ƒë∆∞·ª£c t·∫°o: AMI, instance type, key pair, security group, user data,‚Ä¶

		Auto Scaling Group (ASG):

			L√† nh√≥m EC2 ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi Auto Scaling.

			B·∫°n c·∫•u h√¨nh:

				Min size: s·ªë l∆∞·ª£ng EC2 t·ªëi thi·ªÉu.

				Max size: s·ªë l∆∞·ª£ng EC2 t·ªëi ƒëa.

				Desired capacity: s·ªë l∆∞·ª£ng EC2 mong mu·ªën (ASG s·∫Ω duy tr√¨ con s·ªë n√†y).

		Scaling Policies:

			Quy t·∫Øc ƒë·ªÉ scale in (gi·∫£m) ho·∫∑c scale out (tƒÉng).

			V√≠ d·ª•:

				N·∫øu CPU > 70% trong 5 ph√∫t ‚Üí scale out +1 instance.

				N·∫øu CPU < 30% trong 10 ph√∫t ‚Üí scale in -1 instance.

		Health Checks:

			ASG s·∫Ω ki·ªÉm tra t√¨nh tr·∫°ng EC2. N·∫øu m·ªôt instance unhealthy, n√≥ s·∫Ω terminate v√† kh·ªüi t·∫°o l·∫°i instance m·ªõi.

		Integration v·ªõi Load Balancer

			ASG th∆∞·ªùng g·∫Øn v·ªõi Target Group c·ªßa m·ªôt Elastic Load Balancer (ALB/NLB).

			Khi EC2 scale in/out, ch√∫ng s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c ƒëƒÉng k√Ω/d·ª° kh·ªèi target group.
			
	AWS Part-1 - Route 53:
	
		Amazon Route 53 l√† m·ªôt d·ªãch v·ª• DNS (Domain Name System) v√† qu·∫£n l√Ω t√™n mi·ªÅn c·ªßa AWS.

		C√°c ch·ª©c nƒÉng ch√≠nh c·ªßa Route 53:

			Domain Registration (ƒêƒÉng k√Ω t√™n mi·ªÅn)

				B·∫°n c√≥ th·ªÉ mua v√† qu·∫£n l√Ω domain tr·ª±c ti·∫øp tr√™n AWS (v√≠ d·ª•: mywebsite.com).

			DNS Service (H·ªá th·ªëng ph√¢n gi·∫£i t√™n mi·ªÅn)

				Bi·∫øn t√™n mi·ªÅn d·ªÖ nh·ªõ (mywebsite.com) th√†nh ƒë·ªãa ch·ªâ IP th·∫≠t (192.0.2.1).

				Qu·∫£n l√Ω c√°c b·∫£n ghi DNS nh∆∞:

					A (IPv4 address)

					AAAA (IPv6 address)

					CNAME (alias)

					MX (mail server)

					TXT (text records, th∆∞·ªùng d√πng cho x√°c th·ª±c email, SSL, ‚Ä¶).

			Traffic Routing (ƒêi·ªÅu h∆∞·ªõng l∆∞u l∆∞·ª£ng th√¥ng minh)

				Route 53 h·ªó tr·ª£ nhi·ªÅu ki·ªÉu ƒëi·ªÅu h∆∞·ªõng:

					Simple routing ‚Üí tr·ªè domain ƒë·∫øn 1 server duy nh·∫•t.

					Weighted routing ‚Üí chia t·∫£i theo t·ªâ l·ªá % gi·ªØa nhi·ªÅu server.

					Latency-based routing ‚Üí g·ª≠i user ƒë·∫øn server c√≥ ƒë·ªô tr·ªÖ th·∫•p nh·∫•t.

					Failover routing ‚Üí t·ª± ƒë·ªông chuy·ªÉn sang server kh√°c khi server ch√≠nh b·ªã down.

			Health Checks & Monitoring (Gi√°m s√°t d·ªãch v·ª•)

				Route 53 c√≥ th·ªÉ ki·ªÉm tra t√¨nh tr·∫°ng endpoint (web server, API, ‚Ä¶).

				N·∫øu server kh√¥ng ph·∫£n h·ªìi, Route 53 s·∫Ω t·ª± ƒë·ªông chuy·ªÉn traffic sang server backup.

			·ª®ng d·ª•ng th·ª±c t·∫ø

				Khi b·∫°n c√≥ website ch·∫°y tr√™n EC2 / S3 / Load Balancer, b·∫°n d√πng Route 53 ƒë·ªÉ:

					Tr·ªè domain (myapp.com) ‚Üí Elastic Load Balancer.

					T·ª± ƒë·ªông ƒëi·ªÅu h∆∞·ªõng ng∆∞·ªùi d√πng ƒë·∫øn server g·∫ßn nh·∫•t (multi-region).

					ƒê·∫£m b·∫£o h·ªá th·ªëng v·∫´n online khi m·ªôt server b·ªã l·ªói (failover).
		
	AWS Part-1 - Ki·∫øn th·ª©c:	
		
		- EC2 Key pair name: RSA, ED25519
		- EC2 prite key format: .pem, .ppk
		- EC2 network settings: VPC
		- EC2 network setting: Inbound Security Group Rules
		- EC2 instances -> connect -> ssh client
		- EC2 instance -> security -> security group -> edit inbound rules
		- EC2 -> key pair
		- EC2 network interface
		- EC2 security group: My IP
		- EC2 -> Launch an instance -> Resource types -> Intances, Volumes
		- EC2 -> Instance -> Actions -> Instance settings -> Change instance type
		- EC2 -> Instance -> Actions -> Image and templates -> Create image
		- EC2 -> Images -> AMIs
		- EC2 -> Images -> Action -> Copy AMI
		- EC2 -> Images -> Action -> Edit AMI permissions
		- EC2 -> Images -> Action -> Launch instance from AMI
		- EC2 -> Instance -> Actions -> Security -> Change security groups
		- EC2 -> Instance -> Actions -> Monitor and troubleshoot -> Get system log
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Dissociate Elastic IP address
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Release Elastic IP addresses
		- EC2 -> Instances -> Storage -> Block devices
		- EC2 -> Instances -> Launch Templates -> Actions -> Launch instance from template
		- EC2 -> Instances -> Create image
		- EC2 -> Elastic Block Store -> Volumes
		- EC2 -> Elastic Block Store -> Volumes -> Action -> Attach Volume
		- EC2 -> Elastic Block Store ->  Actions -> Detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Force detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Create snapshot
		- EC2 -> Load Balancing -> Target groups -> Create target group
		- EC2 -> Load Balancing -> Load Balancers -> Create load balancer
		- EC2 -> AMIs -> Launch instance from AMI
		- EC2 -> Auto Scaling groups
		- EC2 -> Instances -> Vao instance -> Actions -> Security -> Modify IAM role
		- CloudWatch -> All alarms -> Create alarm
		- Amazon EFS -> Access points
		- Amazon S3 -> Buckets
		- Amazon S3 -> Buckets -> Objects
		- Amazon S3 -> Buckets -> Properties
		- Amazon S3 -> Buckets -> Permission
		- Amazon S3 -> Buckets -> Manegement -> Lifecycle rules -> Create lifecycle rule
		- Amazon S3 -> Buckets -> Manegement -> Replication rules -> Create replication rules
		- Aurora and RDS
		- Aurora and RDS -> Snapshots		
		- Aurora and RDS -> Databases -> Actions -> Migrate snapshot
		- Aurora and RDS -> Parameter group
		- Aurora and RDS -> Subnet groups
		- Route 53 -> Dashboard -> Create hosted zone
		- IAM -> Access management -> Users -> Add users
		- IAM -> Access management -> Users -> Security credentials -> Access keys -> Create access key
		- IAM -> Access management -> Roles -> Create role
		- IAM -> Users -> Create user
		- IAM -> User -> V√†o user -> Security credentials -> Create access key
		- Certificate Manager
		- Amazon AlasticCache -> Parameter groups -> Create parameter group
		- Amazon AlasticCache -> Subnet groups -> Create subnet group
		- Amazon AlasticCache -> Dashboard -> Create cache -> Create Memcache
		- Amazon MQ -> Broker -> Create brokers
		- Amazon Elastic Beanstalk -> Environment -> Upload and deploy
		- Cloudfront
		
		
		
		

	
	Trong b∆∞·ªõc Set permissions khi t·∫°o IAM User tr√™n AWS, b·∫°n th·∫•y c√≥ 3 t√πy ch·ªçn c·∫•p quy·ªÅn:

		1. Add user to group

			Th√™m user v√†o m·ªôt nh√≥m (IAM Group).

			Nh√≥m n√†y ƒë√£ c√≥ s·∫µn c√°c policy (quy·ªÅn h·∫°n).

			T·∫•t c·∫£ c√°c user trong nh√≥m s·∫Ω k·∫ø th·ª´a quy·ªÅn t·ª´ nh√≥m ƒë√≥.

			ƒê√¢y l√† c√°ch qu·∫£n l√Ω t·∫≠p trung, d·ªÖ duy tr√¨ khi c√≥ nhi·ªÅu user. V√≠ d·ª•: t·∫°o group Developers
			v·ªõi quy·ªÅn AmazonEC2FullAccess, m·ªçi user trong group s·∫Ω c√≥ quy·ªÅn ƒë√≥.

		2. Copy permissions

			Sao ch√©p to√†n b·ªô quy·ªÅn c·ªßa m·ªôt user kh√°c sang user m·ªõi.

			Bao g·ªìm group memberships, attached managed policies, v√† inline policies.

			Ti·ªán khi b·∫°n c√≥ m·ªôt user c≈© l√†m chu·∫©n, v√† mu·ªën user m·ªõi c√≥ quy·ªÅn gi·ªëng h·ªát.

		3. Attach policies directly

			G·∫Øn tr·ª±c ti·∫øp policy (quy·ªÅn h·∫°n) v√†o user.

			B·∫°n ch·ªçn c√°c AWS managed policy (AWS t·∫°o s·∫µn, v√≠ d·ª•: AmazonS3ReadOnlyAccess) ho·∫∑c customer
			managed policy (do b·∫°n t·ª± t·∫°o).

			Linh ho·∫°t cho t·ª´ng user, nh∆∞ng kh√¥ng khuy·∫øn kh√≠ch n·∫øu c√≥ nhi·ªÅu user v√¨ s·∫Ω kh√≥ qu·∫£n l√Ω v·ªÅ l√¢u d√†i.	
		
		
			
			

		Quy tr√¨nh t·∫°o m·ªôt Amazon EC2 Instance (m√°y ·∫£o ch·∫°y tr√™n AWS). Flow g·ªìm c√°c b∆∞·ªõc tu·∫ßn t·ª± nh∆∞ sau:

			Choose an AMI (Amazon Machine Image)

				AMI l√† "b·∫£n m·∫´u" h·ªá ƒëi·ªÅu h√†nh + ph·∫ßn m·ªÅm (Ubuntu, Amazon Linux, Windows Server, ‚Ä¶).

				B·∫°n ch·ªçn AMI ƒë·ªÉ quy·∫øt ƒë·ªãnh m√°y ·∫£o c·ªßa b·∫°n s·∫Ω ch·∫°y h·ªá ƒëi·ªÅu h√†nh n√†o, c√≥ c√†i s·∫µn ph·∫ßn m·ªÅm g√¨.

			Choose an Instance Type

				·ªû b∆∞·ªõc n√†y b·∫°n ch·ªçn lo·∫°i EC2 instance (t√≠nh nƒÉng gi·ªëng nh∆∞ ch·ªçn c·∫•u h√¨nh ph·∫ßn c·ª©ng).

				V√≠ d·ª•: t2.micro (1 vCPU, 1GB RAM ‚Äì mi·ªÖn ph√≠ trong Free Tier), m5.large, c5.xlarge, ‚Ä¶

				M·ªói lo·∫°i kh√°c nhau v·ªÅ CPU, RAM, kh·∫£ nƒÉng m·∫°ng, d√πng cho m·ª•c ƒë√≠ch kh√°c nhau (test, production, t√≠nh to√°n n·∫∑ng‚Ä¶).

		Configuring the Instance

			C·∫•u h√¨nh chi ti·∫øt cho instance:

				Ch·∫°y bao nhi√™u instance

				VPC/Subnet (m·∫°ng n√†o)

				T·ª± ƒë·ªông c·∫•p public IP hay kh√¥ng

				IAM Role (g·∫Øn quy·ªÅn ƒë·ªÉ EC2 truy c·∫≠p d·ªãch v·ª• AWS kh√°c)

				User data (script t·ª± ch·∫°y khi kh·ªüi ƒë·ªông instance)

		Adding Storage

			Ch·ªçn dung l∆∞·ª£ng ·ªï ƒëƒ©a (EBS volume).

			C√≥ th·ªÉ th√™m nhi·ªÅu volume, ch·ªçn lo·∫°i (SSD, HDD, provisioned IOPS‚Ä¶).

			V√≠ d·ª•: 8GB gp2 SSD m·∫∑c ƒë·ªãnh.

		Adding Tags

			G√°n nh√£n (key-value) ƒë·ªÉ qu·∫£n l√Ω d·ªÖ h∆°n.

			V√≠ d·ª•: Name=WebServer01, Environment=Dev.

			Tags gi√∫p t√¨m ki·∫øm, ph√¢n nh√≥m, ho·∫∑c d√πng cho billing (chi ph√≠).

		Configure Security Group

			Security Group l√† t∆∞·ªùng l·ª≠a ·∫£o cho EC2.

			·ªû ƒë√¢y b·∫°n ƒë·ªãnh nghƒ©a inbound/outbound rules (c·ªïng n√†o m·ªü cho IP n√†o).

			V√≠ d·ª•: m·ªü port 22 (SSH) cho IP c·ªßa b·∫°n, m·ªü port 80 (HTTP) cho m·ªçi ng∆∞·ªùi.

		Review

			Xem l·∫°i to√†n b·ªô c·∫•u h√¨nh ƒë√£ ch·ªçn.

			N·∫øu ·ªïn, b·∫°n nh·∫•n "Launch" ƒë·ªÉ kh·ªüi t·∫°o EC2.

			L√∫c n√†y AWS s·∫Ω h·ªèi b·∫°n ch·ªçn key pair (d√πng ƒë·ªÉ SSH v√†o m√°y).

			K·∫øt qu·∫£: B·∫°n c√≥ m·ªôt m√°y ·∫£o EC2 ch·∫°y tr√™n AWS, k·∫øt n·ªëi qua SSH/HTTP/HTTPS t√πy c·∫•u h√¨nh.

	Code trong Advanced details c·ªßa EC2:

		Case 1:
		
			#!/bin/bash
			sudo yum install httpd -y
			sudo systemctl start httpd
			sudo systemctl enable httpd
			mkdir /tmp/test1
			
		Case 2:
		
			#!/bin/bash
			yum install httpd wget unzip -y
			systemctl start httpd
			systemctl enable httpd
			cd /tmp
			wget https://www.tooplate.com/zip-templates/2119_gymso_fitness.zip
			unzip -o 2119_gymso_fitness.zip
			cp -r 2119_gymso_fitness/* /var/www/html/
			systemctl restart httpd
			
		Case 3:
		
			#!/bin/bash

			# Variable Declaration
			#PACKAGE="httpd wget unzip"
			#SVC="httpd"
			URL='https://www.tooplate.com/zip-templates/2098_health.zip'
			ART_NAME='2098_health'
			TEMPDIR="/tmp/webfiles"

			yum --help &> /dev/null

			if [ $? -eq 0 ]
			then
			   # Set Variables for CentOS
			   PACKAGE="httpd wget unzip"
			   SVC="httpd"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo yum install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/

			else
				# Set Variables for Ubuntu
			   PACKAGE="apache2 wget unzip"
			   SVC="apache2"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo apt update
			   sudo apt install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/
			fi 

			
	Code trong vi /etc/fstab:

		Case 1:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1p1    /var/www/html/images ext4       defaults        0 0
			
		Case 2:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1  /var/lib/mysql ext4     defaults        0 0
			
		Case 3:
		
			fs-0001a83c45525fae6 /var/www/html/images efs _netdev,tls,accesspoint=fsap-04b21726d75c2e7eb 0 0

	
			
-- AWS Cloud For Project Set Up or Lift and Shift:

	Flow ch√≠nh:
	
		Lu·ªìng ho·∫°t ƒë·ªông:

			User ‚Üí DNS (GoDaddy/Route53):
			
				Ng∆∞·ªùi d√πng nh·∫≠p domain v√†o browser ‚Üí DNS ph√¢n gi·∫£i domain ‚Üí IP c·ªßa Load Balancer.

			DNS ‚Üí Application Load Balancer:
			
				Request t·ªõi ALB (qua HTTPS/HTTP).

			ALB ‚Üí Tomcat Instances (ASG):
			
				ALB ph√¢n ph·ªëi request ƒë·∫øn m·ªôt trong c√°c EC2 Tomcat.

			Tomcat Instances ‚Üí Backend services:

				N·∫øu c·∫ßn d·ªØ li·ªáu ‚Üí Tomcat k·∫øt n·ªëi t·ªõi MySQL.

				N·∫øu c·∫ßn cache ‚Üí g·ªçi t·ªõi Memcache.

				N·∫øu c·∫ßn message queue ‚Üí g·ªçi t·ªõi RabbitMQ.

				N·∫øu c·∫ßn static file ‚Üí truy v·∫•n t·ª´ S3 bucket.

				Domain n·ªôi b·ªô (db01, mc01, rmq01) ƒë∆∞·ª£c ph√¢n gi·∫£i nh·ªù Private DNS Zone (Route53).

			Response ‚Üí quay ng∆∞·ª£c l·∫°i cho user th√¥ng qua Load Balancer.
	
		C√°c th√†nh ph·∫ßn ch√≠nh:

			DNS Zones (GoDaddy / Route53):

				Ng∆∞·ªùi d√πng (Users) truy c·∫≠p ·ª©ng d·ª•ng qua t√™n mi·ªÅn (domain).

				DNS (v√≠ d·ª• GoDaddy ho·∫∑c Route53) ph√¢n gi·∫£i t√™n mi·ªÅn th√†nh ƒë·ªãa ch·ªâ IP c·ªßa Application Load Balancer.

			Application Load Balancer (ALB):

				Nh·∫≠n request t·ª´ ng∆∞·ªùi d√πng qua HTTP/HTTPS.

				ALB c√≥ Security Group ri√™ng ƒë·ªÉ ki·ªÉm so√°t ai ƒë∆∞·ª£c ph√©p truy c·∫≠p (v√≠ d·ª•: m·ªü c·ªïng 80, 443).

				ALB ph√¢n ph·ªëi request ƒë·∫øn c√°c Tomcat Instances ph√≠a sau.

			Auto Scaling Group (ASG) + Tomcat Instances:

				C√°c EC2 instances ch·∫°y ·ª©ng d·ª•ng (Java/Tomcat).

				ASG ƒë·∫£m b·∫£o s·ªë l∆∞·ª£ng instances ph√π h·ª£p (scale in/out).

				C√≥ Security Group ri√™ng, ch·ªâ cho ph√©p traffic ƒë·∫øn t·ª´ ALB.

			Amazon S3 Bucket:

				L∆∞u tr·ªØ static files (h√¨nh ·∫£nh, video, t√†i li·ªáu).

				·ª®ng d·ª•ng Tomcat c√≥ th·ªÉ ƒë·ªçc/ghi file t·ª´ ƒë√¢y.

			Amazon Route53 (Private DNS Zones):

				D√πng ƒë·ªÉ ph√¢n gi·∫£i n·ªôi b·ªô trong VPC.

				V√≠ d·ª•:

					db01 ‚Üí IP MySQL instance

					mc01 ‚Üí IP Memcache instance

					rmq01 ‚Üí IP RabbitMQ instance

				ƒêi·ªÅu n√†y gi√∫p ·ª©ng d·ª•ng (Tomcat) g·ªçi service b·∫±ng t√™n domain thay v√¨ IP c·ª©ng.

			Backend Instances (c√≥ Security Group ri√™ng):

				MySQL Instances: Database server (c√≥ th·ªÉ l√† RDS ho·∫∑c EC2 c√†i MySQL).

				Memcache Instances: Cache ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.

				RabbitMQ Instances: Message broker, d√πng ƒë·ªÉ giao ti·∫øp phi ƒë·ªìng b·ªô gi·ªØa c√°c service.
				
	T·ª´ EC2 instance -> T·∫°o image -> T·∫°o launch template v√† th√™m image m·ªõi t·∫°o v√†o -> tao auto scaling group
	v√† th√™m launch template m·ªõi t·∫°o v√†o

	- Command:
	
		- cmd: mysql -u admin -padmin123 accounts
		
			mysql ‚Üí g·ªçi MySQL client (d√πng ƒë·ªÉ k·∫øt n·ªëi v√† l√†m vi·ªác v·ªõi MySQL server).

			-u admin ‚Üí ch·ªâ ƒë·ªãnh username ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o MySQL l√† admin.

			-padmin123

				-p d√πng ƒë·ªÉ nh·∫≠p m·∫≠t kh·∫©u.

				·ªû ƒë√¢y vi·∫øt li·ªÅn lu√¥n admin123 ‚Üí nghƒ©a l√† m·∫≠t kh·∫©u l√† admin123.

				(N·∫øu ch·ªâ vi·∫øt -p th√¥i th√¨ MySQL s·∫Ω h·ªèi m·∫≠t kh·∫©u sau khi enter).

			accounts ‚Üí ƒë√¢y l√† t√™n database mu·ªën truy c·∫≠p sau khi login.

				Nghƒ©a l√† sau khi k·∫øt n·ªëi th√†nh c√¥ng, b·∫°n s·∫Ω ·ªü ngay trong database accounts.
		
			K·∫øt n·ªëi t·ªõi MySQL server b·∫±ng user admin, password admin123, v√† ch·ªçn s·∫µn database accounts ƒë·ªÉ l√†m vi·ªác.
			
		- cmd: ping -c 4 db01.vprofile.in
		
			C·∫•u tr√∫c l·ªánh:

				ping: c√¥ng c·ª• ki·ªÉm tra k·∫øt n·ªëi m·∫°ng (ICMP echo request/reply).

				-c 4: option -c (count) ‚Üí g·ª≠i 4 g√≥i tin ICMP r·ªìi d·ª´ng (n·∫øu kh√¥ng c√≥, ping s·∫Ω ch·∫°y v√¥ h·∫°n ƒë·∫øn khi b·∫°n b·∫•m Ctrl+C).

				db01.vprofile.in: hostname / domain m√† b·∫°n mu·ªën ki·ªÉm tra k·∫øt n·ªëi.

			Qu√° tr√¨nh di·ªÖn ra

				DNS Resolution

					M√°y c·ªßa b·∫°n s·∫Ω h·ªèi DNS ƒë·ªÉ l·∫•y ƒë·ªãa ch·ªâ IP c·ªßa db01.vprofile.in.

					N·∫øu trong Route 53 (ho·∫∑c DNS kh√°c) b·∫°n c√≥ record cho db01.vprofile.in, n√≥ s·∫Ω tr·∫£ v·ªÅ
					IP (c√≥ th·ªÉ l√† private ho·∫∑c public).

				G·ª≠i ICMP Echo Request

					Ping s·∫Ω g·ª≠i 4 g√≥i ICMP ƒë·∫øn IP v·ª´a resolve ƒë∆∞·ª£c.

				Nh·∫≠n ICMP Echo Reply

					N·∫øu host tr·∫£ l·ªùi, b·∫°n s·∫Ω th·∫•y th·ªùi gian ph·∫£n h·ªìi (time=xx ms) cho m·ªói g√≥i.

				K·∫øt qu·∫£ th·ªëng k√™

					Sau khi g·ª≠i 4 g√≥i, ping s·∫Ω in th·ªëng k√™:

						S·ªë g√≥i g·ª≠i/nh·∫≠n

						T·ª∑ l·ªá packet loss (%)

						Th·ªùi gian trung b√¨nh / min / max / stddev

			√ù nghƒ©a trong tr∆∞·ªùng h·ª£p n√†y

				db01.vprofile.in th∆∞·ªùng l√† record trong Route 53 ho·∫∑c DNS b·∫°n t·ª± qu·∫£n l√Ω.

				N·∫øu b·∫°n ƒë√£ t·∫°o A record (v√≠ d·ª• db01.vprofile.in -> 10.0.1.5), th√¨ ping n√†y s·∫Ω ki·ªÉm tra xem t·ª´ m√°y b·∫°n c√≥ k·∫øt
				n·ªëi ƒë·∫øn server db01 kh√¥ng.

				N·∫øu record kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng resolve ƒë∆∞·ª£c ‚Üí b√°o l·ªói ‚Äúunknown host‚Äù.

				N·∫øu resolve ƒë∆∞·ª£c nh∆∞ng kh√¥ng ping ƒë∆∞·ª£c ‚Üí c√≥ th·ªÉ b·ªã firewall, security group, ho·∫∑c ICMP b·ªã ch·∫∑n.
		
		- cmd: $ aws configure
		
				AWS Access Key ID [****************ADE6]: AKIA5KBUVI7M7FM7RC5A
				AWS Secret Access Key [****************dUht]: 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
				Default region name [us-east-1]: us-east-1
				Default output format [json]: json
		
		- cmd: vim ~/.aws/credentials
		- cmd: vim ~/.aws/config
		- cmd: aws s3 cp target/vprofile-v2.war s3://vprofile-las-artifactss12311
		- cmd: aws s3 ls s3://vprofile-las-artifactss12311/
		- cmd: aws-cli --classic
		- cmd: aws s3 cp s3://vprofile-las-artifactss12311/vprofile-v2.war /tmp/
		- cmd: systemctl stop tomcat10
		- cmd: systemctl start tomcat10
		- cmd: ls /var/lib/tomcat10/webapps/
		- cmd: cp /tmp/vprofile-v2.war /var/lib/tomcat10/webapps/ROOT.war
		- cmd: ls /var/lib/tomcat10/webapps
		- cmd: sudo yum install -y telnet
		- cmd: telnet db01.vprofile.in 3306
		
		
	- Flow khi b·∫°n mua domain ·ªü GoDaddy v√† mu·ªën d√πng v·ªõi AWS Certificate Manager (ACM):
	
		1. Mua domain ·ªü GoDaddy

			B·∫°n ƒëƒÉng k√Ω domain t·∫°i GoDaddy

			V√≠ d·ª• b·∫°n mua: myawsdemo.com.

			Sau khi thanh to√°n xong, domain n√†y s·∫Ω ƒë∆∞·ª£c qu·∫£n l√Ω trong GoDaddy DNS.

		2. T·∫°o Hosted Zone trong Route 53

			V√†o AWS ‚Üí Route 53 ‚Üí Hosted Zones ‚Üí Create hosted zone.

			Nh·∫≠p t√™n domain c·ªßa b·∫°n (myawsdemo.com).

			Route 53 s·∫Ω sinh ra 4 record NS (Name Servers) v√† 1 record SOA.

		3. C·∫≠p nh·∫≠t DNS ·ªü GoDaddy

			V√†o GoDaddy ‚Üí My Domains ‚Üí DNS Management.

			Thay 4 NS m·∫∑c ƒë·ªãnh c·ªßa GoDaddy b·∫±ng 4 NS t·ª´ AWS Route 53.

			ƒêi·ªÅu n√†y nghƒ©a l√†: t·ª´ gi·ªù m·ªçi request t·ªõi domain s·∫Ω ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi AWS.

			Th·ªùi gian c·∫≠p nh·∫≠t c√≥ th·ªÉ m·∫•t 5 ph√∫t ‚Äì 24h (th∆∞·ªùng l√† v√†i ph√∫t).

		4. Y√™u c·∫ßu ch·ª©ng ch·ªâ trong ACM

			V√†o AWS Certificate Manager ‚Üí Request a certificate.

			Nh·∫≠p domain c·∫ßn SSL:

			myawsdemo.com

			*.myawsdemo.com (n·∫øu b·∫°n mu·ªën wildcard cho subdomain).

			Ch·ªçn DNS validation.

			ACM s·∫Ω ƒë∆∞a ra 1 record CNAME.

		5. Th√™m CNAME v√†o Route 53

			V√†o Route 53 ‚Üí Hosted Zone ‚Üí Create record.

			Th√™m CNAME do ACM cung c·∫•p.

			Ch·ªù AWS x√°c th·ª±c (v√†i ph√∫t ‚Äì 30 ph√∫t).

			Khi tr·∫°ng th√°i certificate = Issued ‚Üí b·∫°n ƒë√£ c√≥ ch·ª©ng ch·ªâ h·ª£p l·ªá.

		6. S·ª≠ d·ª•ng certificate

			CloudFront ‚Üí ch·ªçn certificate trong ph·∫ßn SSL.

			ALB (Application Load Balancer) ‚Üí add certificate v√†o Listener 443.

			API Gateway ‚Üí add certificate cho Custom Domain.
				
	- Tr∆∞·ªùng h·ª£p t·∫°o load blancer nh∆∞ng kh√¥ng mua domain:

		HTTPS ·ªü Load Balancer (ALB/NLB/CLB)

			Khi b·∫°n t·∫°o Load Balancer v√† mu·ªën l·∫Øng nghe HTTPS (port 443), AWS s·∫Ω y√™u c·∫ßu b·∫°n ch·ªçn:

				SSL certificate (t·ª´ AWS Certificate Manager ‚Äì ACM, ho·∫∑c import t·ª´ ngo√†i).

			T·∫°i sao certificate l·∫°i c·∫ßn domain?

				Certificate SSL/TLS lu√¥n g·∫Øn v·ªõi m·ªôt t√™n mi·ªÅn (VD: myapp.com, *.myapp.com).

				Tr√¨nh duy·ªát khi ng∆∞·ªùi d√πng truy c·∫≠p https://myapp.com s·∫Ω ki·ªÉm tra:

				Domain trong URL c√≥ kh·ªõp v·ªõi domain trong certificate kh√¥ng?

				N·∫øu kh·ªõp ‚Üí tr√¨nh duy·ªát hi·ªÉn th·ªã ·ªï kh√≥a xanh üîí.

				N·∫øu kh√¥ng kh·ªõp ‚Üí hi·ªán warning ‚ÄúConnection not secure‚Äù.

				V√¨ v·∫≠y, certificate kh√¥ng th·ªÉ c·∫•p cho DNS m·∫∑c ƒë·ªãnh c·ªßa AWS ELB (xxxxx.elb.amazonaws.com), tr·ª´
				khi b·∫°n c√≥ ch·ª©ng ch·ªâ wildcard ch√≠nh th·ª©c bao tr√πm domain ƒë√≥ (c√°i n√†y b·∫°n kh√¥ng s·ªü h·ªØu ƒë∆∞·ª£c).

			Khi kh√¥ng c√≥ domain

				B·∫°n v·∫´n c√≥ th·ªÉ t·∫°o Load Balancer v·ªõi listener HTTP (port 80) ‚Üí truy c·∫≠p b√¨nh th∆∞·ªùng qua DNS m·∫∑c ƒë·ªãnh.

				Nh∆∞ng n·∫øu ch·ªçn HTTPS (443), AWS s·∫Ω b·∫Øt bu·ªôc b·∫°n ch·ªçn certificate.

				B·∫°n s·∫Ω kh√¥ng c√≥ certificate h·ª£p l·ªá n·∫øu kh√¥ng s·ªü h·ªØu domain ‚Üí kh√¥ng th·ªÉ ho√†n t·∫•t HTTPS chu·∫©n.

				C√≥ th·ªÉ import self-signed certificate ƒë·ªÉ test, nh∆∞ng tr√¨nh duy·ªát s·∫Ω lu√¥n b√°o l·ªói b·∫£o m·∫≠t.

	- T·∫°i sao b·∫°n t·∫°o record trong Route 53 Hosted Zone tr·ªè t·ªõi private IP c·ªßa EC2 v√† khi n√†o n√™n
	l√†m v·∫≠y, k√®m v√≠ d·ª• v√† best-practice:
	
		√ù nghƒ©a / m·ª•c ƒë√≠ch

			DNS n·ªôi b·ªô (Private DNS): Khi b·∫°n t·∫°o Private Hosted Zone (li√™n k·∫øt v·ªõi VPC) v√† th√™m A record tr·ªè
			t·ªõi private IP c·ªßa EC2, c√°c instance trong VPC s·∫Ω c√≥ th·ªÉ resolve t√™n (v√≠ d·ª• db01.internal.example.com)
			th√†nh IP n·ªôi b·ªô (10.x.x.x). D√πng cho giao ti·∫øp n·ªôi b·ªô gi·ªØa service m√† kh√¥ng l·ªô ra Internet.

			D·ªÖ qu·∫£n l√Ω h∆°n: Thay v√¨ hardcode IP v√†o config ·ª©ng d·ª•ng b·∫°n d√πng t√™n d·ªÖ ƒë·ªçc/transfer: mysql -h db01.internal.example.com.
			Khi ph·∫£i thay ƒë·ªïi instance, b·∫°n ch·ªâ c·∫ßn update DNS record, kh√¥ng ph·∫£i edit m·ªçi config.

			Service discovery ƒë∆°n gi·∫£n: D√πng t√™n c·ªë ƒë·ªãnh cho DB, cache, mq, v.v. ƒë·ªÉ c√°c app t√¨m t·ªõi service ƒë√≥.

			Split-horizon / Private vs Public: B·∫°n c√≥ th·ªÉ c√≥ m·ªôt zone public (cho internet) v√† m·ªôt private (cho VPC); private ch·ªâ
			gi·∫£i quy·∫øt trong VPC ‚Äî an to√†n h∆°n.

			H·ªó tr·ª£ failover / swap: N·∫øu b·∫°n thay instance b·∫±ng instance m·ªõi, ƒë·ªïi record sang IP m·ªõi ‚Üí ·ª©ng d·ª•ng chuy·ªÉn sang server m·ªõi
			nhanh h∆°n.

			T√≠ch h·ª£p v·ªõi ALB/ENI/Elastic IP: Thay v√¨ tr·ªè tr·ª±c ti·∫øp IP c·ªßa instance, th∆∞·ªùng t·ªët h∆°n l√† tr·ªè t·ªõi internal ALB (d√πng Alias record)
			ho·∫∑c d√πng ENI c·ªë ƒë·ªãnh ƒë·ªÉ tr√°nh ph·∫£i update DNS khi instance thay ƒë·ªïi.

		V√≠ d·ª• th·ª±c t·∫ø

			T·∫°o Private Hosted Zone internal.example.com li√™n k·∫øt v·ªõi VPC vpc-abc123.

			T·∫°o A record:

				db01.internal.example.com ‚Üí 10.0.1.5

				cache.internal.example.com ‚Üí 10.0.1.6

			Tr√™n EC2 trong c√πng VPC: ping db01.internal.example.com ‚Üí s·∫Ω resolve th√†nh 10.0.1.5.

		C√°ch t·∫°o (t√≥m t·∫Øt)

			AWS Console ‚Üí Route 53 ‚Üí Hosted zones ‚Üí Create hosted zone.

				Type: Private hosted zone for Amazon VPC

				Enter domain: internal.example.com

				Associate with one or more VPCs.

			V√†o zone ‚Üí Create record ‚Üí Type: A ‚Üí Name: db01 ‚Üí Value: 10.0.1.5 ‚Üí Save.

		L∆∞u √Ω & best practices

			Private IP c√≥ th·ªÉ ƒë·ªïi n·∫øu b·∫°n terminate instance; stop/start th∆∞·ªùng gi·ªØ private IPv4 ch√≠nh nh∆∞ng kh√¥ng tuy·ªát
			ƒë·ªëi (n·∫øu b·∫°n detach ENI hay thay instance th√¨ thay ƒë·ªïi). ƒê·ªÉ ·ªïn ƒë·ªãnh:

				G√°n Elastic Network Interface (ENI) v·ªõi private IP tƒ©nh, ho·∫∑c

				D√πng internal load balancer (ALB/NLB) v√† t·∫°o Alias record tr·ªè t·ªõi ALB (kh√¥ng ph·∫£i IP) ‚Äî ALB x·ª≠ l√Ω backend thay ƒë·ªïi.

			TTL: ƒë·∫∑t TTL th·∫•p (v√≠ d·ª• 60s) n·∫øu b·∫°n hay thay ƒë·ªïi record, nh∆∞ng TTL th·∫•p tƒÉng s·ªë query DNS.

			T·ª± ƒë·ªông h√≥a: N·∫øu b·∫°n autoscale, update DNS th·ªß c√¥ng s·∫Ω b·∫•t ti·ªán ‚Äî d√πng AWS Cloud Map / Service Discovery ho·∫∑c t·ª±
			ƒë·ªông ƒëƒÉng k√Ω qua userdata / script / automation (Lambda) ƒë·ªÉ c·∫≠p nh·∫≠t record khi instance kh·ªüi t·∫°o/terminate.

			S·ª≠ d·ª•ng SRV / CNAME khi ph√π h·ª£p (v√≠ d·ª• SRV cho service discovery c√≥ port).

			B·∫£o m·∫≠t: Private hosted zone ch·ªâ tr·∫£ l·ªùi t·ª´ VPC ƒë√£ li√™n k·∫øt ‚Äî an to√†n n·ªôi b·ªô.

			Kh√¥ng tr·ªè public records t·ªõi private IP (s·∫Ω kh√¥ng truy c·∫≠p ƒë∆∞·ª£c t·ª´ Internet).

		Khi n√†o KH√îNG n√™n d√πng tr·ª±c ti·∫øp private IP record

			N·∫øu b·∫°n c√≥ autoscaling ho·∫∑c th∆∞·ªùng xuy√™n thay instance ‚Üí n√™n d√πng internal ALB ho·∫∑c Cloud Map.

			N·∫øu service c·∫ßn load-balancing ho·∫∑c health checks ‚Üí d√πng ALB/NLB thay v√¨ tr·ªè th·∫≥ng v√†o m·ªôt instance.
		
		
--- Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:

	Flow ch√≠nh:
	
		Users (Ng∆∞·ªùi d√πng)

			Ng∆∞·ªùi d√πng g·ª≠i request (HTTP/HTTPS) t·ª´ tr√¨nh duy·ªát ‚Üí v√≠ d·ª•: https://myapp.com.

		Amazon Route 53

			ƒê√¢y l√† DNS service.

			N√≥ s·∫Ω ph√¢n gi·∫£i t√™n mi·ªÅn (myapp.com) ‚Üí IP/public endpoint c·ªßa h·ªá th·ªëng.

		Amazon CloudFront (CDN)

			Gi√∫p cache n·ªôi dung tƒ©nh (·∫£nh, CSS, JS, video) g·∫ßn v·ªõi ng∆∞·ªùi d√πng nh·∫•t.

			L√†m ·ª©ng d·ª•ng nhanh h∆°n, gi·∫£m t·∫£i cho backend.

		Application Load Balancer (ALB)

			ƒêi·ªÅu ph·ªëi request ƒë·∫øn c√°c EC2 instances trong Elastic Beanstalk.

			ƒê·∫£m b·∫£o high availability (HA), scale theo t·∫£i.

		Elastic Beanstalk

			D·ªãch v·ª• t·ª± ƒë·ªông deploy v√† qu·∫£n l√Ω ·ª©ng d·ª•ng.

			Trong h√¨nh, Beanstalk qu·∫£n l√Ω m·ªôt Auto Scaling Group ch·ª©a nhi·ªÅu EC2 ch·∫°y Apache Tomcat (·ª©ng d·ª•ng Java).

			Khi t·∫£i cao ‚Üí t·ª± ƒë·ªông th√™m EC2, khi t·∫£i th·∫•p ‚Üí gi·∫£m EC2.

		Amazon CloudWatch

			Theo d√µi metric (CPU, memory, request count...).

			D√πng ƒë·ªÉ trigger auto scaling trong Elastic Beanstalk.

		Artifacts in S3 Bucket

			Code (WAR file, artifact) ƒë∆∞·ª£c upload v√†o S3 bucket.

			Elastic Beanstalk l·∫•y artifact n√†y ƒë·ªÉ deploy v√†o Tomcat.

		Apache Tomcat

			N∆°i ·ª©ng d·ª•ng Java th·ª±c s·ª± ch·∫°y (x·ª≠ l√Ω business logic).

			V√≠ d·ª•: Spring Boot app ho·∫∑c Java EE app.

		Amazon MQ

			Message broker (gi·ªëng ActiveMQ).

			D√πng khi h·ªá th·ªëng c√≥ asynchronous communication gi·ªØa c√°c service.

			V√≠ d·ª•: app g·ª≠i message v√†o queue, service kh√°c x·ª≠ l√Ω sau.

		MySQL (Amazon RDS)

			Database ch√≠nh, l∆∞u d·ªØ li·ªáu ·ª©ng d·ª•ng.

		Memcached

			Cache server ƒë·ªÉ tƒÉng t·ªëc truy v·∫•n (gi·∫£m t·∫£i cho MySQL).

			L∆∞u tr·ªØ t·∫°m th·ªùi d·ªØ li·ªáu hay d√πng (session, query result).
		
	- Command:
	
		- cmd: apt update && apt install mysql-client git -y
		- cmd: mysql -h vprofile-rds-reach.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pxUScYu55cFzH5NxM8Zc5 accounts < src/main/resources/db_backup.sql
		
--- Continuous Integration with Jenkins:

	Continuous Integration with Jenkins - Flow of Continuous Integration Pipeline:
	
		Developer (Git)

			Developer vi·∫øt code v√† push l√™n GitHub repository.

			ƒê√¢y l√† n∆°i l∆∞u tr·ªØ source code ch√≠nh.

		GitHub ‚Üí Jenkins

			Khi c√≥ commit m·ªõi, Jenkins s·∫Ω ƒë∆∞·ª£c trigger (c√≥ th·ªÉ qua Webhook).

			Jenkins b·∫Øt ƒë·∫ßu pipeline.

		Fetch Code (Git)

			Jenkins s·ª≠ d·ª•ng Git plugin ƒë·ªÉ l·∫•y code t·ª´ GitHub v·ªÅ m√¥i tr∆∞·ªùng build.

		Build (Maven)

			Jenkins d√πng Maven ƒë·ªÉ compile source code v√† build project.

			Maven s·∫Ω qu·∫£n l√Ω dependencies (th∆∞ vi·ªán b√™n ngo√†i) v√† ƒë√≥ng g√≥i project (VD: file .jar ho·∫∑c .war).

		Unit Test (Maven)

			Sau khi build xong, Maven ch·∫°y Unit Test ƒë·ªÉ ƒë·∫£m b·∫£o code ho·∫°t ƒë·ªông ƒë√∫ng.

			N·∫øu test fail ‚Üí pipeline s·∫Ω d·ª´ng.

		Code Analysis (SonarQube)

			Code ƒë∆∞·ª£c g·ª≠i sang SonarQube ƒë·ªÉ ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng:

				Bugs, vulnerabilities, code smells, duplicated code.

			SonarQube c√≥ Quality Gate: n·∫øu code kh√¥ng ƒë·∫°t y√™u c·∫ßu, pipeline d·ª´ng l·∫°i.

		Upload Artifact (Nexus OSS)

			N·∫øu code pass SonarQube, Jenkins upload artifact (jar/war) l√™n Nexus Repository Manager (Sonatype).

			Nexus OSS l√† n∆°i l∆∞u tr·ªØ artifacts, ƒë·ªÉ d√πng cho c√°c m√¥i tr∆∞·ªùng kh√°c (staging, production).
			
	Ch√∫ √Ω l√† ƒë·ªÉ ch·∫°y ƒë∆∞·ª£c Sonar Qube c·∫ßn th√™m authentication token c·ªßa n√≥ v√†o Jenkins m·ªõi ch·∫°y ƒë∆∞·ª£c
	
	Google search:
	
		sonar scanner pipiline script
		
		nexusartifactuploader
		
		slack login
		
		wget for git hash how to add more utilities
		
	Git Weebhook, Poll SCM
	
	LDAP
	
	
	
	
		
	Continuous Integration with Jenkins - M·ª•c ƒë√≠ch c·ªßa Jenkins URL:

		N√≥ ƒë∆∞·ª£c Jenkins (v√† plugin) d√πng l√†m ƒë·ªãa ch·ªâ ch√≠nh ƒë·ªÉ:

			Sinh link ƒë·∫ßy ƒë·ªß trong th√¥ng b√°o
			
				V√≠ d·ª• b·∫°n g·ª≠i Slack, Email, hay Webhook th√¨ Jenkins s·∫Ω t·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫øn job/build c·ª• th·ªÉ.

					N·∫øu kh√¥ng c√≥ Jenkins URL, th√¥ng b√°o c√≥ th·ªÉ ch·ªâ ch·ª©a ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi (v√≠ d·ª• /job/vprofile-pipeline/23/) ‚Üí ng∆∞·ªùi
					ngo√†i s·∫Ω kh√¥ng click ƒë∆∞·ª£c.

					Khi b·∫°n c·∫•u h√¨nh Jenkins URL (v√≠ d·ª• http://jenkins.mycompany.com:8080/), Jenkins s·∫Ω t·∫°o link ƒë·∫ßy ƒë·ªß:

						http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/


			Jenkins agents (slave nodes) k·∫øt n·ªëi v·ªÅ master
			
				M·ªôt s·ªë tr∆∞·ªùng h·ª£p Jenkins agent c·∫ßn bi·∫øt ch√≠nh x√°c URL c·ªßa master ƒë·ªÉ trao ƒë·ªïi th√¥ng tin, t·∫£i config, ho·∫∑c trigger build.

			C√°c plugin ph·ª• thu·ªôc

				Slack, Email-ext plugin, GitHub plugin, Bitbucket plugin... c·∫ßn Jenkins URL ƒë·ªÉ t·∫°o link "Build result" trong notification.

				V√≠ d·ª• Slack message c√≥:

					Build #23 failed. More info at: http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/

	- Commands:
	
		- cmd: sudo apt install fontconfig openjdk-21-jre
		- cmd: sudo wget -O /etc/apt/keyrings/jenkins-keyring.asc \
				https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
		- cmd: echo "deb [signed-by=/etc/apt/keyrings/jenkins-keyring.asc]" \
				  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
				  /etc/apt/sources.list.d/jenkins.list > /dev/null
		- cmd: sudo apt install jenkins
		- cmd: systemctl status jenkins
		- cmd: ls /var/lib/jenkins/
		- cmd: cat /var/lib/jenkins/secrets/initialAdminPassword
		- cmd: apt install openjdk-17-jdk -y
		- cmd: ls /usr/lib/jvm/
		- cmd: sudo apt update && sudo apt install maven -y
		- cmd: systemctl status nexus
		- cmd: ls /opt/nexus/
		- cmd: cat /opt/nexus/sonatype-work/nexus3/admin.password
		- cmd: cd /var/lib/jenkins/
		- cmd: cd /var/lib/jenkins/workspace
		- cmd: systemctl restart jenkins
		- cmd: sudo snap install aws-cli --classic
		- cmd:  # Add Docker's official GPG key:
				sudo apt-get update
				sudo apt-get install ca-certificates curl
				sudo install -m 0755 -d /etc/apt/keyrings
				sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
				sudo chmod a+r /etc/apt/keyrings/docker.asc

				# Add the repository to Apt sources:
				echo \
				  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
				  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
		- cmd: systemctl status docker
		- cmd: docker images
		- cmd: su - jenkins
		- cmd: usermod -a -G docker jenkins
		- cmd: id jenkins
		- cmd: reboot
		- cmd: ssh-keygen.exe
		- cmd: ls ~/.ssh/
		- cmd: git@github.com:ngoctuanqng/jenkinstriggers.git
		- cmd: choco install wget
		- cmd: wget -q --auth-no-challenge --user username --password password --output-document - 'http://JENNKINS_IP:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
			Example:
		
				wget -q --auth-no-challenge --user admin --password ngoctuan99@ --output-document - 'http://54.234.134.51:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
		- cmd: curl -I -X POST http://username:APItoken@Jenkins_IP:8080/job/JOB_NAME/build?token=TOKENNAME
				-H "Jenkins-Crumb:CRUMB"
				
			Example:
		
				curl -I -X POST http://admin:11cde1b5cb464093f62e8f88edfd14388d@54.234.134.51:8080/job/Build/build?token=mybuildtoken
				-H "Jenkins-Crumb:b05d9f0d30cc94463982e15ef5dea44b39cddc6ed3a5a9254fef0bc151afe76b"
		
		- cmd: adduser devops
		- cmd: chown devops.devops /opt/jenkins-slave -R
		- cmd: vim /etc/ssh/sshd_config
		- cmd: systemctl restart ssh
		- cmd: id devops
		


		
		
		
		
		
		
		
		

	- Jenkins: Manage Jenkins -> Tools -> JDK installations -> Add JDK
	- Jenkins: Manage Jenkins -> Tools -> Maven installations
	- Jenkins: Manage Jenkins -> Tools -> SonarQube Scanner installations
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> S3 publisher
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Build Timestamp
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Maven Integration
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Utility Steps
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Nexus Artifact Uploader
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> SonarQube Scanner
	- Jenkins: Manage Jenkins -> System -> Build Timestamp
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Add SonarQube
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Environment variables
	- Jenkins: Manage Jenkins -> System -> Slack
	- Jenkins: Manage Jenkins -> Credentials -> Stores scoped to Jenkins -> System -> Global credentials (unrestricted) -> Add Credentials
	- Jenkins: Manage Jenkins -> Nodes -> New node
	- Jenkins: Manage Jenkins -> Security -> Git Host Key Verification Configuration -> Accept first connection
	- Jenkins: New Item -> Freestyle project -> Source Code Management -> Git
	- Jenkins: New Item -> Freestyle project -> Build Steps -> Invoke top-level Maven targets
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Archive the artifacts
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Publish artifacts to S3 Bucket
	- Jenkins: New Item -> Freestyle project -> Copy from
	- Jenkins: New Item -> Pipeline -> Pipeline script
	- Jenkins: New Item -> Pipeline -> Pipeline script from SCM
	- Jenkins: New Item -> General -> This project is parameterized -> String parameter
	- Jenkins: Jenkins instance -> Build Now	
	- Jenkins: Jenkins instance -> Configure
	- Jenkins: Jenkins instance -> Workspace
	- Jenkins: Jenkins instance -> Stages
	- Jenkins: Jenkins result -> Workspaces	
	- AWS: EC2 -> Instances -> Instance state -> Reboot instance
	- AWS: Amazon Elastic Container Registry
	- AWS: Amazon Elastic Container Service -> Clusters -> Create cluster
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Create
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Update service
	- AWS: Amazon Elastic Container Service -> Task definitions -> Create new task definition
	- Nexus: Browse
	- SonarQube -> Project -> Project instance
	- SonarQube -> Project -> Project instance -> Project setting -> Quality gate
	- SonarQube -> Project -> Project instance -> Project setting -> Webhooks
	- SonarQube -> Quality Gates -> create
	- SonarQube -> Quality Gates -> Quality Gates instance -> Unlock editing -> Add condition
	- Nexus -> Configuration -> Repositories -> Create Repository -> maven2(hosted)
	- Nexus -> Browse -> instance
	- Github -> Settings -> SSH and GPG keys -> New SSH key
	- Github -> Project instance -> Settings -> Webhooks -> Add webhook
		
	- Code trong Execute Shell:
	
		Case 1:
		
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_ID.war
			
		Case 2:
		
			mkdir -p versions
			cp target/vprofile-v2.war version/vpro$VERSION.war

		Case 3:
					
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_TIMESTAMP.war
	
	- Code Jenkinsfile:

		Case 1:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Archiving artifact"
								archiveArtifacts artifacts: '**/*.war'
							}
						}
					}
				}
			}

		Case 2:
		
			def COLOR_MAP = [
				'SUCCESS': 'good',
				'FAILURE': 'danger',
			]

			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Now archiving it..."
								archiveArtifacts artifacts: '**/target/*.war'
							}
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage('Sonar Code Analysis') {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
							withSonarQubeEnv('sonarserver') {
								sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
								-Dsonar.projectName=vprofile \
								-Dsonar.projectVersion=1.0 \
								-Dsonar.sources=src/ \
								-Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
								-Dsonar.junit.reportsPath=target/surefire-reports/ \
								-Dsonar.jacoco.reportsPath=target/jacoco.exec \
								-Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
							}
						}
					}

					stage("Quality Gate") {
					  steps {
						timeout(time: 1, unit: 'HOURS') {
						  waitForQualityGate abortPipeline: true
						}
					  }
					}
					stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
								nexusVersion: 'nexus3',
								protocol: 'http',
								nexusUrl: '172.31.30.253:8081',
								groupId: 'QA',
								version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
								repository: 'vprofile-repo',
								credentialsId: 'nexuslogin',
								artifacts: [
									[artifactId: 'vproapp',
									classifier: '',
									file: 'target/vprofile-v2.war',
									type: 'war']
								]
							)
						}
					}
				}

				post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}
			}

		Case 3:
		
			def COLOR_MAP = [
				'SUCCESS': 'good', 
				'FAILURE': 'danger',
			]
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}

				stages {

					stage('test slack'){
						steps{
							sh 'NotARealCommand'

						}
					}
					stage('Fetch code') {
						steps {
						   git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar6.2'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					 stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
							  nexusVersion: 'nexus3',
							  protocol: 'http',
							  nexusUrl: '172.31.25.14:8081',
							  groupId: 'QA',
							  version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
							  repository: 'vprofile-repo',
							  credentialsId: 'nexuslogin',
							  artifacts: [
								[artifactId: 'vproapp',
								 classifier: '',
								 file: 'target/vprofile-v2.war',
								 type: 'war']
							  ]
							)
						}
					}


				}

			  post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}

			}
			
		Case 4:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					// stage('Remove Container Images'){
					//     steps{
					//         sh 'docker rmi -f $(docker images -a -q)'
					//     }
					// }

			  }
			}
			
		Case 5:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
					cluster = "vprofile"
					service = "vprofileappsvc"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					stage('Remove Container Images'){
						steps{
							sh 'docker rmi -f $(docker images -a -q)'
						}
					}


					stage('Deploy to ecs') {
					  steps {
						withAWS(credentials: 'awscreds', region: 'us-east-1') {
						sh 'aws ecs update-service --cluster ${cluster} --service ${service} --force-new-deployment'
						   }
					  }
					}

			  }
			}
			
	Dockerfile code:
	
		Case 1:
		
			FROM tomcat:10-jdk21
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webapps
			
			
--- Python:

	Google search:
	
		python jenkins library
		
		boto3 python

	- Commands:
	
		- cmd: python
		- cmd: ls -l first-python-code.py
		- cmd: ./first-python-code.py
		- cmd: python first-python-code.py
		- cmd: yum search python3
		- cmd: yum install python3 -y
		- cmd: wget https://bootstrap.pypa.io/get-pip.py
		- cmd: python3 get-pip.py
		- cmd: dnf install python3-pip -y
		- cmd: pip install 'fabric<2.0'
		- cmd: export PATH=$PATH:/usr/local/bin
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i ~/.ssh/id_rsa devops@web01
		- cmd: ssh devops@web01
		- cmd: fab -H web01 -u devops remote_exec
		- cmd: fab -H web01 -u devops web_setup:https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip,2136_kool_form_pack
		


		
		
		
		
		
		
	
	
	- Python code:
	
		Case 1:
		
			print("""
			xin chao
			cac ban
			""")
			
		Case 2:
		
			print('''
			xin chao
			cac ban
			''')
			
		Case 3:

			a = b = c = 65

			print(a)
			print(b)
			print(c)		
			
		Case 4:

			a = 65

			print("a: ",a)		
			
		Case 5:

			w, x, y, z = "alpha", "beta", 12, 5.4

			print("Variable w value is ", w)
			print("Variable w value is ", x)
			print("Variable w value is ", y)
			print("Variable w value is ", z)

			print(type(w))		
			
		Case 6:

			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_list = [str1, "DevOps", 47, num1, 1.5]
			print(first_list)		
			
			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_tuple = (str1, "DevOps", 47, num1, 1.5)
			print(first_tuple)			
			
		Case 7:
		
			user_skill = input("Enter your desired skill: ")
			print(user_skill)
			
		Case 8:
		
			import time
			time.sleep(2)
			print("test")
			
		Case 9:

			def time_activity(*args, **kwargs):
				print(args)
				print(kwargs)
			time_activity(10, 20, 10, hobby="Dance", sport="Boxing", fun="Driving", work="DevOps")
				
		Case 10:
		
			from fabric.api import *

			def greetings(msg):
				print("Good {}".format(msg))

			def system_info():
				print("Disk space.")
				local("df -h")

				print("Memory info.")
				local("free -m")

				print("System Uptime.")
				local("uptime")

			def remote_exec():
				run("hostname")
				run("uptime")
				run("df -h")
				run("free -m")
				
				sudo("yum install unzip zip wget -y")
				
			def web_setup(WEBURL, DIRNAME):
				print("###############################################")
				print("Installing dependencies")
				print("###############################################")
				sudo("yum install httpd wget unzip -y")

				print("###############################################")
				print("Start & enable service.")
				sudo("systemctl start httpd")
				sudo("systemctl enable httpd")

				print("###############################################")
				local("apt install zip unzip -y")

				print("###############################################")
				print("Downloading and pushing website to webservers.")
				print("###############################################")
				local("wget -O website.zip %s" % WEBURL)
				local("unzip -o website.zip")

				print("###############################################")
				with lcd(DIRNAME):
					local("zip -r tooplate.zip * ")
					put("tooplate.zip", "/var/www/html/", use_sudo=True)

				with cd("/var/www/html/"):
					sudo("unzip -o tooplate.zip")

				sudo("systemctl restart httpd")
				print("Website setup is done.")



	Code c·ªßa file /etc/ssh/sshd_config:
	
		Case 1:
		
			PasswordAuthentication yes
			
	Code c·ªßa visudo:
	
		Case 1:
		
			devops  ALL=(ALL)       NOPASSWD: ALL

			
--- Learn Terraform:

	- Commands:
	
		- cmd: choco install terraform
		- cmd: terraform --version
		- cmd: terraform fmt
		- cmd: terraform init
		- cmd: terraform validate
		- cmd: terraform plan
		- cmd: terraform apply
		- cmd: terraform destroy
		
	Amazon S3 -> Buckets -> instance -> Create folder
		
		
		
		
	- Google search:
	
		find ubuntu ami id aws
		
		terraform aws
		
		variable terraform
		
		terraform provisioners
		
	- C√†i ƒë·∫∑t VS Code extension:
	
		HashiCorp Terraform
		
	terraform.tfstate:
	
	File terraform.tfstate l√† tr√°i tim c·ªßa Terraform ‚Äî n√≥ ch·ª©a to√†n b·ªô tr·∫°ng th√°i th·ª±c t·∫ø c·ªßa h·∫°
	t·∫ßng m√† Terraform ƒëang qu·∫£n l√Ω.
	
	ƒê·ªãnh nghƒ©a c·ªët l√µi

		terraform.tfstate l√† file tr·∫°ng th√°i (state file) m√† Terraform d√πng ƒë·ªÉ ghi nh·∫≠n v√† theo d√µi h·∫° t·∫ßng
		th·ª±c t·∫ø sau khi b·∫°n ch·∫°y terraform apply.

			N√≥ m√¥ t·∫£ m·ªçi resource (m√°y ·∫£o, VPC, subnet, load balancer, v.v.) m√† Terraform ƒë√£ t·∫°o,

			V√† li√™n k·∫øt ch√∫ng v·ªõi code Terraform (.tf files).

		Terraform d√πng file n√†y ƒë·ªÉ:

			Bi·∫øt hi·ªán tr·∫°ng h·∫° t·∫ßng ƒëang ra sao,

			So s√°nh v·ªõi mong mu·ªën (trong code),

			T·ª´ ƒë√≥ quy·∫øt ƒë·ªãnh t·∫°o / s·ª≠a / x√≥a g√¨ khi b·∫°n ch·∫°y terraform plan ho·∫∑c terraform apply.
			
	B·∫£n ch·∫•t & c∆° ch·∫ø ho·∫°t ƒë·ªông
	
		Khi b·∫°n ch·∫°y terraform apply

			Terraform ƒë·ªçc code .tf (infrastructure as code).

			N√≥ ki·ªÉm tra file terraform.tfstate ƒë·ªÉ bi·∫øt ƒëang c√≥ g√¨ ngo√†i ƒë·ªùi th·∫≠t.

			N√≥ g·ªçi API l√™n cloud (AWS, GCP, Azure, v.v.) ƒë·ªÉ x√°c minh.

			Sau khi th·ª±c hi·ªán xong thay ƒë·ªïi, Terraform ghi l·∫°i k·∫øt qu·∫£ m·ªõi v√†o terraform.tfstate.
			
	Th√†nh ph·∫ßn ch√≠nh trong terraform.tfstate

		M·ªôt file tfstate l√† JSON, g·ªìm c√°c ph·∫ßn ch√≠nh:
		
			version	Version format c·ªßa Terraform state
			
			terraform_version	Phi√™n b·∫£n Terraform ƒë√£ t·∫°o file
			
			resources	Danh s√°ch to√†n b·ªô resource ƒëang qu·∫£n l√Ω
			
			outputs	C√°c gi√° tr·ªã output (n·∫øu b·∫°n c√≥ output trong code)
			
			serial	S·ªë version n·ªôi b·ªô ƒë·ªÉ Terraform bi·∫øt state ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t bao nhi√™u l·∫ßn
			
	Local state vs Remote state:
	
		Local state:

			M·∫∑c ƒë·ªãnh terraform.tfstate n·∫±m ngay trong th∆∞ m·ª•c d·ª± √°n.

			D·ªÖ d√πng nh∆∞ng nguy hi·ªÉm trong team (v√¨ c√≥ th·ªÉ b·ªã ghi ƒë√®, m·∫•t file, xung ƒë·ªôt...).

		Remote state (an to√†n h∆°n):

			Terraform cho ph√©p l∆∞u state t·ª´ xa (remote backend), v√≠ d·ª•:

				AWS S3

				Azure Blob Storage

				Google Cloud Storage

				Terraform Cloud / Enterprise
				
	L∆∞u √Ω b·∫£o m·∫≠t c·ª±c k·ª≥ quan tr·ªçng

		File terraform.tfstate c√≥ th·ªÉ ch·ª©a th√¥ng tin nh·∫°y c·∫£m, nh∆∞:

			M·∫≠t kh·∫©u, secret key, private IP, token, v.v.

		Kh√¥ng bao gi·ªù commit terraform.tfstate l√™n Git.
		
			H√£y th√™m v√†o .gitignore
		
	- Code file .tf:
	
		Case 1:
		
			data "aws_ami" "amiID" {
			  most_recent = true

			  filter {
				name   = "name"
				values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
			  }

			  filter {
				name   = "virtualization-type"
				values = ["hvm"]
			  }

			  owners = ["099720109477"]
			}
		
		Case 2:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = "us-east-1"
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "42.112.134.249/32"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
		Case 3:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = "dove-key"
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}

				resource "aws_ec2_instance_state" "web_state" {
					instance_id = aws_instance.web.id
					state       = "running"
				}
				
		Case 4:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:
			
				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					es-east-1 = "ami-0e2c8caa4b6378d8c"
				  }
				}
				
		Case 5:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }

				  provisioner "file" {
					source      = "web.sh"
					destination = "/tmp/web.sh"
				  }

				  connection {
					type        = "ssh"
					user        = var.webuser
					private_key = file("dovekey")
					host        = self.public_ip
				  }

				  provisioner "remote-exec" {

					inline = [
					  "chmod +x /tmp/web.sh",
					  "sudo /tmp/web.sh"
					]
				  }

				  provisioner "local-exec" {
					command = "echo ${self.private_ip} >> private_ips.txt"
				  }
				}

				resource "aws_ec2_instance_state" "web-state" {
				  instance_id = aws_instance.web.id
				  state       = "running"
				}

				output "WebPublicIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.public_ip
				}

				output "WebPrivateIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.private_ip
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}

			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "webuser" {
				  default = "ubuntu"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					us-east-1 = "ami-0360c520857e3138f"
				  }
				}
				
			web.sh:
			
				#!/bin/bash
				apt update
				apt install wget unzip apache2 -y
				systemctl start apache2
				systemctl enable apache2
				wget https://www.tooplate.com/zip-templates/2117_infinite_loop.zip
				unzip -o 2117_infinite_loop.zip
				cp -r 2117_infinite_loop/* /var/www/html/
				systemctl restart apache2
				
		Case 6:
		
			Gi·ªëng case 5 nh∆∞ng th√™m file backend.tf
			
			backend.tf:
			
				terraform {
					backend "s3" {
						bucket = "terraformstate3245612112"
						key    = "terraform/backend"
						region = "us-east-1"
					}
				}
				
				
--- Ansible:

	- Commands:
	
		- cmd: ls ~/.ssh/known_hosts
		- cmd: cat ~/.ssh/known_hosts
		- cmd: cat /dev/null > ~/.ssh/known_hosts
		- cmd: sudo apt install software-properties-common
		- cmd: sudo add-apt-repository --yes --update ppa:ansible/ansible
		- cmd: sudo apt install ansible
		- cmd: ansible --version
		- cmd: ansible-config init --disable -t all > ansible.cfg
		- cmd: chmod 400 clientkey.pem
		- cmd: ansible web01 -m ping -i inventory
		- cmd: ansible all -m ping -i inventory
		- cmd: ansible '*' -m ping -i inventory
		- cmd: ansible 'web*' -m ping -i inventory
		- cmd: ansible web01 -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=absent" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.service -a "name=httpd state=started enabled=yes" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.copy -a "src=index.html dest=/var/www/html/index.html" -i inventory --become
		- cmd: ansible webservers -m yum -a "name=httpd state-absent" -i inventory --become
		- cmd: ansible-playbook -i inventory web-db.yaml
		- cmd: ansible-playbook -i inventory web-db.yaml -v
		- cmd: ansible-playbook -i inventory web-db.yaml -vv
		- cmd: ansible-playbook -i inventory web-db.yaml -vvvv
		- cmd: ansible-playbook -i inventory web-db.yaml --syntax-check
		- cmd: ansible-playbook -i inventory web-db.yaml -C
		- cmd: ssh -i clientkey.pem ec2-user@172.31.20.155
		- cmd: yum search python | grep -i mysql
		- cmd: ansible-galaxy collection install community.mysql
		- cmd: vim /etc/ansible/ansible.cfg
		- cmd: ansible-playbook db.yaml
		- cmd: sudo touch /var/log/ansible.log
		- cmd: sudo chown ubuntu.ubuntu /var/log/ansible.log
		- cmd: cat /var/log/ansible.log
		- cmd: ansible-playbook db.yaml -vv
		- cmd: ansible-playbook -e USRNM=cliuser -e COMM=cliuser vars_precedence.yaml
		- cmd: ansible -m setup web01
		- cmd: ansible-playbook print_facts.yaml
		- cmd: rm -rf print_facts.yaml vars_precedence.yaml
		- cmd: cat /etc/chrony.conf
		- cmd: cat /etc/ntpsec/ntp.conf
		- cmd: apt  install tree  # version 2.1.1-2
		- cmd: tree
		- cmd: ansible-galaxy init post-install
		- cmd: vim roles/post-install/vars/main.yml
		- cmd: vim roles/post-install/handlers/main.yml
		- cmd: vim roles/post-install/tasks/main.yml
		- cmd: ansible-galaxy role install geerlingguy.java
		- cmd: cd .ansible/
		- cmd: export AWS_ACCESS_KEY_ID='AK123'
		- cmd: export AWS_SECRET_ACCESS_KEY='abc123'
		- cmd: vim .bashrc
		- cmd: source .bashrc
		- cmd: sudo apt install python3-pip -y
		- cmd: ansible-galaxy collection install amazon.aws
		- cmd: ansible-galaxy collection install amazon.aws --force
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	- Google search:
	
		ansible installing
		
		ansible inventory
		
		ansible to ad hoc command
		
		ansible playbooks
		
		using ansible modules and plugins
		
		mysql ansible modules
		
		ansible configuration file
		
		ansible variables
		
		ansible when condition
		
		ansible loops
		
		ansible files module
		
		ntp server in oregon
		
		ansible handler
		
		ansible roles
		
		ntp servers in india
		
		ansible galaxy
		
		ansible amazon aws
		
		ansible modules
		
	- Code trong file vprofile/excercise1/inventory:
	
		Case 1:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				web02:
				  ansible_host: 172.31.16.152
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				db01:
				  ansible_host: 172.31.20.155
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem

			  children:
			    webservers:
				  hosts:
				    web01:
				    web02:
			    dbservers:
				  hosts:
				    db01:
			    dc_oregon:
				  children:
				    webservers:
				    dbservers:
					
		Case 2:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				web02:
				  ansible_host: 172.31.16.152
				db01:
				  ansible_host: 172.31.20.155

			  children:
				webservers:
				  hosts:
					web01:
					web02:
				dbservers:
				  hosts:
					db01:
				dc_oregon:
				  children:
					webservers:
					dbservers:
				  vars:
					ansible_user: ec2-user
					ansible_ssh_private_key_file: clientkey.pem




	- N·ªôi dung trong file clientkey.pem:

		Case 1:
		
			Ch·ª©a private key c·ªßa web01
		
			-----BEGIN RSA PRIVATE KEY-----
			MIIEogIBAAKCAQEAreGWVicfl8jhOLqPUerGay/AThriECzXc8UcLLam4riCz05+
			TzZP+NeoX6dC83ZX4ssyLazcGj5OMCg2EhFqVZTuDABYsr+ucT/sdXPCP0bjf+mq
			SYT6iHpftoRfm1vk3Um61/HmSzp1vZ6YhE//x1GwWT5+4aOufGS4bCa5xPluEg+E
			IhaEZNRyNfgvc/ylblWVCM1KGkZgdhvenFUQDj8+icCWrvbFvykwW0L8D2WB+zFq
			iSDx1cKLL1zzVCQVgKfKofC3PPm+VMbwOL8HPw65g/OBnzWKrrSjrpWu5s4KKSM+
			3BXW5nRieoPI34ruAwwpYrJKUlR7t6/rBo6QgQIDAQABAoIBACLWvzN11UupMQ8X
			uh2Up7rUL3i2xDKveV+1z6ZZ1mg4xeTZek9Ot4lJVHAN6Ek1nfhP9DbYmqUbdLkL
			ZYILQT3ygBuheiQeacpBH5SM5A+fmXeIjtj6LuRneIPuU+Wh7OI1op0f15+dD/g1
			LaPdD4eVI3tOHUgCbrR3zcfFnpULfGl0+QdkZ7141FmMiAL+8tTjzfWbMYwbPWu2
			ZSIDWx7SZ/wycGDKBejmgJQHFyEn9Cwvexlt+Zh4IK/kTIXXe9XM0ZbVMF8am5D6
			BhSdD7JOB7OVWSghIZFNegtTdPS/+LI7wJj2y3O5KP2QJUGfSvjKMpdpBffoLwV0
			15soOu0CgYEA04abNpRYQbkd21R3F2sN5td2UTE5M31iaxnZXQrGyGCQUdIawb5n
			XNIvIVoH6dV3qFOrx9lmYQOV3B5kQNWPyfOI6q/tWXpsVV9WeUjXhki3TinRDGYq
			FaNlnRh5IvCtwbGIZpnfay4j8UCw8KgnpAqiz45TBQ/YqUR0MCZiPecCgYEA0nDB
			yhsFau6cpNUgcSjeGTCQpCTCckvFusa671x6gjOXC8gxBVzpCcSXxUzXlLANhkmU
			CZFaUG/hwh0FKqAItg0yEhNz7mnYv5Y/w0o5zycqF2QYlB1L6ph/LHh8z82OgQBN
			D0MQEHgkNjejLkaLf080KKYxqZE0d/oWucOxYVcCgYB4gT77oReGmceApGYUWVDa
			KfWl270SsGPZUCic8P6+OQT/GAtWRPrtznA7N+c6N/qrUr+SYzAIJNrDRC0pIoGA
			M9XUndVCHJSLLn09K1pdjh+f0ALgZXOkUCobjU21shfLOTDUAuVdUjP3xTsIX0P2
			GHkYdaSmRZjRFcZ7h+KAEQKBgFjvgGbarpp3h0n+LHzGab65kJdeVbMaJNF/xWb9
			bWTzSqWXEGiU0IPpSr7+b6mOEdkr5V15yXJvJjj0LMfL5IKT5xJOmFMs9oZZiE8P
			YokSoy5Jhj2qd/gIRM7ViOIFnHEWYHrPu81KCPvE3bjj5XaDUabQPfLMxCDkV5Bg
			jOl3AoGAP8u9/AnJsH+7k83fnBQvPHPkoEQ1PtMyclUnWjGbM5BUDdUi0beN1Kme
			PqUDarWVNfwXh4WsTmrmlUe41dInmnmHwf+Cfsn8f02eMV5YnxlUEDfcaOjnJ9+h
			VNXeYgsaoGFp6LiIj70I1X1WkegOT85sfuHuV5tYAZVlF8yzbxI=
			-----END RSA PRIVATE KEY-----
		
	- N·ªôi dung c·ªßa file ansible.cfg:

		Case 1:
		
			host_key_checking=False
			
	- N·ªôi dung c·ªßa file web-db.yaml:
	
		Case 1:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present
					
		Case 2:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
					
	- N·ªôi dung file db.yaml:
	
		Case 1:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present

		Case 2:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

		Case 3:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: vprofile
					password: 'admin943'
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 4:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 5:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
				    msg: " The dbname is {{dbname}}"

				- debug:
				    var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 6:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
					msg: " The dbname is {{dbname}}"

				- debug:
					var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
				  register: dbout

				- name: print dbout variable
				  debug:
					var: dbout


					
	- N·ªôi dung file ansible.cfg:
	
		Case 1:
		
			[defaults]
			host_key_checking=False
			inventory = ./inventory
			forks = 5
			log_path = /var/log/ansible.log

			[privilege_escalation]
			become=True
			become_method=sudo
			become_ask_pass=False

	- N·ªôi dung gile vars_precedence.yaml:
	
		Case 1:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"

		Case 2:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"
				  register: usrout

				- debug:
					var: usrout

		Case 3:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
					USRNM: playuser
					COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment
						
		Case 4:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  gather_facts: False
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment

						
	- N·ªôi dung file group_vars/all:
	
		Case 1:
		
			group_vars/all:
			
				dbname: sky
				dbuser: pilot
				dbpass: aircraft
				
			db.yaml:
			
				- name: DBserver setup
				  hosts: dbservers
				  become: yes
					#vars:
					#dbname: electric
					#dbuser: current
					#dbpass: tesla
				  tasks:
					- debug:
						msg: " The dbname is {{dbname}}"

					- debug:
						var: dbuser

					- name: Install mariadb-server
					  ansible.builtin.yum:
						name: mariadb-server
						state: present

					- name: Install pymysql
					  ansible.builtin.yum:
						name: python3-PyMySQL
						state: present

					- name: Start mariadb service
					  ansible.builtin.service:
						name: mariadb
						state: started
						enabled: yes

					- name: Create a new database with name 'accounts'
					  community.mysql.mysql_db:
						name: "{{dbname}}"
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock

					- name: Create database user with name 'vprofile'
					  community.mysql.mysql_user:
						name: "{{dbuser}}"
						password: "{{dbpass}}"
						priv: '*.*:ALL'
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock
					  register: dbout

					- name: print dbout variable
					  debug:
						var: dbout

		Case 2:
		
			group_vars/all:
			
				USRNM: commonuser
				COMM: variable from groupvars_all file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
		Case 3:
		

			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org


	
	- N·ªôi dung file group_vars/webservers:
	
		Case 1:
		
			group_vars/webservers:
			
				USRNM: webgroup
				COMM: variable from group_vars/webservers file

			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
	- N·ªôi dung file host_vars/web02:
	
		Case 1:
		
			host_vars/web02:
		
				USRNM: web02user
				COMM: variables from host_vars/web02 file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
		
	- N·ªôi dung file print_facts.yaml:
	
		Case 1:
		
			- name: Print facts
			  hosts: all
				#gather_facts: False
			  tasks:
				- name: Print OS name
				  debug:
					var: ansible_distribution
				- name: Print selinux mode
				  debug:
					var: ansible_selinux.mode
				- name: Print RAM memory
				  debug:
					var: ansible_memory_mb.real.free

				- name: Print Processor name
				  debug:
					var: ansible_processor[2]

	- N·ªôi dung file provisioning.yaml:
	
		Case 1:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
					
		Case 2:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd
					
		Case 3:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd


				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"

				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory
					
		Case 4:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

		Case 5:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  vars:
				mydir: /opt/dir22
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: "{{item}}"
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: "{{item}}"
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: "{{mydir}}"
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

				- name: Dump file
				  copy:
					files: files/myfile.txt
					dest: /tmp/myfile.txt

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"
				  
		Case 6:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- post-install
				
		Case 7:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org

		Case 8:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- geerlingguy.java
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org




	- Code file templates/ntpconf_centos:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst
			
	- Code file templates/ntpconf_ubuntu:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst

	- Code file roles/post-install/tasks/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# tasks file for post-install
			- name: Install ntp agent on centos
			  yum:
				name: "{{item}}"
				state: present
			  when: ansible_distribution == "CentOS"
			  loop:
				- chrony
				- wget
				- git
				- zip
				- unzip

			- name: Install ntp agent on Ubuntu
			  apt:
				name: "{{item}}"
				state: present
				update_cache: yes
			  when: ansible_distribution == "Ubuntu"
			  loop:
				- ntp
				- wget
				- git
				- zip
				- unzip


			- name: Start service on centos
			  service:
				name: chronyd
				state: started
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: Start service on ubuntu
			  service:
				name: ntp
				state: started
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

			- name: Banner file
			  copy:
				content: '# This server is managed by ansible. No manual changes please.'
				dest: /etc/motd

			- name: Create a folder
			  file:
				path: "{{mydir}}"
				state: directory

			- name: Deploy ntp agent conf on centos
			  template:
				src: ntpconf_centos.j2
				dest: /etc/chrony.conf
				backup: yes
			  when: ansible_distribution == "CentOS"
			  notify:
				- reStart service on centos

			- name: Deploy ntp agent conf on ubuntu
			  template:
				src: ntpconf_ubuntu.j2
				dest: /etc/ntpsec/ntp.conf
				backup: yes
			  when: ansible_distribution == "Ubuntu"
			  notify:
				- reStart service on ubuntu

			- name: Dump file
			  copy:
				src: myfile.txt
				dest: /tmp/myfile.txt

	- Code file roles/post-install/handlers/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# handlers file for post-install
			- name: reStart service on centos
			  service:
				name: chronyd
				state: restarted
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: reStart service on ubuntu
			  service:
				name: ntp
				state: restarted
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

	- Code file roles/post-install/vars/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# vars file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file roles/post-install/defaults/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# defaults file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file .bashrc:
	
		Case 1:
		
			if ! shopt -oq posix; then
			  if [ -f /usr/share/bash-completion/bash_completion ]; then
				. /usr/share/bash-completion/bash_completion
			  elif [ -f /etc/bash_completion ]; then
				. /etc/bash_completion
			  fi
			fi

			export AWS_ACCESS_KEY_ID='AK123'
			export AWS_SECRET_ACCESS_KEY='abc123'
			
	- Code file test-aws.yml:
	
		Case 1:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					
		Case 2:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout
					
		Case 3:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 4:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 5:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-west-2
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

				- name: start an instance
				  amazon.aws.ec2_instance:
					name: "public-compute-instance"
					key_name: "sample"
					  #vpc_subnet_id: subnet-5ca1ab1e
					instance_type: t3.micro
					security_group: default
					  #network_interfaces:
					  #- assign_public_ip: true
					image_id: ami-0caa91d6b7bee0ed0
					exact_count: 1
					region: us-west-2
					tags:
					  Environment: Testing


--- AWS Part -2:

	- VPC -> Your VPCs -> Create VPC
	- VPC -> Subnets -> Create subnet
	- VPC -> Subnets -> instance -> Route table
	- VPC -> Internet gateways -> Create internet gateway
	- VPC -> Internet gateways -> instance -> Actions -> Attach to VPC
	- VPC -> Route tables -> instance -> Routes
	- VPC -> Route tables -> instance -> Subnet associations -> Edit subnet associations
	- VPC -> Route tables -> instance -> Create route table
	- VPC -> Elastic IPs -> Allocate Elastic IP address
	- VPC -> NAT gateways -> Create NAT gateway
	- VPC -> Peering connections -> Create peering connection
	- VPC -> Peering connections -> Actions -> Accept request
	- IAM -> Roles -> Create role
	- EC2 -> Instances -> instance -> Actions -> Security -> Modify IAM role
	- CloudWatch -> Logs -> Log groups -> Log streams
	- CloudWatch -> Logs -> Metric filters -> Create metric filter
	
	
	
	- Google search:
	
		Terraform AWS VPC
		
		Terraform registry
		
		access logs loadbalancer
	
	- Commands:
	
		- cmd: scp -i vpro-bastion-key.pem web-key.pem ubuntu@54.177.221.62:/home/ubuntu/
		- cmd: chmod 400 web-key.pem
		- cmd: yum install httpd wget unzip -y
		- cmd: wget https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip
		- cmd: tail -f access_log
		- cmd: tar czvf wave-web01-httpdlogs19122020.tar.gz *
		- cmd: cat /dev/null > access_log error_log
		- cmd: ls -ltr /tmp/logs-wave/
		- cmd: yum install awscli -y
		- cmd: aws s3 ls
		- cmd: aws s3 sync /tmp/logs-wave/ s3://wave-web-logs-111222/
		- cmd: yum install awslogs -y
		- cmd: sudo yum install amazon-cloudwatch-agent -y
		- cmd: rm -rf .aws/credentials
		- cmd: systemctl restart awslogsd
		- cmd: systemctl enable awslogsd
		- cmd: sudo systemctl status awslogsd
		- cmd: sudo cat /etc/awslogs/awslogs.conf
		- cmd: service awslogsd restart
		- cmd: cat /var/log/awslogs.log
		- cmd: vim /etc/awslogs/awslogs.conf
		
		
		
		
		
		
		
		
	
	AWS Part -2 - M√¥ h√¨nh m√¥ ph·ªèng m·ªôt ‚ÄúCorporate Datacenter‚Äù (trung t√¢m d·ªØ li·ªáu doanh nghi·ªáp
	truy·ªÅn th·ªëng), d√πng ƒë·ªÉ so s√°nh v·ªõi Amazon VPC (Virtual Private Cloud) trong AWS:

		T·ªïng quan

			H√¨nh n√†y m√¥ t·∫£ m·ªôt trung t√¢m d·ªØ li·ªáu v·∫≠t l√Ω (on-premises), n∆°i doanh nghi·ªáp qu·∫£n l√Ω h·∫° t·∫ßng m·∫°ng, m√°y ch·ªß, router, switch,...

				To√†n b·ªô h·ªá th·ªëng ƒë∆∞·ª£c chia nh·ªè th√†nh nhi·ªÅu subnet ‚Äî t∆∞∆°ng t·ª± nh∆∞ c√°ch AWS chia VPC th√†nh subnet.

				C√°c subnet n√†y k·∫øt n·ªëi v·ªõi nhau qua router trung t√¢m.

				Network ACL (Access Control List) ƒë∆∞·ª£c ƒë·∫∑t gi·ªØa router v√† subnet ƒë·ªÉ ki·ªÉm so√°t l∆∞u l∆∞·ª£ng.

		C√°c th√†nh ph·∫ßn ch√≠nh trong s∆° ƒë·ªì
		
			Router (b·ªô ƒë·ªãnh tuy·∫øn)

				N·∫±m ·ªü tr√™n c√πng.

				C√≥ nhi·ªám v·ª• k·∫øt n·ªëi c√°c subnet kh√°c nhau trong c√πng m·∫°ng n·ªôi b·ªô.

				C≈©ng c√≥ th·ªÉ k·∫øt n·ªëi trung t√¢m d·ªØ li·ªáu n√†y v·ªõi Internet ho·∫∑c c√°c m·∫°ng kh√°c (WAN, VPN,...).

			Network ACL

				L√† b·ªô l·ªçc b·∫£o m·∫≠t ·ªü m·ª©c subnet.

				Quy·∫øt ƒë·ªãnh g√≥i tin n√†o ƒë∆∞·ª£c ph√©p ƒëi v√†o ho·∫∑c ƒëi ra subnet (theo IP, port, protocol,...).

				T∆∞∆°ng t·ª± Network ACL trong AWS VPC, n·∫±m gi·ªØa router v√† subnet.

			Subnet (m·∫°ng con)

				M·ªói m√†u trong h√¨nh l√† m·ªôt subnet:

					üüß 172.20.1.0/24

					üü¶ 192.168.0.0/24

					üü© 10.0.0.0/16

				M·ªói subnet ƒë·∫°i di·ªán cho m·ªôt ph√¢n ƒëo·∫°n m·∫°ng ri√™ng bi·ªát trong doanh nghi·ªáp.
				
					V√≠ d·ª•:

						M·ªôt subnet cho b·ªô ph·∫≠n k·∫ø to√°n.

						M·ªôt subnet cho b·ªô ph·∫≠n k·ªπ thu·∫≠t.

						M·ªôt subnet cho server n·ªôi b·ªô.

		B√™n trong m·ªói subnet

			M·ªói subnet c√≥:

				Switch ‚Äì ƒë·ªÉ k·∫øt n·ªëi c√°c m√°y t√≠nh n·ªôi b·ªô trong c√πng subnet.

				Network of computers ‚Äì nh√≥m m√°y tr·∫°m ho·∫∑c m√°y ch·ªß.

				Mainframe ‚Äì c√°c m√°y ch·ªß l·ªõn, ch·ªãu tr√°ch nhi·ªám x·ª≠ l√Ω d·ªØ li·ªáu trung t√¢m.

			T·∫•t c·∫£ m√°y trong c√πng subnet c√≥ th·ªÉ giao ti·∫øp tr·ª±c ti·∫øp v·ªõi nhau qua switch m√† kh√¥ng c·∫ßn ƒëi qua router.
			
	AWS Part -2 - Ki·∫øn tr√∫c m·∫°ng c∆° b·∫£n c·ªßa m·ªôt h·ªá th·ªëng ch·∫°y tr√™n AWS VPC (Virtual Private Cloud):
	
		T·ªïng quan: Amazon VPC (Virtual Private Cloud)

			VPC l√† m·ªôt m·∫°ng ri√™ng ·∫£o b√™n trong AWS, n∆°i b·∫°n c√≥ th·ªÉ tri·ªÉn khai c√°c t√†i nguy√™n nh∆∞ EC2, RDS, Load Balancer...
			VPC cho ph√©p b·∫°n ki·ªÉm so√°t ho√†n to√†n m·∫°ng l∆∞·ªõi, g·ªìm:

				D·∫£i IP (CIDR)

				Subnet

				Routing table

				Security Group v√† Network ACL
				
		C√°c th√†nh ph·∫ßn trong h√¨nh
		
			Internet Gateway (IGW)

					L√† c·ªïng k·∫øt n·ªëi gi·ªØa VPC v√† Internet.

					Cho ph√©p c√°c instance trong public subnet c√≥ th·ªÉ giao ti·∫øp ra/v√†o Internet.

					IGW g·∫Øn tr·ª±c ti·∫øp v√†o VPC.

				Vai tr√≤:
				
					C√°c web server mu·ªën ƒë∆∞·ª£c truy c·∫≠p c√¥ng khai ph·∫£i ƒëi qua IGW.
					
			Public Subnet

				ƒê√¢y l√† subnet c√≥ route ra Internet Gateway.

				Ch·ª©a c√°c t√†i nguy√™n c·∫ßn c√¥ng khai ra ngo√†i, v√≠ d·ª•:
				
					Web Server, NAT Gateway, Load Balancer, v.v.

				Trong h√¨nh:

					C√≥ hai WEB SERVER trong public subnet.

					C√≥ VPC NAT Gateway n·∫±m c√πng subnet.
					
			Private Subnet

				Subnet kh√¥ng c√≥ route tr·ª±c ti·∫øp ra Internet.

				Ch·ªâ c√≥ th·ªÉ truy c·∫≠p Internet gi√°n ti·∫øp th√¥ng qua NAT Gateway ho·∫∑c t·ª´ public subnet.

				D√πng cho c√°c t√†i nguy√™n n·ªôi b·ªô c·∫ßn b·∫£o m·∫≠t cao:

					App Server

					Database Server

				Trong h√¨nh:

					C√≥ 1 APP SERVER v√† 2 DB SERVER trong private subnet.
					
			VPC NAT Gateway

					‚ÄúNAT‚Äù = Network Address Translation.

					N·∫±m trong public subnet v√† c√≥ Elastic IP g·∫Øn v√†o.

					Cho ph√©p c√°c instance trong private subnet truy c·∫≠p Internet ƒë·ªÉ c·∫≠p nh·∫≠t package, t·∫£i d·ªØ li·ªáu, v.v.,
					nh∆∞ng Internet kh√¥ng th·ªÉ truy c·∫≠p ng∆∞·ª£c l·∫°i v√†o private subnet.

				V√≠ d·ª•:

					APP SERVER trong private subnet mu·ªën t·∫£i c·∫≠p nh·∫≠t t·ª´ Internet ‚Üí ƒëi qua NAT Gateway ‚Üí Internet.

					Nh∆∞ng Internet kh√¥ng th·ªÉ v√†o tr·ª±c ti·∫øp APP SERVER.
					
			Availability Zone (AZ)

				AWS chia m·ªói region (v√≠ d·ª•: us-east-1) th√†nh nhi·ªÅu AZ ‚Äî c√°c trung t√¢m d·ªØ li·ªáu v·∫≠t l√Ω ri√™ng bi·ªát.

				Trong h√¨nh:

					Availability Zone A: ch·ª©a public subnet (WEB SERVER, NAT Gateway)

					Availability Zone B: ch·ª©a private subnet (APP SERVER, DB SERVER)

				üß≠ M·ª•c ti√™u: tƒÉng ƒë·ªô tin c·∫≠y v√† t√≠nh s·∫µn s√†ng (high availability).
				
		Lu·ªìng ho·∫°t ƒë·ªông th·ª±c t·∫ø

			Ng∆∞·ªùi d√πng truy c·∫≠p web qua Internet ‚Üí ƒëi v√†o Internet Gateway ‚Üí ƒë·∫øn WEB SERVER trong public subnet.

			WEB SERVER c√≥ th·ªÉ:

				G·ª≠i request ƒë·∫øn APP SERVER trong private subnet.

				APP SERVER x·ª≠ l√Ω, truy v·∫•n DB SERVER (c≈©ng trong private subnet).

			N·∫øu APP SERVER c·∫ßn ra Internet (v√≠ d·ª• t·∫£i dependency) ‚Üí n√≥ ƒëi qua NAT Gateway trong public subnet ‚Üí Internet.
			
		B·∫£o m·∫≠t

			Security Group (SG): ki·ªÉm so√°t traffic theo instance (firewall c·∫•p EC2).
			
				V√≠ d·ª•:

					Web server SG cho ph√©p inbound HTTP (80) v√† HTTPS (443).

					App/DB server SG ch·ªâ cho ph√©p inbound t·ª´ web server.

			Network ACL (NACL): ki·ªÉm so√°t traffic theo subnet (firewall c·∫•p m·∫°ng).
			
	AWS Part -2 - Ki·∫øn tr√∫c m·∫°ng VPC trong AWS:
	
		M·ª•c ti√™u c·ªßa s∆° ƒë·ªì

			H√¨nh n√†y cho ta th·∫•y c√°ch AWS chia m·∫°ng trong VPC th√†nh hai ph·∫ßn:

				Public Subnet: cho ph√©p EC2 giao ti·∫øp tr·ª±c ti·∫øp v·ªõi Internet

				Private Subnet: ho√†n to√†n c√°ch ly, ch·ªâ giao ti·∫øp n·ªôi b·ªô ho·∫∑c qua VPN/NAT
				
		C√°c th√†nh ph·∫ßn ch√≠nh trong h√¨nh
		
			Amazon VPC

				L√† ‚Äúm·∫°ng ·∫£o ri√™ng‚Äù c·ªßa b·∫°n trong AWS (Virtual Private Cloud).

				B·∫°n c√≥ th·ªÉ ki·ªÉm so√°t to√†n b·ªô m·∫°ng, IP, routing, subnet, b·∫£o m·∫≠t‚Ä¶

				M·ªói VPC n·∫±m trong m·ªôt region (v√≠ d·ª•: us-east-1).
				
			Public Subnet

				Subnet n√†y c√≥ route ra Internet Gateway ‚Üí cho ph√©p c√°c instance b√™n trong giao ti·∫øp tr·ª±c ti·∫øp v·ªõi Internet.

				Trong h√¨nh:

					C√≥ 1 EC2 instance

					G·∫Øn Security Group (b·∫£o m·∫≠t c·∫•p instance)

					C√≥ Network ACL (b·∫£o m·∫≠t c·∫•p subnet)

					C√≥ Route Table (ƒë·ªãnh tuy·∫øn 172.16.0.0, 172.16.1.0, 172.16.2.0)

				T√°c d·ª•ng:
				
					D√†nh cho c√°c t√†i nguy√™n c·∫ßn c√¥ng khai, v√≠ d·ª•:

						Web server

						Load balancer

						NAT Gateway
						
			Private Subnet

				Subnet n√†y kh√¥ng c√≥ route tr·ª±c ti·∫øp ra Internet Gateway, ch·ªâ c√≥ th·ªÉ k·∫øt n·ªëi n·ªôi b·ªô.

				Trong h√¨nh:

					C≈©ng c√≥ EC2 instance

					C√≥ Security Group, Network ACL, Route Table ri√™ng

				T√°c d·ª•ng:
				
					D√†nh cho t√†i nguy√™n ch·ªâ d√πng n·ªôi b·ªô, nh∆∞:

						App Server

						Database Server

						Internal API
						
			Route Table (B·∫£ng ƒë·ªãnh tuy·∫øn)

				ƒêi·ªÅu khi·ªÉn ‚Äúh∆∞·ªõng ƒëi‚Äù c·ªßa traffic trong VPC.

				M·ªói subnet ƒë∆∞·ª£c g√°n 1 Route Table.

					Public Subnet: c√≥ route 0.0.0.0/0 ‚Üí Internet Gateway

					Private Subnet: kh√¥ng c√≥ route ƒë√≥ ‚Üí ch·∫∑n Internet tr·ª±c ti·∫øp

				Trong h√¨nh: Route Table li·ªát k√™ c√°c d·∫£i IP n·ªôi b·ªô (172.16.x.x) d√πng ƒë·ªÉ ƒë·ªãnh tuy·∫øn
				gi·ªØa c√°c subnet trong VPC.
				
			Network ACL (Access Control List)

				L√† t∆∞·ªùng l·ª≠a c·∫•p subnet, ki·ªÉm so√°t inbound/outbound traffic.

				C√≥ th·ªÉ cho ph√©p ho·∫∑c ch·∫∑n d·ª±a tr√™n rule theo IP, port, protocol.
				
			Security Group

				T∆∞·ªùng l·ª≠a c·∫•p instance.

				Ki·ªÉm so√°t traffic inbound/outbound cho t·ª´ng EC2.

				M·∫∑c ƒë·ªãnh: ch·∫∑n m·ªçi inbound, cho ph√©p outbound.
				
			Router

				L√† router logic b√™n trong VPC (AWS qu·∫£n l√Ω t·ª± ƒë·ªông).

				K·∫øt n·ªëi c√°c subnet v·ªõi nhau v√† v·ªõi gateway (Internet ho·∫∑c VPN).

				B·∫°n kh√¥ng c·∫•u h√¨nh tr·ª±c ti·∫øp router, m√† ƒëi·ªÅu khi·ªÉn qua Route Table.
				
			Internet Gateway (IGW)

				C·ªïng k·∫øt n·ªëi VPC ‚Üî Internet.

				C·∫ßn thi·∫øt n·∫øu EC2 trong public subnet mu·ªën c√≥ IP c√¥ng c·ªông v√† truy c·∫≠p ra ngo√†i.

				Ch·ªâ ho·∫°t ƒë·ªông v·ªõi subnet c√≥ route t·ªõi IGW.
				
			VPN Gateway

				Cho ph√©p k·∫øt n·ªëi VPC v·ªõi datacenter on-premises (qua ƒë∆∞·ªùng VPN b·∫£o m·∫≠t).

				D√†nh cho m√¥ h√¨nh hybrid cloud (v·ª´a d√πng AWS v·ª´a d√πng h·∫° t·∫ßng n·ªôi b·ªô).
				
		Lu·ªìng ho·∫°t ƒë·ªông minh h·ªça

			EC2 trong Public Subnet c√≥ th·ªÉ giao ti·∫øp Internet tr·ª±c ti·∫øp qua Internet Gateway.

			EC2 trong Private Subnet:

				Kh√¥ng th·ªÉ ƒëi ra Internet tr·ª±c ti·∫øp.

				C√≥ th·ªÉ ƒëi qua VPN Gateway ƒë·ªÉ k·∫øt n·ªëi v·ªÅ h·ªá th·ªëng n·ªôi b·ªô (corporate network).

			Traffic n·ªôi b·ªô gi·ªØa c√°c subnet (172.16.0.x ‚Üî 172.16.2.x) ƒëi qua Router c·ªßa VPC, theo Route Table.
			
	AWS Part -2 - Thi·∫øt k·∫ø ki·∫øn tr√∫c m·∫°ng (VPC Blueprint) trong AWS ·ªü region us-west-1, v·ªõi ƒë·∫ßy ƒë·ªß subnet,
	route table, NAT, Internet Gateway, v√† VPC Peering:
	
		Th√¥ng tin t·ªïng quan

			Region: us-west-1 (California)

			VPC Range: 172.20.0.0/16
			
				‚Üí d·∫£i IP t·ªïng cho to√†n b·ªô VPC (65,536 ƒë·ªãa ch·ªâ IP kh·∫£ d·ª•ng).
				
				‚Üí c√°c subnet con s·∫Ω chia nh·ªè d·∫£i n√†y (theo /24).

		C·∫•u tr√∫c Subnet

			VPC n√†y c√≥ 4 subnet chia l√†m 2 public v√† 2 private, ƒë·∫∑t ·ªü 2 Availability Zones (AZs) kh√°c nhau:

				Subnet 			CIDR				Lo·∫°i		AZ	Ghi ch√∫
				172.20.1.0/24	Public Subnet 1		us-west-1a	pub-sub-1
				172.20.2.0/24	Public Subnet 2		us-west-1b	pub-sub-2
				172.20.3.0/24	Private Subnet 1	us-west-1a	priv-sub-1
				172.20.4.0/24	Private Subnet 2	us-west-1b	priv-sub-2

			M·ª•c ƒë√≠ch:

				Public subnet: ch·ª©a c√°c t√†i nguy√™n c·∫ßn truy c·∫≠p t·ª´ Internet (v√≠ d·ª•: Bastion host, Load Balancer, NAT Gateway).

				Private subnet: ch·ª©a c√°c ·ª©ng d·ª•ng n·ªôi b·ªô (App Server, Database, Cache‚Ä¶).

		Internet Gateway (IGW)

			C√≥ 1 Internet Gateway g·∫Øn v√†o VPC n√†y.

			Cho ph√©p c√°c instance trong public subnet giao ti·∫øp tr·ª±c ti·∫øp v·ªõi Internet.
			
		NAT Gateway (2 c√°i)

			ƒê·∫∑t trong public subnet, m·ªói AZ m·ªôt c√°i.

			D√πng ƒë·ªÉ cho ph√©p c√°c instance trong private subnet truy c·∫≠p Internet ra ngo√†i, nh∆∞ng Internet kh√¥ng
			truy c·∫≠p ng∆∞·ª£c l·∫°i ƒë∆∞·ª£c.

			V√≠ d·ª•: private EC2 c√≥ th·ªÉ t·∫£i g√≥i update, patch, v.v‚Ä¶ m√† v·∫´n gi·ªØ an to√†n.
			
		Elastic IP (EIP)

			M·ªói NAT Gateway c·∫ßn m·ªôt ƒë·ªãa ch·ªâ IP tƒ©nh (EIP) ƒë·ªÉ ra Internet.

			Do ƒë√≥, ·ªü ƒë√¢y c√≥ 1 EIP (ho·∫∑c 2 n·∫øu m·ªói NAT Gateway 1 IP).
			
		Route Tables (B·∫£ng ƒë·ªãnh tuy·∫øn)

			C√≥ 2 Route Tables:

				T√™n Route Table		D√†nh cho		ƒê·ªãnh tuy·∫øn ch√≠nh
				Public Subnet RT	Public Subnet	0.0.0.0/0 ‚Üí Internet Gateway
				Private Subnet RT	Private Subnet	0.0.0.0/0 ‚Üí NAT Gateway

			Gi·∫£i th√≠ch:

				Public subnet c√≥ route tr·ª±c ti·∫øp ra IGW.

				Private subnet kh√¥ng c√≥ route ra IGW, ch·ªâ route qua NAT Gateway.
				
		Bastion Host

			1 Bastion host n·∫±m trong public subnet.

			L√† EC2 trung gian, d√πng ƒë·ªÉ SSH v√†o c√°c instance trong private subnet.

			V√¨ private subnet kh√¥ng c√≥ IP public, n√™n b·∫°n SSH v√†o Bastion tr∆∞·ªõc ‚Üí r·ªìi t·ª´ ƒë√≥ SSH
			ti·∫øp v√†o c√°c private EC2.
			
		Network ACL (NACL)

			C√≥ nh·∫Øc ƒë·∫øn ‚ÄúNACL‚Äù ·ªü cu·ªëi ‚Äî nghƒ©a l√† s·∫Ω c√≥ Network ACL ƒë·ªÉ ki·ªÉm so√°t traffic ·ªü c·∫•p subnet.

			D√πng ƒë·ªÉ tƒÉng th√™m l·ªõp b·∫£o m·∫≠t, th∆∞·ªùng c·∫•u h√¨nh song song v·ªõi Security Group.
			
		VPC Peering

			D√≤ng cu·ªëi ‚Äú1 more VPC ‚Üí VPC Peering‚Äù nghƒ©a l√†:

				C√≥ m·ªôt VPC kh√°c trong c√πng ho·∫∑c kh√°c region.

				S·∫Ω t·∫°o VPC Peering ƒë·ªÉ hai VPC giao ti·∫øp v·ªõi nhau qua m·∫°ng ri√™ng (kh√¥ng qua Internet).

			·ª®ng d·ª•ng:

			D√πng ƒë·ªÉ k·∫øt n·ªëi VPC c·ªßa team kh√°c, m√¥i tr∆∞·ªùng dev/staging, ho·∫∑c h·ªá th·ªëng on-premises (khi kh√¥ng d√πng VPN).
			
	- Google search:
	
		online subnet calculator
		
	- Code trong file /etc/awslogs/awslogs.conf:
	
		Case 1:
		
		
			[/var/log/messages]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/messages
			buffer_duration = 5000
			log_stream_name = web01-sys-logs
			initial_position = start_of_file
			log_group_name = wave-web

			[/var/log/httpd/access_log]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/httpd/access_log
			buffer_duration = 5000
			log_stream_name = web01-httpd-access
			initial_position = start_of_file
			log_group_name = wave-web

	- Code trong s3-policy.json:
	
		Case 1:
		
			{
			  "Version":"2012-10-17",		 	 	 
			  "Statement": [
				{
				  "Effect": "Allow",
				  "Principal": {
					"AWS": "arn:aws:iam::127311923021:root"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*"
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*",
				  "Condition": {
					"StringEquals": {
						"s3:x-amz-acl": "bucket-owner-full-control"
					}
				  }
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:GetBucketAcl",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222"
				}
			  ]
			}


--- AWS CI CD Project:

	- Elastic Beanstalk -> Create application
	- Codebuild -> Getting started -> Create project
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Start build
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Phase details
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Edit
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Build logs
	- Codebuild -> Pipeline -> Pipelines -> Create pipeline
	- Bitbucket -> Settings -> Personal Bitbucket settings -> SSH keys -> Add key
	- Bitbucket -> Settings -> Personal Bitbucket settings -> App passwords
	
	- Google search:
	
		bitbucket
		
		codebuild buildspec file documentation

	- Commands:
	
		- cmd: chmod 400 vprobeankey.pem
		- cmd: dnf search mysql
		- cmd: dnf install mariadb1011 -y
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts
		- cmd: wget https://raw.githubusercontent.com/hkhcoder/vprofile-project/refs/heads/aws-ci/src/main/resources/db_backup.sql
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts < db_backup.sql
		- cmd: ls .ssh/
		- cmd: cat vprobit_rsa.pub
		- cmd: ssh -T git@bitbucket.org
		- cmd: ls -ltr
		- cmd: cat .git/config
		- cmd: git fetch --tags
		- cmd: git remote rm origin
		- cmd: git remote add origin git@bitbucket.org:devopscicd1122/vproapp.git
		- cmd: git push origin --all
		
		
		
		
	- Code trong file config:
	
		Case 1:
		
			# bitbucket.org
			Host bitbucket.org
			  PreferredAuthentications publickey
			  IdentityFile ~/.ssh/vprobit_rsa
			  
--- Docker:

	- Commands:
	
		- cmd: docker images
		- cmd: sudo vim /etc/group
		- cmd: sudo usermod -aG docker ubuntu
		- cmd: id ubuntu
		- cmd: docker run hello-world
		- cmd: docker ps
		- cmd: docker ps -a
		- cmd: docker pull nginx:stable-alpine3.21-perl
		- cmd: docker pull nginx
		- cmd: docker run --name myweb -p 7090:80 -d nginx
		- cmd: docker stop 2c0ebbbbc6cc
		- cmd: docker start myweb
		- cmd: ps -ef
		- cmd: cd /var/lib/docker/
		- cmd: root@ip-172-31-35-202:/var/lib/docker# cd containers/
		- cmd: du -sh 6e18faf905a703aad2a20e13e47edcd43a8e3fa8463dc22a76ef653133f04fe5
		- cmd: root@ip-172-31-35-202:/var/lib/docker# ls image/overlay2/
		- cmd: docker exec myweb ls /
		- cmd: docker exec myweb /bin/bash
		- cmd: docker exec -it myweb /bin/bash
		- cmd: apt install procps -y
		- cmd: docker rmi nginx:stable-alpine3.21-perl
		- cmd: docker rm 2c0ebbbbc6cc
		- cmd: docker pull ubuntu
		- cmd: docker run ubuntu
		- cmd: docker run -it ubuntu /bin/bash
		- cmd: docker rm sleepy_clarke amazing_ganguly tender_kilby
		- cmd: docker rmi ubuntu hello-world
		- cmd: docker inspect nginx
		- cmd: docker run -d -P nginx
		- cmd: docker logs peaceful_einstein
		- cmd: docker run -P nginx
		- cmd: docker run -d -P mysql:5.7
		- cmd: docker run -d -P -e MYSQL_ROOT_PASSWORD=mypass mysql:5.7
		- cmd: docker pull mysql:5.7
		- cmd: docker inspect mysql:5.7
		- cmd: docker exec -it vprodb /bin/bash
		- cmd: bash-4.2# cd /var/lib/mysql
		- cmd: docker stop vprodb
		- cmd: docker rm vprodb
		- cmd: docker volume
		- cmd: docker volume create mydbdata
		- cmd: docker volume ls
		- cmd: docker run --name vprodb -d -e MYSQL_ROOT_PASSWORD=secretpass -p 3030:3306 -v mydbdata:/var/lib/mysql mysql:5.7
		- cmd: ls /var/lib/docker/volumes/
		- cmd: ls /var/lib/docker/volumes/mydbdata/_data/
		- cmd: 	sudo apt update
				sudo apt install -y mysql-client
		- cmd: sudo apt install unzip -y
		- cmd: tar czvf nano.tar.gz *
		- cmd: mv nano.tar.gz ../
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg
		- cmd: docker build -t nanoimg:V2 .
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg:V2
		- cmd: docker build --no-cache -t nanoimg:V2 .
		
			Ch·∫°y ƒë·ªÉ tr√°nh n√≥ ch·∫°y v·ªõi cache c≈©
			
		- cmd: docker login
		- cmd: docker build -t ngoctuan99/nanoimg:V2 .
		- cmd: docker push ngoctuan99/nanoimg:V2
		- cmd: ubuntu@ip-172-31-35-202:~/EntryCMD$ docker build -t printer:v1 -f cmd/Dockerfile .
		- cmd: docker build -t printer:v2 entry/
		- cmd: docker-compose up
		- cmd: curl http://localhost:8000
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose top
		- cmd: git clone -b docker https://github.com/devopshydclub/vprofile-project.git
		
		
		
		
			
		
		
	

		
		
		
		
	
	- Google search:
	
		docker ubuntu install
		
		dockerhub official images
		
		hub docker
		
		docker compose install
		
		docker compose get started
		
	- Code trong Dockerfile:
	
		Case 1:
		
			FROM ubuntu:latest
			LABEL "Author"="Imran"
			LABEL "Project"="nano"
			ENV DEBIAN_FRONTEND=noninteractive
			RUN apt update && apt install git -y
			RUN apt install apache2 -y
			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			EXPOSE 80
			WORKDIR /var/www/html
			VOLUME /var/log/apache2
			ADD nano.tar.gz /var/www/html
			#COPY nano.tar.gz /var/www/html
			
		Case 2:
		
			# syntax=docker/dockerfile:1
			FROM python:3.10-alpine
			WORKDIR /code
			ENV FLASK_APP=app.py
			ENV FLASK_RUN_HOST=0.0.0.0
			RUN apk add --no-cache gcc musl-dev linux-headers
			COPY requirements.txt requirements.txt
			RUN pip install -r requirements.txt
			EXPOSE 5000
			COPY . .
			CMD ["flask", "run"]
			
		Case 3:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
				volumes:
				  - .:/code
				environment:
				  FLASK_ENV: development
			  redis:
				image: "redis:alpine"

		Case 4:
		
			FROM ubuntu:latest
			CMD ["echo", "hello"]
			
		Case 5:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			
		Case 6:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			cmd ["hello"]
			
		Case 7:
		
			FROM tomcat:8-jre11
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webappsubuntu@ip-172-31-35-202:~/composetest/vprofile-project/Docker-files/app$
			
		Case 8:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11

			RUN rm -rf /usr/local/tomcat/webapps/*

			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]



	-  Code trong app.py:
	
		Case 1:
		
			import time

			import redis
			from flask import Flask

			app = Flask(__name__)
			cache = redis.Redis(host='redis', port=6379)

			def get_hit_count():
				retries = 5
				while True:
					try:
						return cache.incr('hits')
					except redis.exceptions.ConnectionError as exc:
						if retries == 0:
							raise exc
						retries -= 1
						time.sleep(0.5)

			@app.route('/')
			def hello():
				count = get_hit_count()
				return 'Hello World! I have been seen {} times.\n'.format(count)
				
	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
			  redis:
				image: "redis:alpine"

	- Code trong file requirements.txt:
	
		Case 1:
		
			flask
			redis
			
--- Containerization:

	Git bash kh√¥ng th·ªÉ ch·∫°y nhi·ªÅu l·ªánh 1 l√∫c ƒë∆∞·ª£cA

	- Google search:
	
		Docker ubuntu install
		
		Dockerfile reference
		
	Containerization -  S·ª± kh√°c nhau gi·ªØa / v√† /root:
	
		Trong Ubuntu (v√† Linux n√≥i chung), hai th∆∞ m·ª•c / v√† /root ho√†n to√†n kh√°c nhau v·ªÅ vai tr√≤ v√† quy·ªÅn h·∫°n.
		
		/ (root directory)

			/ ƒë∆∞·ª£c g·ªçi l√† th∆∞ m·ª•c g·ªëc c·ªßa to√†n b·ªô h·ªá th·ªëng t·∫≠p tin (root filesystem).

			T·∫•t c·∫£ m·ªçi th·ª© trong Linux ‚Äî file h·ªá th·ªëng, ng∆∞·ªùi d√πng, thi·∫øt b·ªã, ·ª©ng d·ª•ng, v.v. ‚Äî ƒë·ªÅu n·∫±m d∆∞·ªõi th∆∞ m·ª•c n√†y.

			N√≥ gi·ªëng nh∆∞ ·ªï C: trong Windows, nh∆∞ng r·ªông h∆°n, v√¨ n√≥ ch·ª©a lu√¥n c√°c ‚Äú·ªï ƒëƒ©a‚Äù v√† mount kh√°c.

			C·∫•u tr√∫c th∆∞·ªùng th·∫•y trong /:
	
				/
				‚îú‚îÄ‚îÄ bin        ‚Üí Ch·ª©a l·ªánh c∆° b·∫£n (ls, cp, mv, cat, v.v.)
				‚îú‚îÄ‚îÄ boot       ‚Üí Ch·ª©a file kh·ªüi ƒë·ªông h·ªá th·ªëng (kernel, grub)
				‚îú‚îÄ‚îÄ dev        ‚Üí Thi·∫øt b·ªã (·ªï c·ª©ng, USB, tty,...)
				‚îú‚îÄ‚îÄ etc        ‚Üí File c·∫•u h√¨nh h·ªá th·ªëng
				‚îú‚îÄ‚îÄ home       ‚Üí Th∆∞ m·ª•c ng∆∞·ªùi d√πng b√¨nh th∆∞·ªùng (vd: /home/nhuan)
				‚îú‚îÄ‚îÄ lib        ‚Üí Th∆∞ vi·ªán h·ªá th·ªëng
				‚îú‚îÄ‚îÄ root       ‚Üí Th∆∞ m·ª•c c√° nh√¢n c·ªßa t√†i kho·∫£n root
				‚îú‚îÄ‚îÄ tmp        ‚Üí File t·∫°m th·ªùi
				‚îú‚îÄ‚îÄ usr        ‚Üí Ch∆∞∆°ng tr√¨nh, th∆∞ vi·ªán d√πng chung
				‚îú‚îÄ‚îÄ var        ‚Üí Log, mail, cache,...
				
			/root

				/root l√† th∆∞ m·ª•c c√° nh√¢n (home directory) c·ªßa ng∆∞·ªùi d√πng root (superuser).

				Khi b·∫°n ƒëƒÉng nh·∫≠p b·∫±ng t√†i kho·∫£n root, ƒë√¢y l√† n∆°i m·∫∑c ƒë·ªãnh b·∫°n s·∫Ω ‚Äúƒë·ª©ng‚Äù (cd).

				T∆∞∆°ng t·ª± nh∆∞ /home/username c·ªßa ng∆∞·ªùi d√πng th∆∞·ªùng, nh∆∞ng ƒë∆∞·ª£c ƒë·∫∑t ·ªü ƒë√¢y v√¨:

				Root c·∫ßn truy c·∫≠p ƒë∆∞·ª£c ngay c·∫£ khi /home ch∆∞a mount (v√≠ d·ª• trong ch·∫ø ƒë·ªô kh√¥i ph·ª•c).

				B·∫£o m·∫≠t v√† t√°ch bi·ªát v·ªõi user th√¥ng th∆∞·ªùng.

	- Commands:
	
		- cmd: usermod -aG docker vagrant
		- cmd: id vagrant
		- cmd: docker compose up -d
		- cmd: docker compose down
		- cmd: docker volume ls
		- cmd: docker volume rm vagrant_vproappdata vagrant_vprodbdata
		- cmd: docker volume prune
		- cmd: docker system prune -a
		- cmd: code .
		- cmd: id
		- cmd: docker-compose --version
		- cmd: docker-compose build
		
		
		
		
		
	
	- Code trong file Dockerfile:
	
		Case 1:
		
			FROM maven:3.9.9-eclipse-temurin-21-jammy AS BUILD_IMAGE
			RUN git clone https://github.com/hkhcoder/vprofile-project.git
			RUN cd vprofile-project && git checkout containers && mvn install

			FROM tomcat:10-jdk21
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 2:
		
			FROM mysql:8.0.33
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			ENV MYSQL_ROOT_PASSWORD="vprodbpass"
			ENV MYSQL_DATABASE="accounts"

			ADD db_backup.sql docker-entrypoint-initdb.d/db_backup.sql
			
		Case 3:
		
			FROM nginx
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /etc/nginx/conf.d/default.conf
			COPY nginvproapp.conf /etc/nginx/conf.d/vproapp.conf
			
		Case 4:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			#ADD ../../vprofile-project
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 5:
		
			FROM node:14 AS web-build
			WORKDIR /usr/src/app
			COPY ./ ./client
			RUN cd client && npm install && npm run build --prod

			# Use official nginx image as the base image
			FROM nginx:latest

			# Copy the build output to replace the default nginx contents.
			COPY --from=web-build /usr/src/app/client/dist/client/ /usr/share/nginx/html
			COPY nginx.conf /etc/nginx/conf.d/default.conf

			# Expose port 4200
			EXPOSE 4200
			
		Case 6:
		
			FROM node:14 AS nodeapi-build
			WORKDIR /usr/src/app
			COPY ./ ./nodeapi/
			RUN cd nodeapi && npm install

			FROM node:14
			WORKDIR /usr/src/app/
			COPY --from=nodeapi-build /usr/src/app/nodeapi/ ./
			RUN ls
			EXPOSE 5000
			CMD ["/bin/sh", "-c", "cd /usr/src/app/ && npm start"]
			# Test3

		Case 7:
		
			FROM openjdk:8 AS BUILD_IMAGE
			WORKDIR /usr/src/app/
			RUN apt update && apt install maven -y
			COPY ./ /usr/src/app/
			RUN mvn install -DskipTests

			FROM openjdk:8

			WORKDIR /usr/src/app/
			COPY --from=BUILD_IMAGE /usr/src/app/target/book-work-0.0.1-SNAPSHOT.jar ./book-work-0.0.1.jar

			EXPOSE 9000
			ENTRYPOINT ["java","-jar","book-work-0.0.1.jar"]
			# Test


			
	- Code trong file nginvproapp.conf:
	
		Case 1:
		
			upstream vproapp {
			 server vproapp:8080;
			}
			server {
			  listen 80;
			location / {
			  proxy_pass http://vproapp;
			}
			}

	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
				vprodb:
				  build:
					context: ./Docker-files/db
				  image: ngoctuan99/vprofiledb
				  container_name: vprodb
				  ports:
					- "3306:3306"
				  volumes:
					- vprodbdata:/var/lib/mysql
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vprocache01:
				  image: memcached
				  container_name: vprocache01
				  ports:
					- "11211:11211"

				vpromq01:
				  image: rabbitmq
				  container_name: vpromq01
				  ports:
					- "5672:5672"
				  environment:
					- RABBITMQ_DEFAULT_USER=guest
					- RABBITMQ_DEFAULT_PASS=guest

				vproapp:
				  build:
					context: ./Docker-files/app
				  image: ngoctuan99/vprofileapp
				  container_name: vproapp
				  ports:
					- "8080:8080"
				  volumes:
					- vproappdata:/usr/local/tomcat/webapps
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vproweb:
				  build:
					context: ./Docker-files/web
				  image: ngoctuan99/vprofileweb
				  container_name: vproweb
				  ports:
					- "80:80"

			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books

			  
	- Code trong file nginx.conf:
	
		Case 1:
		
			server {
				listen       4200;
				listen  [::]:4200;
				server_name  localhost;

				#access_log  /var/log/nginx/host.access.log  main;

				location / {
					root   /usr/share/nginx/html;
					index  index.html index.htm;
				}

				#error_page  404              /404.html;

				# redirect server error pages to the static page /50x.html
				#
				error_page   500 502 503 504  /50x.html;
				location = /50x.html {
					root   /usr/share/nginx/html;
				}

				# proxy the PHP scripts to Apache listening on 127.0.0.1:80
				#
				#location ~ \.php$ {
				#    proxy_pass   http://127.0.0.1;
				#}

				# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
				#
				#location ~ \.php$ {
				#    root           html;
				#    fastcgi_pass   127.0.0.1:9000;
				#    fastcgi_index  index.php;
				#    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
				#    include        fastcgi_params;
				#}

				# deny access to .htaccess files, if Apache's document root
				# concurs with nginx's one
				#
				#location ~ /\.ht {
				#    deny  all;
				#}
			}

	- Code trong file default.conf:
	
		Case 1:
		
			#upstream api {
			#    server api:5000; 
			#}
			#upstream webapi {
			#    server webapi:9000;
			#}
			upstream client {
				server client:4200;
			}
			server {
				listen 80;
				location / {
					proxy_set_header Host $host;
					proxy_set_header X-Real-IP $remote_addr;
					proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
					proxy_set_header X-Forwarded-Proto $scheme; 

					proxy_http_version 1.1;
					proxy_set_header Upgrade $http_upgrade;
					proxy_set_header Connection "upgrade";
					proxy_pass http://client/;
				}
				location /api {
			#        rewrite ^/api/(.*) /$1 break; # works for both /api and /api/
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			#        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme; 
			#        proxy_http_version 1.1;

					proxy_pass http://api:5000;
				}
				location /webapi {
			#        rewrite ^/webapi/(.*) /$1 break;
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			##        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme;
					proxy_pass http://webapi:9000;
				}
			}

	- Code trong Advanced detials c·ªßa EC2:
	
		Case 1:
		
			#!/bin/bash

			# Install docker on Ubuntu
			sudo apt-get update
				sudo apt-get install \
				 ca-certificates \
				 curl \
				 gnupg \
				 lsb-release -y
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
				echo \
				"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
				$(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

			# Install docker-compose
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io -y
				sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
				sudo chmod +x /usr/local/bin/docker-compose

			# Add ubuntu user into docker group
				sudo usermod -a -G docker ubuntu

--- Kubernetes:

	- Route 53 -> Dashboard -> Create hosted zone
	- Amazon Elastic Kubernetes Service (Amazon EKS)

	- Google search:
	
		kubernetes docs
		
		minikube get started
		
		install kops
		
		install kubectl on linux
		
		kubernetes pod
		
		kubernetes deployment
		
		install kubectl on window
		
		kubernetes replicaset
		
		kubernetes deployment
		
		dockerfile reference
		
		define a command and arguments for a container
		
		kubernetes volume
		
		kubernetes configmaps
		
		kubernetes secrets
		
		kubernetes ingress
		
		kubectl cheatsheet
		
		kubernetes taint
		
		kubernetes limit
		
		kubernetes jobs
		
		kubernetes cronjob
		
		kubernetes daemonset
		
		k8slens.dev
		
		terraform modules
		
		
		
		
		
	C·∫ßn check code trong source t·ª´ git clone (emartapp)
		
		
		
		

	- Commands:
	
		- cmd: kops export kubecfg kubevpro.vnstudio.info --state=s3://kopsstate112211 --admin
		- cmd: choco install minikube kubernetes-cli -y
		- cmd: minikube.exe --help
		- cmd: minikube start
		- cmd: minikube start --no-vtx-check
		- cmd: minikube delete
		- cmd: cat .kube/config
		- cmd: kubectl.exe get nodes
		- cmd: kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.0
		- cmd: kubectl.exe get pod
		- cmd: kubectl.exe get deploy
		- cmd: kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0
		- cmd: minikube service hello-minikube --url
		- cmd: minikube service hello-minikube
		- cmd: kubectl.exe delete deploy hello-minikube
		- cmd: minikube.exe stop
		- cmd: minikube.exe delete
		- cmd: chmod +x kops
		- cmd: sudo mv kops /usr/local/bin/kops
		- cmd: kubectl version --client
		- cmd: cat .kube/config
		- cmd:
		
				kops create cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate956
				--zones=us-east-1a,us-east-1b --node-count=2 --node-size=t3.small --control-plane-size=t3.medium
				--dns-zone=kubevpro.vnstudio.info --node-volume-size=12 --control-plane-volume-size=12
				--ssh-public-key ~/.ssh/id_ed25519.pub
				
		- cmd: kops update cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes --admin
		- cmd: kubectl get nodes
		- cmd: kubectl config view
		- cmd: kubectl get ns
		- cmd: kubectl get all
		- cmd: kubectl get all --all-namespaces
		- cmd: kubectl get svc -n kube-system
		- cmd: kubectl create ns kubekart
		- cmd: kubectl run nginx1 --image=nginx -n kubekart
		- cmd: kubectl apply -f pod1.yaml
		- cmd: kubectl get pod -n kubekart
		- cmd: kubectl delete ns kubekart
		- cmd: kubectl create -f pod-setup.yml
		- cmd: kubectl describe pod webapp-pod
		- cmd: kubectl get pod webapp-pod -o yaml
		- cmd: kubectl get pod webapp-pod -o yaml > webpod-definition.yml
		- cmd: kubectl edit pod webapp-pod
		- cmd: kubectl delete pod nginx12
		- cmd: kubectl logs web2
		- cmd: history | grep test47
		- cmd: kubectl run nginx12 --image=nginx
		- cmd: kubectl get svc
		- cmd: kubectl describe svc helloworld-service
		- cmd: kubectl describe pod | grep IP
		- cmd: kubectl delete svc helloworld-services
		- cmd: kubectl get rs
		- cmd: kubectl delete pod frontend-z2qch frontend-zr67k
		- cmd: kubectl scale --replicas=1 rs/frontend
		- cmd: kubectl edit rs frontend
		- cmd: kubectl get deploy
		- cmd: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		- cmd: kubectl rollout undo deployment/nginx-deployment
		- cmd: kubectl describe pod nginx-deployment-647677fc66-62mt4 | grep Image
		- cmd: kubectl rollout history deployment/nginx-deployment
		- cmd: kubectl delete deploy nginx-deployment
		- cmd: kubectl apply -f samplecm.yaml
		- cmd: kubectl get cm game-demo -o yaml
		
		- cmd:
		
			ubuntu@ip-172-31-36-50:~$ kubectl exec --stdin --tty configmap-demo-pod -- /bin/sh
			/ # ls /config/
			game.properties            user-interface.properties
			/ # cd /config/
			/config # cat game.properties
			enemy.types=aliens,monsters
			player.maximum-lives=5
			/config # cat user-interface.properties
			color.good=purple
			color.bad=yellow
			allow.textmode=true
			/config # echo $PLAYER_INITIAL_LIVES
			3
			/config # echo $UI_PROPERTIES_FILE_NAME
			user-interface.properties
			/config # exit
			
		- cmd: echo -n "admin" | base64
		- cmd: echo -n "mysecretpass" | base64
		- cmd: echo 'c2VjcmV0cGFzcw==' | base64 --decode
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl get all -n ingress-nginx
		- cmd:
		
			ubuntu@ip-172-31-36-50:~/vprofile$ # Controller
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReated DNS Cname Record for LB
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReate Ingress
			
		- cmd: kubectl delete ingress vpro-ingress
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl config view
		- cmd: kubectl run nginxpod --image=nginx --dry-run=client -o yaml > ngpod.yaml
		- cmd: cat ngpod.yaml
		- cmd: kubectl create deployment ngdep --image=nginx --dry-run=client -o yaml > ngdep.yaml
		- cmd: cat ngdep.yaml
		- cmd: kubectl get ds -A
		- cmd: kubectl get pod -n kube-system
		- cmd: choco uninstall terraform
		- cmd: choco install terraform -y
		- cmd: terraform init -upgrade
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vpro-eks
		- cmd: cat ~/.kube/config
		- cmd: choco install kubernetes-cli -y
		- cmd: kubectl get pods
		
		
		
		
		
		
		
		
		
		
		
		

		
		

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	
		

	Kubernetes - S∆° ƒë·ªì ki·∫øn tr√∫c c·ªßa Kubernetes (K8s):
	
		T·ªïng quan v·ªÅ Kubernetes Architecture

			Kubernetes chia h·ªá th·ªëng th√†nh hai ph·∫ßn ch√≠nh:

				Master Node (Control Plane) ‚Äì qu·∫£n l√Ω to√†n b·ªô cluster

				Worker Nodes ‚Äì n∆°i th·∫≠t s·ª± ch·∫°y c√°c container (·ª©ng d·ª•ng c·ªßa b·∫°n)
				
		Master Node (Control Plane)

			ƒê√¢y l√† b·ªô n√£o c·ªßa Kubernetes, ch·ªãu tr√°ch nhi·ªám ƒëi·ªÅu ph·ªëi v√† qu·∫£n l√Ω to√†n b·ªô cluster.
			Trong h√¨nh, b·∫°n th·∫•y c√°c th√†nh ph·∫ßn ch√≠nh sau:

			API Server

				L√† trung t√¢m giao ti·∫øp c·ªßa Kubernetes.

				M·ªçi l·ªánh (kubectl apply, kubectl get pods, ‚Ä¶) ƒë·ªÅu ƒëi qua API Server.

				N√≥ nh·∫≠n file YAML (b√™n tr√°i h√¨nh), ch·ª©a ƒë·ªãnh nghƒ©a Pod, Service, Deployment, v.v.

				Sau ƒë√≥ ghi th√¥ng tin ƒë√≥ v√†o etcd (database).

				Hi·ªÉu ƒë∆°n gi·∫£n: API Server l√† ‚Äúc·ª≠a ch√≠nh‚Äù c·ªßa cluster.

			etcd

				L√† c∆° s·ªü d·ªØ li·ªáu ph√¢n t√°n (key-value store) c·ªßa Kubernetes.

				L∆∞u to√†n b·ªô tr·∫°ng th√°i c·ªßa cluster: pods, nodes, c·∫•u h√¨nh m·∫°ng, secrets, ‚Ä¶

				N·∫øu m·∫•t etcd, b·∫°n m·∫•t to√†n b·ªô ‚Äútr√≠ nh·ªõ‚Äù c·ªßa h·ªá th·ªëng.

			‚öôController Manager

				Theo d√µi tr·∫°ng th√°i cluster qua etcd.

				N·∫øu ph√°t hi·ªán s·ª± kh√°c bi·ªát gi·ªØa tr·∫°ng th√°i mong mu·ªën v√† tr·∫°ng th√°i th·ª±c t·∫ø, n√≥ s·∫Ω h√†nh ƒë·ªông ƒë·ªÉ kh√¥i ph·ª•c l·∫°i.

				V√≠ d·ª•:

					YAML khai b√°o c·∫ßn 3 pods, nh∆∞ng th·ª±c t·∫ø ch·ªâ c√≥ 2 pods ch·∫°y ‚Üí Controller Manager s·∫Ω t·∫°o th√™m 1 pod m·ªõi.

			Scheduler

				Quy·∫øt ƒë·ªãnh pod s·∫Ω ƒë∆∞·ª£c ch·∫°y ·ªü node n√†o.

				D·ª±a tr√™n t√†i nguy√™n c√≤n tr·ªëng (CPU, RAM), c√°c r√†ng bu·ªôc (nodeSelector, affinity, v.v.)

				Sau khi ch·ªçn node, Scheduler b√°o cho kubelet ·ªü node ƒë√≥ ƒë·ªÉ tri·ªÉn khai pod.
				
		Worker Nodes

			M·ªói Worker Node l√† m·ªôt m√°y (ho·∫∑c VM) th·∫≠t n∆°i container th·ª±c s·ª± ch·∫°y.
			
			Trong h√¨nh b·∫°n th·∫•y ba Worker nodes, m·ªói node g·ªìm:

				Kubelet

					Agent ch·∫°y tr√™n m·ªói node, nh·∫≠n l·ªánh t·ª´ API Server.

					Khi Scheduler quy·∫øt ƒë·ªãnh g√°n Pod v√†o node n√†y, Kubelet s·∫Ω:

						G·ªçi Docker (ho·∫∑c container runtime kh√°c nh∆∞ containerd)

						K√©o image v·ªÅ

						T·∫°o v√† kh·ªüi ƒë·ªông container

						B√°o l·∫°i tr·∫°ng th√°i pod v·ªÅ API Server

				Kube Proxy

					Qu·∫£n l√Ω m·∫°ng v√† load balancing gi·ªØa c√°c Pods.

					Gi√∫p c√°c Pods ·ªü c√°c node kh√°c nhau c√≥ th·ªÉ li√™n l·∫°c v·ªõi nhau.

					Duy tr√¨ service IP v√† routing rules trong h·ªá th·ªëng.

				Docker (ho·∫∑c container runtime)

					L√† c√¥ng c·ª• th·∫≠t s·ª± ch·∫°y container.

					Kubernetes ch·ªâ ‚Äúra l·ªánh‚Äù, Docker th·ª±c thi vi·ªác t·∫°o v√† qu·∫£n l√Ω container.

				Pods

					ƒê∆°n v·ªã tri·ªÉn khai nh·ªè nh·∫•t trong Kubernetes.

					M·ªôt Pod c√≥ th·ªÉ ch·ª©a m·ªôt ho·∫∑c nhi·ªÅu container, ch·∫°y c√πng tr√™n m·ªôt node.

					N·∫øu Pod ch·∫øt, Kubernetes s·∫Ω t·ª± t·∫°o l·∫°i Pod m·ªõi theo YAML ƒë·ªãnh nghƒ©a (b·∫°n th·∫•y d·∫•u ‚ÄúX ƒë·ªè‚Äù ·ªü node
					cu·ªëi h√¨nh ‚Üí m·ªôt Pod ch·∫øt, K8s s·∫Ω kh·ªüi t·∫°o l·∫°i).
					
		Lu·ªìng ho·∫°t ƒë·ªông t·ªïng qu√°t

			Dev g·ª≠i file YAML ƒë·∫øn API Server (kubectl apply -f deployment.yaml)

			API Server l∆∞u c·∫•u h√¨nh v√†o etcd

			Controller Manager ki·ªÉm tra v√† ra l·ªánh t·∫°o Pod

			Scheduler ch·ªçn node ph√π h·ª£p

			Kubelet ·ªü node ƒë√≥ nh·∫≠n l·ªánh ‚Üí ch·∫°y Pod b·∫±ng Docker

			Kube Proxy ƒë·∫£m b·∫£o Pod c√≥ th·ªÉ giao ti·∫øp v·ªõi c√°c Pod kh√°c
			
	- Code trong file pod1.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: nginx12
			  namespace: kubekart
			spec:
			  containers:
			  - name: nginx
				image: nginx:1.14.2
				ports:
				- containerPort: 80

	- Code trong file vproapppod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: vproapp
			  labels:
				app: vproapp
			spec:
			  containers:
				- name: appcontainer
				  image: imranvisualpath/freshtomapp:V7
				  ports:
					- name: vproapp-port
					  containerPort: 8080
	- Code trong file vproapp-nodeport.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 8090
				nodePort: 30001
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: NodePort

	- Code trong file vproapp-loadbalancer.yml
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 80
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: LoadBalancer

	- Code trong file replset.yaml
	
		Case 1:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 3
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
		Case 2:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 5
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
	- Code trong file deployment.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: nginx-deployment
			  labels:
				app: nginx
			spec:
			  replicas: 3
			  selector:
				matchLabels:
				  app: nginx
			  template:
				metadata:
				  labels:
					app: nginx
				spec:
				  containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80

	- Code trong file com.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: command-demo
			  labels:
				purpose: demonstrate-command
			spec:
			  containers:
			  - name: command-demo-container
				image: debian
				command: ["printenv"]
				args: ["HOSTNAME", "KUBERNETES_PORT"]
			  restartPolicy: OnFailure
			  
	- Code trong file mysqlpod.yaml:
	
		Case 1: ch·∫°y g·∫∑p l·ªói v√† ƒë∆∞·ª£c fix nh∆∞ ·ªü Case 2
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: dbpod
			spec:
			  containers:
			  - image: mysql:5.7
				name: mysql
				volumeMounts:
				- mountPath: /var/lib/mysql
				  name: dbvol
			  volumes:
			  - name: dbvol
				hostPath:
				  # directory location on host
				  path: /data
				  # this field is optional
				  type: DirectoryOrCreate
				  
		Case 2:
		
			apiVersion: v1

			kind: Pod

			metadata:

			  name: dbpod

			spec:

			  containers:

			  - image: mysql:5.7

				name: mysql

				env:

				- name: MYSQL_ROOT_PASSWORD

				  value: examplepassword

				- name: MYSQL_DATABASE

				  value: exampledb

				- name: MYSQL_USER

				  value: exampleuser

				- name: MYSQL_PASSWORD

				  value: examplepassword

				volumeMounts:

				- mountPath: /var/lib/mysql

				  name: dbvol

			  volumes:

			  - name: dbvol

				hostPath:

				  # directory location on host

				  path: /data

				  # this field is optional

				  type: DirectoryOrCreate
				  
	- Code trong file samplecm.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: ConfigMap
			metadata:
			  name: game-demo
			data:
			  # property-like keys; each key maps to a simple value
			  player_initial_lives: "3"
			  ui_properties_file_name: "user-interface.properties"

			  # file-like keys
			  game.properties: |
				enemy.types=aliens,monsters
				player.maximum-lives=5    
			  user-interface.properties: |
				color.good=purple
				color.bad=yellow
				allow.textmode=true    

	- Code trong file readcmpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: configmap-demo-pod
			spec:
			  containers:
				- name: demo
				  image: alpine
				  command: ["sleep", "3600"]
				  env:
					# Define the environment variable
					- name: PLAYER_INITIAL_LIVES # Notice that the case is different here
												 # from the key name in the ConfigMap.
					  valueFrom:
						configMapKeyRef:
						  name: game-demo           # The ConfigMap this value comes from.
						  key: player_initial_lives # The key to fetch.
					- name: UI_PROPERTIES_FILE_NAME
					  valueFrom:
						configMapKeyRef:
						  name: game-demo
						  key: ui_properties_file_name
				  volumeMounts:
				  - name: config
					mountPath: "/config"
					readOnly: true
			  volumes:
			  # You set volumes at the Pod level, then mount them into containers inside that Pod
			  - name: config
				configMap:
				  # Provide the name of the ConfigMap you want to mount.
				  name: game-demo
				  # An array of keys from the ConfigMap to create as files
				  items:
				  - key: "game.properties"
					path: "game.properties"
				  - key: "user-interface.properties"
					path: "user-interface.properties"
					
	- Code trong file mysecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			
	- Code trong file readsecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			ubuntu@ip-172-31-36-50:~$ cat readsecret.yaml
			apiVersion: v1
			kind: Pod
			metadata:
			  name: secret-env-pod
			spec:
			  containers:
				- name: mycontainer
				  image: redis
				  env:
					- name: SECRET_USERNAME
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: username
						  optional: false  # same as default; "mysecret" must exist and include a key named "username"

					- name: SECRET_PASSWORD
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: password
						  optional: false  # same as default; "mysecret" must exist and include a key named "password"

			  restartPolicy: Never
			  
	- Code trong file vprodep.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080

	- Code trong file vprosvc.yaml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080
			ubuntu@ip-172-31-36-50:~/vprofile$ cat vprosvc.yaml
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-app
			spec:
			  ports:
			  - port: 8080
				protocol: TCP
				targetPort: 8080
			  selector:
				run: my-app
			  type: ClusterIP

	- Code trong file vprosvc.yaml:

		apiVersion: v1
		kind: Service
		metadata:
		  name: my-app
		spec:
		  ports:
		  - port: 8080
			protocol: TCP
			targetPort: 8080
		  selector:
			run: my-app
		  type: ClusterIP
	- Code trong file vproingress.yaml:
	
		Case 1:
		
			apiVersion: networking.k8s.io/v1
			kind: Ingress
			metadata:
			  name: vpro-ingress
			  annotations:
				nginx.ingress.kubernetes.io/use-regex: "true"
			spec:
			  ingressClassName: nginx
			  rules:
			  - host: vprofile.vnstudio.info
				http:
				  paths:
				  - path: /
					pathType: Prefix
					backend:
					  service:
						name: my-app
						port:
						  number: 8080
						  
	- Code trong file ngpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  labels:
				run: nginxpod
			  name: nginxpod
			spec:
			  containers:
			  - image: nginx
				name: nginxpod

	- Code trong file sampleds.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: DaemonSet
			metadata:
			  name: fluentd-elasticsearch
			  namespace: kube-system
			  labels:
				k8s-app: fluentd-logging
			spec:
			  selector:
				matchLabels:
				  name: fluentd-elasticsearch
			  template:
				metadata:
				  labels:
					name: fluentd-elasticsearch
				spec:
				  tolerations:
				  # these tolerations are to have the daemonset runnable on control plane nodes
				  # remove them if your control plane nodes should not run pods
				  - key: node-role.kubernetes.io/control-plane
					operator: Exists
					effect: NoSchedule
				  - key: node-role.kubernetes.io/master
					operator: Exists
					effect: NoSchedule
				  containers:
				  - name: fluentd-elasticsearch
					image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
					resources:
					  limits:
						memory: 200Mi
					  requests:
						cpu: 100m
						memory: 200Mi
					volumeMounts:
					- name: varlog
					  mountPath: /var/log
				  # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
				  # preempts running Pods
				  # priorityClassName: important
				  terminationGracePeriodSeconds: 30
				  volumes:
				  - name: varlog
					hostPath:
					  path: /var/log

	- Code trong file eks-cluster.tf:
	
		Case 1:
		
			module "eks" {
			  source = "terraform-aws-modules/eks/aws"
			  version = "19.0.4"

			  cluster_name = local.cluster_name
			  cluster_version = "1.30"

			  vpc_id = module.vpc.vpc_id
			  subnet_ids = module.vpc.private_subnets
			  cluster_endpoint_public_access = true

			  eks_managed_node_group_defaults = {
				ami_type = "AL2_x86_64"

			  }

			  eks_managed_node_groups = {
				one = {
				  name = "node-group-1"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 3
				  desired_size = 2
				}

				two = {
				  name = "node-group-2"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 2
				  desired_size = 1
				}
			  }
			}

	- Code trong file main.tf:
	
		Case 1:
		
			provider "kubernetes" {
			  host = module.eks.cluster_endpoint
			  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
			}

			provider "aws" {
			  region = var.region
			}

			data "aws_availability_zones" "available" {}

			locals {
			  cluster_name = var.clusterName
			}

	- Code trong file outputs.tf:
	
		Case 1:
		
			output "cluster_name" {
			  description = "Amazon Web Service EKS Cluster Name"
			  value = module.eks.cluster_name
			}

			output "cluster_endpoint" {
			  description = "Endpoint for Amazon Web Service EKS "
			  value = module.eks.cluster_endpoint
			}

			output "region" {
			  description = "Amazon Web Service EKS Cluster region"
			  value = var.region
			}


			output "cluster_security_group_id" {
			  description = "Security group ID for the Amazon Web Service EKS Cluster "
			  value = module.eks.cluster_security_group_id
			}

	- Code trong file terraform.tf:
	
		Case 1:
		
			terraform {
			  required_providers {
				aws = {
				  source = "hashicorp/aws"
				  version = "~> 4.46.0"
				}

				random = {
				  source = "hashicorp/random"
				  version = "~> 3.4.3"
				}

				tls = {
				  source = "hashicorp/tls"
				  version = "~> 4.0.4"
				}

				cloudinit = {
				  source = "hashicorp/cloudinit"
				  version = "~> 2.2.0"
				}

				kubernetes = {
				  source = "hashicorp/kubernetes"
				  version = "~> 2.16.1"
				}
			  }

			  backend "s3" {
				bucket         	   = "terraform-eks112211"
				key              	   = "state/terraform.tfstate"
				region         	   = "us-east-1"
			  }

			  required_version = "~> 1.3"
			}
			
	- Code trong file variables.tf:
	
		Case 1:
		
			variable "region" {
			  description = "AWS region"
			 type = string
			 default = "us-east-1"
			}

			variable "clusterName" {
			  description = "Name of the EKS cluster"
			 type = string
			 default = "vpro-eks"
			}
			
	- Code trong file vpc.tf:
	
		Case 1:
		
			module "vpc" {
			  source = "terraform-aws-modules/vpc/aws"
			  version = "3.14.2"

			  name = "vprofile-eks"

			  cidr = "172.20.0.0/16"
			  azs = slice(data.aws_availability_zones.available.names, 0, 3)

			  private_subnets = ["172.20.1.0/24", "172.20.2.0/24", "172.20.3.0/24"]
			  public_subnets = ["172.20.4.0/24", "172.20.5.0/24", "172.20.6.0/24"]

			  enable_nat_gateway = true
			  single_nat_gateway = true
			  enable_dns_hostnames = true

			  public_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/elb" = 1
			  }

			  private_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/internal-elb" = 1
			  }
			}


--- App Deployment on Kubernetes Cluster:

	- Google search:
	
		kubernetes persistent volume claim
		
		kubernetes deploy yaml
		
		https://github.com/marketplace?type=
		
		helm docs install helm
		
		

	- Commands:
	
		- cmd: echo -n "vprodbpass" | base64
		- cmd: echo -n "guest" | base64
		- cmd: kubectl create -f .
		- cmd: kubectl delete -f .
		- cmd: kubectl get sc
		- cmd: kubectl get pods -n kube-system | grep ingress
		- cmd: kubectl describe ingress vpro-ingress
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
		- cmd: kops delete cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes
		- cmd: choco install kubernetes-helm
		- cmd: helm create vprofilecharts
		- cmd: aws eks list-clusters --region us-east-1
		
		
		
--- GitOps Project:

	- Amazon Container Registry
	- Gibhub -> Project instance -> Setting -> Branches -> Add branch protection rules -> Require a pull request before merging
	- Gibhub -> Project instance -> Pull requests -> New pull request
	- SonarCloud -> New... -> Create new organization -> create one manually
	- SonarCloud -> My Account -> Security -> Generate Token
	- SonarCloud -> My organization -> instance -> Quality Gates -> Create -> Add Condition
	- SonarCloud -> My Projects -> instance -> Administration -> Quality Gate -> Use a specific quality gate
	
	
	- Google search:
	
		terraform aws modules vpc
		
		


	- Command:
	
		- cmd: cd ~/.ssh
		- cmd: export GIT_SSH_COMMAND="ssh -i ~/.ssh/actions"
		- cmd: git config core.sshCommand "ssh -i ~/.ssh/actions -F /dev/null"
		- cmd: git config --global user.name ngoctuan99
		- cmd: git config --global user.email ngoctuanqng1@gmail.com
		- cmd: git fetch upstream
		- cmd: git merge stage
		- cmd: git fetch origin stage
		- cmd: git checkout main
		- cmd: git merge origin/stage
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vprofile-eks
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: helm list
		- cmd: helm uninstall vprofile-stack
		- cmd: terraform init -backend-config="bucket=vprofileactions112211"
		
		
		
	GitOps Project - S∆° ƒë·ªì t·ªïng th·ªÉ c·ªßa GitOps CI/CD pipeline trong m√¥i tr∆∞·ªùng DevOps, k·∫øt h·ª£p GitHub,
	Terraform, EKS, Docker, Maven, Helm, SonarCloud:
	
		Visual Studio Code (VS Code):
		
			L√† m√¥i tr∆∞·ªùng local IDE c·ªßa developer.

			Dev s·∫Ω code, commit v√† push l√™n GitHub.

			Repo n√†y c√≥ th·ªÉ g·ªìm:

				main branch (s·∫£n xu·∫•t)

				stage branch (m√¥i tr∆∞·ªùng staging/test)

			üí° Dev l√†m vi·ªác ch·ªß y·∫øu ·ªü stage, sau khi test xong m·ªõi merge v·ªÅ main.
			
		GitHub (Git + Actions):
		
			ƒê√¢y l√† trung t√¢m ƒëi·ªÅu ph·ªëi CI/CD.

			G·ªìm 2 workflow ch√≠nh:
			
				Terraform Workflow (b√™n tr√°i):

					D√†nh cho h·∫° t·∫ßng (Infrastructure as Code).

					Khi c√≥ thay ƒë·ªïi trong repo ‚ÄúIaC‚Äù (infrastructure code), GitHub Actions s·∫Ω:

						Fetch stage branch ‚Üí L·∫•y code h·∫° t·∫ßng t·ª´ nh√°nh stage.

						Terraform Plan/Test ‚Üí Ki·ªÉm tra xem vi·ªác apply c√≥ an to√†n kh√¥ng, h·∫° t·∫ßng c√≥ thay ƒë·ªïi g√¨.

						Khi PR ƒë∆∞·ª£c merge v√†o main, th√¨ GitHub Actions ch·∫°y:

							terraform apply


								ƒë·ªÉ tri·ªÉn khai (apply) thay ƒë·ªïi l√™n AWS.
		
				Build, Test & Deploy Workflow (b√™n ph·∫£i):

					D√†nh cho ·ª©ng d·ª•ng (Application Code).

					Khi code app thay ƒë·ªïi, GitHub Actions s·∫Ω:

						Fetch code t·ª´ repo (branch t∆∞∆°ng ·ª©ng).

						Maven ‚Üí build v√† ch·∫°y unit test (Java project).

						SonarCloud ‚Üí qu√©t ch·∫•t l∆∞·ª£ng code (code quality & security scan).

						Docker ‚Üí build image v√† push l√™n Amazon ECR (Elastic Container Registry).

						Helm Charts ‚Üí tri·ªÉn khai image m·ªõi l√™n Amazon EKS (Kubernetes cluster).		
		
		AWS Cloud (ph·∫ßn trung t√¢m)

			ƒê√¢y l√† n∆°i to√†n b·ªô h·∫° t·∫ßng v√† app ch·∫°y.

			C√°c th√†nh ph·∫ßn ch√≠nh:

				Amazon ECR: n∆°i l∆∞u tr·ªØ Docker image (container registry c·ªßa AWS).

				Amazon EKS: Kubernetes cluster ‚Äî n∆°i ·ª©ng d·ª•ng th·ª±c t·∫ø ƒë∆∞·ª£c deploy.

				VPC Subnet: m·∫°ng n·ªôi b·ªô AWS, ch·ª©a EKS cluster, ECR, security groups, subnet, etc.

			M·ªëi quan h·ªá:

				Terraform qu·∫£n l√Ω h·∫° t·∫ßng AWS (EKS, VPC, ECR,...).

				GitHub Actions qu·∫£n l√Ω pipeline build v√† deploy app v√†o ƒë√≥.
				
		Quy tr√¨nh t·ªïng th·ªÉ:
		
			Giai ƒëo·∫°n	H√†nh ƒë·ªông												C√¥ng c·ª•
			Dev Code	Code v√† push l√™n GitHub (stage)							VS Code, Git
			IaC Test	GitHub Actions ch·∫°y terraform plan						Terraform
			Merge		Khi PR t·ª´ stage ‚Üí main, GitHub ch·∫°y terraform apply		Terraform
			Build		Build app (Maven), test code (SonarCloud)				GitHub Actions
			Image		Docker build ‚Üí push l√™n ECR								Docker, ECR
			Deploy		Helm deploy image l√™n EKS								Helm, Kubernetes
			Run	App 	ch·∫°y th·ª±c t·∫ø tr√™n AWS									AWS EKS
		
		
		
	- Code trong file terraform.yml:

		Case 1:
		
			name: "Vprofile IAC"
			on:
			  push:
				  branches:
					  - main
					  - stage
				  paths:
					  - terraform/**
			  pull_request:
				  branches:
					  - main
				  paths:
					  - terraform/**

			env:
			  # Credentials for deployment to AWS
			  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
			  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
			  # S3 bucket for the Terraform state
			  BUCKET_TF_STATE: ${{ secrets.BUCKET_TF_STATE }}
			  AWS_REGION: us-east-1
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  terraform:
				name: "Apply terraform code changes"
				runs-on: ubuntu-latest
				defaults:
				  run:
					shell: bash
					working-directory: ./terraform
				steps:
				  - name: Checkout source code
					uses: actions/checkout@v2
				  - name: Setup Terraform with specified version on the runner
					uses: hashicorp/setup-terraform@v2
					with:
					  terraform_version: 1.6.3
				  - name: Terraform init
					id: init
					run: terraform init -backend-config="bucket=$BUCKET_TF_STATE"
				  - name: Terraform format
					id: fmt
					run: terraform fmt -check
				  - name: Terraform validate
					id: validate
					run: terraform validate
				  - name: Terraform plan
					id: plan
					run: terraform plan -no-color -input=false -out planfile
				  - name: Terraform plan status
					if: steps.plan.outcome == 'failure'
					run: exit 1

				  - name: Terraform Apply
					id: apple
					if: github.ref == 'refs/heads/main' && github.event_name == 'push'
					run: terraform apply -auto-approve -input=false -parallelism=1 planfile

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					id: getconfig
					if: steps.apple.outcome == 'success'
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Install Ingress controller
					if: steps.apple.outcome == 'success' && steps.getconfig.outcome == 'success'
					run: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		
	- Code trong file vproappdep.yml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: vproapp
			  labels: 
				app: vproapp
			spec:
			  replicas: 1
			  selector:
				matchLabels:
				  app: vproapp
			  template:
				metadata:
				  labels:
					app: vproapp
				spec:
				  containers:
				  - name: vproapp
					image: "{{ .Values.appimage }}:{{ .Values.apptag }}"
					ports:
					- name: vproapp-port
					  containerPort: 8080
				  initContainers:
				  - name: init-mydb
					image: busybox
					command: ['sh', '-c', 'until nslookup vprodb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']
				  - name: init-memcache
					image: busybox
					command: ['sh', '-c', 'until nslookup vprocache01.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']

	- Code trong file main.yml:

		Case 1:
		
			name: vprofile actions
			on: workflow_dispatch
			env:
			  AWS_REGION: us-east-1
			  ECR_REPOSITORY: vprofileapp
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  Testing:
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Maven test
					run: mvn test

				  - name: Checkstyle
					run: mvn checkstyle:checkstyle

				  # Setup java 11 to be default (sonar-scanner requirement as of 5.x)
				  - name: Set Java 11
					uses: actions/setup-java@v3
					with:
					  distribution: 'temurin' # See 'supported distributions' for availabel
					  java-version: '11'

				  # Setup sonar-scanner
				  - name: Setup SonarQube
					uses: warchant/setup-sonar-scanner@v7

				  # Run sonar-scanner
				  - name: SonarQube Scan
					run: sonar-scanner
						-Dsonar.host.url=${{ secrets.SONAR_URL }}
						-Dsonar.login=${{ secrets.SONAR_TOKEN }}
						-Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }}
						-Dsonar.projectKey=${{ secrets.SONAR_PROJECT_KEY }}
						-Dsonar.sources=src/
						-Dsonar.junit.reportsPath=target/surefire-reports/
						-Dsonar.jacoco.reportsPath=target/jacoco.exec
						-Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml
						-Dsonar.java.binaries=target/test-classes/com/visualpathit/account

				  # Check the Quality Gate status.
				  - name: SonarQube Quality Gate check
					id: sonarqube-quality-gate-check
					uses: sonarsource/sonarqube-quality-gate-action@master
					# Force to fail step after specific time.
					timeout-minutes: 5
					env:
					  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
					  SONAR_HOST_URL: ${{ secrets.SONAR_URL }} # OPTIONAL

			  BUILD_AND_PUBLISH:
				needs: Testing
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Build & Upload image to ECR
					uses: appleboy/docker-ecr-action@master
					with:
						access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
						secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
						registry: ${{ secrets.REGISTRY }}
						repo: ${{ env.ECR_REPOSITORY }}
						region: ${{ env.AWS_REGION }}
						tags: latest,${{ github.run_number }}
						daemon_off: false
						dockerfile: ./Dockerfile
						context: ./

			  DeployToEKS:
				needs: BUILD_AND_PUBLISH
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Print config file
					run: cat ~/.kube/config

				  - name: Login to ECR
					run: |
					  kubectl delete secret regcred --ignore-not-found
					  kubectl create secret docker-registry regcred --docker-server=${{ secrets.REGISTRY }} --docker-username=AWS --docker-password=$(aws ecr get-login-password)

				  - name: Deploy Helm
					uses: bitovi/github-actions-deploy-eks-helm@v1.2.8
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ secrets.AWS_REGION }}
					  cluster-name: ${{ env.EKS_CLUSTER }}
					  #configfiles: .github/values/dev.yaml
					  chart-path: helm/vprofilecharts
					  namespace: default
					  value: appimage=${{ secrets.REGISTRY }}/${{ env.ECR_REPOSITORY }},apptag=${{ github.run_number }}
					  name: vprofile-stack
		
		
		
		
		