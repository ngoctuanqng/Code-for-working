- Danh sÃ¡ch chÆ°a lÃ m Ä‘Æ°á»£c:

	Cáº§n Ä‘á»c hiá»ƒu pháº§n VPC trong AWS part 2

	AWS Part 1:
	
		EC2 Load Balancer
		
	AWS Cloud For Project Set Up | Lift & Shift:
	
		ChÆ°a Ä‘Äƒng nháº­p web thÃ nh cÃ´ng vá»›i user, password lÃ  admin_vp
		
	Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:
	
		Build & Deploy Artifact:
		
			Pháº§n target group kiá»ƒm tra váº«n unhealthy
			
	Jenkins:
	
		Jenkins Master and Slave tá»« chá»— nÃ y vá» sau, xem xÃ©t chá»¥p áº£nh má»›i vÃ  lÃ m láº¡i
		
	Terraform:
	
		ChÆ°a upload Ä‘Æ°á»£c file lÃªn s3 á»Ÿ pháº§n cuá»‘i
		
	Ansible:
	
		Chá»— roles cÃ³ pháº§n cháº¡y bá»‹ lá»—i
		
	AWS Part 2:
	
		Bá»‹ lá»—i táº¡i Website in VPC
		
	Docker:
	
		Building images
		
			ChÆ°a thá»ƒ show Ä‘Ãºng trang web
			
	Containerization:
	
		Pháº§n cuá»‘i cháº¡y theo source git cá»§a tÃ¡c giáº£ chÆ°a Ä‘Æ°á»£c
		
	App Deployment on Kubernetes Cluster:
	
		Deploy App on K8s Cluster:
		
			Ingress chÆ°a cÃ³ address
			
	GitOps Project:
	
		Deploy to EKS:
		
			ChÆ°a cháº¡y lÃªn trang web Ä‘Æ°á»£c


- Iaas, Paas
- ORACLE VM VIRTUAL BOX
- AWS Command line Interface











- GIT BASH

	Git Bash lÃ  má»™t á»©ng dá»¥ng giáº£ láº­p dÃ²ng lá»‡nh (terminal) dÃ nh cho Windows.

	NÃ³ cung cáº¥p:

		Git CLI (Command Line Interface) â†’ Ä‘á»ƒ cháº¡y cÃ¡c lá»‡nh Git (git init, git clone, git commit, â€¦).

		Bash shell â†’ má»™t mÃ´i trÆ°á»ng giá»‘ng Linux/Unix Ä‘á»ƒ cháº¡y cÃ¡c lá»‡nh bash (ls, pwd, cd, rm, touch, â€¦).

	NÃ³i Ä‘Æ¡n giáº£n: Git Bash giÃºp ngÆ°á»i dÃ¹ng Windows cÃ³ tráº£i nghiá»‡m giá»‘ng nhÆ° Ä‘ang lÃ m viá»‡c trÃªn Linux khi dÃ¹ng Git.
	
	Táº¡i sao cáº§n Git Bash?

		TrÃªn Linux/Mac, Git thÆ°á»ng Ä‘i kÃ¨m vá»›i Bash sáºµn cÃ³.

		NhÆ°ng Windows khÃ´ng cÃ³ Bash gá»‘c â†’ vÃ¬ váº­y Git for Windows ra Ä‘á»i, kÃ¨m theo Git Bash Ä‘á»ƒ:

			DÃ¹ng lá»‡nh Git thuáº­n tiá»‡n.

			DÃ¹ng cÃ¡c lá»‡nh bash cÆ¡ báº£n (thay vÃ¬ chá»‰ Command Prompt hay PowerShell).
			
	CÃ¡c thÃ nh pháº§n chÃ­nh

		Git â†’ cÃ´ng cá»¥ quáº£n lÃ½ phiÃªn báº£n.

		Bash â†’ shell dá»±a trÃªn MSYS2 (má»™t mÃ´i trÆ°á»ng giÃºp mang Linux tools sang Windows).

		Unix tools â†’ báº¡n cÃ³ thá»ƒ dÃ¹ng nhiá»u lá»‡nh quen thuá»™c trong Linux (ssh, scp, cat, nano, vim, â€¦).

- VAGRANT

	Vagrant lÃ  má»™t cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ (open-source) dÃ¹ng Ä‘á»ƒ táº¡o, quáº£n lÃ½ vÃ  tá»± Ä‘á»™ng hÃ³a mÃ´i trÆ°á»ng mÃ¡y áº£o (VMs).

	NÃ³ giÃºp láº­p trÃ¬nh viÃªn, tester, DevOpsâ€¦ cÃ³ thá»ƒ dá»±ng mÃ´i trÆ°á»ng phÃ¡t triá»ƒn giá»‘ng nhau chá»‰ vá»›i má»™t file cáº¥u
	hÃ¬nh (Vagrantfile).

	Vagrant thÆ°á»ng cháº¡y trÃªn VirtualBox, VMware, Hyper-V hoáº·c Docker.
	
	Æ¯u Ä‘iá»ƒm cá»§a Vagrant

		TÃ¡i táº¡o mÃ´i trÆ°á»ng dá»… dÃ ng

		Chá»‰ cáº§n 1 file Vagrantfile, báº¥t ká»³ ai cÅ©ng cÃ³ thá»ƒ táº¡o ra mÃ´i trÆ°á»ng y há»‡t.

		TrÃ¡nh lá»—i kiá»ƒu: "MÃ¡y anh cháº¡y Ä‘Æ°á»£c, mÃ¡y em khÃ´ng cháº¡y".

		TÃ­ch há»£p vá»›i nhiá»u provider

		Há»— trá»£ VirtualBox, VMware, Docker, AWS EC2...

		Tá»± Ä‘á»™ng hÃ³a

		CÃ³ thá»ƒ cÃ i Ä‘áº·t pháº§n má»m, cáº¥u hÃ¬nh há»‡ Ä‘iá»u hÃ nh thÃ´ng qua provisioning tools (Shell script, Ansible, Puppet, Chef...).

		Dá»… dÃ¹ng

		Lá»‡nh chÃ­nh thÆ°á»ng chá»‰ cÃ³ vagrant up, vagrant halt, vagrant destroy.
		
	CÃ¡ch hoáº¡t Ä‘á»™ng

		Báº¡n cÃ i Vagrant + má»™t provider (vÃ­ dá»¥ VirtualBox).

		Táº¡o file Vagrantfile mÃ´ táº£ mÃ´i trÆ°á»ng (OS, RAM, CPU, máº¡ng, pháº§n má»m cáº§n cÃ i).
		
		Cháº¡y lá»‡nh:
		
			vagrant up
			
				Vagrant sáº½ táº£i box (giá»‘ng nhÆ° 1 template OS) tá»« Vagrant Cloud, táº¡o mÃ¡y áº£o, cáº¥u hÃ¬nh máº¡ng, cÃ i pháº§n má»m.

- CHOCOLATEY/BREW
- SUBLIME TEXT EDITOR
- AWS CLI

- IAM WITH MFA
- Homebrew
- SonarSource:

	SonarSource lÃ  má»™t cÃ´ng ty cÃ´ng nghá»‡ chuyÃªn phÃ¡t triá»ƒn cÃ¡c cÃ´ng cá»¥ giÃºp phÃ¢n tÃ­ch cháº¥t
	lÆ°á»£ng mÃ£ nguá»“n vÃ  phÃ¡t hiá»‡n váº¥n Ä‘á» báº£o máº­t trong pháº§n má»m.
	
- SonarQube:

	CÃ´ng cá»¥ phÃ¢n tÃ­ch code phá»• biáº¿n nháº¥t cá»§a SonarSource.

	Há»— trá»£ hÆ¡n 25 ngÃ´n ngá»¯ láº­p trÃ¬nh (Java, C#, Python, JavaScript, C/C++, Goâ€¦).

	TÃ­ch há»£p vÃ o CI/CD Ä‘á»ƒ tá»± Ä‘á»™ng kiá»ƒm tra code má»—i khi build.

	GiÃºp phÃ¡t hiá»‡n:

		Bug (lá»—i cÃ³ thá»ƒ gÃ¢y crash hoáº·c káº¿t quáº£ sai).

		Code Smell (Ä‘oáº¡n code khÃ³ báº£o trÃ¬, khÃ´ng tá»‘i Æ°u).

		Security Vulnerability (lá»— há»•ng báº£o máº­t).

- SONARCLOUD:

	Dá»‹ch vá»¥ SaaS trÃªn cloud (khÃ´ng cáº§n cÃ i SonarQube server).

	DÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch code cá»§a dá»± Ã¡n public hoáº·c private trÃªn GitHub, GitLab, Bitbucket, Azure DevOps.
	
	

	Homebrew lÃ  má»™t trÃ¬nh quáº£n lÃ½ gÃ³i (package manager) phá»• biáº¿n trÃªn macOS (vÃ  Linux).

	NÃ³i Ä‘Æ¡n giáº£n, Homebrew giÃºp báº¡n cÃ i Ä‘áº·t, quáº£n lÃ½ vÃ  cáº­p nháº­t pháº§n má»m báº±ng cÃ¡c
	lá»‡nh trong terminal, thay vÃ¬ pháº£i táº£i file .dmg hoáº·c .pkg vÃ  cÃ i thá»§ cÃ´ng.
	
- choco
- MFA device

	MFA (Multi-Factor Authentication) = XÃ¡c thá»±c Ä‘a yáº¿u tá»‘.

	Khi báº­t MFA, ngoÃ i username + password, báº¡n cáº§n thÃªm mÃ£ OTP tá»« thiáº¿t bá»‹ MFA thÃ¬ má»›i Ä‘Äƒng nháº­p Ä‘Æ°á»£c.

	GiÃºp tÄƒng cÆ°á»ng báº£o máº­t cho tÃ i khoáº£n AWS (nháº¥t lÃ  vá»›i root user hoáº·c IAM user cÃ³ quyá»n cao).
	
	MFA Device (thiáº¿t bá»‹ MFA) trong AWS:
	
		MFA Device = nguá»“n phÃ¡t mÃ£ OTP Ä‘á»ƒ Ä‘Äƒng nháº­p AWS.
		
		AWS há»— trá»£ nhiá»u loáº¡i thiáº¿t bá»‹ MFA:

		Virtual MFA device (phá»• biáº¿n nháº¥t):

			DÃ¹ng app trÃªn Ä‘iá»‡n thoáº¡i, vÃ­ dá»¥:

				Google Authenticator

				Authy

				Microsoft Authenticator

				LastPass Authenticator

			Khi Ä‘Äƒng nháº­p AWS, báº¡n má»Ÿ app â†’ nháº­p mÃ£ OTP hiá»ƒn thá»‹ (thay Ä‘á»•i má»—i 30 giÃ¢y).

		Hardware MFA device (pháº§n cá»©ng):

			Thiáº¿t bá»‹ váº­t lÃ½ nhá» (giá»‘ng USB token).

			VÃ­ dá»¥: Gemalto, Yubikey.

			PhÃ¹ há»£p cho mÃ´i trÆ°á»ng cáº§n báº£o máº­t cao.

		U2F security key (khÃ³a báº£o máº­t chuáº©n FIDO):

			DÃ¹ng USB/NFC key (nhÆ° Yubico YubiKey).

			Chá»‰ cáº§n cáº¯m vÃ o mÃ¡y hoáº·c cháº¡m vÃ o khi AWS yÃªu cáº§u xÃ¡c thá»±c.

		SMS MFA (Ã­t dÃ¹ng):

			AWS gá»­i mÃ£ OTP qua tin nháº¯n SMS.

			Tuy nhiÃªn kÃ©m an toÃ n (dá»… bá»‹ hack SIM), nÃªn Ã­t Ä‘Æ°á»£c khuyÃªn dÃ¹ng.
			
	CÃ¡ch hoáº¡t Ä‘á»™ng:
	
		ÄÄƒng nháº­p vÃ o AWS Console báº±ng username & password.

		AWS yÃªu cáº§u MFA code.

		Báº¡n nháº­p mÃ£ OTP tá»« MFA device (app hoáº·c hardware).

		Náº¿u Ä‘Ãºng â†’ vÃ o Ä‘Æ°á»£c AWS Console.
		
	Táº¡i sao nÃªn dÃ¹ng MFA?
	
		NgÄƒn hacker truy cáº­p ngay cáº£ khi máº­t kháº©u bá»‹ lá»™.

		Báº£o vá»‡ root account (tÃ i khoáº£n quáº£n trá»‹ tá»‘i cao trong AWS).

		TÄƒng cÆ°á»ng tuÃ¢n thá»§ báº£o máº­t (PCI-DSS, ISO, SOC2â€¦).
		
	AWS MFA Device = thiáº¿t bá»‹ phÃ¡t mÃ£ OTP (virtual app, USB key, hardware tokenâ€¦) Ä‘á»ƒ Ä‘Äƒng nháº­p AWS an toÃ n hÆ¡n.

- .csv file AWS
- Cloudwatch AWS
- SNS topic AWS
- EC2 AWS

	EC2 (Elastic Compute Cloud) lÃ  dá»‹ch vá»¥ mÃ¡y chá»§ áº£o (virtual server) trÃªn ná»n táº£ng AWS (Amazon Web Services).

	NÃ³i nÃ´m na: EC2 giá»‘ng nhÆ° báº¡n thuÃª má»™t cÃ¡i mÃ¡y tÃ­nh trong Ä‘Ã¡m mÃ¢y Ä‘á»ƒ cÃ i Ä‘áº·t há»‡ Ä‘iá»u hÃ nh,
	pháº§n má»m, web server, databaseâ€¦ vÃ  cháº¡y á»©ng dá»¥ng cá»§a báº¡n.
	
	Khi táº¡o má»™t EC2 Instance, báº¡n cáº§n chá»n:
	
		AMI (Amazon Machine Image) â†’ giá»‘ng nhÆ° chá»n há»‡ Ä‘iá»u hÃ nh (Ubuntu, Amazon Linux, Windowsâ€¦).

		Instance type â†’ cáº¥u hÃ¬nh pháº§n cá»©ng (t2.micro, m5.large, â€¦).

		Storage (EBS) â†’ dung lÆ°á»£ng á»• cá»©ng.

		Security Group â†’ giá»‘ng firewall, cho phÃ©p má»Ÿ port (vÃ­ dá»¥: 22 cho SSH, 80 cho HTTP, 443 cho HTTPS).

		Key Pair â†’ Ä‘á»ƒ Ä‘Äƒng nháº­p vÃ o server qua SSH.
		
	EC2 dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?
	
		Host website hoáº·c API.

		Cháº¡y database (MySQL, PostgreSQL, MongoDBâ€¦).

		Cháº¡y á»©ng dá»¥ng microservices, container (Docker, Kubernetes).

		LÃ m mÃ¡y chá»§ dev/test cho láº­p trÃ¬nh viÃªn.

		Xá»­ lÃ½ big data, AI/ML, game server.
		
	EC2 = MÃ¡y chá»§ áº£o trong AWS â†’ báº¡n muá»‘n cháº¡y app/web gÃ¬ thÃ¬ chá»‰ cáº§n táº¡o má»™t EC2 instance rá»“i cÃ i Ä‘áº·t nhÆ° trÃªn mÃ¡y tÃ­nh tháº­t.

- AWS Certificate Manager:

	ACM lÃ  dá»‹ch vá»¥ cá»§a AWS Ä‘á»ƒ cáº¥p phÃ¡t, quáº£n lÃ½ vÃ  triá»ƒn khai chá»©ng chá»‰ SSL/TLS cho á»©ng dá»¥ng, website vÃ  dá»‹ch vá»¥ AWS.

	NÃ³ giÃºp báº¡n báº£o máº­t káº¿t ná»‘i HTTPS mÃ  khÃ´ng cáº§n tá»± mua hoáº·c tá»± quáº£n lÃ½ certificate thá»§ cÃ´ng.

	NÃ³i ngáº¯n gá»n: ACM = dá»‹ch vá»¥ quáº£n lÃ½ chá»©ng chá»‰ SSL/TLS trong AWS.
	
	TÃ­nh nÄƒng chÃ­nh

		Cáº¥p phÃ¡t (Provision)

			ACM cÃ³ thá»ƒ cáº¥p chá»©ng chá»‰ SSL/TLS miá»…n phÃ­ cho domain cá»§a báº¡n.

			VÃ­ dá»¥: www.myapp.com cÃ³ thá»ƒ xin chá»©ng chá»‰ tá»« ACM thay vÃ¬ mua ngoÃ i.

		Triá»ƒn khai (Deploy)

			DÃ¹ng trá»±c tiáº¿p vá»›i cÃ¡c dá»‹ch vá»¥ AWS:

				Elastic Load Balancer (ELB)

				CloudFront (CDN)

				API Gateway

				App Runner / Elastic Beanstalk

		Gia háº¡n tá»± Ä‘á»™ng (Auto-renewal)

			Chá»©ng chá»‰ do ACM cáº¥p Ä‘Æ°á»£c tá»± Ä‘á»™ng gia háº¡n trÆ°á»›c khi háº¿t háº¡n â†’ khÃ´ng lo downtime vÃ¬ quÃªn renew SSL.

		Import Certificate (Nháº­p chá»©ng chá»‰)

			Náº¿u báº¡n Ä‘Ã£ cÃ³ chá»©ng chá»‰ tá»« CA khÃ¡c (VÃ­ dá»¥: DigiCert, GoDaddy, Letâ€™s Encrypt), báº¡n cÃ³ thá»ƒ import vÃ o ACM Ä‘á»ƒ quáº£n lÃ½.

		Quáº£n lÃ½ táº­p trung

			LÆ°u trá»¯ vÃ  phÃ¢n phá»‘i chá»©ng chá»‰ an toÃ n.

			KhÃ´ng cáº§n copy file .crt vÃ  .key thá»§ cÃ´ng.

- RSA 2048 AWS
- VMware
- Host OS
- Guest OS
- VM
- Snapshot
- Hypervisor
- CentOS VM, Ubuntu VM

	CentOS VM

		CentOS (Community ENTerprise Operating System) lÃ  má»™t báº£n phÃ¢n phá»‘i Linux dá»±a trÃªn Red Hat Enterprise Linux (RHEL).

		ThÆ°á»ng dÃ¹ng trong doanh nghiá»‡p, server production vÃ¬:

			á»”n Ä‘á»‹nh, Ã­t thay Ä‘á»•i.

			Chu ká»³ phÃ¡t hÃ nh dÃ i (7â€“10 nÄƒm).

			TÆ°Æ¡ng thÃ­ch vá»›i pháº§n má»m viáº¿t cho RHEL.

		Package Manager: yum (CentOS 7), dnf (CentOS 8+).

		ThÃ­ch há»£p: Web server (Apache, Nginx), Database server (MySQL, PostgreSQL), mÃ´i trÆ°á»ng Enterprise.

		Tuy nhiÃªn: CentOS Linux chÃ­nh thá»©c bá»‹ ngá»«ng phÃ¡t triá»ƒn tá»« 2021, thay tháº¿ báº±ng CentOS Stream (rolling release, cáº­p nháº­t nhanh hÆ¡n).
		
	Ubuntu VM

		Ubuntu lÃ  báº£n phÃ¢n phá»‘i Linux dá»±a trÃªn Debian.

		ThÃ¢n thiá»‡n hÆ¡n vá»›i ngÆ°á»i dÃ¹ng, tÃ i liá»‡u vÃ  cá»™ng Ä‘á»“ng há»— trá»£ cá»±c nhiá»u.

		Package Manager: apt (Advanced Package Tool).

		CÃ³ nhiá»u phiÃªn báº£n:

			Ubuntu Server â†’ cháº¡y trÃªn cloud, server.

			Ubuntu Desktop â†’ dÃ¹ng lÃ m há»‡ Ä‘iá»u hÃ nh cÃ¡ nhÃ¢n.

		PhÃ¹ há»£p: Development, AI/ML, Cloud (AWS, Azure, GCP), container (Docker, Kubernetes).
		
- RHEL (Red Hat Enterprise Linux) vÃ  Debian:

	RHEL (Red Hat Enterprise Linux)

		Nguá»“n gá»‘c: Dá»±a trÃªn Linux kernel, phÃ¡t triá»ƒn bá»Ÿi Red Hat.

		Äá»‹nh hÆ°á»›ng: Há»‡ Ä‘iá»u hÃ nh thÆ°Æ¡ng máº¡i, enterprise, dÃ¹ng trong doanh nghiá»‡p vÃ  data center.

		Há»— trá»£: CÃ³ há»£p Ä‘á»“ng support tá»« Red Hat (SLA, cáº­p nháº­t báº£o máº­t, bug fix).

		Package manager: rpm (Red Hat Package Manager) + yum/dnf.

		Æ¯u Ä‘iá»ƒm:

			á»”n Ä‘á»‹nh, chu ká»³ release lÃ¢u dÃ i (7â€“10 nÄƒm).

			ÄÆ°á»£c chá»©ng nháº­n Ä‘á»ƒ cháº¡y nhiá»u pháº§n má»m enterprise (Oracle DB, SAP, VMware...).

			Há»— trá»£ tá»‘t cho server váº­t lÃ½ vÃ  mÃ¡y áº£o.

		NhÆ°á»£c Ä‘iá»ƒm:

			Pháº£i tráº£ phÃ­ Ä‘á»ƒ dÃ¹ng báº£n chÃ­nh thá»©c vÃ  nháº­n cáº­p nháº­t.

			TÃ i liá»‡u cá»™ng Ä‘á»“ng Ã­t hÆ¡n so vá»›i Ubuntu/Debian.

		Báº£n miá»…n phÃ­, cá»™ng Ä‘á»“ng hÃ³a cá»§a RHEL: CentOS, Rocky Linux, AlmaLinux.
		
	Debian

		Nguá»“n gá»‘c: Má»™t trong nhá»¯ng báº£n Linux lÃ¢u Ä‘á»i nháº¥t, phÃ¡t triá»ƒn bá»Ÿi cá»™ng Ä‘á»“ng (ra Ä‘á»i 1993).

		Äá»‹nh hÆ°á»›ng: Há»‡ Ä‘iá»u hÃ nh miá»…n phÃ­, mÃ£ nguá»“n má»Ÿ, táº­p trung vÃ o sá»± á»•n Ä‘á»‹nh vÃ  tá»± do pháº§n má»m.

		Package manager: dpkg + apt.

		Æ¯u Ä‘iá»ƒm:

			Ráº¥t á»•n Ä‘á»‹nh, uy tÃ­n lÃ¢u Ä‘á»i.

			LÃ  ná»n táº£ng cho nhiá»u distro phá»• biáº¿n (nhÆ° Ubuntu, Linux Mint...).

			HoÃ n toÃ n miá»…n phÃ­.

			Cá»™ng Ä‘á»“ng há»— trá»£ rá»™ng rÃ£i.

		NhÆ°á»£c Ä‘iá»ƒm:

			KhÃ´ng cÃ³ há»— trá»£ thÆ°Æ¡ng máº¡i chÃ­nh thá»©c (trá»« khi thÃ´ng qua bÃªn thá»© 3).

			Chu ká»³ phÃ¡t hÃ nh hÆ¡i cháº­m â†’ Ã­t cáº­p nháº­t tÃ­nh nÄƒng má»›i so vá»›i Ubuntu.

- Linux distros

	Linux Distro lÃ  gÃ¬?

		Linux Distro (Linux Distribution) = báº£n phÃ¢n phá»‘i Linux.

		Linux chá»‰ cÃ³ kernel (nhÃ¢n há»‡ Ä‘iá»u hÃ nh). Muá»‘n dÃ¹ng Ä‘Æ°á»£c, ngÆ°á»i ta pháº£i thÃªm:

			TrÃ¬nh quáº£n lÃ½ gÃ³i (package manager)

			ThÆ° viá»‡n há»‡ thá»‘ng

			CÃ´ng cá»¥ quáº£n trá»‹

			MÃ´i trÆ°á»ng desktop (GUI) (náº¿u cáº§n)

			á»¨ng dá»¥ng máº·c Ä‘á»‹nh

		Khi káº¿t há»£p táº¥t cáº£ nhá»¯ng thÃ nh pháº§n nÃ y láº¡i, ta cÃ³ má»™t Linux Distribution â€“ tá»©c lÃ  má»™t "phiÃªn báº£n Linux" hoÃ n chá»‰nh.
		
	VÃ­ dá»¥ cÃ¡c Linux Distro phá»• biáº¿n:

		Dá»±a trÃªn Debian: Ubuntu, Linux Mint, Kali Linux.

		Dá»±a trÃªn RHEL: CentOS, Rocky Linux, AlmaLinux, Fedora.

		DÃ nh cho báº£o máº­t: Kali Linux, Parrot OS.

		DÃ nh cho ngÆ°á»i má»›i: Ubuntu, Linux Mint, Zorin OS.

		DÃ nh cho enterprise: RHEL, SUSE, Oracle Linux.

		DÃ nh cho developer/hacker thÃ­ch tuá»³ biáº¿n: Arch Linux, Gentoo.

- Putty
- Oracle VM Virtualbox, Virtualbox
- ISO file
- Vagrant
- BIOS

	BIOS = Basic Input/Output System.

	LÃ  má»™t pháº§n má»m há»‡ thá»‘ng cáº¥p tháº¥p Ä‘Æ°á»£c lÆ°u trong chip nhá»› ROM/Flash trÃªn mainboard.

	Khi báº¡n báº­t mÃ¡y tÃ­nh, BIOS sáº½ lÃ  chÆ°Æ¡ng trÃ¬nh cháº¡y Ä‘áº§u tiÃªn, trÆ°á»›c cáº£ há»‡ Ä‘iá»u hÃ nh (Windows, Linux, macOSâ€¦).
	
	Chá»©c nÄƒng chÃ­nh cá»§a BIOS

		POST (Power-On Self-Test)

			Kiá»ƒm tra pháº§n cá»©ng: RAM, CPU, bÃ n phÃ­m, á»• cá»©ngâ€¦ cÃ³ hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng khÃ´ng.

		Khá»Ÿi táº¡o pháº§n cá»©ng

			Gáº¯n driver cÆ¡ báº£n cho bÃ n phÃ­m, chuá»™t, card mÃ n hÃ¬nhâ€¦

		Chá»n thiáº¿t bá»‹ khá»Ÿi Ä‘á»™ng (Bootloader)

			Quyáº¿t Ä‘á»‹nh mÃ¡y sáº½ boot tá»« HDD/SSD, USB, CD/DVD hay Network.

		Cung cáº¥p giao diá»‡n cáº¥u hÃ¬nh (BIOS Setup Utility)

			Cho phÃ©p báº¡n chá»‰nh thá»i gian, thá»© tá»± boot, máº­t kháº©u BIOS, Ã©p xung CPU/RAM, báº­t/táº¯t thiáº¿t bá»‹â€¦
			
	BIOS = pháº§n má»m cháº¡y ngay khi báº­t mÃ¡y, nhiá»‡m vá»¥ lÃ  kiá»ƒm tra pháº§n cá»©ng vÃ  náº¡p há»‡ Ä‘iá»u hÃ nh.

	NgÃ y nay, pháº§n lá»›n mÃ¡y tÃ­nh má»›i Ä‘Ã£ dÃ¹ng UEFI, nhÆ°ng ngÆ°á»i ta váº«n hay gá»i chung lÃ  "BIOS".

- Vtx
- Reboot
- Ethernet (emp0s8), Ethernet (emp0s3)
- ACPI shutdown trong Oracle VM
- SSH
- NAT trong Oracle VM
- Vagrantfile
- Linux distros
- Desktop Linux OS

	Desktop Linux OS (hay cÃ²n gá»i lÃ  Linux Desktop Operating System) lÃ  má»™t há»‡ Ä‘iá»u hÃ nh Linux Ä‘Æ°á»£c thiáº¿t káº¿
	Ä‘á»ƒ sá»­ dá»¥ng trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n, laptop hoáº·c workstation vá»›i giao diá»‡n Ä‘á»“ há»a (GUI) thÃ¢n thiá»‡n, giá»‘ng
	nhÆ° Windows hay macOS.
	
	ThÃ nh pháº§n chÃ­nh cá»§a Desktop Linux OS

		Kernel (Linux kernel): lÃµi cá»§a há»‡ Ä‘iá»u hÃ nh, quáº£n lÃ½ pháº§n cá»©ng.

		Distro (báº£n phÃ¢n phá»‘i): vÃ­ dá»¥ Ubuntu, Fedora, Linux Mint, Manjaroâ€¦ â†’ cung cáº¥p bá»™ cÃ´ng cá»¥, á»©ng dá»¥ng, package manager.

		Desktop Environment (DE): giao diá»‡n Ä‘á»“ há»a ngÆ°á»i dÃ¹ng, nhÆ°:

			GNOME (Ubuntu)

			KDE Plasma (Kubuntu, openSUSE)

			XFCE, LXQt (nháº¹, tiáº¿t kiá»‡m tÃ i nguyÃªn)

		á»¨ng dá»¥ng ngÆ°á»i dÃ¹ng: trÃ¬nh duyá»‡t (Firefox, Chrome), bá»™ office (LibreOffice), media player, email client...
		
	Äáº·c Ä‘iá»ƒm cá»§a Desktop Linux OS

		Miá»…n phÃ­ & mÃ£ nguá»“n má»Ÿ.

		TÃ¹y biáº¿n cao (cÃ³ thá»ƒ thay Ä‘á»•i giao diá»‡n, DE, theme).

		á»”n Ä‘á»‹nh, báº£o máº­t (Ã­t virus hÆ¡n Windows).

		Há»— trá»£ pháº§n cá»©ng khÃ¡ rá»™ng, nhÆ°ng váº«n cÃ²n háº¡n cháº¿ vá»›i má»™t sá»‘ driver (vÃ­ dá»¥ card Ä‘á»“ há»a, mÃ¡y in Ä‘á»i má»›i).

		Nhiá»u báº£n phÃ¢n phá»‘i khÃ¡c nhau phá»¥c vá»¥ nhu cáº§u khÃ¡c nhau:

			ThÃ¢n thiá»‡n cho ngÆ°á»i má»›i: Ubuntu, Linux Mint, Zorin OS

			DÃ nh cho dev/pro: Fedora, openSUSE, Debian

			SiÃªu nháº¹: Lubuntu, Puppy Linux
			
	Desktop Linux OS lÃ  phiÃªn báº£n Linux Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i sáºµn vá»›i giao diá»‡n Ä‘á»“ há»a vÃ  á»©ng dá»¥ng cáº§n thiáº¿t cho ngÆ°á»i
	dÃ¹ng cÃ¡ nhÃ¢n, giá»‘ng nhÆ° Windows/macOS, nhÆ°ng miá»…n phÃ­ vÃ  tÃ¹y biáº¿n máº¡nh máº½.

- Server Linux OS

	Server Linux OS (Linux Server Operating System) lÃ  má»™t há»‡ Ä‘iá»u hÃ nh Linux Ä‘Æ°á»£c tá»‘i Æ°u Ä‘á»ƒ cháº¡y trÃªn mÃ¡y
	chá»§ (server) thay vÃ¬ mÃ¡y tÃ­nh cÃ¡ nhÃ¢n. NÃ³ cung cáº¥p mÃ´i trÆ°á»ng á»•n Ä‘á»‹nh, báº£o máº­t, vÃ  hiá»‡u suáº¥t cao Ä‘á»ƒ cháº¡y
	cÃ¡c dá»‹ch vá»¥ máº¡ng (web, database, mail, file server, container...).
	
	Äáº·c Ä‘iá»ƒm cá»§a Server Linux OS

		KhÃ´ng cáº§n giao diá»‡n Ä‘á»“ há»a (GUI) â†’ thÆ°á»ng chá»‰ cháº¡y á»Ÿ cháº¿ Ä‘á»™ command line (CLI) Ä‘á»ƒ tiáº¿t kiá»‡m tÃ i nguyÃªn.

		Tá»‘i Æ°u cho hiá»‡u suáº¥t vÃ  báº£o máº­t â†’ cháº¡y lÃ¢u dÃ i, Ã­t cáº§n reboot.

		Há»— trá»£ nhiá»u dá»‹ch vá»¥ server: web server (Apache, Nginx), database (MySQL, PostgreSQL), container (Docker, Kubernetes)...

		Cáº­p nháº­t á»•n Ä‘á»‹nh, vÃ²ng Ä‘á»i dÃ i (LTS â€“ Long Term Support).

		Kháº£ nÄƒng má»Ÿ rá»™ng (scalability) Ä‘á»ƒ phá»¥c vá»¥ nhiá»u ngÆ°á»i dÃ¹ng cÃ¹ng lÃºc.
		
	CÃ¡c báº£n phÃ¢n phá»‘i (distro) Server Linux phá»• biáº¿n

		Ubuntu Server (dá»… dÃ¹ng, cá»™ng Ä‘á»“ng lá»›n, há»— trá»£ LTS).

		CentOS Stream (tiáº¿p ná»‘i CentOS, gáº§n vá»›i Red Hat Enterprise Linux).

		Red Hat Enterprise Linux (RHEL) (báº£n thÆ°Æ¡ng máº¡i, há»— trá»£ doanh nghiá»‡p).

		Debian Server (á»•n Ä‘á»‹nh, Ã­t thay Ä‘á»•i, ráº¥t phá»• biáº¿n).

		SUSE Linux Enterprise Server (SLES) (doanh nghiá»‡p, há»— trá»£ máº¡nh máº½).
		
	Server Linux OS lÃ  Linux Ä‘Æ°á»£c tinh chá»‰nh cho mÃ´i trÆ°á»ng server, chÃº trá»ng hiá»‡u suáº¥t, báº£o máº­t, Ä‘á»™ á»•n Ä‘á»‹nh.

	Desktop Linux OS thÃ¬ thiÃªn vá» tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng cuá»‘i vá»›i GUI vÃ  á»©ng dá»¥ng vÄƒn phÃ²ng/giáº£i trÃ­.
	
	Khi nÃ o cáº§n dÃ¹ng Linux Server?

		Báº¡n sáº½ dÃ¹ng Linux Server khi:

			Cáº§n má»™t mÃ¡y cháº¡y dá»‹ch vá»¥ cho nhiá»u ngÆ°á»i khÃ¡c truy cáº­p (web server, database server, mail server, file server...).

			Cáº§n há»‡ thá»‘ng á»•n Ä‘á»‹nh, cháº¡y 24/7 (khÃ´ng táº¯t mÃ¡y nhÆ° PC cÃ¡ nhÃ¢n).

			Cáº§n má»™t mÃ´i trÆ°á»ng báº£o máº­t, nháº¹, Ã­t tá»‘n tÃ i nguyÃªn (thÆ°á»ng khÃ´ng cÃ³ GUI, chá»‰ dÃ¹ng dÃ²ng lá»‡nh).

			Triá»ƒn khai trong data center, cloud (AWS, Azure, GCP) hoáº·c trÃªn mÃ¡y chá»§ váº­t lÃ½.

		TrÆ°á»ng há»£p deploy web báº±ng AWS thÃ¬ sao?

			Khi báº¡n deploy website lÃªn AWS, báº¡n thÆ°á»ng sáº½ cháº¡y trÃªn má»™t dá»‹ch vá»¥ nhÆ° EC2 (mÃ¡y áº£o trong AWS).

			MÃ¡y EC2 nÃ y thÆ°á»ng Ä‘Æ°á»£c cÃ i má»™t distro Linux Server (vÃ­ dá»¥: Ubuntu Server, Amazon Linux, CentOS Stream, Debian Server...).

			Sau Ä‘Ã³ báº¡n sáº½ cÃ i nginx, Apache (httpd), Node.js, Java, MySQL, v.v. trÃªn Ä‘Ã³ Ä‘á»ƒ cháº¡y á»©ng dá»¥ng.

- RPM based, Debian based trong Linux

	Trong tháº¿ giá»›i Linux, khi ngÆ°á»i ta nÃ³i â€œRPM-basedâ€ vÃ  â€œDebian-basedâ€, há» Ä‘ang nÃ³i vá» há»‡ quáº£n lÃ½
	gÃ³i (package management system) mÃ  báº£n phÃ¢n phá»‘i (distro) Ä‘Ã³ sá»­ dá»¥ng.
	
	Debian-based:
	
		Nguá»“n gá»‘c: tá»« Debian.

		TrÃ¬nh quáº£n lÃ½ gÃ³i: dpkg (Debian Package).

		Äá»‹nh dáº¡ng gÃ³i: .deb.

		CÃ´ng cá»¥ cao hÆ¡n: apt-get, apt, aptitude (dÃ¹ng Ä‘á»ƒ cÃ i Ä‘áº·t, update tá»« repository).

		Äáº·c Ä‘iá»ƒm:

			Kho pháº§n má»m ráº¥t lá»›n, cá»™ng Ä‘á»“ng máº¡nh.

			ThÃ¢n thiá»‡n cho ngÆ°á»i má»›i (vÃ­ dá»¥ Ubuntu).

		VÃ­ dá»¥ distro Debian-based:

			Debian

			Ubuntu (vÃ  cÃ¡c biáº¿n thá»ƒ: Kubuntu, Xubuntu, â€¦)

			Linux Mint

			Kali Linux

			Pop!_OS
			
	RPM-based:

		Nguá»“n gá»‘c: tá»« Red Hat.

		TrÃ¬nh quáº£n lÃ½ gÃ³i: rpm (Red Hat Package Manager).

		Äá»‹nh dáº¡ng gÃ³i: .rpm.

		CÃ´ng cá»¥ cao hÆ¡n:

			yum (Yellowdog Updater, Modified â€“ dÃ¹ng trong CentOS, RHEL cÅ©).

			dnf (Dandified YUM â€“ thay tháº¿ yum trong Fedora, RHEL 8+).

		Äáº·c Ä‘iá»ƒm:

			á»”n Ä‘á»‹nh, nhiá»u báº£n thÆ°Æ¡ng máº¡i (RHEL, SUSE).

			ThÆ°á»ng dÃ¹ng trong mÃ´i trÆ°á»ng doanh nghiá»‡p.

		VÃ­ dá»¥ distro RPM-based:

			Red Hat Enterprise Linux (RHEL)

			CentOS, CentOS Stream

			Fedora

			openSUSE, SUSE Linux Enterprise

- ftp
- telnet
- jenkin
- tar
- bitbucket
- codecommit
- yum
- vagrant cloud

	Vagrant Cloud lÃ  má»™t dá»‹ch vá»¥ online do HashiCorp cung cáº¥p, Ä‘Ã³ng vai trÃ² nhÆ°
	má»™t â€œkho lÆ°u trá»¯ boxâ€ (Vagrant boxes) cho cá»™ng Ä‘á»“ng vÃ  doanh nghiá»‡p.

	Báº¡n cÃ³ thá»ƒ xem nÃ³ giá»‘ng nhÆ° Docker Hub nhÆ°ng dÃ nh cho Vagrant.
	
	CÃ¡c chá»©c nÄƒng chÃ­nh cá»§a Vagrant Cloud

		Chia sáº» box (Vagrant Boxes)

			Táº£i vá» cÃ¡c box cÃ³ sáºµn (Ubuntu, CentOS, Debian, Windows, â€¦).

			VÃ­ dá»¥:	

				vagrant init hashicorp/bionic64
				vagrant up

		ÄÄƒng táº£i box cá»§a báº¡n

			Náº¿u báº¡n táº¡o má»™t mÃ¡y áº£o Vagrant vá»›i cáº¥u hÃ¬nh Ä‘áº·c biá»‡t, báº¡n cÃ³ thá»ƒ upload lÃªn
			Vagrant Cloud Ä‘á»ƒ chia sáº» cho Ä‘á»“ng Ä‘á»™i hoáº·c cá»™ng Ä‘á»“ng.

		Quáº£n lÃ½ box private/public

			Public box: ai cÅ©ng cÃ³ thá»ƒ táº£i.

			Private box: chá»‰ nhÃ³m/báº¡n bÃ¨ hoáº·c cÃ´ng ty cá»§a báº¡n cÃ³ quyá»n truy cáº­p.
			
		TÃ­ch há»£p vá»›i Vagrant CLI

			Báº¡n cÃ³ thá»ƒ login báº±ng CLI:
			
				vagrant login
				
		Team & Enterprise

			Cho phÃ©p táº¡o tá»• chá»©c, nhÃ³m, phÃ¢n quyá»n Ä‘á»ƒ quáº£n lÃ½ box trong ná»™i bá»™ cÃ´ng ty.

- daemon
- hostmanager
- memcache
- apache tomcat
- nginx
- EPEL repository
- systemctl command
- OSI
- DNS & DHCP
- .ppk va .pem trong key pair aws
- AWS EC2
- AWS AMI
- tag trong EC2
- security group trong AMI
- AWS IAM

	IAM (Identity and Access Management) lÃ  dá»‹ch vá»¥ cá»§a AWS dÃ¹ng Ä‘á»ƒ quáº£n lÃ½ ngÆ°á»i dÃ¹ng, nhÃ³m vÃ  quyá»n truy cáº­p Ä‘áº¿n cÃ¡c tÃ i nguyÃªn AWS.

	NÃ³ giÃºp báº¡n kiá»ƒm soÃ¡t ai cÃ³ thá»ƒ truy cáº­p vÃ o dá»‹ch vá»¥ nÃ o vÃ  Ä‘Æ°á»£c lÃ m gÃ¬ trong AWS.

	NÃ³i ngáº¯n gá»n: IAM = Há»‡ thá»‘ng phÃ¢n quyá»n & báº£o máº­t trong AWS.
	
	ThÃ nh pháº§n chÃ­nh trong IAM:
	
		User (NgÆ°á»i dÃ¹ng)

			Äáº¡i diá»‡n cho má»™t cÃ¡ nhÃ¢n (developer, adminâ€¦) hoáº·c má»™t á»©ng dá»¥ng.

			Má»—i user cÃ³ credentials (username/password hoáº·c access key/secret key).

		Group (NhÃ³m)

			Táº­p há»£p nhiá»u user.

			Quyá»n Ä‘Æ°á»£c gÃ¡n cho group â†’ táº¥t cáº£ user trong group Ä‘á»u cÃ³ quyá»n Ä‘Ã³.

		Role (Vai trÃ²)

			DÃ¹ng cho dá»‹ch vá»¥ AWS hoáº·c á»©ng dá»¥ng Ä‘á»ƒ cÃ³ quyá»n truy cáº­p tÃ i nguyÃªn.

			VÃ­ dá»¥: EC2 cÃ³ thá»ƒ assume má»™t role Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u tá»« S3.

		Policy (ChÃ­nh sÃ¡ch)

			TÃ i liá»‡u JSON Ä‘á»‹nh nghÄ©a quyá»n truy cáº­p (cho phÃ©p hay tá»« chá»‘i).

			VÃ­ dá»¥: policy cho phÃ©p user Ä‘á»c file trong S3 bucket.
			
	Chá»©c nÄƒng chÃ­nh cá»§a IAM:
	
		Quáº£n lÃ½ ngÆ°á»i dÃ¹ng & nhÃ³m (user, group).

		Cáº¥p quyá»n chi tiáº¿t báº±ng policy (theo nguyÃªn táº¯c least privilege â€“ chá»‰ cáº¥p quyá»n tá»‘i thiá»ƒu cáº§n thiáº¿t).

		Há»— trá»£ MFA (Multi-Factor Authentication) Ä‘á»ƒ tÄƒng báº£o máº­t.

		TÃ­ch há»£p vá»›i dá»‹ch vá»¥ khÃ¡c nhÆ° EC2, Lambda, S3, RDSâ€¦
		
	VÃ­ dá»¥ thá»±c táº¿:
	
		Dev A chá»‰ cáº§n Ä‘á»c dá»¯ liá»‡u trong S3 â†’ táº¡o IAM user + policy s3:GetObject.

		Dev B cáº§n quyá»n quáº£n lÃ½ EC2 â†’ gÃ¡n policy AmazonEC2FullAccess.

		EC2 server cáº§n quyá»n truy cáº­p DynamoDB â†’ táº¡o IAM Role cho EC2.
		
	AWS IAM = Quáº£n lÃ½ danh tÃ­nh & quyá»n truy cáº­p trong AWS, giÃºp Ä‘áº£m báº£o an toÃ n vÃ  kiá»ƒm soÃ¡t chÃ­nh xÃ¡c ai Ä‘Æ°á»£c
	lÃ m gÃ¬ trÃªn tÃ i nguyÃªn AWS.
	
- AWS IAM security credentials

	Trong AWS IAM, security credentials lÃ  cÃ¡c thÃ´ng tin xÃ¡c thá»±c mÃ  má»™t IAM user hoáº·c role dÃ¹ng Ä‘á»ƒ truy cáº­p tÃ i nguyÃªn AWS.

	CÃ³ 2 cÃ¡ch chÃ­nh Ä‘á»ƒ truy cáº­p AWS:

		AWS Management Console (giao diá»‡n web) â†’ dÃ¹ng username + password (+ MFA náº¿u báº­t).

		AWS CLI / SDK / API â†’ dÃ¹ng Access keys (Access key ID + Secret access key).
		
	CÃ¡c loáº¡i Security Credentials trong IAM:
	
		Password (Console access)

			DÃ¹ng Ä‘á»ƒ Ä‘Äƒng nháº­p vÃ o AWS Management Console.

			CÃ³ thá»ƒ yÃªu cáº§u user Ä‘á»•i máº­t kháº©u khi láº§n Ä‘áº§u Ä‘Äƒng nháº­p.
			
		Access Keys (Programmatic access)

			Gá»“m 2 pháº§n:

				Access key ID â†’ giá»‘ng username.

				Secret access key â†’ giá»‘ng máº­t kháº©u.

			DÃ¹ng Ä‘á»ƒ truy cáº­p AWS qua CLI, SDK, API.

			VÃ­ dá»¥ khi cáº¥u hÃ¬nh AWS CLI:
			
				aws configure
				AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
				AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
				Default region name [None]: ap-southeast-1
				Default output format [None]: json
				
		SSH Keys (cho EC2 Instance Connect)

			DÃ¹ng Ä‘á»ƒ káº¿t ná»‘i SSH vÃ o cÃ¡c mÃ¡y EC2.	

		Server Certificates

			DÃ¹ng cho AWS services nhÆ° Elastic Load Balancing (ELB) hoáº·c CloudFront Ä‘á»ƒ há»— trá»£ HTTPS (SSL/TLS).
			
		MFA Devices

			Virtual (Google Authenticator, Authy), Hardware (YubiKeyâ€¦), SMS.

			DÃ¹ng Ä‘á»ƒ tÄƒng báº£o máº­t cho login.
			
		X.509 Certificates (Ã­t dÃ¹ng hiá»‡n nay)

			DÃ¹ng cho cÃ¡c dá»‹ch vá»¥ cÅ© hoáº·c tÃ­ch há»£p Ä‘áº·c biá»‡t.
			
	Best Practices (khuyáº¿n nghá»‹ báº£o máº­t)

		KhÃ´ng bao giá» dÃ¹ng Access Key cá»§a root user ğŸš«.

		Táº¡o IAM user/role riÃªng cho tá»«ng á»©ng dá»¥ng.

		Báº­t MFA cho root user vÃ  user quan trá»ng.

		DÃ¹ng IAM Role thay vÃ¬ access key náº¿u app cháº¡y trÃªn AWS (VD: EC2 â†’ S3).

		Rotate access key thÆ°á»ng xuyÃªn (xÃ³a key cÅ©, táº¡o key má»›i).

		Chá»‰ cáº¥p quyá»n theo nguyÃªn táº¯c least privilege (tá»‘i thiá»ƒu cáº§n thiáº¿t).
		
	Security Credentials trong AWS IAM = táº­p há»£p thÃ´ng tin xÃ¡c thá»±c (password, access keys,
	SSH keys, MFAâ€¦) mÃ  user/role dÃ¹ng Ä‘á»ƒ truy cáº­p AWS má»™t cÃ¡ch an toÃ n.
	
- Console Access:

	Console Access nghÄ©a lÃ  má»™t IAM User (hoáº·c root user) cÃ³ thá»ƒ Ä‘Äƒng nháº­p vÃ o AWS Management Console (giao diá»‡n web cá»§a AWS) báº±ng:

		Username/Password

		(MFA code náº¿u Ä‘Ã£ báº­t Multi-Factor Authentication

	NÃ³i ngáº¯n gá»n: Console Access = quyá»n truy cáº­p AWS qua giao diá»‡n web (console.aws.amazon.com).
	
- AWS Billing:

	AWS Billing lÃ  há»‡ thá»‘ng quáº£n lÃ½ chi phÃ­, hÃ³a Ä‘Æ¡n vÃ  thanh toÃ¡n trong tÃ i khoáº£n AWS.
	
	NÃ³ cho phÃ©p báº¡n:

		Xem chi phÃ­ sá»­ dá»¥ng dá»‹ch vá»¥ AWS (theo ngÃ y, theo dá»‹ch vá»¥, theo region).

		Quáº£n lÃ½ hÃ³a Ä‘Æ¡n (invoice) vÃ  phÆ°Æ¡ng thá»©c thanh toÃ¡n.

		Thiáº¿t láº­p cáº£nh bÃ¡o chi phÃ­ (budget alerts) Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ ngÃ¢n sÃ¡ch.

		PhÃ¢n tÃ­ch chi phÃ­ theo tá»«ng dá»± Ã¡n, team, hoáº·c mÃ´i trÆ°á»ng (prod/dev/test).
		
	CÃ¡c thÃ nh pháº§n chÃ­nh trong AWS Billing

		Bills (HÃ³a Ä‘Æ¡n)

			Hiá»ƒn thá»‹ chi tiáº¿t chi phÃ­ theo tá»«ng dá»‹ch vá»¥ (EC2, S3, RDS, Lambdaâ€¦).

			CÃ³ thá»ƒ táº£i vá» dáº¡ng PDF hoáº·c CSV.

		Payment Methods (PhÆ°Æ¡ng thá»©c thanh toÃ¡n)

			AWS cháº¥p nháº­n tháº» tÃ­n dá»¥ng/ghi ná»£ (Visa, Mastercard, AmExâ€¦) vÃ  má»™t sá»‘ phÆ°Æ¡ng thá»©c khÃ¡c.

		Budgets (NgÃ¢n sÃ¡ch)

			Äáº·t ngÃ¢n sÃ¡ch chi phÃ­ (vÃ­ dá»¥ $50/thÃ¡ng).

			Nháº­n cáº£nh bÃ¡o qua email hoáº·c SNS náº¿u vÆ°á»£t má»©c.

		Cost Explorer

			CÃ´ng cá»¥ trá»±c quan hÃ³a chi phÃ­ (biá»ƒu Ä‘á»“, bÃ¡o cÃ¡o).

			Cho phÃ©p phÃ¢n tÃ­ch xu hÆ°á»›ng chi tiÃªu, dá»± Ä‘oÃ¡n chi phÃ­ trong tÆ°Æ¡ng lai.

		Free Tier Usage

			Hiá»ƒn thá»‹ má»©c sá»­ dá»¥ng AWS Free Tier (miá»…n phÃ­ 12 thÃ¡ng hoáº·c vÄ©nh viá»…n cho má»™t sá»‘ dá»‹ch vá»¥).

			GiÃºp trÃ¡nh vÆ°á»£t giá»›i háº¡n free mÃ  phÃ¡t sinh phÃ­ ngoÃ i Ã½ muá»‘n.

		Consolidated Billing (cho AWS Organizations)

			Náº¿u báº¡n cÃ³ nhiá»u tÃ i khoáº£n AWS trong má»™t Organization, cÃ³ thá»ƒ gá»™p hÃ³a Ä‘Æ¡n â†’ nháº­n giáº£m giÃ¡ volume.
			
	Best Practices vá» Billing

		LuÃ´n báº­t Billing Alerts Ä‘á»ƒ theo dÃµi chi phÃ­.

		DÃ¹ng AWS Budgets Ä‘á»ƒ cáº£nh bÃ¡o vÆ°á»£t má»©c ngÃ¢n sÃ¡ch.

		Gáº¯n Tags cho tÃ i nguyÃªn (vÃ­ dá»¥: Project: Ecommerce, Env: Dev) Ä‘á»ƒ phÃ¢n tÃ­ch chi phÃ­ theo dá»± Ã¡n.

		XÃ³a tÃ i nguyÃªn khÃ´ng dÃ¹ng (EC2, EBS, Elastic IP, Load Balancerâ€¦) vÃ¬ AWS váº«n tÃ­nh phÃ­ náº¿u cÃ²n tá»“n táº¡i.
		
	AWS Billing = há»‡ thá»‘ng quáº£n lÃ½ chi phÃ­, hÃ³a Ä‘Æ¡n vÃ  thanh toÃ¡n trong AWS, giÃºp báº¡n kiá»ƒm soÃ¡t vÃ  tá»‘i
	Æ°u chi tiÃªu khi dÃ¹ng dá»‹ch vá»¥ AWS.
	
- AWS CloudWatch:

	Amazon CloudWatch lÃ  dá»‹ch vá»¥ giÃ¡m sÃ¡t (monitoring) vÃ  quan sÃ¡t (observability) cá»§a AWS.
	
	NÃ³ giÃºp báº¡n theo dÃµi:

		Metrics (chá»‰ sá»‘: CPU, RAM, Network, Disk I/Oâ€¦).

		Logs (nháº­t kÃ½ á»©ng dá»¥ng, há»‡ Ä‘iá»u hÃ nh, dá»‹ch vá»¥ AWS).

		Events (sá»± kiá»‡n há»‡ thá»‘ng, thay Ä‘á»•i tráº¡ng thÃ¡i).

		Alarms (cáº£nh bÃ¡o khi vÆ°á»£t ngÆ°á»¡ng).
		
	ThÃ nh pháº§n chÃ­nh cá»§a CloudWatch

		Metrics:

			Thu tháº­p sá»‘ liá»‡u vá» tÃ i nguyÃªn AWS.

			VÃ­ dá»¥:

			EC2: CPUUtilization, NetworkIn/Out.

			RDS: FreeStorageSpace, DatabaseConnections.

			Lambda: Invocations, Duration, Errors.

		Alarms:

			Äáº·t ngÆ°á»¡ng cáº£nh bÃ¡o dá»±a trÃªn metrics.

			VÃ­ dá»¥: CPU EC2 > 80% trong 5 phÃºt â†’ gá»­i cáº£nh bÃ¡o qua SNS (email/SMS) hoáº·c tá»± Ä‘á»™ng scale up.

		Logs:

			LÆ°u vÃ  phÃ¢n tÃ­ch log tá»«:

			á»¨ng dá»¥ng (Java, Python, NodeJSâ€¦).

			Há»‡ Ä‘iá»u hÃ nh (EC2).

			Lambda (log tá»± Ä‘á»™ng vÃ o CloudWatch Logs).

		Events (hoáº·c EventBridge):

			Theo dÃµi sá»± kiá»‡n há»‡ thá»‘ng AWS (vÃ­ dá»¥: EC2 stop/start).

			CÃ³ thá»ƒ tá»± Ä‘á»™ng kÃ­ch hoáº¡t Lambda function, SNS notification, hoáº·c Step Functions.

		Dashboards:

			Táº¡o báº£ng Ä‘iá»u khiá»ƒn tÃ¹y chá»‰nh vá»›i biá»ƒu Ä‘á»“ metrics, logs.

			VÃ­ dá»¥: giÃ¡m sÃ¡t toÃ n bá»™ há»‡ thá»‘ng microservices trÃªn 1 dashboard.
			
	VÃ­ dá»¥ thá»±c táº¿

		Báº¡n cÃ³ má»™t EC2 cháº¡y Spring Boot:

		CloudWatch Metrics theo dÃµi CPU, RAM, Disk.

		CloudWatch Logs lÆ°u láº¡i log á»©ng dá»¥ng (/var/log/...).

		CloudWatch Alarm cáº£nh bÃ¡o khi CPU > 70%.

		CloudWatch Event kÃ­ch hoáº¡t Auto Scaling Ä‘á»ƒ thÃªm EC2 má»›i khi táº£i cao.
		
	Lá»£i Ã­ch

		GiÃºp tá»± Ä‘á»™ng giÃ¡m sÃ¡t há»‡ thá»‘ng â†’ khÃ´ng cáº§n kiá»ƒm tra thá»§ cÃ´ng.

		Dá»… dÃ ng tÃ¬m lá»—i nhá» log táº­p trung.

		Há»— trá»£ tá»‘i Æ°u chi phÃ­ (theo dÃµi sá»­ dá»¥ng tÃ i nguyÃªn).

		Káº¿t há»£p vá»›i Auto Scaling, Lambda, SNS Ä‘á»ƒ pháº£n á»©ng tá»± Ä‘á»™ng.
		
	AWS CloudWatch = cÃ´ng cá»¥ giÃ¡m sÃ¡t metrics, logs, events vÃ  táº¡o cáº£nh bÃ¡o cho há»‡ thá»‘ng AWS.

	NÃ³ giá»‘ng nhÆ° Prometheus + Grafana + ELK, nhÆ°ng Ä‘Æ°á»£c tÃ­ch há»£p sáºµn trong AWS.

- AWS IAM access key
- AWS Elastic Block Storage
- Volumn in configure storage of IAM S3
- AWS EC2 Image Builder
- AWS EC2 launch templates
- AWS Cloud Watch
- AWS Route53
- AWS RDS
- AWS cloudfront
- AWS Amazon S3
- AWS Amazon SNS
- Amazon Elastic File System
- AWS EFS
- AWS NFS
- AWS Auto Scaling
- AWS RDS
- public access trong RDS
- AWS S3
- Elastic Beanstalk
- Amazon ElastiCache
- Amazon Elastic Container Registry
- Poll SCM
- Webhooks
- CRUMB
- event-driven
- boto3 python
- python fabric
- Terraform
- Ansible
- ad hoc command
- ansible palybook
- Amazon VPC
- AWS CloudWatch Logs
- Bitbucket
- AWS CodeBuild
- AWS CodePipeline
- docker pull nginx:mainline-alpine-perl
- docker engine
- minikube
- Kops
- key pair va security group trong ec2 instance
- curl
- kubectl binary
- kubernetes cli
- kubernetes namespace
- kubernetes taints and tolerations
- Amazon Elastic Kubernetes Service
- kubernetes ingress
- git fork
- SSH key
- AWS Elastic Container Registry
- kubernetes cluster
- QA, BA trong doanh nghiep
- Nesus
- file TAR
- helm chart
- Server váº­t lÃ½ (physical server) vÃ  Server áº£o (virtual server / VM):

	Server váº­t lÃ½ (physical server)

		LÃ  mÃ¡y chá»§ tháº­t Ä‘áº·t trong data center (cÃ³ CPU, RAM, á»• cá»©ng, card máº¡ng).

		Báº¡n cÃ³ thá»ƒ cÃ i há»‡ Ä‘iá»u hÃ nh lÃªn nÃ³: Linux Server (Ubuntu Server, CentOS, Debian Server, â€¦)
		hoáº·c Windows Server.

		Sau khi cÃ i xong, báº¡n thao tÃ¡c quáº£n lÃ½ báº±ng SSH (náº¿u Linux) hoáº·c RDP (náº¿u Windows).

	Server áº£o (virtual server / VM)

		LÃ  mÃ¡y chá»§ áº£o hÃ³a táº¡o ra tá»« pháº§n má»m nhÆ° VMware, VirtualBox, KVM, hoáº·c dá»‹ch vá»¥ Cloud
		nhÆ° AWS EC2, Azure VM, Google Compute Engine.

		BÃªn trong server áº£o Ä‘Ã³, báº¡n cÅ©ng cÃ i Linux Server (hoáº·c Windows Server).

		Khi truy cáº­p vÃ  thao tÃ¡c, tráº£i nghiá»‡m khÃ´ng khÃ¡c gÃ¬ server váº­t lÃ½.

	NghÄ©a lÃ :

		DÃ¹ báº¡n dÃ¹ng server váº­t lÃ½ hay server áº£o, náº¿u há»‡ Ä‘iá»u hÃ nh cÃ i bÃªn trong lÃ  Linux Server, thÃ¬ báº¡n váº«n
		sáº½ thao tÃ¡c báº±ng dÃ²ng lá»‡nh Linux Server (CLI qua SSH).



Missing in Jenkin lession:
	- Agent/Node/Slave in Jenkins
	- Using Agent/Note/Slave
Missing Build tools lession


--- VÃ²ng Ä‘á»i DevOps:

	1. CODE

	Láº­p trÃ¬nh viÃªn viáº¿t code (Java, HTML, CSS, JS, â€¦).

	ÄÃ¢y lÃ  nguá»“n gá»‘c cá»§a má»i thá»© trong pipeline.

	2. CODE BUILD

	Code Ä‘Æ°á»£c build thÃ nh gÃ³i cháº¡y Ä‘Æ°á»£c (JAR, WAR, Docker image, â€¦).

	CÃ´ng cá»¥ thÆ°á»ng dÃ¹ng: Maven, Gradle, Jenkins, GitLab CI, GitHub Actions.

	3. CODE TEST

	Cháº¡y test tá»± Ä‘á»™ng (unit test, integration test).

	Äáº£m báº£o code khÃ´ng bá»‹ lá»—i logic.

	4. CODE ANALYSIS

	PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng code (coding style, security scan, bug detection).

	CÃ´ng cá»¥: SonarQube, PMD, Checkstyle.

	5. DELIVERY

	Chuáº©n bá»‹ pháº§n má»m Ä‘á»ƒ Ä‘Æ°a qua mÃ´i trÆ°á»ng staging hoáº·c production.

	Bao gá»“m artifact repository (Nexus, Artifactory).

	6. DB / Security / OS CHANGES

	CÃ¡c thay Ä‘á»•i liÃªn quan tá»›i database migration, cáº¥u hÃ¬nh há»‡ Ä‘iá»u hÃ nh, báº£o máº­t.

	ThÆ°á»ng Ä‘Æ°á»£c tá»± Ä‘á»™ng hÃ³a báº±ng Ansible, Terraform, Liquibase, Flyway.

	7. SOFTWARE TESTING

	Test á»Ÿ má»©c há»‡ thá»‘ng (system testing, performance testing, user acceptance testing).

	Má»¥c tiÃªu: kiá»ƒm thá»­ toÃ n bá»™ á»©ng dá»¥ng trong mÃ´i trÆ°á»ng staging.

	8. DEPLOY TO PROD

	Deploy á»©ng dá»¥ng sang production environment.

	CÃ´ng cá»¥: Docker, Kubernetes, Jenkins, ArgoCD.

	9. GO LIVE

	á»¨ng dá»¥ng báº¯t Ä‘áº§u cháº¡y cho ngÆ°á»i dÃ¹ng tháº­t.

	ÄÃ²i há»i monitoring (Prometheus, Grafana, ELK stack) Ä‘á»ƒ theo dÃµi hoáº¡t Ä‘á»™ng.

	10. USER APPROVAL

	NgÆ°á»i dÃ¹ng/khÃ¡ch hÃ ng kiá»ƒm tra vÃ  xÃ¡c nháº­n á»©ng dá»¥ng Ä‘Ã¡p á»©ng yÃªu cáº§u.

	Feedback Ä‘Æ°á»£c Ä‘Æ°a láº¡i cho team Dev, táº¡o vÃ²ng láº·p má»›i.


--- CÃ¡c lá»‡nh cmd:

	- cmd: choco list
	
		============================================================
		Lá»†NH: choco list
		============================================================

		1. Má»¤C ÄÃCH CHUNG
		   - Hiá»ƒn thá»‹ danh sÃ¡ch cÃ¡c package liÃªn quan tá»›i Chocolatey.
		   - CÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ:
			   â€¢ TÃ¬m package trÃªn nguá»“n (remote) hoáº·c
			   â€¢ Liá»‡t kÃª package Ä‘Ã£ cÃ i cá»¥c bá»™ (tÃ¹y phiÃªn báº£n choco).
		   - ThÆ°á»ng dÃ¹ng Ä‘á»ƒ kiá»ƒm tra package sáºµn cÃ³, kiá»ƒm tra phiÃªn báº£n, hoáº·c xuáº¥t danh sÃ¡ch package Ä‘Ã£ cÃ i.

		------------------------------------------------------------
		2. CÃš PHÃP CÆ  Báº¢N
		   choco list <filter> [<options/switches>]

		   VÃ­ dá»¥ Ä‘Æ¡n giáº£n:
			 choco list
			 choco list git
			 choco list --local-only

		   (ThÃ´ng tin chÃ­nh thá»©c vÃ  cÃ¡c option Ä‘Æ°á»£c láº¥y tá»« tÃ i liá»‡u lá»‡nh `choco list` cá»§a Chocolatey). :contentReference[oaicite:0]{index=0}

		------------------------------------------------------------
		3. Ã NGHÄ¨A CÃC THÃ€NH PHáº¦N
		   - <filter>
			   â†’ Tá»« khÃ³a tÃ¬m kiáº¿m (cÃ³ thá»ƒ lÃ  tÃªn package hoáº·c pháº§n cá»§a tÃªn).
			   â†’ Náº¿u khÃ´ng truyá»n filter, choco sáº½ tráº£ vá» nhiá»u káº¿t quáº£ (tuá»³ source/phiÃªn báº£n).

		   - <options/switches>
			   â†’ CÃ¡c tuá»³ chá»n Ä‘iá»u chá»‰nh hÃ nh vi hiá»ƒn thá»‹ (xem pháº§n 4 cho cÃ¡c tuá»³ chá»n thÆ°á»ng dÃ¹ng).

		------------------------------------------------------------
		4. CÃC TUá»² CHá»ŒN / SWITCH THÆ¯á»œNG DÃ™NG (chá»n lá»c, giáº£i thÃ­ch)
		   NOTE: danh sÃ¡ch dÆ°á»›i Ä‘Ã¢y tÃ³m táº¯t cÃ¡c tuá»³ chá»n quan trá»ng nháº¥t; tham kháº£o docs Ä‘á»ƒ xem Ä‘áº§y Ä‘á»§. :contentReference[oaicite:1]{index=1}

		   - --local-only  (cÅ©ng tháº¥y dÆ°á»›i dáº¡ng -l hoáº·c -lo á»Ÿ má»™t sá»‘ tÃ i liá»‡u/phiÃªn báº£n)
			   â†’ Chá»‰ liá»‡t kÃª cÃ¡c package Ä‘Ã£ **cÃ i cá»¥c bá»™** (installed packages).
			   â†’ LÆ°u Ã½: cÃ¡ch xá»­ lÃ½ flag nÃ y cÃ³ khÃ¡c nhau giá»¯a cÃ¡c phiÃªn báº£n Chocolatey (v2+ cÃ³ thay Ä‘á»•i). :contentReference[oaicite:2]{index=2}

		   - -i, --include-programs, --include-programs
			   â†’ Bao gá»“m cáº£ pháº§n má»m hiá»ƒn thá»‹ trong "Programs and Features" (Windows) mÃ  khÃ´ng do Chocolatey quáº£n lÃ½.

		   - --exact  (or -e)
			   â†’ Chá»‰ tráº£ vá» package cÃ³ tÃªn **chÃ­nh xÃ¡c** báº±ng filter.

		   - --id-only
			   â†’ Chá»‰ in ra Package IDs (Ã­t thÃ´ng tin hÆ¡n, há»¯u Ã­ch cho scripting).

		   - --pre, --prerelease
			   â†’ Bao gá»“m cÃ¡c phiÃªn báº£n prerelease (pre-release) trong káº¿t quáº£.

		   - --version=<value>
			   â†’ Lá»c káº¿t quáº£ theo má»™t **phiÃªn báº£n cá»¥ thá»ƒ** cá»§a package.

		   - --page=<n>  vÃ   --page-size=<n>
			   â†’ PhÃ¢n trang káº¿t quáº£ (page: trang cáº§n láº¥y; page-size: sá»‘ item má»—i trang).
			   â†’ Máº·c Ä‘á»‹nh choco tráº£ vá» táº¥t cáº£ káº¿t quáº£, nhÆ°ng cÃ³ thá»ƒ giá»›i háº¡n báº±ng page/page-size.

		   - -r, --limit-output
			   â†’ Giáº£m bá»›t output vá» thÃ´ng tin thiáº¿t yáº¿u (Ã­t noise hÆ¡n).

		   - --include-headers
			   â†’ Khi dÃ¹ng --limit-output, hiá»ƒn thá»‹ header (yÃªu cáº§u CLI 2.5.0+).

		   - --by-id-only / --by-tag-only / --id-starts-with
			   â†’ Lá»c theo cÃ¡ch so khá»›p id hoáº·c tag.

		   - --ignore-pinned
			   â†’ Bá» qua viá»‡c hiá»ƒn thá»‹ cÃ¡c package bá»‹ "pinned".

		   - -d, --debug; -v, --verbose; --trace
			   â†’ Thay Ä‘á»•i má»©c logging Ä‘á»ƒ debug hoáº·c xem chi tiáº¿t.

		   (Nguá»“n: trang tÃ i liá»‡u lá»‡nh `choco list` tá»« Chocolatey). :contentReference[oaicite:3]{index=3}

		------------------------------------------------------------
		5. Káº¾T QUáº¢ MáºªU & Ã NGHÄ¨A
		   - Káº¿t quáº£ thÃ´ng thÆ°á»ng (remote search):
			   git 2.41.0
			   git.install 2.41.0
			   git-lfs 3.4.0
			 â†’ Dáº¡ng: <packageId> <version>

		   - Khi dÃ¹ng --local-only:
			   7zip 19.00
			   git 2.41.0
			 â†’ Liá»‡t kÃª package Ä‘Ã£ cÃ i kÃ¨m phiÃªn báº£n; dÃ²ng cuá»‘i cÃ³ thá»ƒ in tÃ³m táº¯t "X packages installed."

		   - Khi dÃ¹ng --id-only: chá»‰ in
			   git
			   7zip
			 â†’ dÃ¹ng cho scripting/pipe.

		------------------------------------------------------------
		6. EXIT CODES (tÃ³m táº¯t há»¯u Ã­ch cho scripts)
		   - 0: thÃ nh cÃ´ng
		   - 1 (hoáº·c -1): lá»—i
		   - 2: (enhanced exit codes enabled) khÃ´ng cÃ³ káº¿t quáº£
		   â†’ TÃ¹y thuá»™c vÃ o cáº¥u hÃ¬nh feature `useEnhancedExitCodes`. :contentReference[oaicite:4]{index=4}

		------------------------------------------------------------
		7. VÃ Dá»¤ THá»°C HÃ€NH (COPY-PASTE)

		   # 1) TÃ¬m package tÃªn chá»©a "git"
		   choco list git

		   # 2) Liá»‡t kÃª package Ä‘Ã£ cÃ i cá»¥c bá»™ (backup danh sÃ¡ch)
		   choco list --local-only > installed-packages.txt

		   # 3) Chá»‰ in ID (phÃ¹ há»£p cho piping)
		   choco list git --id-only

		   # 4) Bao gá»“m chÆ°Æ¡ng trÃ¬nh tá»« Programs and Features
		   choco list --local-only --include-programs

		   # 5) Lá»c chÃ­nh xÃ¡c tÃªn package
		   choco list --exact git

		   # 6) Hiá»ƒn thá»‹ Ã­t output (dÃ¹ng trong automation)
		   choco list git --limit-output

		   # 7) PhÃ¢n trang (vÃ­ dá»¥ láº¥y trang 2, 25 item/trang)
		   choco list --page=2 --page-size=25

		   (CÃ¡c vÃ­ dá»¥ trÃªn dá»±a theo cÃº phÃ¡p vÃ  options trong docs cá»§a Chocolatey). :contentReference[oaicite:5]{index=5}

		------------------------------------------------------------
		8. LÆ¯U Ã & Máº¸O
		   - PhiÃªn báº£n Chocolatey khÃ¡c nhau cÃ³ thá»ƒ thay Ä‘á»•i hÃ nh vi má»™t sá»‘ flag (vÃ­ dá»¥ `--local-only` trÆ°á»›c Ä‘Ã¢y phá»• biáº¿n, trong choco v2+ cÃ¡ch máº·c Ä‘á»‹nh cÃ³ thay Ä‘á»•i). Kiá»ƒm tra `choco --version` vÃ  `choco list --help` náº¿u káº¿t quáº£ khÃ¡c ká»³ vá»ng. :contentReference[oaicite:6]{index=6}
		   - Äá»ƒ backup toÃ n bá»™ package list rá»“i restore trÃªn mÃ¡y khÃ¡c:
			   choco list --local-only > packages.txt
			   choco install < packages.txt
		   - DÃ¹ng `--limit-output` vÃ  `--id-only` khi viáº¿t script Ä‘á»ƒ giáº£m parsing complexity.
		   - Náº¿u cáº§n debug: thÃªm `-d` hoáº·c `--trace` Ä‘á»ƒ xem chi tiáº¿t.

		------------------------------------------------------------
		9. TÃ€I NGUYÃŠN THAM KHáº¢O
		   - Documentation: Chocolatey â€” List Command (choco list). :contentReference[oaicite:7]{index=7}

		------------------------------------------------------------
		10. TÃ“M Táº®T NGáº®N Gá»ŒN
		   - `choco list` lÃ  lá»‡nh Ä‘á»ƒ **tÃ¬m vÃ /hoáº·c liá»‡t kÃª** package liÃªn quan tá»›i Chocolatey.
		   - DÃ¹ng `--local-only` Ä‘á»ƒ xem package Ä‘Ã£ cÃ i; dÃ¹ng cÃ¡c flag nhÆ° `--exact`, `--id-only`, `--include-programs` Ä‘á»ƒ Ä‘iá»u chá»‰nh output cho scripting hay automation.
		   - LuÃ´n kiá»ƒm tra `choco list --help` / docs chÃ­nh thá»©c náº¿u cáº§n chi tiáº¿t theo phiÃªn báº£n.

		============================================================
	
	
	- cmd: choco install virtualbox --version=7.0.8 -y
	
	
	
	- cmd: choco install vagrant --version=2.3.7 -y
	- cmd: choco install corretto17jdk -y
	- cmd: choco install maven -y
	- cmd: choco install awscli -y
	- cmd: choco install intellijidea-community -y
	- cmd: choco install vscode -y
	- cmd: choco install sublimetext3 -y
	
	
--- VM setup:

	- cmd: ip addr show
	
		ÄÃ¢y lÃ  lá»‡nh trong Linux dÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ thÃ´ng tin cÃ¡c Ä‘á»‹a chá»‰ IP trÃªn mÃ¡y. NÃ³ thuá»™c bá»™ cÃ´ng
		cá»¥ iproute2 (thay tháº¿ dáº§n ifconfig).
		
		Káº¿t quáº£ khi cháº¡y:
		
			[root@localhost apache-tomcat-10.1.46]# ip addr show
			1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
				link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
				inet 127.0.0.1/8 scope host lo
				   valid_lft forever preferred_lft forever
				inet6 ::1/128 scope host
				   valid_lft forever preferred_lft forever
			2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:e4:ec:85 brd ff:ff:ff:ff:ff:ff
				inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute enp0s3
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fee4:ec85/64 scope link noprefixroute
				   valid_lft forever preferred_lft forever
			3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:00:b4:c3 brd ff:ff:ff:ff:ff:ff
				inet 192.168.56.10/24 brd 192.168.56.255 scope global noprefixroute enp0s8
				   valid_lft forever preferred_lft forever
				inet6 fe80::a00:27ff:fe00:b4c3/64 scope link
				   valid_lft forever preferred_lft forever
			4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:3c:20:80 brd ff:ff:ff:ff:ff:ff
				inet 192.168.1.25/24 brd 192.168.1.255 scope global dynamic noprefixroute enp0s9
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fe3c:2080/64 scope link
				   valid_lft forever preferred_lft forever
				   
			Loopback (lo):
			
				lo: card loopback, dÃ¹ng Ä‘á»ƒ giao tiáº¿p ná»™i bá»™ trong mÃ¡y (localhost).

				127.0.0.1: Ä‘á»‹a chá»‰ IP Ä‘áº·c biá»‡t Ä‘á»ƒ tham chiáº¿u chÃ­nh mÃ¡y Ä‘Ã³.

				mtu 65536: kÃ­ch thÆ°á»›c gÃ³i tin tá»‘i Ä‘a.

				IPv6 tÆ°Æ¡ng á»©ng: ::1.
				
			enp0s3 (NAT interface):
			
				ÄÃ¢y lÃ  card máº¡ng NAT do VirtualBox táº¡o.

				10.0.2.15/24: IP gÃ¡n tá»± Ä‘á»™ng trong máº¡ng NAT cá»§a VirtualBox.

				dynamic: IP Ä‘Æ°á»£c cáº¥p tá»« DHCP.

				DÃ¹ng Ä‘á»ƒ mÃ¡y áº£o cÃ³ thá»ƒ truy cáº­p internet thÃ´ng qua host.

			enp0s8 (Host-Only Adapter):
			
				ÄÃ¢y lÃ  card Host-Only Network.

				IP 192.168.56.10: cho phÃ©p giao tiáº¿p giá»¯a mÃ¡y áº£o vÃ  host, nhÆ°ng khÃ´ng ra internet.

				ThÆ°á»ng dÃ¹ng khi báº¡n cáº¥u hÃ¬nh private_network trong Vagrant (192.168.56.x).
				
			enp0s9 (Bridge Adapter):
			
				ÄÃ¢y lÃ  card Bridge (ná»‘i tháº³ng vÃ o máº¡ng LAN/WiFi thá»±c táº¿ cá»§a báº¡n).

				IP 192.168.1.25: Ä‘Æ°á»£c cáº¥p tá»« DHCP cá»§a router WiFi/LAN.

				Cho phÃ©p mÃ¡y áº£o hoáº¡t Ä‘á»™ng nhÆ° má»™t mÃ¡y riÃªng trong máº¡ng tháº­t, cÃ¡c mÃ¡y khÃ¡c trong máº¡ng LAN cÃ³ thá»ƒ
				truy cáº­p trá»±c tiáº¿p.


	- cmd: ssh ngoctuanqng1@192.168.1.17
	
		ÄÃ¢y lÃ  lá»‡nh káº¿t ná»‘i Ä‘áº¿n má»™t mÃ¡y tÃ­nh khÃ¡c qua giao thá»©c SSH (Secure Shell).	

		PhÃ¢n tÃ­ch tá»«ng pháº§n:

			ssh: chÆ°Æ¡ng trÃ¬nh dÃ¹ng Ä‘á»ƒ Ä‘Äƒng nháº­p tá»« xa an toÃ n, cho phÃ©p báº¡n cháº¡y lá»‡nh trÃªn má»™t mÃ¡y khÃ¡c qua máº¡ng.

			ngoctuanqng1: tÃªn user mÃ  báº¡n muá»‘n Ä‘Äƒng nháº­p vÃ o mÃ¡y Ä‘Ã­ch.

			@: phÃ¢n tÃ¡ch giá»¯a user vÃ  Ä‘á»‹a chá»‰ mÃ¡y.

			192.168.1.17: Ä‘á»‹a chá»‰ IP cá»§a mÃ¡y Ä‘Ã­ch trong máº¡ng LAN (á»Ÿ Ä‘Ã¢y lÃ  mÃ¡y báº¡n muá»‘n SSH vÃ o).
			
		Quy trÃ¬nh khi cháº¡y lá»‡nh:

			MÃ¡y báº¡n má»Ÿ káº¿t ná»‘i SSH Ä‘áº¿n Ä‘á»‹a chá»‰ 192.168.1.17 qua cá»•ng 22 (máº·c Ä‘á»‹nh).

			Server SSH trÃªn mÃ¡y 192.168.1.17 pháº£n há»“i, yÃªu cáº§u xÃ¡c thá»±c.

			Báº¡n nháº­p password (hoáº·c dÃ¹ng SSH key) cho user ngoctuanqng1.

			Náº¿u Ä‘Äƒng nháº­p thÃ nh cÃ´ng, báº¡n sáº½ cÃ³ má»™t terminal shell cá»§a mÃ¡y 192.168.1.17 vÃ  cÃ³ thá»ƒ cháº¡y lá»‡nh tá»« xa.
			
		BÃ¢y giá» báº¡n Ä‘ang á»Ÿ trong mÃ¡y 192.168.1.17 vá»›i quyá»n user ngoctuanqng1.

		Má»™t sá»‘ tuá»³ chá»n hay dÃ¹ng:

			Chá»‰ Ä‘á»‹nh cá»•ng khÃ¡c (náº¿u SSH server khÃ´ng cháº¡y trÃªn port 22):

				ssh -p 2222 ngoctuanqng1@192.168.1.17
				
			Cháº¡y 1 lá»‡nh tá»« xa mÃ  khÃ´ng cáº§n login interactive:
			
				ssh ngoctuanqng1@192.168.1.17 "hostname"
				
				Sáº½ in ra hostname cá»§a mÃ¡y Ä‘Ã­ch rá»“i thoÃ¡t.
	
	- cmd: hostname
	
		ÄÃ¢y lÃ  lá»‡nh trong Linux/Unix dÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ hoáº·c Ä‘áº·t tÃªn cá»§a mÃ¡y tÃ­nh (hostname).	
			
		Khi cháº¡y Ä‘Æ¡n giáº£n hostname
			
			NÃ³ sáº½ in ra tÃªn mÃ¡y (hostname) hiá»‡n táº¡i.
			
				$ hostname
				localhost.localdomain
				
				localhost â†’ tÃªn ngáº¯n (short hostname).

				localhost.localdomain â†’ tÃªn Ä‘áº§y Ä‘á»§ (FQDN â€“ Fully Qualified Domain Name).
				
			Äáº·t láº¡i hostname (cáº§n quyá»n sudo)
			
				sudo hostname myserver
				
				hostname táº¡m thá»i Ä‘á»•i thÃ nh myserver (chá»‰ cÃ³ hiá»‡u lá»±c Ä‘áº¿n khi reboot).
				
		Kiá»ƒm tra cÃ¡c tÃ¹y chá»n khÃ¡c:

			hostname -i â†’ in ra Ä‘á»‹a chá»‰ IP gáº¯n vá»›i hostname.

			hostname -f â†’ in ra FQDN (tÃªn Ä‘áº§y Ä‘á»§ vá»›i domain).

			hostname -s â†’ in ra tÃªn ngáº¯n (short hostname).
			
		LiÃªn há»‡ thá»±c táº¿:

			Hostname giÃºp phÃ¢n biá»‡t cÃ¡c mÃ¡y trong máº¡ng ná»™i bá»™ hoáº·c há»‡ thá»‘ng server.

			Khi báº¡n SSH vÃ o mÃ¡y khÃ¡c, dÃ²ng prompt thÆ°á»ng hiá»ƒn thá»‹ user@hostname Ä‘á»ƒ biáº¿t mÃ¬nh Ä‘ang á»Ÿ Ä‘Ã¢u.
	
	- cmd: pwd
	
		ÄÃ¢y lÃ  lá»‡nh trong Linux/Unix dÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘Æ°á»ng dáº«n thÆ° má»¥c hiá»‡n táº¡i (thÆ° má»¥c mÃ  báº¡n Ä‘ang Ä‘á»©ng trong terminal).

		pwd = print working directory.	
	
	- cmd: mkdir /f/vagrant-vms
	
		mkdir (make directory): lá»‡nh dÃ¹ng Ä‘á»ƒ táº¡o thÆ° má»¥c má»›i trong Linux/Unix.

		/f/vagrant-vms: Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c muá»‘n táº¡o.

		NghÄ©a lÃ  báº¡n Ä‘ang muá»‘n táº¡o má»™t thÆ° má»¥c cÃ³ tÃªn vagrant-vms bÃªn trong thÆ° má»¥c /f.

		Má»™t vÃ i lÆ°u Ã½ quan trá»ng

			Trong Linux, Ä‘Æ°á»ng dáº«n /f/... cÃ³ nghÄ©a lÃ  trong thÆ° má»¥c gá»‘c / cÃ³ má»™t thÆ° má»¥c con tÃªn lÃ  f.
			
			Trong Windows + Git Bash / WSL, cÃº phÃ¡p /f/... thÆ°á»ng Ã¡m chá»‰ á»• Ä‘Ä©a F: Ä‘Æ°á»£c mount vÃ o Linux environment.

				VÃ­ dá»¥:

					/c/... â†’ á»• Ä‘Ä©a C:

					/d/... â†’ á»• Ä‘Ä©a D:

					/f/... â†’ á»• Ä‘Ä©a F:
					
					=> NghÄ©a lÃ  báº¡n Ä‘ang táº¡o thÆ° má»¥c vagrant-vms trong á»• Ä‘Ä©a F:.
	
	- cmd: vagrant init eurolinux-vagrant/centos-stream-9
	
		ÄÃ¢y lÃ  lá»‡nh cá»§a Vagrant dÃ¹ng Ä‘á»ƒ khá»Ÿi táº¡o (init) má»™t mÃ´i trÆ°á»ng mÃ¡y áº£o má»›i dá»±a trÃªn box cÃ³
		tÃªn eurolinux-vagrant/centos-stream-9.
		
		Giáº£i thÃ­ch chi tiáº¿t tá»«ng pháº§n:

			vagrant: cÃ´ng cá»¥ quáº£n lÃ½ mÃ´i trÆ°á»ng áº£o (VM) dÃ¹ng chung vá»›i VirtualBox, VMware, Hyper-V, Docker...

			init: khá»Ÿi táº¡o project Vagrant má»›i. Lá»‡nh nÃ y sáº½ táº¡o ra má»™t file Vagrantfile trong thÆ° má»¥c hiá»‡n táº¡i.

			eurolinux-vagrant/centos-stream-9: tÃªn cá»§a box (hÃ¬nh áº£nh há»‡ Ä‘iá»u hÃ nh template) Ä‘Æ°á»£c láº¥y tá»« Vagrant Cloud.

				eurolinux-vagrant â†’ publisher/organization Ä‘Ã£ táº¡o box.

				centos-stream-9 â†’ tÃªn box (á»Ÿ Ä‘Ã¢y lÃ  CentOS Stream 9).
				
		Káº¿t quáº£ khi cháº¡y lá»‡nh:

			Táº¡o file Vagrantfile trong thÆ° má»¥c hiá»‡n táº¡i (náº¿u chÆ°a cÃ³).
			
			VÃ­ dá»¥ ná»™i dung cÆ¡ báº£n:

				Vagrant.configure("2") do |config|
				  config.vm.box = "eurolinux-vagrant/centos-stream-9"
				end

			File nÃ y Ä‘á»‹nh nghÄ©a mÃ¡y áº£o mÃ  Vagrant sáº½ quáº£n lÃ½.
	
	
	- cmd: vagrant init ubuntu/jammy64
	- cmd: cat Vagrantfile
	- cmd: vagrant up
	- cmd: vagrant box list
	- cmd: vagrant status
	- cmd: vagrant ssh
	- cmd: whoami
	- cmd: exit
	- cmd: vagrant halt
	- cmd: vagrant reload
	- cmd: vagrant destroy
	- cmd: vagrant global-status
	- cmd: vagrant global-status --prune
	- cmd: sudo -i
	- cmd: history

	-- Manual:
	
		Oracle VM Virtualbox (Hypervisor)
		
		ISO file (CentOS & Ubuntu)
		
		Login tool (Git Bash & Putty)
		
		Táº£i file iso cá»§a centos vÃ  ubuntu Ä‘á»ƒ import vÃ o Oracle VM Virtualbox
		
	-- Automated:
	
		VirtualBox (Hypervisor)
		
		Vagrant (Creates vms with Vagrantfile)
		
		commands (vagrant up)
		
		VÃ o vagrant cloud Ä‘á»ƒ tÃ¬m lá»‡nh cá»§a vagrant Ä‘á»ƒ kÃ©o file vá» centos 9
		(eurolinux-vagrant/centos-stream-9) vÃ  ubuntu jammy (ubuntu/jammy64)
		
	Táº£i cÃ¡c file iso rá»“i import vÃ o storage cá»§a tá»«ng vm
	
	-- Oracle VM VirtualBox lÃ  gÃ¬?

		LÃ  pháº§n má»m áº£o hÃ³a (virtualization), cho phÃ©p báº¡n cháº¡y nhiá»u há»‡ Ä‘iá»u hÃ nh (OS)
		khÃ¡c nhau trÃªn cÃ¹ng má»™t mÃ¡y tÃ­nh tháº­t.

		VÃ­ dá»¥: mÃ¡y tháº­t cá»§a báº¡n cháº¡y Windows, nhÆ°ng báº¡n cÃ³ thá»ƒ cÃ i thÃªm má»™t mÃ¡y áº£o Ubuntu
		Linux trong VirtualBox Ä‘á»ƒ há»c, test mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Windows.
		
		Táº¡i sao cáº§n dÃ¹ng VirtualBox?
		
			Há»c táº­p & Thá»­ nghiá»‡m

				Báº¡n cÃ³ thá»ƒ cÃ i nhiá»u há»‡ Ä‘iá»u hÃ nh (Ubuntu, CentOS, Kali, Windows Server, â€¦)
				Ä‘á»ƒ há»c IT, máº¡ng, báº£o máº­t, DevOps, láº­p trÃ¬nh.

				Thá»­ nghiá»‡m pháº§n má»m, cáº¥u hÃ¬nh há»‡ thá»‘ng, cÃ i package mÃ  khÃ´ng sá»£ lÃ m há»ng mÃ¡y tháº­t.

			Máº¡ng & Server

				Giáº£ láº­p mÃ´i trÆ°á»ng server: cÃ i Apache/Nginx, MySQL, Kafka, RabbitMQ, Dockerâ€¦

				Dá»… dÃ ng cáº¥u hÃ¬nh máº¡ng áº£o (NAT, Bridged, Host-only) â†’ há»c cÃ¡ch cÃ¡c server giao
				tiáº¿p vá»›i nhau.

			PhÃ¡t triá»ƒn pháº§n má»m

				Táº¡o mÃ´i trÆ°á»ng riÃªng biá»‡t Ä‘á»ƒ test code, deploy á»©ng dá»¥ng.

				CÃ³ thá»ƒ snapshot (lÆ°u tráº¡ng thÃ¡i mÃ¡y áº£o) â†’ náº¿u há»ng thÃ¬ rollback láº¡i ngay.

			An toÃ n & Báº£o máº­t

				Cháº¡y thá»­ pháº§n má»m láº¡, file nghi ngá» virus trong mÃ¡y áº£o.

				MÃ¡y áº£o cÃ¡ch ly vá»›i mÃ¡y tháº­t, háº¡n cháº¿ rá»§i ro.

			Phá»• biáº¿n trong DevOps / Cloud

				TrÆ°á»›c khi triá»ƒn khai tháº­t trÃªn server (AWS, GCP, Azure), báº¡n cÃ³ thá»ƒ mÃ´ phá»ng mÃ´i
				trÆ°á»ng cloud ngay trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n báº±ng VirtualBox.

				Káº¿t há»£p vá»›i Vagrant Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o mÃ´i trÆ°á»ng dev/test.
				
		Khi nÃ o khÃ´ng cáº§n VirtualBox?

			Náº¿u báº¡n chá»‰ muá»‘n cháº¡y Linux thÆ°á»ng xuyÃªn â†’ cÃ³ thá»ƒ dual
			boot (cÃ i song song Windows + Linux).

			Náº¿u báº¡n chá»‰ cáº§n mÃ´i trÆ°á»ng dev nhanh â†’ cÃ³ thá»ƒ dÃ¹ng WSL
			(Windows Subsystem for Linux) thay cho VM.
	
	-- File ISO lÃ  gÃ¬?

		ISO file = má»™t báº£n sao toÃ n bá»™ (image) cá»§a Ä‘Ä©a CD/DVD.

		NÃ³ chá»©a Ä‘áº§y Ä‘á»§ há»‡ Ä‘iá»u hÃ nh hoáº·c pháº§n má»m, giá»‘ng nhÆ° báº¡n cáº§m má»™t cÃ¡i Ä‘Ä©a cÃ i Ä‘áº·t.

		Trong VirtualBox, file ISO Ä‘Æ°á»£c dÃ¹ng nhÆ° Ä‘Ä©a cÃ i Ä‘áº·t há»‡ Ä‘iá»u hÃ nh cho mÃ¡y áº£o (VM).

		VÃ­ dá»¥:

		Náº¿u báº¡n muá»‘n cÃ i Ubuntu trÃªn VirtualBox â†’ báº¡n cáº§n táº£i file ubuntu-22.04.iso.

		Náº¿u báº¡n muá»‘n cÃ i Windows 10 â†’ báº¡n cáº§n file Win10.iso.

		Náº¿u báº¡n muá»‘n cÃ i CentOS/RHEL â†’ táº£i file CentOS-Stream-9.iso hoáº·c rhel.iso.
		
	-- Setting trong Oracle VM Virtualbox:
	
		General: ThÃ´ng tin chung cá»§a VM (tÃªn, phiÃªn báº£n OS, mÃ´ táº£).

		System: CPU, RAM, Boot order.

		Display: cáº¥u hÃ¬nh card mÃ n hÃ¬nh, Ä‘á»™ phÃ¢n giáº£i.

		Storage: gáº¯n á»• cá»©ng áº£o (VDI, VMDK) hoáº·c file ISO cÃ i há»‡ Ä‘iá»u hÃ nh.

		Audio: báº­t/táº¯t Ã¢m thanh.

		Network: cáº¥u hÃ¬nh máº¡ng cho mÃ¡y áº£o.

		USB, Shared Folders, User Interface: cáº¥u hÃ¬nh USB, thÆ° má»¥c chia sáº», giao diá»‡n ngÆ°á»i dÃ¹ng.
		
	-- Controller trong VirtualBox lÃ  gÃ¬?
	
		Controller lÃ  bá»™ Ä‘iá»u khiá»ƒn (controller) giáº£ láº­p Ä‘á»ƒ quáº£n lÃ½ cÃ¡c thiáº¿t bá»‹ lÆ°u trá»¯ (Storage Devices) gáº¯n vÃ o mÃ¡y áº£o.

		NÃ³ hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° card Ä‘iá»u khiá»ƒn á»• cá»©ng/á»• Ä‘Ä©a trong mÃ¡y tháº­t.

		CÃ¡c controller phá»• biáº¿n: IDE, SATA, SCSI, SAS, NVMe, Floppy.
		
		Controller: IDE

			IDE (Integrated Drive Electronics) lÃ  chuáº©n cÅ©, thÆ°á»ng dÃ¹ng cho á»• Ä‘Ä©a CD/DVD vÃ  á»• cá»©ng tháº¿ há»‡ cÅ©.

			Trong VirtualBox:

			IDE thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ mount file ISO (nhÆ° Ä‘Ä©a cÃ i CentOS, Ubuntu).

			á» áº£nh cá»§a báº¡n:

				Controller: IDE cÃ³ 1 thiáº¿t bá»‹ gáº¯n lÃ  Empty (á»• CD/DVD trá»‘ng).

				Khi báº¡n chá»n vÃ  gáº¯n file ISO â†’ VirtualBox coi nhÆ° báº¡n bá» "Ä‘Ä©a CD áº£o" vÃ o á»• IDE nÃ y Ä‘á»ƒ boot vÃ  cÃ i OS.

		Controller: SATA

			SATA (Serial ATA) lÃ  chuáº©n má»›i hÆ¡n, nhanh hÆ¡n IDE, thÆ°á»ng dÃ¹ng cho á»• cá»©ng hiá»‡n Ä‘áº¡i.

			Trong VirtualBox:

				Controller: SATA Ä‘ang chá»©a centosvm.vdi â†’ Ä‘Ã¢y chÃ­nh lÃ  á»• cá»©ng áº£o (Virtual Disk Image) cá»§a mÃ¡y CentOS.

				Há»‡ Ä‘iá»u hÃ nh (CentOS) sau khi cÃ i sáº½ náº±m trong á»• nÃ y.
		
	ACPI lÃ  gÃ¬?

		ACPI (Advanced Configuration and Power Interface) lÃ  má»™t chuáº©n do Intel, Microsoft vÃ  Toshiba phÃ¡t triá»ƒn.

		NÃ³ cho phÃ©p há»‡ Ä‘iá»u hÃ nh quáº£n lÃ½ nguá»“n Ä‘iá»‡n cá»§a pháº§n cá»©ng (power management).

		Nhá» ACPI, OS cÃ³ thá»ƒ:

			Táº¯t/má»Ÿ mÃ¡y báº±ng pháº§n má»m.

			ÄÆ°a mÃ¡y vÃ o cháº¿ Ä‘á»™ Sleep/Hibernate.

			Giáº£m tá»‘c Ä‘á»™ CPU khi khÃ´ng cáº§n thiáº¿t Ä‘á»ƒ tiáº¿t kiá»‡m Ä‘iá»‡n.

		ACPI Shutdown trong VirtualBox lÃ  gÃ¬?

			Khi báº¡n chá»n ACPI Shutdown trong VirtualBox (hoáº·c trong cÃ¡c hypervisor khÃ¡c), nÃ³ sáº½ gá»­i tÃ­n hiá»‡u
			ACPI Ä‘áº¿n há»‡ Ä‘iá»u hÃ nh trong mÃ¡y áº£o.

			HÃ nh Ä‘á»™ng nÃ y tÆ°Æ¡ng tá»± nhÆ° nháº¥n nÃºt Power trÃªn mÃ¡y tÃ­nh tháº­t (nhÆ°ng khÃ´ng pháº£i "táº¯t cá»©ng").

			Há»‡ Ä‘iá»u hÃ nh trong VM sáº½ nháº­n tÃ­n hiá»‡u vÃ  tá»± thá»±c hiá»‡n quÃ¡ trÃ¬nh shutdown an toÃ n:

				ÄÃ³ng á»©ng dá»¥ng.

				Ghi dá»¯ liá»‡u cÃ²n trong RAM xuá»‘ng Ä‘Ä©a.

				Sau Ä‘Ã³ táº¯t mÃ¡y.

			KhÃ¡c vá»›i Power Off

				ACPI Shutdown: Táº¯t an toÃ n, giá»‘ng nhÆ° báº¡n báº¥m nÃºt Shutdown trong Windows/Linux.

				Power Off (Hard Power Off): Táº¯t cá»©ng, giá»‘ng nhÆ° rÃºt dÃ¢y nguá»“n â†’ cÃ³ thá»ƒ gÃ¢y máº¥t dá»¯ liá»‡u hoáº·c há»ng
				file há»‡ thá»‘ng.
				
				
--- Linux:

	Command line:

		- cmd: cat /etc/os-release
		- cmd: cd
		
			cd (khÃ´ng tham sá»‘) â†’ quay vá» thÆ° má»¥c home cá»§a user.
			
		- cmd: cd /tmp/
		
			/tmp/ cÃ³ dáº¥u / á»Ÿ Ä‘áº§u â†’ Ä‘Ã¢y lÃ  Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i.

			NghÄ©a lÃ : Ä‘i tháº³ng vÃ o thÆ° má»¥c /tmp náº±m á»Ÿ gá»‘c cá»§a há»‡ thá»‘ng (/).

			DÃ¹ báº¡n Ä‘ang Ä‘á»©ng á»Ÿ Ä‘Ã¢u thÃ¬ káº¿t quáº£ váº«n giá»‘ng nhau.		
		
		- cmd: uptime
		- cmd: free -m
		- cmd: mkdir ops bakupdir
		
			mkdir = make directory, dÃ¹ng Ä‘á»ƒ táº¡o thÆ° má»¥c má»›i.

				ops = tÃªn thÆ° má»¥c thá»© nháº¥t báº¡n muá»‘n táº¡o.

				bakupdir = tÃªn thÆ° má»¥c thá»© hai báº¡n muá»‘n táº¡o.
				
			Khi viáº¿t hai tÃªn thÆ° má»¥c nhÆ° váº­y, mkdir sáº½ táº¡o cÃ¹ng lÃºc 2 thÆ° má»¥c trong thÆ° má»¥c hiá»‡n táº¡i	
		
		- cmd: touch devopsfile{1..10}.txt
		- cmd: cp devopsfile1.txt dev/
		- cmd: ls dev/
		- cmd: ls /home/vagrant/dev/
		- cmd: cp --help
		- cmd: mv devopsfile3.txt ops/
		- cmd: mv testfile1.txt testfile22.txt
		
			ÄÃ¢y lÃ  lá»‡nh di chuyá»ƒn (move) hoáº·c Ä‘á»•i tÃªn (rename) file/thÆ° má»¥c trong Linux/Unix.
			
			VÃ¬ cáº£ hai tÃªn Ä‘á»u á»Ÿ cÃ¹ng má»™t thÆ° má»¥c â†’ lá»‡nh nÃ y Ä‘á»•i tÃªn file tá»« testfile1.txt thÃ nh testfile22.txt.
			
		- cmd: touch testfile1.txt
		- cmd: mv *.txt textdir/
		- cmd: rm devopsfile10.txt
		- cmd: rm -r mobile
		- cmd: mkdir testdir{1..5}
		- cmd: rm -rf *
		- cmd: history
		- cmd: cat /etc/os-release
		
			ÄÃ¢y lÃ  lá»‡nh Ä‘á»ƒ xem thÃ´ng tin vá» há»‡ Ä‘iá»u hÃ nh Linux Ä‘ang cháº¡y.
			
		- cmd: sudo yum install vim -y
		- cmd: vim firstfile.txt
		- cmd: ls -l
		- cmd: file anaconda-ks.cfg
		- cmd: file yum
		- cmd: file /bin/pwd
		- cmd: mkdir -p /opt/dev/ops/devops/test
		- cmd: ln -s /opt/dev/ops/devops/test/command.txt cmds
		- cmd: unlink cmds
		- cmd: ls -lt
		- cmd: ls -ltr
		- cmd: ls -ltr /etc/
		- cmd: hostname centos7.devops. in
		- cmd: ls -ltr /etc/
		- cmd: grep firewall anaconda-ks.cfg
		- cmd: grep Firewall anaconda-ks.cfg
		- cmd: grep -i Firewall anaconda-ks.cfg
		- cmd: grep -i firewall < anaconda-ks.cfg
		- cmd: grep -i firewall *
		- cmd: grep -iR firewall *
		- cmd: grep -R SELINUX /etc/*
		- cmd: grep -vi firewall anaconda-ks.cfg
		- cmd: less anaconda-ks.cfg
		- cmd: more anaconda-ks.cfg
		- cmd: head anaconda-ks.cfg
		- cmd: head -20 anaconda-ks.cfg
		- cmd: tail anaconda-ks.cfg
		- cmd: tail -2 anaconda-ks.cfg
		- cmd: tail -f anaconda-ks.cfg
		- cmd: tail -f yum.log
		- cmd: cut -d: -f1 /etc/passwd
		- cmd: cut -d: -f3 /etc/passwd
		- cmd: awk -F': ' ' {print $1}' /etc/passwd
		- cmd: sed 's/coronavirus/covid19/g' samplefile.txt
		- cmd: sed -i 's/covid19/nothing/g' samplefile.txt
		- cmd: sed -i 's/coronavirus/covid19/g' samplefile.txt
		- cmd: uptime > /tmp/sysinfo.txt
		- cmd: ls > /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m
		- cmd: df -h
		- cmd: echo "Good Morning"
		- cmd: echo "#############################" > /tmp/sysinfo.txt
		- cmd: date > /tmp/sysinfo.txt
		- cmd: echo "#############################" >> /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m >> /tmp/sysinfo.txt
		- cmd: df -h >> /tmp/sysinfo.txt
		- cmd: yum install vim -y > /dev/null
		- cmd: cat /dev/null > /tmp/sysinfo.txt
		- cmd: free -m > /dev/null
		- cmd: freeeee -m 2>> /tmp//error.log
		- cmd: free -m 1>> /tmp//error.log
		- cmd: free -m &>> /tmp//error.log
		- cmd: freesdsd -m &>> /tmp//error.log
		- cmd: wc -l /etc/passwd
		- cmd: wc -l < /etc/passwd
		- cmd: ls | wc -l
		- cmd: ls | grep host*
		- cmd: ls | grep host
		- cmd: tail /var/log/messages | grep -i vagrant
		- cmd: tail -20 /var/log/messages | grep -i vagrant
		- cmd: free -m | grep Mem
		- cmd: ls -l | tail
		- cmd: ls -l | head
		- cmd: find /etc -name host*
		- cmd: find / -name host*
		- cmd: yum install mlocate -y
		- cmd: updatedb
		- cmd: locate host
		- cmd: head -1 /etc/passwd
		- cmd: grep vagrant /etc/passwd
		- cmd: id vagrant
		- cmd: useradd ansible
		- cmd: useradd jenkins
		- cmd: useradd aws
		- cmd: tail -4 /etc/passwd
		- cmd: tail -4 /etc/group
		- cmd: id ansible
		- cmd: groupadd devops
		- cmd: ls -l
		- cmd: ls -ld /opt/devopsdir
		- cmd: chown -R ansible:devops /opt/devopsdir
		- cmd: ls -ld /opt/devopsdir
		- cmd: chmod o-x /opt/devopsdir
		- cmd: chmod g+w /opt/devopsdir
		- cmd: su - miles
		- cmd: su - aws
		- cmd: chown aws.devops /opt/webdata
		- cmd: chmod -R 770 /opt/webdata
		- cmd: chmod -R 754 /opt/webdata/
		- cmd: sudo yum install git -y
		- cmd: sudo useradd test
		- cmd: passwd ansible
		- cmd: su - ansible
		- cmd: sudo useradd test12
		- cmd: visudo
		- cmd: ls -l /etc/sudoers
		- cmd: cd /etc/sudoers.d/
		- cmd: cat vagrant
		- cmd: cat *
		- cmd: rpm -qa
		- cmd: telnet
		- cmd: arch
		- cmd: uname -m
		- cmd: curl https://rpmfind.net/linux/RPM/centos-stream/9/appstream/x86_64/telnet-0.17-85.el9.x86_64.html
		- cmd: curl https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/telnet-0.17-85.el9.x86_64.rpm -o telnet-0.17-85.el9.x86_64.rpm
		- cmd: rpm -ivh telnet-0.17-85.el9.x86_64.rpm
		- cmd: telnet
		- cmd: rpm -qa | grep telnet
		- cmd: rpm -e telnet-0.17-85.el9.x86_64
		- cmd: wget https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/httpd-2.4.57-8.el9.x86_64.rpm
		- cmd: cd /etc/yum.repos.d/
		- cmd: yum search httpd
		- cmd: yum install httpd
		- cmd: dnf remove httpd
		- cmd: dnf install httpd -y
		- cmd: dnf --help
		- cmd: yum upgrade
		- cmd: yum install jenkins
		- cmd: sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
		- cmd: sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
		- cmd: sudo yum upgrade
		- cmd: dnf repolist
		- cmd: dnf install epel-release -y
		- cmd: dnf history
		- cmd: systemctl status httpd
		- cmd: systemctl start httpd
		- cmd: sudo reboot
		- cmd: systemctl enable httpd
		- cmd: systemctl status sshd
		- cmd: systemctl is-active httpd
		- cmd: systemctl is-enabled httpd
		- cmd: cat /etc/systemd/system/multi-user.target.wants/httpd.service
		- cmd: top
		- cmd: ps aux
		- cmd: ps -ef
		- cmd: ps -ef | grep httpd | grep -v 'grep'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}' | xargs kill -9
		- cmd: file jenkins_06122020.tar.gz
		- cmd: tar -czvf kenkins_06122020.tar.gz jenkins
		- cmd: tar -xzvf jenkins_06122020.tar.gz
		- cmd: tar -xzvf jenkins_06122020.tar.gz -C/pop/
		- cmd: tar --help
		- cmd: yum install zip unzip -y
		- cmd: zip -r jenkins_06122020.zip jenkins
		- cmd: ls -ltr jenkins*
		- cmd: rm -rf jenkins
		- cmd: unzip jenkins_06122020.zip
		- cmd: userdel -r devops
		- cmd: export EDITOR=vim
		- cmd: wget http://archive.ubuntu.com/ubuntu/pool/universe/t/tree/tree_2.0.2-1_amd64.deb
		- cmd: dpkg -i tree_2.0.2-1_amd64.deb
		- cmd: dpkg -l
		- cmd: dpkg -l | grep tree
		- cmd: dpkg -r tree
		- cmd: apt update
		- cmd: apt search tree
		- cmd: apt install tree
		- cmd: apt install apache2
		- cmd: systemctl status apache2
		- cmd: apt upgrade
		- cmd: apt remove apache2
		- cmd: apt purge apache2
		
	
	TrÃªn host: báº¡n khÃ´ng cÃ³ sudo â†’ lá»—i.

	Sau khi vagrant ssh: báº¡n Ä‘ang á»Ÿ trong mÃ¡y áº£o, nÆ¡i sudo Ä‘Ã£ Ä‘Æ°á»£c cÃ i vÃ  cáº¥u hÃ¬nh sáºµn â†’ cháº¡y Ä‘Æ°á»£c sudo -i.
	
	$ vagrant ssh
	
		A Vagrant environment or target machine is required to run this
		command. Run `vagrant init` to create a new Vagrant environment. Or,
		get an ID of a target machine from `vagrant global-status` to run
		this command on. A final option is to change to a directory with a
		Vagrantfile and to try again.
		
		CÃ³ nghÄ©a lÃ  báº¡n Ä‘ang cháº¡y vagrant ssh nhÆ°ng Vagrant khÃ´ng biáº¿t báº¡n muá»‘n SSH vÃ o mÃ¡y áº£o nÃ o.
		
		Báº¡n chÆ°a táº¡o mÃ¡y áº£o báº±ng vagrant init + vagrant up.

		Hoáº·c báº¡n Ä‘ang khÃ´ng Ä‘á»©ng trong thÆ° má»¥c cÃ³ chá»©a file Vagrantfile.

		Hoáº·c chÆ°a cÃ³ mÃ¡y áº£o nÃ o Ä‘ang cháº¡y.
		
	CÃ¡ch thoÃ¡t ra khá»i vim:

		Nháº¥n phÃ­m Esc (Ä‘á»ƒ cháº¯c cháº¯n khÃ´ng cÃ²n á»Ÿ cháº¿ Ä‘á»™ nháº­p vÄƒn báº£n ná»¯a).

		Sau Ä‘Ã³ gÃµ má»™t trong cÃ¡c lá»‡nh sau rá»“i báº¥m Enter:

		:q â†’ thoÃ¡t náº¿u chÆ°a chá»‰nh sá»­a gÃ¬.

		:q! â†’ thoÃ¡t bá» qua thay Ä‘á»•i (khÃ´ng lÆ°u).

		:wq â†’ lÆ°u rá»“i thoÃ¡t.

		:x â†’ tÆ°Æ¡ng tá»± :wq (lÆ°u rá»“i thoÃ¡t).

	Vim:
	
		Lá»‡nh Shift + g
		
			Di chuyá»ƒn con trá» xuá»‘ng dÃ²ng cuá»‘i cÃ¹ng cá»§a file.
			
		Lá»‡nh yy

			yy = yank line (sao chÃ©p 1 dÃ²ng hiá»‡n táº¡i).

			Khi báº¡n Ä‘ang á»Ÿ cháº¿ Ä‘á»™ Normal mode (báº¥m Esc Ä‘á»ƒ cháº¯c cháº¯n):

			yy sáº½ copy nguyÃªn cáº£ dÃ²ng nÆ¡i con trá» Ä‘ang Ä‘á»©ng vÃ o bá»™ nhá»› táº¡m (clipboard cá»§a Vim).

			VÃ­ dá»¥:
			
				Con trá» Ä‘ang á»Ÿ dÃ²ng sá»‘ 5, gÃµ yy â†’ dÃ²ng sá»‘ 5 Ä‘Æ°á»£c copy.
				
		Lá»‡nh yyyy

		Lá»‡nh p

			p = put (dÃ¡n sau con trá»).

			Sau khi copy báº±ng yy, báº¡n cÃ³ thá»ƒ dÃ¡n dÃ²ng Ä‘Ã³ xuá»‘ng ngay dÆ°á»›i con trá» báº±ng p.

			VÃ­ dá»¥:

				Báº¡n Ä‘á»©ng á»Ÿ dÃ²ng sá»‘ 5, gÃµ yy.

				Sau Ä‘Ã³ xuá»‘ng dÃ²ng sá»‘ 10, gÃµ p.

				DÃ²ng sá»‘ 5 sáº½ Ä‘Æ°á»£c chÃ¨n vÃ o sau dÃ²ng sá»‘ 10.
				
		Lá»‡nh dd
		
		Lá»‡nh ddddd
		
		Lá»‡nh u
		
		Lá»‡nh /
		
		Lá»‡nh :%s/coronavirus/covid19/g
		
			Thay toÃ n bá»™ táº¥t cáº£ cÃ¡c chá»¯ "coronavirus" trong file thÃ nh "covid19".
			
			Giáº£i thÃ­ch tá»«ng pháº§n:
		
				: â†’ vÃ o command mode trong Vim (gÃµ lá»‡nh).

				% â†’ pháº¡m vi Ã¡p dá»¥ng: toÃ n bá»™ file.

				Náº¿u chá»‰ viáº¿t :s/.../.../ thÃ¬ nÃ³ chá»‰ thay trong dÃ²ng hiá»‡n táº¡i.

				% nghÄ©a lÃ  tá»« dÃ²ng 1 Ä‘áº¿n dÃ²ng cuá»‘i.

				s â†’ viáº¿t táº¯t cá»§a substitute (thay tháº¿).

				coronavirus â†’ lÃ  chuá»—i tÃ¬m kiáº¿m (cÃ¡i cáº§n thay).

				covid19 â†’ lÃ  chuá»—i thay tháº¿.

				g â†’ global trong dÃ²ng â†’ thay táº¥t cáº£ cÃ¡c láº§n xuáº¥t hiá»‡n trong má»—i dÃ²ng.

				Náº¿u bá» g, thÃ¬ trÃªn má»—i dÃ²ng chá»‰ thay láº§n xuáº¥t hiá»‡n Ä‘áº§u tiÃªn.
		
		Lá»‡nh :%s/covid19//g
		
			XÃ³a toÃ n bá»™ táº¥t cáº£ cÃ¡c chá»¯ covid19 trong toÃ n bá»™ file.
			
			Giáº£i thÃ­ch ngáº¯n gá»n:

				:% â†’ Ã¡p dá»¥ng cho toÃ n bá»™ file.

				s â†’ substitute (thay tháº¿).

				covid19 â†’ chuá»—i cáº§n tÃ¬m.

				// â†’ thay tháº¿ báº±ng chuá»—i rá»—ng (nghÄ©a lÃ  xÃ³a Ä‘i).

				g â†’ thay táº¥t cáº£ cÃ¡c láº§n xuáº¥t hiá»‡n trÃªn má»—i dÃ²ng.
		
	Grep:
	
		grep trong Linux lÃ  má»™t lá»‡nh dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m chuá»—i (string/pattern) trong file hoáº·c
		trong output cá»§a lá»‡nh khÃ¡c. NÃ³ lÃ  viáº¿t táº¯t cá»§a Global Regular Expression Print.
		
		CÃ¡ch hoáº¡t Ä‘á»™ng

			grep sáº½ quÃ©t qua tá»«ng dÃ²ng dá»¯ liá»‡u.

			Náº¿u dÃ²ng nÃ o khá»›p vá»›i chuá»—i hoáº·c biá»ƒu thá»©c chÃ­nh quy (regex) mÃ  báº¡n chá»‰ Ä‘á»‹nh â†’ nÃ³ sáº½ in dÃ²ng Ä‘Ã³ ra mÃ n hÃ¬nh.
			
	dnf:
	
		Trong Linux (Ä‘áº·c biá»‡t lÃ  CentOS, RHEL, Fedora), dnf lÃ  má»™t trÃ¬nh quáº£n lÃ½ gÃ³i (package manager).
		
		Giáº£i thÃ­ch ngáº¯n gá»n

			dnf viáº¿t táº¯t cá»§a Dandified YUM.

			NÃ³ lÃ  phiÃªn báº£n má»›i thay tháº¿ cho yum tá»« RHEL/CentOS 8 trá»Ÿ Ä‘i.

			DÃ¹ng Ä‘á»ƒ:

			CÃ i Ä‘áº·t pháº§n má»m.

			Gá»¡ bá» pháº§n má»m.

			Cáº­p nháº­t há»‡ thá»‘ng.

			Quáº£n lÃ½ kho (repository).
			
	yum:
	
		Trong Linux (Ä‘áº·c biá»‡t lÃ  cÃ¡c báº£n CentOS, RHEL, Fedora cÅ©), yum lÃ  má»™t trÃ¬nh quáº£n lÃ½ gÃ³i (package manager).
		
		Giáº£i thÃ­ch ngáº¯n gá»n

			yum viáº¿t táº¯t cá»§a Yellowdog Updater, Modified.

			NÃ³ giÃºp báº¡n cÃ i Ä‘áº·t, cáº­p nháº­t, gá»¡ bá» vÃ  quáº£n lÃ½ pháº§n má»m trÃªn há»‡ thá»‘ng Linux dá»±a trÃªn gÃ³i RPM.

			yum lÃ m viá»‡c vá»›i cÃ¡c repository (kho pháº§n má»m) Ä‘á»ƒ tá»± Ä‘á»™ng táº£i vá» vÃ  xá»­ lÃ½ phá»¥ thuá»™c giá»¯a cÃ¡c gÃ³i (dependencies).

		Quan há»‡ giá»¯a yum vÃ  dnf

			yum lÃ  cÃ´ng cá»¥ cÅ© (dÃ¹ng trÃªn CentOS 7, RHEL 7 trá»Ÿ xuá»‘ng).

			Tá»« CentOS/RHEL 8 trá»Ÿ Ä‘i, yum Ä‘Ã£ Ä‘Æ°á»£c thay tháº¿ bá»Ÿi dnf.

			Tuy nhiÃªn, Ä‘á»ƒ trÃ¡nh gÃ¢y rá»‘i cho ngÆ°á»i dÃ¹ng quen yum, nhiá»u há»‡ thá»‘ng má»›i váº«n Ä‘á»ƒ lá»‡nh yum tá»“n táº¡i, nhÆ°ng thá»±c
			cháº¥t nÃ³ chá»‰ lÃ  symlink (liÃªn káº¿t) trá» tá»›i dnf.
			
	apt:
	
		apt (Advanced Package Tool) lÃ  trÃ¬nh quáº£n lÃ½ gÃ³i (package manager) dÃ¹ng trong cÃ¡c há»‡ Ä‘iá»u hÃ nh Linux
		thuá»™c há» Debian (nhÆ° Ubuntu, Debian, Linux Mint).

		NÃ³ giÃºp báº¡n cÃ i Ä‘áº·t, nÃ¢ng cáº¥p, gá»¡ bá» vÃ  quáº£n lÃ½ pháº§n má»m má»™t cÃ¡ch dá»… dÃ ng, thay vÃ¬ pháº£i táº£i thá»§ cÃ´ng tá»«ng file .deb.
		
		
		
--- GIT:

	- Commands:
	
		- cmd: git init
		- cmd: ls -a
		- cmd: touch saturn{1..10}.py
		- cmd: git status
		- cmd: git add .
		- cmd: git commit -m "new files commited"
		- cmd: git config --global user.email "ngoctuanqng1@gmail.com"
		- cmd: git config --global user.name  "ngoctuanqng1"
		- cmd: git remote add origin https://github.com/ngoctuanqng/gitpractice.git
		- cmd: cat .git/config
		- cmd: git branch -m main
		- cmd: git push -u origin main
		- cmd: git add satural.py
		- cmd: git push origin main
		- cmd: git log
		- cmd: git log --oneline
		- cmd: git show 625b034
		- cmd: git pull
		- cmd: git branch -c sprintl
		- cmd: git branch -a
		- cmd: git checkout sprintl
		- cmd: git rm saturn6.py saturn7.py saturn8.py
		- cmd: git mv saturn1.py saturn11.py
		- cmd: touch jupiter{1..4}.rb
		- cmd: git push origin sprintl
		- cmd: touch sun earth venus mercury
		- cmd: git switch sprint1
		- cmd: git merge sprintl
		- cmd: git clone https://github.com/ngoctuanqng/gitpractice.git
		- cmd: git checkout jupiter1.rb
		- cmd: git diff
		- cmd: git diff --cached
		- cmd: git restore --staged jupiter1.rb
		- cmd: git diff 8dff644..cbb01a8
		- cmd: git revert HEAD
		- cmd: git reset --hard 8dff644
		- cmd: cat .git/config
		- cmd: rm -rf .ssh/*
		- cmd: cat .ssh/id_rsa.pub
		- cmd: git tag
		- cmd: git show v2.0.0
		- cmd: git tag -a v3.5.3 -m "Release 3.5.3"
		- cmd: systemctl restart httpd
		
		
		
		
		
	Ta cÃ³ thá»ƒ táº¡o á»Ÿ local trÆ°á»›c, sau Ä‘Ã³ táº¡o á»Ÿ remote, dÃ¹ng lá»‡nh git remote add origin Ä‘á»ƒ Ä‘áº©y local vÃ o remote
	
	touch a vÃ  touch a/ lÃ  khÃ¡c nhau, touch a sáº½ táº¡o file a, cÃ²n touch a/ sáº½ táº¡o folder tÃªn lÃ  a
	

--- Vagrant and Linux Servers:

	- Command:
	
		- cmd: vagrant destroy --force
		- cmd: ls ~/.vagrant.d/
		- cmd: cat /proc/cpuinfo
		- cmd: vagrant reload --provision
		- cmd: vi /etc/hostname
		- cmd: hostname finance
		- cmd: yum install httpd wget vim unzip zip -y
		- cmd: unzip 2138_aqua_nova.zip
		- cmd: rm -r 2138_aqua_nova
		- cmd: cp -r * /var/www/html/
		- cmd: systemctl status firewalld
		- cmd: systemctl stop firewalld
		- cmd: systemctl disable firewalld
		- cmd: vagrant init ubuntu/focal64
		- cmd: sudo apt update
		
		- cmd: sudo apt install apache2 \
                 ghostscript \
                 libapache2-mod-php \
                 mysql-server \
                 php \
                 php-bcmath \
                 php-curl \
                 php-imagick \
                 php-intl \
                 php-json \
                 php-mbstring \
                 php-mysql \
                 php-xml \
                 php-zip
				 
		- cmd: sudo mkdir -p /srv/www
		- cmd: sudo chown www-data: /srv/www
		- cmd: curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www
		- cmd: vim /etc/apache2/sites-available/wordpress.conf
		- cmd: sudo a2ensite wordpress
		- cmd: sudo a2enmod rewrite
		- cmd: sudo a2dissite 000-default
		- cmd: sudo service apache2 reload
		- cmd: sudo mysql -u root
		- cmd: CREATE DATABASE wordpress;
		- cmd: show databases;
		- cmd: CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
		- cmd: GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
		- cmd: FLUSH PRIVILEGES;
		- cmd: sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php
		- cmd: vim /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data vim /srv/www/wordpress/wp-config.php
		- cmd: vagrant ssh web02
		- cmd: vagrant destroy web01
		- cmd: vagrant up web03
		- cmd: ls /usr/lib/systemd/system/
		- cmd: cat /usr/lib/systemd/system/httpd.service
		- cmd: ls /etc/httpd/
		- cmd: ls /var/log/httpd/
		- cmd: wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.46/bin/apache-tomcat-10.1.46.tar.gz
		- cmd: tar xzvf apache-tomcat-10.1.46.tar.gz
		- cmd: dnf install java-17-openjdk -y
		- cmd: [root@localhost apache-tomcat-10.1.46]# bin/startup.sh
		- cmd: ps -ef | grep tomcat
		- cmd: kill 9886
		- cmd: [root@localhost ~]# useradd --home-dir /opt/tomcat --shell /sbin/nologin tomcat
		- cmd: [root@localhost ~]# cp -r apache-tomcat-10.1.46/* /opt/tomcat
		- cmd: [root@localhost ~]# chown -R tomcat.tomcat /opt/tomcat/
		- cmd: vim /etc/systemd/system/tomcat.service
		
			[Unit]
			Description=Tomcat
			After=network.target

			[Service]
			Type=forking

			User=tomcat
			Group=tomcat

			workingDirectory=/opt/tomcat

			Environment=JAVA_HOME=/usr/lib/jvm/jre

			Environment=CATALINA_HOME=/opt/tomcat
			Environment=CATALINE_BASE=/opt/tomcat

			ExecStart=/opt/tomcat/bin/startup.sh
			ExecStop=/opt/tomcat/bin/shutdown.sh

			[Install]
			WantedBy=multi-user.target

		- cmd: systemctl daemon-reload
		- cmd: [root@localhost ~]# systemctl start tomcat
		- cmd: [root@localhost ~]# systemctl status tomcat
		- cmd: [root@localhost ~]# systemctl enable tomcat
		
		
		
		
		
	
		
		
		
	GÃ¡n cÃ¡c ná»™i dung cá»§a trang web vÃ o /var/www/html/ Ä‘á»ƒ cÃ³ thá»ƒ cháº¡y lÃªn trang web mong muá»‘n
	
	Download tomcat 10
	
	Truy cáº­p Tomcat:
	
		Tomcat theo máº·c Ä‘á»‹nh sáº½ nghe trÃªn port 8080, trá»« khi báº¡n Ä‘Ã£ sá»­a file server.xml.
		
		Trong output ip addr show, card enp0s8 cÃ³ Ä‘á»‹a chá»‰: inet 192.168.1.11/24 brd 192.168.1.255
		
		ÄÃ¢y lÃ  IP Ä‘á»ƒ báº¡n truy cáº­p tá»« host hoáº·c cÃ¡c mÃ¡y khÃ¡c trong cÃ¹ng máº¡ng 192.168.1.x.
		
		Báº¡n cáº§n má»Ÿ trÃ¬nh duyá»‡t vÃ  nháº­p:
		
			http://192.168.1.11:8080/
	
	Intall and configure WordPress:
	
		https://ubuntu.com/tutorials/install-and-configure-wordpress#7-configure-wordpress
		
			WordPress lÃ  gÃ¬?

			WordPress lÃ  má»™t CMS (Content Management System) â€“ há»‡ thá»‘ng quáº£n trá»‹ ná»™i dung.

			NÃ³ giÃºp báº¡n táº¡o website/blog mÃ  khÃ´ng cáº§n viáº¿t code nhiá»u.

			Ráº¥t phá»• biáº¿n Ä‘á»ƒ lÃ m: blog, website tin tá»©c, website cÃ´ng ty, tháº­m chÃ­ thÆ°Æ¡ng máº¡i Ä‘iá»‡n
			tá»­ (vá»›i plugin nhÆ° WooCommerce).

		â€œInstall and Configure WordPressâ€ (cÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh WordPress) Ä‘á»ƒ lÃ m gÃ¬?

			Install (cÃ i Ä‘áº·t):

				Táº£i mÃ£ nguá»“n WordPress vá» mÃ¡y chá»§ (server Linux).

				Äáº·t nÃ³ vÃ o thÆ° má»¥c web server (Apache/nginx).

				Káº¿t ná»‘i vá»›i cÆ¡ sá»Ÿ dá»¯ liá»‡u (MySQL/MariaDB).

				Sau bÆ°á»›c nÃ y, server cá»§a báº¡n cÃ³ WordPress, nhÆ°ng nÃ³ chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ cháº¡y.

			Configure (cáº¥u hÃ¬nh):

				Táº¡o file wp-config.php Ä‘á»ƒ WordPress biáº¿t pháº£i káº¿t ná»‘i tá»›i database nÃ o, username/password lÃ  gÃ¬.

				Thiáº¿t láº­p cÃ¡c thÃ´ng sá»‘ nhÆ° ngÃ´n ngá»¯, timezone, theme, plugin.

				Truy cáº­p qua trÃ¬nh duyá»‡t Ä‘á»ƒ hoÃ n táº¥t wizard (báº¡n Ä‘áº·t tÃªn site, tÃ i khoáº£n admin, máº­t kháº©uâ€¦).

				Sau bÆ°á»›c nÃ y, báº¡n cÃ³ má»™t website WordPress cháº¡y Ä‘Æ°á»£c, sáºµn sÃ ng dÃ¹ng.

		TÃ³m láº¡i

			Install WordPress = Ä‘Æ°a WordPress vÃ o server.

			Configure WordPress = káº¿t ná»‘i WordPress vá»›i database + thiáº¿t láº­p Ä‘á»ƒ nÃ³ cháº¡y thÃ nh má»™t website hoÃ n chá»‰nh.
			
	Promt Ä‘á»ƒ táº¡o multivm:
	
		Multivm vagrantfile web01 ubuntu20, web02 with ubuntu20 & db01 with centos 7. Private IP for all the
		vms. Provisioning for db01. Set hostname also
		
		
		
		
		
	Ná»™i dung file Vagrantfile khi má»Ÿ má»™t sá»‘ comment:
	
		Case 1 (ubuntu):

			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end
			
		Case 2 (ubuntu):
		
			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.synced_folder "E:\\devops_practice\\scripts\\shellscripts", "/opt/scripts"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end			

		Case 3 (centos):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.10"
			  config.vm.network "public_network"
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip git -y
				mkdir /opt/devopsdir
				free -m
				uptime
			  SHELL
			end
			
		Case 4 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.33.10"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end

		Case 5 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.56.14"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end
			
			ChÃº thá»ƒ truy cáº­p vÃ o link 192.168.56.14
			
		Case 6 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.22"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			end
			
		Case 7 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.26"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			end
			
		Case 8 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.28"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip vim -y
				systemctl start httpd
				systemctl enabled httpd
				mkdir -p /tmp/finance
				cd /tmp/finance
				wget https://www.tooplate.com/zip-templates/2138_aqua_nova.zip
				unzip -o 2138_aqua_nova.zip
				cp -r 2138_aqua_nova/* /var/www/html/
				systemctl restart httpd
				cd /tmp/
				rm -rf /tmp/finance
			  SHELL
			end
			
		Case 9 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.30"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
			  sudo apt update -y
			  sudo apt install -y apache2 \
							  ghostscript \
							  libapache2-mod-php \
							  mysql-server \
							  php \
							  php-bcmath \
							  php-curl \
							  php-imagick \
							  php-intl \
							  php-json \
							  php-mbstring \
							  php-mysql \
							  php-xml \
							  php-zip

			  sudo mkdir -p /srv/www
			  sudo chown www-data: /srv/www
			  curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www

			  cat > /etc/apache2/sites-available/wordpress.conf <<EOF
			<VirtualHost *:80>
				DocumentRoot /srv/www/wordpress
				<Directory /srv/www/wordpress>
					Options FollowSymLinks
					AllowOverride Limit Options FileInfo
					DirectoryIndex index.php
					Require all granted
				</Directory>
				<Directory /srv/www/wordpress/wp-content>
					Options FollowSymLinks
					Require all granted
				</Directory>
			</VirtualHost>
			EOF

			  sudo a2ensite wordpress
			  sudo a2enmod rewrite
			  sudo a2dissite 000-default

			  mysql -u root -e CREATE DATABASE wordpress;
			  mysql -u root -e CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
			  mysql -u root -e GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
			  mysql -u root -e FLUSH PRIVILEGES;

			  sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php

			  systemctl restart mysql
			  systemctl restart apache2
			  SHELL
			end
			
			LÃºc Ä‘áº§u lÃ  sudo apt update vÃ  sudo apt install apache2 nhÆ°ng sau Ä‘Ã³ thÃªm -y Ä‘á»ƒ nÃ³ táº¡o Ä‘á»“ng Ã½, náº¿u
			khÃ´ng cÃ³ nÃ³ sáº½ absort rá»“i gÃ¢y ra lá»—i

		Case 10 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  # ---------- Web01 ----------
			  config.vm.define "web01" do |web01|
				web01.vm.box = "ubuntu/focal64"
				web01.vm.hostname = "web01"
				web01.vm.network "private_network", ip: "192.168.56.41"
			  end

			  # ---------- Web02 ----------
			  config.vm.define "web02" do |web02|
				web02.vm.box = "ubuntu/focal64"
				web02.vm.hostname = "web02"
				web02.vm.network "private_network", ip: "192.168.56.42"
			  end

			  # ---------- DB01 ----------
			  config.vm.define "db01" do |db01|
				db01.vm.box = "centos/7"
				db01.vm.hostname = "db01"
				db01.vm.network "private_network", ip: "192.168.56.43"
				# Provisioning script for DB01
				db01.vm.provision "shell", inline: <<-SHELL
				  yum install -y wget unzip mariadb-server -y
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL
			  end
			end

			CÃ¡i nÃ y sáº½ khá»Ÿi táº¡o nhiá»u vm 1 lÃºc
			
			CÃ³ thá»ƒ thao tÃ¡c vá»›i tá»«ng vm theo cÃ¡c lá»‡nh sau:
			
				vagrant ssh web02
				
				vagrant destroy web01
				
				vagrant destroy -f web01
				
				vagrant up web03
				
				vagrant destroy --force
				
					XÃ³a táº¥t cáº£ cÃ¡c vm
				
			Code trÃªn centos 7 cÅ© nÃªn Ä‘Ã£ bá»‹ EOL nÃªn sáº½ Ä‘Æ°á»£c thÃªm vá»›i dÃ²ng code sau Ä‘á»ƒ fix:

				db01.vm.provision "shell", inline: <<-SHELL
				  # --- Fix CentOS 7 EOL repo ---
				  sed -i 's|mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-Base.repo
				  sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-Base.repo
				  yum clean all
				  yum makecache

				  # --- Install MariaDB ---
				  yum install -y wget unzip mariadb-server
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL

	httpd:
	
		httpd trong Linux thÆ°á»ng dÃ¹ng Ä‘á»ƒ chá»‰ Apache HTTP Server â€“ má»™t trong nhá»¯ng web server phá»•
		biáº¿n vÃ  lÃ¢u Ä‘á»i nháº¥t trÃªn tháº¿ giá»›i.
		
		Giáº£i thÃ­ch

		httpd = HTTP Daemon (daemon nghÄ©a lÃ  dá»‹ch vá»¥ cháº¡y ngáº§m trÃªn há»‡ thá»‘ng).

		Khi cÃ i Apache trÃªn Linux (vÃ­ dá»¥ CentOS, RHEL, Fedora), gÃ³i dá»‹ch vá»¥ sáº½ Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  httpd.

		Nhiá»‡m vá»¥ chÃ­nh:

			Nháº­n request HTTP/HTTPS tá»« client (trÃ¬nh duyá»‡t, curl, APIâ€¦).

			Xá»­ lÃ½ vÃ  tráº£ vá» response (HTML, CSS, JS, JSON, file tÄ©nh, dá»¯ liá»‡u Ä‘á»™ng qua PHP/Python/Javaâ€¦).

		httpd cÃ³ thá»ƒ má»Ÿ rá»™ng báº±ng module (vÃ­ dá»¥: mod_ssl cho HTTPS, mod_rewrite cho rewrite URL, mod_php Ä‘á»ƒ cháº¡y PHP).
		
	Lá»‡nh Ä‘á»ƒ in ná»™i dung cá»§a file html ra mÃ n hÃ¬nh:

		[root@Finance ~]# cd /var/www/html/
		[root@Finance html]# ls
		[root@Finance html]# vim index.html
		index.html
		[root@Finance html]# systemctl restart httpd

	Kiá»ƒm tra xem cÃ³ táº¥t cáº£ cÃ¡i nÃ o Ä‘ang cháº¡y:
	
		vagrant global-status
		
		
--- Variables, JSON and YAML:

	- Commands:
	
		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ skill="DevOps"

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo $skill
		DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo skill
		skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo "I am learning $skill"
		I am learning DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo 'I am learning $skill'
		I am learning $skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ Num=123
		
	Python edittors online
	
	Json editor
	
	Yaml editor
	
--- Networking:

	- Commands:
	
		- cmd: sudo apt update
		- cmd: sudo apt install -y net-tools
		- cmd: ifconfig
		- cmd:
		
				vagrant@web01:~$ ping 192.168.56.41
				PING 192.168.56.41 (192.168.56.41) 56(84) bytes of data.
				64 bytes from 192.168.56.41: icmp_seq=1 ttl=64 time=0.021 ms
				64 bytes from 192.168.56.41: icmp_seq=2 ttl=64 time=0.037 ms
				64 bytes from 192.168.56.41: icmp_seq=3 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=4 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=5 ttl=64 time=0.042 ms

		- cmd: root@web01:~# vi /etc/hosts
		
				127.0.0.1       localhost

				# The following lines are desirable for IPv6 capable hosts
				::1     ip6-localhost   ip6-loopback
				fe00::0 ip6-localnet
				ff00::0 ip6-mcastprefix
				ff02::1 ip6-allnodes
				ff02::2 ip6-allrouters
				ff02::3 ip6-allhosts
				127.0.1.1       ubuntu-focal    ubuntu-focal

				127.0.2.1 web01 web01

				192.168.56.41 web01
				
		- cmd: ping web01
		- cmd: logout
		- cmd: $ tracert www.google.in

			   Tracing route to www.google.in [2404:6800:4003:c11::5e]
			   over a maximum of 30 hops:
			   
			     1     1 ms     9 ms     1 ms  2405:4803:d75f:ab0:5e3a:45ff:fe2e:8f78
			     2     *        *        *     Request timed out.
			     3     *        *        *     Request timed out.
			     4    73 ms     3 ms     3 ms  2405:4802:f500::37d
			     5    71 ms     *        *     2405:4800::1117:5118:a
			     6     *        *        *     Request timed out.
			     7    49 ms    48 ms    47 ms  2405:4800::101:1112:1
			     8    48 ms    45 ms    46 ms  2001:4860:0:1::5bc2
			     9    49 ms    51 ms    51 ms  2001:4860::c:4000:db82
			    10   157 ms    82 ms    82 ms  2001:4860::c:4003:1c94
			    11     *        *        *     Request timed out.
			    12     *        *        *     Request timed out.
			    13     *        *        *     Request timed out.
			    14     *        *        *     Request timed out.
			    15     *        *        *     Request timed out.
			    16     *        *        *     Request timed out.
			    17     *        *        *     Request timed out.
			    18     *        *        *     Request timed out.
			    19     *        *        *     Request timed out.
			    20    82 ms   166 ms    82 ms  se-in-f94.1e100.net [2404:6800:4003:c11::5e]
			   
			   Trace complete.
		
		- cmd: vagrant@web01:~$ netstat -antp
		
			   (Not all processes could be identified, non-owned process info
			    will not be shown, you would have to be root to see it all.)
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED -
			   tcp6       0      0 :::22                   :::*                    LISTEN      -
			   
		- cmd: root@web01:~# netstat -antp
		
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      598/systemd-resolve
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      856/sshd: /usr/sbin
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED 3245/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED 3358/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED 2250/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED 3146/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED 3492/sshd: vagrant
			   tcp6       0      0 :::22                   :::*                    LISTEN      856/sshd: /usr/sbin


		- cmd:  root@web01:~# ps -ef | grep apache2
				
				root        3602    3589  0 05:53 pts/4    00:00:00 grep --color=auto apache2
				
		- cmd:  root@web01:~# ss -tunlp
		
				Netid          State           Recv-Q          Send-Q                      Local Address:Port                   Peer Address:Port          Process
				udp            UNCONN          0               0                           127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=12))
				udp            UNCONN          0               0                        10.0.2.15%enp0s3:68                          0.0.0.0:*              users:(("systemd-network",pid=1932,fd=20))
				tcp            LISTEN          0               4096                        127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=13))
				tcp            LISTEN          0               128                               0.0.0.0:22                          0.0.0.0:*              users:(("sshd",pid=856,fd=3))
				tcp            LISTEN          0               128                                  [::]:22                             [::]:*              users:(("sshd",pid=856,fd=4))
				
		- cmd: apt install nmap -y
		- cmd: nmap
		- cmd:  root@web01:~# nmap localhost
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 05:56 UTC
				Nmap scan report for localhost (127.0.0.1)
				Host is up (0.0000030s latency).
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.44 seconds
				
		- cmd:  root@web01:~# nmap web01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 06:47 UTC
				Nmap scan report for web01 (127.0.2.1)
				Host is up (0.0000040s latency).
				Other addresses for web01 (not scanned): 192.168.56.41
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.12 seconds

		- cmd:  root@web01:~# dig www.google.com

				; <<>> DiG 9.18.30-0ubuntu0.20.04.2-Ubuntu <<>> www.google.com
				;; global options: +cmd
				;; Got answer:
				;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54039
				;; flags: qr rd ra; QUERY: 1, ANSWER: 6, AUTHORITY: 0, ADDITIONAL: 1

				;; OPT PSEUDOSECTION:
				; EDNS: version: 0, flags:; udp: 65494
				;; QUESTION SECTION:
				;www.google.com.                        IN      A

				;; ANSWER SECTION:
				www.google.com.         285     IN      A       142.251.12.99
				www.google.com.         285     IN      A       142.251.12.147
				www.google.com.         285     IN      A       142.251.12.104
				www.google.com.         285     IN      A       142.251.12.106
				www.google.com.         285     IN      A       142.251.12.105
				www.google.com.         285     IN      A       142.251.12.103

				;; Query time: 36 msec
				;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)
				;; WHEN: Sun Sep 21 06:49:44 UTC 2025
				;; MSG SIZE  rcvd: 139
				
		- cmd:  root@web01:~# nslookup www.google.com
		
				Server:         127.0.0.53
				Address:        127.0.0.53#53

				Non-authoritative answer:
				Name:   www.google.com
				Address: 142.251.12.103
				Name:   www.google.com
				Address: 142.251.12.105
				Name:   www.google.com
				Address: 142.251.12.106
				Name:   www.google.com
				Address: 142.251.12.104
				Name:   www.google.com
				Address: 142.251.12.147
				Name:   www.google.com
				Address: 142.251.12.99
				Name:   www.google.com
				Address: 2404:6800:4003:c11::63
				Name:   www.google.com
				Address: 2404:6800:4003:c11::67
				Name:   www.google.com
				Address: 2404:6800:4003:c11::69
				Name:   www.google.com
				Address: 2404:6800:4003:c11::68
				
		- cmd:  root@web01:~# route -n
		
				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				10.0.2.2        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# route

				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				default         _gateway        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				_gateway        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# arp
		
				Address                  HWtype  HWaddress           Flags Mask            Iface
				_gateway                 ether   52:54:00:12:35:02   C                     enp0s3
				10.0.2.3                 ether   52:54:00:12:35:03   C                     enp0s3
				192.168.56.42                    (incomplete)                              enp0s8
				
		- cmd: mtr
		- cmd: mtr www.google.com
		
		- cmd:  root@web01:~# nmap db01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 07:06 UTC
				Failed to resolve "db01".
				WARNING: No targets were specified, so 0 hosts scanned.
				Nmap done: 0 IP addresses (0 hosts up) scanned in 0.06 seconds
				
		- cmd:  root@web01:~# telnet 192.168.56.43 3306
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				HHost '192.168.56.41' is not allowed to connect to this MariaDB serverConnection closed by foreign host.
				
		- cmd: sudo yum install -y net-tools
		
		- cmd:  root@web01:~# telnet 192.168.56.43 22
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				SSH-2.0-OpenSSH_7.4



	OSI model
	
	Switch
	
	Router
	
	Firewall
	
	Gateway IP
	
	Wireless Access Point
	
	Port
	
	TCP, UDP
	
--- Introducing Containers:

	- Commands:
	
		- cmd: sudo install -m 0755 -d /etc/apt/keyrings
		- cmd: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
		- cmd: echo \
			  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
		- cmd: sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
		
		- cmd:  root@ubuntu-focal:~# systemctl status docker
		
				â— docker.service - Docker Application Container Engine
					 Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
					 Active: active (running) since Sun 2025-09-21 07:47:33 UTC; 1min 12s ago
				TriggeredBy: â— docker.socket
					   Docs: https://docs.docker.com
				   Main PID: 3868 (dockerd)
					  Tasks: 9
					 Memory: 21.3M
					 CGroup: /system.slice/docker.service
							 â””â”€3868 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

				Sep 21 07:47:33 ubuntu-focal systemd[1]: Started Docker Application Container Engine.
				
		- cmd: docker run hello-world
		- cmd:  root@ubuntu-focal:~# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
				hello-world   latest    1b44b5a3e06a   6 weeks ago   10.1kB
				
		- cmd:  root@ubuntu-focal:~# docker ps
		
				CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
				
		- cmd:  root@ubuntu-focal:~# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND    CREATED              STATUS                          PORTS     NAMES
				00001e808420   hello-world   "/hello"   About a minute ago   Exited (0) About a minute ago             cool_agnesi
				
		- cmd: docker run --name web01 -d -p 9080:80 nginx
		- cmd: docker inspect web01
		- cmd:  root@ubuntu-focal:~# curl http://172.17.0.2:80
		
				<!DOCTYPE html>
				<html>
				<head>
				<title>Welcome to nginx!</title>
				<style>
				html { color-scheme: light dark; }
				body { width: 35em; margin: 0 auto;
				font-family: Tahoma, Verdana, Arial, sans-serif; }
				</style>
				</head>
				<body>
				<h1>Welcome to nginx!</h1>
				<p>If you see this page, the nginx web server is successfully installed and
				working. Further configuration is required.</p>

				<p>For online documentation and support please refer to
				<a href="http://nginx.org/">nginx.org</a>.<br/>
				Commercial support is available at
				<a href="http://nginx.com/">nginx.com</a>.</p>

				<p><em>Thank you for using nginx.</em></p>
				</body>
				</html>
				
		- cmd: docker build -t tesimg .
		- cmd: docker run -d -P tesimg
		- cmd:  root@ubuntu-focal:~/images# docker ps
		
				CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                       NAMES
				f41b4289fc35   tesimg    "/usr/sbin/apache2ctâ€¦"   2 minutes ago    Up 2 minutes    0.0.0.0:32768->80/tcp, [::]:32768->80/tcp   elegant_moore
				44fde054c460   nginx     "/docker-entrypoint.â€¦"   25 minutes ago   Up 25 minutes   0.0.0.0:9080->80/tcp, [::]:9080->80/tcp     web01
				
				Con sá»‘ 32768 trong output docker ps nÃ y chÃ­nh lÃ  cá»•ng ngáº«u nhiÃªn trÃªn mÃ¡y host mÃ  Docker Ã¡nh
				xáº¡ Ä‘áº¿n cá»•ng 80 trong container.

		- cmd: docker stop web01 elegant_moore
		
		- cmd:  root@ubuntu-focal:~/images# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS                        PORTS     NAMES
				f41b4289fc35   tesimg        "/usr/sbin/apache2ctâ€¦"   6 minutes ago    Exited (137) 28 seconds ago             elegant_moore
				44fde054c460   nginx         "/docker-entrypoint.â€¦"   29 minutes ago   Exited (0) 38 seconds ago               web01
				00001e808420   hello-world   "/hello"                 32 minutes ago   Exited (0) 32 minutes ago               cool_agnesi
				
		- cmd: docker rm elegant_moore web01 cool_agnesi
		
		- cmd:  root@ubuntu-focal:~/images# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED          SIZE
				tesimg        latest    c197410a4d04   13 minutes ago   271MB
				nginx         latest    41f689c20910   5 weeks ago      192MB
				hello-world   latest    1b44b5a3e06a   6 weeks ago      10.1kB
				
		- cmd: docker rmi c197410a4d04 41f689c20910 1b44b5a3e06a
		- cmd: wget https://raw.githubusercontent.com/devopshydclub/vprofile-project/refs/heads/docker/compose/docker-compose.yml
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose down
		- cmd: docker system prune -a
		- cmd: git clone https://github.com/devopshydclub/emartapp.git
		
		
		
		
		
	Khi báº¡n copyâ€“paste vÃ o vim thÃ¬ lá»—i format YAML thÆ°á»ng xáº£y ra do:

		DÃ­nh tab thay vÃ¬ space.

		Paste vÃ o vim á»Ÿ cháº¿ Ä‘á»™ autoindent gÃ¢y lá»‡ch dÃ²ng.

		Hoáº·c paste tá»« Windows â†’ Linux thÃ¬ dÃ­nh kÃ½ tá»± áº©n (CRLF).

		Kháº¯c phá»¥c:
		
			TrÆ°á»›c khi paste, gÃµ: :set paste
			
	Docker compose lÃ  Ä‘á»ƒ run cÃ¡c image chá»© khÃ´ng pháº£i Ä‘á»ƒ build cÃ¡c image
				

		



		

	Ná»™i dung file Vagrantfile khi má»Ÿ má»™t sá»‘ comment:
	
		Case 1:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
			   sudo apt-get update
			   sudo apt-get install \
				ca-certificates \
				curl \
				gnupg -y

			   sudo install -m 0755 -d /etc/apt/keyrings
			   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
			   sudo chmod a+r /etc/apt/keyrings/docker.gpg
			   echo \
			     "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			     "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			     sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
			   sudo apt-get update
			   sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
			  SHELL
			end
			
		Case 2:
		
			FROM ubuntu:latest AS BUILD_IMAGE
			RUN apt update && apt install wget unzip -y
			RUN wget https://www.tooplate.com/zip-templates/2128_tween_agency.zip
			RUN unzip 2128_tween_agency.zip && cd 2128_tween_agency && tar -czf tween.tgz * && mv tween.tgz /root/tween.tgz

			FROM ubuntu:latest
			LABEL "project"="Marketing"
			ENV DEBIAN_FRONTEND=noninteractive

			RUN apt update && apt install apache2 git wget -y
			COPY --from=BUILD_IMAGE /root/tween.tgz /var/www/html/
			RUN cd /var/www/html/ && tar xzf tween.tgz

			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			VOLUME /var/log/apache2
			WORKDIR /var/www/html
			EXPOSE 80

		Case 3:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
				sudo apt-get update
				sudo apt-get install \
					ca-certificates \
					curl \
					gnupg -y

				sudo install -m 0755 -d /etc/apt/keyrings
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
				sudo chmod a+r /etc/apt/keyrings/docker.gpg
				echo \
				  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
				  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
				sudo curl -L "https://github.com/docker/compose/releases/download/v2.1.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

				chmod +x /usr/local/bin/docker-compose
			   SHELL
			end
			
	Ná»™i dung file docker-compose.yml:
	
		Case 1:
		
			version: '3.8'
			services:
			  vprodb:
				image: vprocontainers/vprofiledb
				ports:
				  - "3306:3306"
				volumes:
				  - vprodbdata:/var/lib/mysql
				environment:
				  - MYSQL_ROOT_PASSWORD=vprodbpass

			  vprocache01:
				image: memcached
				ports:
				  - "11211:11211"

			  vpromq01:
				image: rabbitmq
				ports:
				  - "15672:15672"
				environment:
				  - RABBITMQ_DEFAULT_USER=guest
				  - RABBITMQ_DEFAULT_PASS=guest

			  vproapp:
				image: vprocontainers/vprofileapp
				ports:
				  - "8080:8080"
				volumes: 
				  - vproappdata:/usr/local/tomcat/webapps

			  vproweb:
				image: vprocontainers/vprofileweb
				ports:
				  - "80:80"
			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books
		  
--- Bash Scripting:

	- Commands:
		
		- cmd: vim /etc/hostname
		
			File nÃ y Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tÃªn hostname
			
		- cmd: hostname scriptbox
		- cmd: hostname
		- cmd: yum install vim -y
		- cmd: chmod +x firstscript.sh
		- cmd: chmod +x dismantle.sh
		- cmd: w
		- cmd: who
		- cmd: free -m | grep Mem
		- cmd: free -m | grep Mem | awk '{print $4}'
		- cmd: source .bashrc
		- cmd: ip addr show | grep -v LOOPBACK | grep -ic mtu
		- cmd: cat /var/run/httpd/httpd.pid
		- cmd: ssh-copy-id vagrant@192.168.10.13
		- cmd: ssh vagrant@web01
		- cmd: passwd devops
		- cmd: vim /etc/ssh/sshd_config
		- cmd: ssh devops@web01 uptime
		- cmd: ssh-keygen
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i .ssh/id_rsa devops@web01 uptime
		- cmd: cat .ssh/id_rsa
		- cmd: cat .ssh/id_rsa.pub
		- cmd: for host in `cat remhosts`; do echo $host;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host hostname;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host uptime;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host sudo yum install git -y;done
		- cmd: echo "testfile" >testfile.txt
		- cmd: scp testfile.txt devops@web01:/tmp/
		- cmd: scp testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/home/devops/
		
		
		
		
	Varantfile:

		Vagrant.configure("2") do |config|

		  config.vm.define "scriptbox" do |scriptbox|
			scriptbox.vm.box = "eurolinux-vagrant/centos-stream-9"
				scriptbox.vm.network "private_network", ip: "192.168.10.12"
				scriptbox.vm.provider "virtualbox" do |vb|
			 vb.memory = "1024"
		   end
		  end

		  config.vm.define "web01" do |web01|
			web01.vm.box = "eurolinux-vagrant/centos-stream-9"
				web01.vm.network "private_network", ip: "192.168.10.13"
		  end

		  config.vm.define "web02" do |web02|
			web02.vm.box = "eurolinux-vagrant/centos-stream-9"
				web02.vm.network "private_network", ip: "192.168.10.14"
		  end

		   config.vm.define "web03" do |web03|
			web03.vm.box = "ubuntu/bionic64"
				web03.vm.network "private_network", ip: "192.168.10.15"
		  end
		end
	
		
		
		
	Ná»™i dung file script:
	
		Case 1:
		
			#!/bin/bash

			echo "Welcome to bash script."
			echo

			echo "The uptime of the system is: "
			uptime

			echo "Memory Utilization"
			free -m

			echo "Disk Utilization"
			df -h
			
		Case 2:
		
			#!/bin/bash

			### This script prints system info ###

			echo "Welcome to bash script."
			echo

			#checking system uptime
			echo "#################################"
			echo "The uptime of the system is: "
			uptime

			# Memory Utilization
			echo "#################################"
			echo "Memory Utilization"
			free -m

			# Disk Utilization
			echo "#################################"
			echo "Disk Utilization"
			df -h
			
		Case 3:
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip
			unzip 2098_health.zip
			sudo cp -r 2098_health.zip/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo
			
		Case 4:
		
			CÃ¡i nÃ y cÃ³ thá»ƒ cháº¡y lÃªn trang web
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip > /dev/null
			unzip 2098_health.zip > /dev/null
			sudo cp -r 2098_health/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo

			sudo systemctl status httpd
			ls /var/www/html/
			
		Case 5:
		
			[root@scriptbox ~]# SKILL="DevOps"
			[root@scriptbox ~]# echo $SKILL
			DevOps
			[root@scriptbox ~]# echo SKILL
			SKILL
			[root@scriptbox ~]# PACKAGE="httpd wget unzip"
			[root@scriptbox ~]# yum install $PACKAGE -y
			Last metadata expiration check: 0:12:59 ago on Sun 21 Sep 2025 11:19:55 AM UTC.
			Package httpd-2.4.62-7.el9.x86_64 is already installed.
			Package wget-1.21.1-8.el9.x86_64 is already installed.
			Package unzip-6.0-59.el9.x86_64 is already installed.
			Dependencies resolved.
			Nothing to do.
			Complete!
			
		Case 6:
		
			[root@scriptbox scripts]# ls
			firstscript.sh  websetup.sh
			[root@scriptbox scripts]# mv firstscript.sh 1_firstscript.sh
			[root@scriptbox scripts]# mv websetup.sh 2_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh
			[root@scriptbox scripts]# cp 2_websetup.sh 3_vars_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh
		
		Case 7:
		
			#!/bin/bash
			ART_NAME="2098_health"
			SVC="httpd"
			URL="https://www.tooplate.com/zip-templates/2098_health.zip"
			ART_NAME="2098_health"
			TEMPDIR="/tmp/webfiles"

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install $PACKAGE -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start $SVC
			sudo systemctl enable $SVC
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p $TEMPDIR
			cd $TEMPDIR
			echo

			wget $URL > /dev/null
			unzip $ART_NAME.zip > /dev/null
			sudo cp -r $ART_NAME/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart $SVC
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf $TEMPDIR
			echo

			sudo systemctl status $SVC
			ls /var/www/html/
			
		Case 8:
		
			#!/bin/bash
			sudo systemctl stop httpd
			sudo rm -rf /var/www/html/*
			sudo yum remove httpd wget unzip -y
			
		Case 9:
		
			#!/bin/bash

			echo "Value of 0 is "
			echo $0

			echo "Value of 1"
			echo $1

			echo "Value of 2"
			echo $2

			echo "Value of 3"
			echo $3
			
				[root@scriptbox scripts]# ./4_args.sh Linux AWS Ansible Jenkins
				Value of 0 is
				./4_args.sh
				Value of 1
				Linux
				Value of 2
				AWS
				Value of 3
				Ansible
				
		Case 10:
		
			[root@scriptbox scripts]# echo $USER
			root
			[root@scriptbox scripts]# echo $HOSTNAME
			scriptbox
			[root@scriptbox scripts]# echo $RANDOM
			11889
			[root@scriptbox scripts]# SKILL="DevOps"
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# SKILL='DevOps'
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# echo "Ihave got $SKILL skill."
			Ihave got DevOps skill.
			[root@scriptbox scripts]# echo 'Ihave got $SKILL skill.'
			Ihave got $SKILL skill.
			[root@scriptbox scripts]# echo "Ihave got \$SKILL skill."
			Ihave got $SKILL skill.

			[root@scriptbox scripts]# uptime
			 13:25:12 up  3:03,  1 user,  load average: 0.02, 0.04, 0.03
			[root@scriptbox scripts]# UP="uptime"
			[root@scriptbox scripts]# echo $UP
			uptime
			[root@scriptbox scripts]# UP=`uptime`
			[root@scriptbox scripts]# echo $UP
			13:25:42 up 3:03, 1 user, load average: 0.01, 0.03, 0.03
			
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-21 10:44 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         330         155           3         414         436
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         330         155           3         414         436
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			155
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 155 mb.
			
		Case 11:
		
			#!/bin/bash

			echo "Welcome $USER on $HOSTNAME."
			echo "##############################################"

			FREERAM=$(free -m | grep Mem | awk '{print $4}')
			LOAD=`uptime | awk '{print $9}'`
			ROOTFREE=$(df -h | grep '/dev/sdal' | awk '{print $4}')

			echo "##############################################"
			echo "Available free RAM is $FREERAM MB"
			echo "##############################################"
			echo "Current Load Average $LOAD"
			echo "##############################################"
			echo "Free ROOT partition size is $ROOTFREE"
			
		Case 12:
		
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo CURRENT_USER
			CURRENT_USER
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-22 13:05 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         282         383           3         229         485
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         282         383           3         229         485
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			382
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 382 mb.
			[root@scriptbox scripts]# ./6_command_subs.sh
			Welcome root on scriptbox.
			##############################################
			##############################################
			Available free RAM is 382 MB
			##############################################
			Current Load Average 0.04,
			##############################################
			Free ROOT partition size is
			
		Case 13:
		
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo SEASON
			SEASON
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON

			[root@scriptbox ~]# cd /opt/scripts/

		Case 14:
		
			#!/bin/bash
			echo "The $SEASON season is more than expected, this time."
			
			[root@scriptbox scripts]# chmod +x testvars.sh
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# ./testvars.sh
			The  season is more than expected, this time.
			[root@scriptbox scripts]# export SEASON
			[root@scriptbox scripts]# ./testvars.sh
			The Monsoon season is more than expected, this time.
			
		Case 15:
		
			# .bashrc
			# User specific aliases and functions
			alias rm='rm -i'
			alias cp='cp -i'
			alias mv='mv -i'
			# Source global definitions
			if [ -f /etc/bashrc ]; then
					. /etc/bashrc
			fi
			export SEASON="Monsoon"


			[root@scriptbox ~]# echo $SEASON
			[root@scriptbox ~]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# vi .bashrc
			[root@scriptbox ~]# sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
		
		Case 16:
			
			#!/bin/bash
			export SEASON='Winter'
			
			[vagrant@scriptbox ~]$ echo $SEASON
			Winter
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
			
		Case 17:
		
			#!/bin/bash
			echo "Enter youe skills:"
			read SKILL
			echo "Your $SKILL skill is in high Demand in the IT Industry."
			read -p 'Username: ' USR
			read -sp 'Password: ' pass
			echo
			echo "Login Successfull: Welcome USER $USR,"

			[root@scriptbox scripts]# chmod +x 7_userInput.sh
			[root@scriptbox scripts]# ./7_userInput.sh
			Enter youe skills:
			CloudComputing
			Your CloudComputing skill is in high Demand in the IT Industry.
			Username: ngoctuanqng1
			Password:
			Login Successfull: Welcome USER ngoctuanqng1,

		Case 18:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# chmod +x 8if1.sh
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 120
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:40:30 PM UTC 2025
			Script execution completed successfully.
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 50
			Script execution completed successfully.
		
		Case 19:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			else
					echo "You have entered number less than 100."
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 60
			You have entered number less than 100.
			Script execution completed successfully.
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 110
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:43:41 PM UTC 2025
			Script execution completed successfully.
			
		Case 20:
		
			#!/bin/bash
			value=$(ip addr show | grep -v LOOPBACK | grep -ic mtu)
			if [ $value -eq 1 ]
			then
					echo "1 Active Network Interface found."
			elif [ $value -gt 1 ]
			then
					echo "Found Multiple active Interface."
			else
					echo "No Active interface found."
			fi
			
			[root@scriptbox ~]# ./9_ifelif.sh
			Found Multiple active Interface.
			[root@scriptbox ~]# ls
			9_ifelif.sh  anaconda-ks.cfg  original-ks.cfg
			[root@scriptbox ~]# ls /opt/scripts/
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  dismantle.sh  testvars.sh
			[root@scriptbox ~]# mv ./9_ifelif.sh /opt/scripts/
			[root@scriptbox ~]# cd /opt/scripts/
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  9_ifelif.sh  dismantle.sh  testvars.sh
			
		Case 21:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null
			if [ $? -eq 0 ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
			[root@scriptbox scripts]# ./11_monit.sh
			#######################################################
			Mon Sep 22 04:12:59 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 22:
		
			#!/bin/bash
			# mm hh dom MM DOW COMMAND
			# mm hh dom MM DOW COMMAND
			# 30 20 * * 1-5 COMMAND
			* * * * * /opt/scripts/11_monit.sh &>> /var/log/monit_httpd.log

			[root@scriptbox scripts]# crontab -e
			no crontab for root - using an empty one
			crontab: installing new crontab
			[root@scriptbox scripts]# systemctl stop httpd
			[root@scriptbox scripts]# ls /var/log/monit_httpd.log
			/var/log/monit_httpd.log
			[root@scriptbox scripts]# cat /var/log/monit_httpd.log
			#######################################################
			Mon Sep 22 04:20:08 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################

			#######################################################
			Mon Sep 22 04:21:03 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 23:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null

			if [ -f /var/run/httpd/httpd.pid ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
		Case 24:
		
			#!/bin/bash
			for VAR1 in java .net python ruby php
			do
					echo "Looping...."
					sleep 1
					echo "#####################################"
					echo "Value of VAR1 is $VAR1."
					echo "#####################################"
					date
			done
			
			[root@scriptbox scripts]# chmod +x 13_for.sh
			[root@scriptbox scripts]# ./13_for.sh
			Looping....
			#####################################
			Value of VAR1 is java.
			#####################################
			Mon Sep 22 04:28:16 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is .net.
			#####################################
			Mon Sep 22 04:28:18 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is python.
			#####################################
			Mon Sep 22 04:28:19 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is ruby.
			#####################################
			Mon Sep 22 04:28:20 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is php.
			#####################################
			Mon Sep 22 04:28:21 PM UTC 2025
			
		Case 25:
		
			#!/bin/bash
			MYUSERS="alpha beta gamma"
			for usr in $MYUSERS
			do
					echo "Adding user $usr."
					useradd $usr
					id $usr
					echo "##############################"
			done
			
			[root@scriptbox scripts]# chmod +x 14_for.sh
			[root@scriptbox scripts]# ./14_for.sh
			Adding user alpha.
			uid=1001(alpha) gid=1001(alpha) groups=1001(alpha)
			##############################
			Adding user beta.
			uid=1002(beta) gid=1002(beta) groups=1002(beta)
			##############################
			Adding user gamma.
			uid=1003(gamma) gid=1003(gamma) groups=1003(gamma)
			##############################
	
		Case 26:
		
			#!/bin/bash
			counter=0
			while [ $counter -lt 5 ]
			do
					echo "Looping...."
					echo "Value of counter is $counter."
					counter=$(( $counter + 1))
					sleep 1
			done
			echo "Out of the loop"
			
			[root@scriptbox ~]# chmod +x 15_while.sh
			[root@scriptbox ~]# ./15_while.sh
			Looping....
			Value of counter is 0.
			Looping....
			Value of counter is 1.
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 3.
			Looping....
			Value of counter is 4.
			Out of the loop
			
		Case 27:
		
            #!/bin/bash
            counter=2
            while true
            do
                echo "Looping...."
                echo "Value of counter is $counter."
                counter=$(( $counter * 2))
                sleep 1
            done
            echo "Out of the loop"
			
			[root@scriptbox ~]# ./16_while.sh
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 4.
			Looping....
			Value of counter is 8.
			Looping....
			Value of counter is 16.
			Looping....
			Value of counter is 32.
			Looping....
			Value of counter is 64.
			Looping....
			Value of counter is 128.
			...
			
		Case 28:
		
			127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
			::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
			192.168.10.13 web01
			192.168.10.14 web02
			192.168.10.15 web03
			
			[root@localhost ~]# ping web01
			PING web01 (192.168.10.13) 56(84) bytes of data.
			64 bytes from web01 (192.168.10.13): icmp_seq=1 ttl=64 time=3.83 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=2 ttl=64 time=2.57 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=3 ttl=64 time=1.44 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=4 ttl=64 time=3.46 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=5 ttl=64 time=2.18 ms

		Case 29:
		
			web01: visudo
		
				## Allow root to run any commands anywhere
				root    ALL=(ALL)       ALL
				devops ALL=(ALL)        NOPASSWD: ALL
				
			web03: /etc/ssh/sshd_config

				PasswordAuthentication yes
				ChallengeResponseAuthentication no
			
		Case 30:
		
			multios_websetup.sh:
		
				#!/bin/bash

				# Variable Declaration
				#PACKAGE="httpd wget unzip"
				#SVC="httpd"
				URL='https://www.tooplate.com/zip-templates/2098_health.zip'
				ART_NAME='2098_health'
				TEMPDIR="/tmp/webfiles"

				yum --help &> /dev/null

				if [ $? -eq 0 ]
				then
				   # Set Variables for CentOS
				   PACKAGE="httpd wget unzip"
				   SVC="httpd"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo yum install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/

				else
					# Set Variables for Ubuntu
				   PACKAGE="apache2 wget unzip"
				   SVC="apache2"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo apt update
				   sudo apt install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/
				fi 

		Case 31:
		
			#!/bin/bash
			USR='devops'
			for host in `cat remhosts`
			do
					echo
					echo "#######################################"
					echo "Connecting to $host"
					echo "Pushing Script to $host"
					scp multios_websetup.sh $USR@$host:/tmp/
					echo "Executing Script on $host"
					ssh $USR@$host sudo /tmp/multios_websetup.sh
					ssh $USR@$host sudo rm -rf /tmp/multios_websetup.sh
					echo "#######################################"
					echo
			done

--- AWS Part-1:

	Setting EC2
	
	Khi táº¡o inbound rules trong security group chÃº Ã½ Ä‘áº¿n port, pháº£i Ä‘Ãºng port má»›i cháº¡y ra Ä‘Æ°á»£c trang web
	
	ChÃº Ã½ khi truy cáº­p ec2 vá»›i ssh key pair thÃ¬ cáº§n import Ä‘Ãºng file key pair, náº¿u khÃ´ng sáº½ bá»‹ denied
	
	My IP mÃ  báº¡n thÆ°á»ng tháº¥y khi cáº¥u hÃ¬nh Inbound rule trong Security Group cá»§a AWS nghÄ©a lÃ :

		AWS tá»± Ä‘á»™ng láº¥y Ä‘á»‹a chá»‰ IP public hiá»‡n táº¡i cá»§a mÃ¡y tÃ­nh báº¡n Ä‘ang dÃ¹ng
		(mÃ¡y laptop/PC/Ä‘iá»‡n thoáº¡i Ä‘ang truy cáº­p vÃ o AWS Console).

		Khi báº¡n chá»n My IP, rule sáº½ chá»‰ cho phÃ©p Ä‘á»‹a chá»‰ IP Ä‘Ã³ Ä‘Æ°á»£c truy cáº­p vÃ o EC2 qua cá»•ng
		báº¡n chá»n (vÃ­ dá»¥ SSH port 22, hoáº·c HTTP port 80).

	ChÆ°a cháº¡y Ä‘Æ°á»£c mkfs

		Tuy nhiÃªn láº¡i cháº¡y Ä‘Æ°á»£c lá»‡nh mkfs.t4 /dev/nvme1n1p1
	
	
	
	
	
	
	
	
	

	AWS Part-1 - Command:
	
		- cmd: ssh -i "web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ssh -i "E:/devops learning download/AWS_Part-1/web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ss -tunlp | grep 80
		- cmd: curl http://localhost
		- cmd: apt update && apt install apache2 -y
		- cmd: apt install unzip -y
		- cmd: systemctl restart apache2
		- cmd: ps -ef | grep apache2
		- cmd: ss -tunlp | grep 2668
		- cmd: choco install awscli -y
		- cmd: aws --version
		- cmd: aws configure
		- cmd: ls ~/.aws/
		- cmd: cat ~/.aws/config
		- cmd: cat ~/.aws/credentials
		- cmd: aws sts get-caller-identity
		- cmd: aws ec2 describe-instances

		- cmd: sudo amazon-linux-extras install epel -y
		- cmd: sudo yum install stress -y
		- cmd: nohup stress -c 4 -t 300 &
		- cmd: top
		
			Lá»‡nh top trong Linux lÃ  má»™t cÃ´ng cá»¥ theo dÃµi tiáº¿n trÃ¬nh (process monitoring) cháº¡y trong real-time.		
		
			CÃ´ng dá»¥ng chÃ­nh

				Hiá»ƒn thá»‹ tÃ¬nh tráº¡ng há»‡ thá»‘ng (CPU, RAM, uptime, load average).

				Liá»‡t kÃª danh sÃ¡ch tiáº¿n trÃ¬nh (process) Ä‘ang cháº¡y, sáº¯p xáº¿p theo má»©c Ä‘á»™ tiÃªu thá»¥ tÃ i nguyÃªn.

				GiÃºp báº¡n nhanh chÃ³ng biáº¿t tiáº¿n trÃ¬nh nÃ o Ä‘ang chiáº¿m CPU hoáº·c RAM nhiá»u nháº¥t.

			Má»™t sá»‘ phÃ­m táº¯t há»¯u Ã­ch trong top

				P â†’ sáº¯p xáº¿p theo CPU usage.

				M â†’ sáº¯p xáº¿p theo RAM usage.

				k â†’ kill process (nháº­p PID).

				q â†’ thoÃ¡t khá»i top.
				
			top = Task Manager trong Linux, giÃºp báº¡n giÃ¡m sÃ¡t CPU, RAM, vÃ  tiáº¿n trÃ¬nh theo thá»i gian thá»±c.
		
		- cmd: fdisk -l	
		
			Lá»‡nh fdisk -l trong Linux Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ liá»‡t kÃª (list) táº¥t cáº£ cÃ¡c phÃ¢n vÃ¹ng (partitions)
			vÃ  á»• Ä‘Ä©a hiá»‡n cÃ³ trÃªn há»‡ thá»‘ng.
			
			fdisk: cÃ´ng cá»¥ quáº£n lÃ½ phÃ¢n vÃ¹ng trÃªn Linux (táº¡o, xÃ³a, thay Ä‘á»•i partition).

			-l (list): hiá»ƒn thá»‹ danh sÃ¡ch cÃ¡c thiáº¿t bá»‹ lÆ°u trá»¯ vÃ  thÃ´ng tin chi tiáº¿t.
		
		- cmd: df -h
		
			Lá»‡nh df -h trong Linux dÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ dung lÆ°á»£ng á»• Ä‘Ä©a vÃ  cÃ¡c phÃ¢n vÃ¹ng Ä‘ang Ä‘Æ°á»£c
			mount theo cÃ¡ch dá»… Ä‘á»c (human-readable).
			
			df = disk filesystem (thÃ´ng tin há»‡ thá»‘ng tá»‡p).

			-h = human readable â†’ hiá»ƒn thá»‹ dung lÆ°á»£ng báº±ng Ä‘Æ¡n vá»‹ KB, MB, GB, TB thay vÃ¬ block.	

			KhÃ¡c vá»›i fdisk -l:

				df -h â†’ xem dung lÆ°á»£ng Ä‘ang sá»­ dá»¥ng cá»§a phÃ¢n vÃ¹ng Ä‘Ã£ mount.

				fdisk -l â†’ xem cáº¥u trÃºc phÃ¢n vÃ¹ng cá»§a á»• Ä‘Ä©a (dÃ¹ chÆ°a mount).			
		
		- cmd: mount /dev/nvme1n1p1 /var/www/html/images
		
			Ã nghÄ©a:

				mount: gáº¯n (mount) má»™t phÃ¢n vÃ¹ng/á»• Ä‘Ä©a vÃ o má»™t thÆ° má»¥c trÃªn há»‡ thá»‘ng.

				/dev/nvme1n1p1: thiáº¿t bá»‹ váº­t lÃ½/phÃ¢n vÃ¹ng (á»Ÿ Ä‘Ã¢y lÃ  á»• NVMe, partition sá»‘ 1).

				/var/www/html/images: thÆ° má»¥c mount point (nÆ¡i ná»™i dung cá»§a phÃ¢n vÃ¹ng sáº½ xuáº¥t hiá»‡n).
				
			Quy trÃ¬nh hoáº¡t Ä‘á»™ng:

				Sau khi cháº¡y lá»‡nh, toÃ n bá»™ ná»™i dung gá»‘c trong /var/www/html/images (náº¿u cÃ³) sáº½ bá»‹ áº©n
				Ä‘i (khÃ´ng máº¥t, chá»‰ bá»‹ che khuáº¥t) vÃ  Ä‘Æ°á»£c thay tháº¿ bá»Ÿi ná»™i dung cá»§a phÃ¢n vÃ¹ng /dev/nvme1n1p1.

				Khi unmount (umount /var/www/html/images), dá»¯ liá»‡u gá»‘c trong thÆ° má»¥c láº¡i hiá»‡n ra.
				
			CÃ¡c lÆ°u Ã½:
			
				ThÆ° má»¥c mount point pháº£i tá»“n táº¡i trÆ°á»›c

				Thiáº¿t bá»‹ pháº£i cÃ³ há»‡ thá»‘ng tá»‡p (filesystem) há»£p lá»‡ (ext4, xfs, vfat...).

				Náº¿u muá»‘n mount tá»± Ä‘á»™ng khi khá»Ÿi Ä‘á»™ng láº¡i, cáº§n chá»‰nh file /etc/fstab.
				
		- cmd: mount -a
		
			Ã nghÄ©a:

				mount: gáº¯n (mount) phÃ¢n vÃ¹ng/thiáº¿t bá»‹ vÃ o há»‡ thá»‘ng.

				-a (all): mount táº¥t cáº£ cÃ¡c má»¥c Ä‘Æ°á»£c khai bÃ¡o trong file cáº¥u
				hÃ¬nh /etc/fstab (ngoáº¡i trá»« nhá»¯ng dÃ²ng cÃ³ option noauto).
				
			Khi nÃ o dÃ¹ng:

				Sau khi báº¡n chá»‰nh sá»­a file /etc/fstab Ä‘á»ƒ thÃªm má»›i phÃ¢n vÃ¹ng cáº§n mount tá»±
				Ä‘á»™ng khi khá»Ÿi Ä‘á»™ng láº¡i.

				Thay vÃ¬ pháº£i reboot, báº¡n cÃ³ thá»ƒ cháº¡y mount -a Ä‘á»ƒ kiá»ƒm tra ngay.

		- cmd: umount /var/www/html/images
		
			Ã nghÄ©a:

				umount (khÃ´ng cÃ³ chá»¯ n á»Ÿ Ä‘áº§u) dÃ¹ng Ä‘á»ƒ thÃ¡o (unmount) má»™t phÃ¢n vÃ¹ng khá»i há»‡ thá»‘ng.

				/var/www/html/images lÃ  mount point mÃ  trÆ°á»›c Ä‘Ã³ báº¡n Ä‘Ã£ mount thiáº¿t bá»‹ vÃ o.
				
			Káº¿t quáº£ khi cháº¡y:

				Sau khi umount, dá»¯ liá»‡u trÃªn phÃ¢n vÃ¹ng /dev/nvme1n1p1 sáº½ khÃ´ng cÃ²n xuáº¥t
				hiá»‡n á»Ÿ /var/www/html/images.

				Ná»™i dung gá»‘c cá»§a thÆ° má»¥c /var/www/html/images (náº¿u cÃ³ trÆ°á»›c Ä‘Ã³) sáº½ hiá»‡n ra láº¡i.
		
		- cmd: vi /etc/fstab
		- cmd: systemctl daemon-reload
		
		
		- cmd: fdisk /dev/nvme1n1
		
			ThÆ° má»¥c sáº½ Ä‘Æ°á»£c láº¥y theo lá»‡nh giÃ¡ trá»‹ cá»§a Disk theo lá»‡nh sau:
			
				[ec2-user@ip-172-31-16-206 ~]$ fdisk -l
				fdisk: cannot open /dev/nvme0n1: Permission denied
				fdisk: cannot open /dev/nvme1n1: Permission denied
				[ec2-user@ip-172-31-16-206 ~]$ sudo -i
				[root@ip-172-31-16-206 ~]# fdisk -l
				Disk /dev/nvme0n1: 8 GiB, 8589934592 bytes, 16777216 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				Disklabel type: gpt
				Disk identifier: 7129DF59-1D55-477D-A6E9-19E29F26FE4F

				Device           Start      End  Sectors  Size Type
				/dev/nvme0n1p1    2048     6143     4096    2M BIOS boot
				/dev/nvme0n1p2    6144  1030143  1024000  500M EFI System
				/dev/nvme0n1p3 1030144  3078143  2048000 1000M Linux extended boot
				/dev/nvme0n1p4 3078144 16777182 13699039  6.5G Linux root (x86-64)


				Disk /dev/nvme1n1: 5 GiB, 5368709120 bytes, 10485760 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				
			Ã nghÄ©a

				fdisk: cÃ´ng cá»¥ quáº£n lÃ½ phÃ¢n vÃ¹ng (partition) trÃªn á»• Ä‘Ä©a.

				/dev/nvme1n1: thiáº¿t bá»‹ NVMe (á»• Ä‘Ä©a váº­t lÃ½, chÆ°a chá»n phÃ¢n vÃ¹ng cá»¥ thá»ƒ).

				Khi cháº¡y lá»‡nh nÃ y, báº¡n sáº½ má»Ÿ trÃ¬nh tÆ°Æ¡ng tÃ¡c (interactive mode) Ä‘á»ƒ thao tÃ¡c vá»›i báº£ng phÃ¢n
				vÃ¹ng cá»§a á»• Ä‘Ä©a Ä‘Ã³.
				
			Lá»‡nh con	Chá»©c nÄƒng
			m			Hiá»ƒn thá»‹ menu trá»£ giÃºp (help).
			p			In (print) báº£ng phÃ¢n vÃ¹ng hiá»‡n táº¡i.
			n			Táº¡o (new) phÃ¢n vÃ¹ng má»›i.
			d			XÃ³a (delete) má»™t phÃ¢n vÃ¹ng.
			t			Thay Ä‘á»•i loáº¡i phÃ¢n vÃ¹ng (partition type).
			w			LÆ°u thay Ä‘á»•i (write) xuá»‘ng Ä‘Ä©a vÃ  thoÃ¡t.
			q			ThoÃ¡t mÃ  khÃ´ng lÆ°u gÃ¬ (quit).
				
			LÆ°u Ã½ quan trá»ng

				Ráº¥t nguy hiá»ƒm: náº¿u báº¡n d (xÃ³a) rá»“i w, dá»¯ liá»‡u sáº½ máº¥t.
				
		- cmd: lsof /var/www/html/images
		
			Ã nghÄ©a

				lsof = list open files â†’ liá»‡t kÃª táº¥t cáº£ file Ä‘ang Ä‘Æ°á»£c má»Ÿ trÃªn há»‡ thá»‘ng.

				/var/www/html/images = Ä‘Æ°á»ng dáº«n báº¡n muá»‘n kiá»ƒm tra.

			Khi cháº¡y, nÃ³ sáº½ hiá»ƒn thá»‹ cÃ¡c process (tiáº¿n trÃ¬nh) nÃ o Ä‘ang dÃ¹ng thÆ° má»¥c hoáº·c file trong thÆ° má»¥c nÃ y.		
		
		- cmd: yum install lsof -y
		- cmd: yum install mariadb-server -y
		- cmd: systemctl start mariadb
		- cmd: systemctl status mariadb
		- cmd: systemctl stop mariadb
		- cmd: stress-ng --cpu 2 --timeout 60s
		- cmd: stress-ng --cpu $(nproc) --timeout 300s
		- cmd: stress-ng --cpu $(nproc) --cpu-load 60 --timeout 600s
		- cmd: sudo yum install -y amazon-efs-utils
		- cmd: mount -fav
		- cmd: apt install mysql-client -y
		
		
	AWS Part-1 - Amazon EFS (Elastic File System):	

		Amazon EFS (Elastic File System) lÃ  dá»‹ch vá»¥ lÆ°u trá»¯ file Ä‘Æ°á»£c AWS cung cáº¥p, dÃ¹ng Ä‘á»ƒ chia sáº» dá»¯ liá»‡u giá»¯a
		nhiá»u EC2 instance hoáº·c container má»™t cÃ¡ch dá»… dÃ ng.
		
		Äáº·c Ä‘iá»ƒm chÃ­nh cá»§a EFS:

			LÆ°u trá»¯ dáº¡ng file (NFS â€“ Network File System), khÃ´ng pháº£i block nhÆ° EBS vÃ  khÃ´ng pháº£i object nhÆ° S3.

			Tá»± Ä‘á»™ng má»Ÿ rá»™ng: dung lÆ°á»£ng tÄƒng/giáº£m theo nhu cáº§u, báº¡n chá»‰ tráº£ tiá»n cho dung lÆ°á»£ng dÃ¹ng thá»±c táº¿.

			Chia sáº» giá»¯a nhiá»u EC2: nhiá»u mÃ¡y áº£o (EC2) cÃ³ thá»ƒ mount vÃ o cÃ¹ng má»™t EFS vÃ  Ä‘á»c/ghi dá»¯ liá»‡u chung.

			High availability: dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ tá»± Ä‘á»™ng trong nhiá»u AZ (Availability Zone) trong cÃ¹ng 1 region.

			Hiá»‡u nÄƒng cao, cÃ³ thá»ƒ dÃ¹ng cho web server, big data, container, CI/CD.
		
	AWS Part-1 - Security group:	
		
		0.0.0.0/0 â†’ toÃ n bá»™ IPv4 address space (má»i Ä‘á»‹a chá»‰ IPv4 Ä‘á»u truy cáº­p Ä‘Æ°á»£c).

		::/0 â†’ toÃ n bá»™ IPv6 address space (má»i Ä‘á»‹a chá»‰ IPv6 Ä‘á»u truy cáº­p Ä‘Æ°á»£c).
		
		ChÃº Ã½ khi táº¡o load balancer thÃ¬ trong nÃ³ sáº½ cÃ³ 1 security group vÃ­ dá»¥ nhÆ° sg-b, trong instance sáº½ cÃ³ 1 security group vÃ­
		dá»¥ nhÆ° a-sg, thÃ¬ trong a-sg pháº£i add thÃªm b-sg Ä‘á»ƒ nÃ³ cháº¥p nháº­n truy cáº­p tá»« load balancer
		
		ChÃº Ã½ khi gÃ¡n public IP cá»§a mÃ¡y vÃ o security group, chÃºng ta táº¯t mÃ¡y rá»“i má»Ÿ láº¡i thÃ¬ sáº½ cÃ³ public API má»›i,
		lÃºc nÃ y cáº§n cáº­p nháº­t nÃ³ trong security group Ä‘á»ƒ cÃ³ public IP má»›i nháº¥t
		
	AWS Part-1 - Security group NFS:
	
		Trong Security Group cá»§a AWS, khi báº¡n tháº¥y NFS thÃ¬ nÃ³ lÃ  viáº¿t táº¯t cá»§a:

		NFS (Network File System): giao thá»©c chia sáº» file qua máº¡ng, thÆ°á»ng dÃ¹ng bá»Ÿi Amazon EFS (Elastic File System).
		
		ThÃ´ng tin cÆ¡ báº£n vá» NFS trong Security Group:

			Protocol: TCP

			Port: 2049 (cá»•ng máº·c Ä‘á»‹nh cá»§a NFS)

			Ã nghÄ©a: Cho phÃ©p EC2 (hoáº·c cÃ¡c mÃ¡y khÃ¡c trong VPC) káº¿t ná»‘i Ä‘áº¿n EFS Ä‘á»ƒ Ä‘á»c/ghi file.
			
		VÃ­ dá»¥ Security Group cho EFS:

			Äá»ƒ EC2 cÃ³ thá»ƒ mount EFS:

				Security Group cá»§a EFS pháº£i má»Ÿ inbound rule:

				Type: NFS

				Protocol: TCP

				Port Range: 2049

				Source: Security Group cá»§a EC2 (hoáº·c CIDR cá»§a VPC, vÃ­ dá»¥ 10.0.0.0/16)
		
	AWS Part-1 - EBS:
	
		EBS trong AWS lÃ  viáº¿t táº¯t cá»§a Amazon Elastic Block Store. ÄÃ¢y lÃ  dá»‹ch vá»¥ cung cáº¥p lÆ°u
		trá»¯ dáº¡ng block (block storage) cho cÃ¡c EC2 instance.		
		
		Äáº·c Ä‘iá»ƒm chÃ­nh cá»§a Amazon EBS:

			Block storage:

				Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u thÃ nh cÃ¡c khá»‘i (block), tÆ°Æ¡ng tá»± nhÆ° á»• cá»©ng gáº¯n vÃ o mÃ¡y tÃ­nh.

				CÃ³ thá»ƒ format thÃ nh há»‡ thá»‘ng file (ext4, xfs, NTFS, â€¦) hoáº·c dÃ¹ng trá»±c tiáº¿p cho
				database, á»©ng dá»¥ng cáº§n truy cáº­p I/O nhanh.

			Gáº¯n vá»›i EC2 Instance:

				Má»™t volume EBS giá»‘ng nhÆ° á»• cá»©ng áº£o gáº¯n vÃ o EC2.

				CÃ³ thá»ƒ gáº¯n má»™t volume cho nhiá»u instance (cháº¿ Ä‘á»™ read-only) hoáº·c cho 1 instance (read/write).

			TÃ­nh Ä‘Ã n há»“i (Elastic):

				CÃ³ thá»ƒ tÄƒng/giáº£m dung lÆ°á»£ng mÃ  khÃ´ng cáº§n dá»«ng instance.

				Há»— trá»£ thay Ä‘á»•i loáº¡i volume (vÃ­ dá»¥ tá»« HDD sang SSD).

			TÃ­nh bá»n vá»¯ng (Durability):

				EBS Ä‘Æ°á»£c lÆ°u trá»¯ trong Availability Zone (AZ), cÃ³ kháº£ nÄƒng replicate trong AZ Ä‘á»ƒ trÃ¡nh
				máº¥t dá»¯ liá»‡u khi cÃ³ lá»—i pháº§n cá»©ng.

			Snapshot:

				CÃ³ thá»ƒ táº¡o báº£n sao (snapshot) cá»§a volume vÃ  lÆ°u trÃªn Amazon S3.

				Snapshot dÃ¹ng Ä‘á»ƒ backup hoáº·c táº¡o volume má»›i tá»« báº£n copy.

	AWS Part-1 - EBS Snapshot:
	
		Trong Amazon EBS, snapshot chÃ­nh lÃ  cÆ¡ cháº¿ lÆ°u láº¡i dá»¯ liá»‡u cá»§a má»™t volume táº¡i má»™t
		thá»i Ä‘iá»ƒm cá»¥ thá»ƒ (point-in-time).
					
		Cá»¥ thá»ƒ hÆ¡n:

			Má»™t EBS volume giá»‘ng nhÆ° á»• cá»©ng gáº¯n vÃ o EC2 instance.

			Khi báº¡n táº¡o snapshot:

				AWS sáº½ "chá»¥p láº¡i" toÃ n bá»™ dá»¯ liá»‡u cá»§a volume táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³.

				Snapshot nÃ y Ä‘Æ°á»£c lÆ°u trong S3 ná»™i bá»™ cá»§a AWS (báº¡n khÃ´ng trá»±c tiáº¿p tháº¥y trong
				bucket S3, mÃ  quáº£n lÃ½ qua EC2 console/CLI).
				
		Má»¥c Ä‘Ã­ch chÃ­nh cá»§a snapshot:

			Sao lÆ°u dá»¯ liá»‡u (backup)

				Äá»ƒ phÃ²ng sá»± cá»‘ máº¥t dá»¯ liá»‡u, há»ng volume, hoáº·c EC2 bá»‹ terminate.

			Phá»¥c há»“i (restore)

				Báº¡n cÃ³ thá»ƒ táº¡o láº¡i má»™t volume má»›i tá»« snapshot â†’ cÃ³ dá»¯ liá»‡u giá»‘ng há»‡t volume ban Ä‘áº§u táº¡i thá»i Ä‘iá»ƒm chá»¥p.

			Táº¡o báº£n copy (clone)

				DÃ¹ng snapshot Ä‘á»ƒ táº¡o volume má»›i cho mÃ´i trÆ°á»ng test/dev mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n volume gá»‘c.

			Di chuyá»ƒn dá»¯ liá»‡u giá»¯a region

				Snapshot cÃ³ thá»ƒ copy sang region khÃ¡c, sau Ä‘Ã³ táº¡o volume á»Ÿ Ä‘Ã³.
			
		LÆ°u Ã½ quan trá»ng:

			Snapshot EBS lÃ  incremental:

				Snapshot Ä‘áº§u tiÃªn = toÃ n bá»™ dá»¯ liá»‡u.

				Snapshot tiáº¿p theo = chá»‰ lÆ°u pháº§n dá»¯ liá»‡u thay Ä‘á»•i tá»« láº§n trÆ°á»›c â†’ tiáº¿t kiá»‡m chi phÃ­.

			Náº¿u xÃ³a 1 snapshot, dá»¯ liá»‡u cá»§a snapshot Ä‘Ã³ Ä‘Æ°á»£c giá»¯ láº¡i trong cÃ¡c snapshot
			sau (AWS Ä‘áº£m báº£o khÃ´ng máº¥t dá»¯ liá»‡u miá»…n lÃ  cÃ²n Ã­t nháº¥t 1 snapshot trong chuá»—i).
			
	WS Part-1 - EC2 Launch Template:
	
		Launch Template lÃ  má»™t â€œmáº«u cáº¥u hÃ¬nhâ€ (template) chá»©a sáºµn thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ táº¡o EC2 instance.

		NÃ³ giÃºp báº¡n chuáº©n hÃ³a vÃ  tÃ¡i sá»­ dá»¥ng cáº¥u hÃ¬nh khi khá»Ÿi táº¡o nhiá»u EC2.

		CÃ³ thá»ƒ coi nhÆ° lÃ  má»™t "blueprint" cho instance.

		ThÃ´ng tin lÆ°u trong Launch Template:

			Khi táº¡o Launch Template, báº¡n cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a sáºµn:

				AMI (Amazon Machine Image) â†’ há»‡ Ä‘iá»u hÃ nh, pháº§n má»m cÃ i sáºµn.

				Instance type (t2.micro, t3.small, v.v.).

				Key pair (dÃ¹ng Ä‘á»ƒ SSH).

				Security group.

				VPC + Subnet.

				EBS volume mapping (á»• cá»©ng gáº¯n vÃ o instance).

				IAM role cho EC2.

				User data (script cháº¡y khi instance khá»Ÿi Ä‘á»™ng).

				Network settings (private/public IP).
				
		TÃ­nh nÄƒng chÃ­nh:

			TÃ¡i sá»­ dá»¥ng cáº¥u hÃ¬nh:

				KhÃ´ng pháº£i nháº­p tay láº¡i tá»«ng thÃ´ng tin khi launch EC2 má»›i.

			Versioning (phiÃªn báº£n hÃ³a):

				Má»—i khi báº¡n thay Ä‘á»•i Launch Template â†’ sáº½ táº¡o ra má»™t version má»›i.

				CÃ³ thá»ƒ quay láº¡i version cÅ© hoáº·c chá»n version máº·c Ä‘á»‹nh.

			TÃ­ch há»£p vá»›i Auto Scaling vÃ  Spot Fleet:

				Khi báº¡n táº¡o Auto Scaling Group hoáº·c EC2 Spot Fleet, thay vÃ¬ cáº¥u hÃ¬nh phá»©c táº¡p â†’ chá»‰ cáº§n trá» Ä‘áº¿n Launch Template.

			So vá»›i Launch Configuration (LC):

				Launch Template lÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a Launch Configuration.

				Há»— trá»£ nhiá»u tÃ­nh nÄƒng hÆ¡n (versioning, multiple instance types, T2/T3 Unlimited, placement groups...).

				AWS khuyÃªn dÃ¹ng Launch Template thay vÃ¬ Launch Configuration.
				
		VÃ­ dá»¥ thá»±c táº¿

			Báº¡n muá»‘n triá»ƒn khai Auto Scaling Group cho web server:

			Táº¡o Launch Template vá»›i cáº¥u hÃ¬nh:

				AMI Ubuntu 22.04

				Instance type: t3.micro

				Security Group má»Ÿ cá»•ng 80/443

				User Data: cÃ i Nginx

			Khi Auto Scaling cáº§n táº¡o thÃªm EC2 â†’ nÃ³ sáº½ dá»±a vÃ o Launch Template nÃ y, Ä‘áº£m báº£o táº¥t cáº£ instance Ä‘á»u giá»‘ng nhau.
			
	AWS Part-1 - Load Balancing:
	
		Load Balancing = ká»¹ thuáº­t phÃ¢n phá»‘i lÆ°u lÆ°á»£ng (traffic) Ä‘áº¿n nhiá»u server (targets) thay vÃ¬ dá»“n háº¿t vÃ o má»™t server.

		Má»¥c tiÃªu:

			TÄƒng tÃ­nh sáºµn sÃ ng (High Availability) â†’ náº¿u 1 server há»ng, traffic tá»± Ä‘á»™ng chuyá»ƒn sang server khÃ¡c.

			TÄƒng kháº£ nÄƒng má»Ÿ rá»™ng (Scalability) â†’ thÃªm server má»›i Ä‘á»ƒ gÃ¡nh thÃªm táº£i.

			CÃ¢n báº±ng hiá»‡u nÄƒng (Performance) â†’ trÃ¡nh 1 server bá»‹ quÃ¡ táº£i trong khi server khÃ¡c ráº£nh.

			Trong AWS, Elastic Load Balancer (ELB) chÃ­nh lÃ  dá»‹ch vá»¥ thá»±c hiá»‡n load balancing cho EC2 vÃ 
			cÃ¡c dá»‹ch vá»¥ backend khÃ¡c.
			
		ThÃ nh pháº§n chÃ­nh trong Load Balancing (EC2 + ELB):

			Load Balancer (ALB, NLB, GWLB, CLB):

				ThÃ nh pháº§n Ä‘á»©ng trÆ°á»›c client.

				Nháº­n request tá»« user â†’ phÃ¢n phá»‘i Ä‘áº¿n backend.

			Listener:

				Quy Ä‘á»‹nh protocol & port (VD: HTTP:80, HTTPS:443, TCP:3306).

				CÃ³ thá»ƒ cÃ³ nhiá»u listener trong má»™t Load Balancer.

			Target Group:

				Táº­p há»£p backend (EC2 instances, IPs, Lambda, ECS tasks).

				Load Balancer sáº½ gá»­i request Ä‘áº¿n Target Group theo rule.

			Health Check:

				Load Balancer kiá»ƒm tra tráº¡ng thÃ¡i (healthy/unhealthy) cá»§a tá»«ng target.

				Chá»‰ gá»­i request Ä‘áº¿n target cÃ²n "healthy".
				
		CÃ¡c loáº¡i Load Balancer trong AWS

			Application Load Balancer (ALB)

				DÃ¹ng cho HTTP/HTTPS.

				CÃ³ tÃ­nh nÄƒng routing theo path, host, header (Layer 7).

				VÃ­ dá»¥: /api/* vÃ o backend API, /img/* vÃ o backend static server.

			Network Load Balancer (NLB)

				DÃ¹ng cho TCP/UDP (Layer 4).

				Hiá»‡u nÄƒng cá»±c cao, Ä‘á»™ trá»… tháº¥p, xá»­ lÃ½ hÃ ng triá»‡u request/s.

			Gateway Load Balancer (GWLB)

				DÃ¹ng Ä‘á»ƒ phÃ¢n phá»‘i traffic qua firewall/appliance (Layer 3).

				Classic Load Balancer (CLB)

				Dá»‹ch vá»¥ Ä‘á»i cÅ©, Ã­t dÃ¹ng cho há»‡ thá»‘ng má»›i.
				
		VÃ­ dá»¥ thá»±c táº¿:

			Báº¡n cÃ³ 3 EC2 cháº¡y web server:

			Cáº¥u hÃ¬nh 1 Application Load Balancer (ALB) vá»›i listener HTTP:80.

			Gáº¯n 3 EC2 nÃ y vÃ o Target Group.

			Khi ngÆ°á»i dÃ¹ng truy cáº­p website:

				Request Ä‘áº¿n ALB.

				ALB chá»n 1 EC2 trong target group (theo Round Robin hoáº·c rule khÃ¡c).

				Náº¿u 1 EC2 bá»‹ down â†’ ALB ngá»«ng gá»­i traffic Ä‘áº¿n nÃ³.
			
	AWS Part-1 - Target Group cá»§a Load Balancing:
	
		Target Group lÃ  táº­p há»£p cÃ¡c backend (targets) â€“ tá»©c lÃ  nÆ¡i nháº­n request tá»« Load Balancer.

		CÃ¡c target cÃ³ thá»ƒ lÃ :

			EC2 instances

			IP addresses (private IP)

			Lambda functions

			ECS tasks (trong case dÃ¹ng vá»›i container)

		NÃ³i Ä‘Æ¡n giáº£n: Target Group = danh sÃ¡ch cÃ¡c server/á»©ng dá»¥ng mÃ  Load Balancer sáº½ phÃ¢n phá»‘i traffic Ä‘áº¿n.
		
		CÃ¡ch hoáº¡t Ä‘á»™ng

			NgÆ°á»i dÃ¹ng gá»­i request â†’ Load Balancer nháº­n request.

			Load Balancer sáº½ chuyá»ƒn request Ä‘áº¿n má»™t Target Group Ä‘Ã£ gáº¯n vá»›i nÃ³.

			Trong Target Group, LB phÃ¢n phá»‘i request Ä‘áº¿n cÃ¡c target cá»¥ thá»ƒ (EC2, IP, ECS task...) dá»±a
			theo policy (Round Robin, Least Outstanding Request, â€¦).

			Target Group cÃ³ health check Ä‘á»ƒ xÃ¡c Ä‘á»‹nh target nÃ o cÃ²n â€œsá»‘ngâ€ (healthy). LB chá»‰ gá»­i traffic Ä‘áº¿n cÃ¡c target healthy.
			
		CÃ¡c loáº¡i Target Group (tÃ¹y loáº¡i Load Balancer):

			Application Load Balancer (ALB): target group cÃ³ thá»ƒ chá»©a EC2, IP, ECS tasks, Lambda.

			Network Load Balancer (NLB): target group thÆ°á»ng chá»©a EC2 hoáº·c IP.

			Gateway Load Balancer (GWLB): target group chá»©a appliances (vÃ­ dá»¥ firewall áº£o).
			
		Health Check:

			Má»—i Target Group cÃ³ thá»ƒ cáº¥u hÃ¬nh health check riÃªng (HTTP, HTTPS, TCP).

			Náº¿u target fail health check â†’ LB táº¡m dá»«ng gá»­i request Ä‘áº¿n Ä‘Ã³.
			
		VÃ­ dá»¥ thá»±c táº¿:

			Báº¡n cÃ³ 2 EC2 web server cháº¡y Nginx:

				Táº¡o Target Group web-servers.

				Add 2 EC2 instance (10.0.1.10, 10.0.1.11) vÃ o Target Group.

				Cáº¥u hÃ¬nh ALB rule: náº¿u request Ä‘áº¿n / â†’ forward Ä‘áº¿n Target Group web-servers.

				Khi ALB nháº­n request, nÃ³ sáº½ forward sang 1 trong 2 EC2 Ä‘Ã³ (theo Round Robin).
				
	AWS Part-1 - EC2 Auto Scaling:
	
		Trong AWS, Auto Scaling (thÆ°á»ng gá»i lÃ  EC2 Auto Scaling) lÃ  dá»‹ch vá»¥ giÃºp báº¡n tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh
		sá»‘ lÆ°á»£ng tÃ i nguyÃªn (EC2 instances) Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u táº£i, mÃ  khÃ´ng cáº§n can thiá»‡p thá»§ cÃ´ng.
		
		CÃ¡c thÃ nh pháº§n chÃ­nh trong AWS Auto Scaling

		Launch Template / Launch Configuration:

			Äá»‹nh nghÄ©a cÃ¡ch EC2 sáº½ Ä‘Æ°á»£c táº¡o: AMI, instance type, key pair, security group, user data,â€¦

		Auto Scaling Group (ASG):

			LÃ  nhÃ³m EC2 Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi Auto Scaling.

			Báº¡n cáº¥u hÃ¬nh:

				Min size: sá»‘ lÆ°á»£ng EC2 tá»‘i thiá»ƒu.

				Max size: sá»‘ lÆ°á»£ng EC2 tá»‘i Ä‘a.

				Desired capacity: sá»‘ lÆ°á»£ng EC2 mong muá»‘n (ASG sáº½ duy trÃ¬ con sá»‘ nÃ y).

		Scaling Policies:

			Quy táº¯c Ä‘á»ƒ scale in (giáº£m) hoáº·c scale out (tÄƒng).

			VÃ­ dá»¥:

				Náº¿u CPU > 70% trong 5 phÃºt â†’ scale out +1 instance.

				Náº¿u CPU < 30% trong 10 phÃºt â†’ scale in -1 instance.

		Health Checks:

			ASG sáº½ kiá»ƒm tra tÃ¬nh tráº¡ng EC2. Náº¿u má»™t instance unhealthy, nÃ³ sáº½ terminate vÃ  khá»Ÿi táº¡o láº¡i instance má»›i.

		Integration vá»›i Load Balancer

			ASG thÆ°á»ng gáº¯n vá»›i Target Group cá»§a má»™t Elastic Load Balancer (ALB/NLB).

			Khi EC2 scale in/out, chÃºng sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c Ä‘Äƒng kÃ½/dá»¡ khá»i target group.
			
	AWS Part-1 - Route 53:
	
		Amazon Route 53 lÃ  má»™t dá»‹ch vá»¥ DNS (Domain Name System) vÃ  quáº£n lÃ½ tÃªn miá»n cá»§a AWS.

		CÃ¡c chá»©c nÄƒng chÃ­nh cá»§a Route 53:

			Domain Registration (ÄÄƒng kÃ½ tÃªn miá»n)

				Báº¡n cÃ³ thá»ƒ mua vÃ  quáº£n lÃ½ domain trá»±c tiáº¿p trÃªn AWS (vÃ­ dá»¥: mywebsite.com).

			DNS Service (Há»‡ thá»‘ng phÃ¢n giáº£i tÃªn miá»n)

				Biáº¿n tÃªn miá»n dá»… nhá»› (mywebsite.com) thÃ nh Ä‘á»‹a chá»‰ IP tháº­t (192.0.2.1).

				Quáº£n lÃ½ cÃ¡c báº£n ghi DNS nhÆ°:

					A (IPv4 address)

					AAAA (IPv6 address)

					CNAME (alias)

					MX (mail server)

					TXT (text records, thÆ°á»ng dÃ¹ng cho xÃ¡c thá»±c email, SSL, â€¦).

			Traffic Routing (Äiá»u hÆ°á»›ng lÆ°u lÆ°á»£ng thÃ´ng minh)

				Route 53 há»— trá»£ nhiá»u kiá»ƒu Ä‘iá»u hÆ°á»›ng:

					Simple routing â†’ trá» domain Ä‘áº¿n 1 server duy nháº¥t.

					Weighted routing â†’ chia táº£i theo tá»‰ lá»‡ % giá»¯a nhiá»u server.

					Latency-based routing â†’ gá»­i user Ä‘áº¿n server cÃ³ Ä‘á»™ trá»… tháº¥p nháº¥t.

					Failover routing â†’ tá»± Ä‘á»™ng chuyá»ƒn sang server khÃ¡c khi server chÃ­nh bá»‹ down.

			Health Checks & Monitoring (GiÃ¡m sÃ¡t dá»‹ch vá»¥)

				Route 53 cÃ³ thá»ƒ kiá»ƒm tra tÃ¬nh tráº¡ng endpoint (web server, API, â€¦).

				Náº¿u server khÃ´ng pháº£n há»“i, Route 53 sáº½ tá»± Ä‘á»™ng chuyá»ƒn traffic sang server backup.

			á»¨ng dá»¥ng thá»±c táº¿

				Khi báº¡n cÃ³ website cháº¡y trÃªn EC2 / S3 / Load Balancer, báº¡n dÃ¹ng Route 53 Ä‘á»ƒ:

					Trá» domain (myapp.com) â†’ Elastic Load Balancer.

					Tá»± Ä‘á»™ng Ä‘iá»u hÆ°á»›ng ngÆ°á»i dÃ¹ng Ä‘áº¿n server gáº§n nháº¥t (multi-region).

					Äáº£m báº£o há»‡ thá»‘ng váº«n online khi má»™t server bá»‹ lá»—i (failover).
		
	AWS Part-1 - Kiáº¿n thá»©c:	
		
		- EC2 Key pair name: RSA, ED25519
		- EC2 prite key format: .pem, .ppk
		- EC2 network settings: VPC
		- EC2 network setting: Inbound Security Group Rules
		- EC2 instances -> connect -> ssh client
		- EC2 instance -> security -> security group -> edit inbound rules
		- EC2 -> key pair
		- EC2 network interface
		- EC2 security group: My IP
		- EC2 -> Launch an instance -> Resource types -> Intances, Volumes
		- EC2 -> Instance -> Actions -> Instance settings -> Change instance type
		- EC2 -> Instance -> Actions -> Image and templates -> Create image
		- EC2 -> Images -> AMIs
		- EC2 -> Images -> Action -> Copy AMI
		- EC2 -> Images -> Action -> Edit AMI permissions
		- EC2 -> Images -> Action -> Launch instance from AMI
		- EC2 -> Instance -> Actions -> Security -> Change security groups
		- EC2 -> Instance -> Actions -> Monitor and troubleshoot -> Get system log
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Dissociate Elastic IP address
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Release Elastic IP addresses
		- EC2 -> Instances -> Storage -> Block devices
		- EC2 -> Instances -> Launch Templates -> Actions -> Launch instance from template
		- EC2 -> Instances -> Create image
		- EC2 -> Elastic Block Store -> Volumes
		- EC2 -> Elastic Block Store -> Volumes -> Action -> Attach Volume
		- EC2 -> Elastic Block Store ->  Actions -> Detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Force detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Create snapshot
		- EC2 -> Load Balancing -> Target groups -> Create target group
		- EC2 -> Load Balancing -> Load Balancers -> Create load balancer
		- EC2 -> AMIs -> Launch instance from AMI
		- EC2 -> Auto Scaling groups
		- EC2 -> Instances -> Vao instance -> Actions -> Security -> Modify IAM role
		- CloudWatch -> All alarms -> Create alarm
		- Amazon EFS -> Access points
		- Amazon S3 -> Buckets
		- Amazon S3 -> Buckets -> Objects
		- Amazon S3 -> Buckets -> Properties
		- Amazon S3 -> Buckets -> Permission
		- Amazon S3 -> Buckets -> Manegement -> Lifecycle rules -> Create lifecycle rule
		- Amazon S3 -> Buckets -> Manegement -> Replication rules -> Create replication rules
		- Aurora and RDS
		- Aurora and RDS -> Snapshots		
		- Aurora and RDS -> Databases -> Actions -> Migrate snapshot
		- Aurora and RDS -> Parameter group
		- Aurora and RDS -> Subnet groups
		- Route 53 -> Dashboard -> Create hosted zone
		- IAM -> Access management -> Users -> Add users
		- IAM -> Access management -> Users -> Security credentials -> Access keys -> Create access key
		- IAM -> Access management -> Roles -> Create role
		- IAM -> Users -> Create user
		- IAM -> User -> VÃ o user -> Security credentials -> Create access key
		- Certificate Manager
		- Amazon AlasticCache -> Parameter groups -> Create parameter group
		- Amazon AlasticCache -> Subnet groups -> Create subnet group
		- Amazon AlasticCache -> Dashboard -> Create cache -> Create Memcache
		- Amazon MQ -> Broker -> Create brokers
		- Amazon Elastic Beanstalk -> Environment -> Upload and deploy
		- Cloudfront
		
		
		
		

	
	Trong bÆ°á»›c Set permissions khi táº¡o IAM User trÃªn AWS, báº¡n tháº¥y cÃ³ 3 tÃ¹y chá»n cáº¥p quyá»n:

		1. Add user to group

			ThÃªm user vÃ o má»™t nhÃ³m (IAM Group).

			NhÃ³m nÃ y Ä‘Ã£ cÃ³ sáºµn cÃ¡c policy (quyá»n háº¡n).

			Táº¥t cáº£ cÃ¡c user trong nhÃ³m sáº½ káº¿ thá»«a quyá»n tá»« nhÃ³m Ä‘Ã³.

			ÄÃ¢y lÃ  cÃ¡ch quáº£n lÃ½ táº­p trung, dá»… duy trÃ¬ khi cÃ³ nhiá»u user. VÃ­ dá»¥: táº¡o group Developers
			vá»›i quyá»n AmazonEC2FullAccess, má»i user trong group sáº½ cÃ³ quyá»n Ä‘Ã³.

		2. Copy permissions

			Sao chÃ©p toÃ n bá»™ quyá»n cá»§a má»™t user khÃ¡c sang user má»›i.

			Bao gá»“m group memberships, attached managed policies, vÃ  inline policies.

			Tiá»‡n khi báº¡n cÃ³ má»™t user cÅ© lÃ m chuáº©n, vÃ  muá»‘n user má»›i cÃ³ quyá»n giá»‘ng há»‡t.

		3. Attach policies directly

			Gáº¯n trá»±c tiáº¿p policy (quyá»n háº¡n) vÃ o user.

			Báº¡n chá»n cÃ¡c AWS managed policy (AWS táº¡o sáºµn, vÃ­ dá»¥: AmazonS3ReadOnlyAccess) hoáº·c customer
			managed policy (do báº¡n tá»± táº¡o).

			Linh hoáº¡t cho tá»«ng user, nhÆ°ng khÃ´ng khuyáº¿n khÃ­ch náº¿u cÃ³ nhiá»u user vÃ¬ sáº½ khÃ³ quáº£n lÃ½ vá» lÃ¢u dÃ i.	
		
		
			
			

		Quy trÃ¬nh táº¡o má»™t Amazon EC2 Instance (mÃ¡y áº£o cháº¡y trÃªn AWS). Flow gá»“m cÃ¡c bÆ°á»›c tuáº§n tá»± nhÆ° sau:

			Choose an AMI (Amazon Machine Image)

				AMI lÃ  "báº£n máº«u" há»‡ Ä‘iá»u hÃ nh + pháº§n má»m (Ubuntu, Amazon Linux, Windows Server, â€¦).

				Báº¡n chá»n AMI Ä‘á»ƒ quyáº¿t Ä‘á»‹nh mÃ¡y áº£o cá»§a báº¡n sáº½ cháº¡y há»‡ Ä‘iá»u hÃ nh nÃ o, cÃ³ cÃ i sáºµn pháº§n má»m gÃ¬.

			Choose an Instance Type

				á» bÆ°á»›c nÃ y báº¡n chá»n loáº¡i EC2 instance (tÃ­nh nÄƒng giá»‘ng nhÆ° chá»n cáº¥u hÃ¬nh pháº§n cá»©ng).

				VÃ­ dá»¥: t2.micro (1 vCPU, 1GB RAM â€“ miá»…n phÃ­ trong Free Tier), m5.large, c5.xlarge, â€¦

				Má»—i loáº¡i khÃ¡c nhau vá» CPU, RAM, kháº£ nÄƒng máº¡ng, dÃ¹ng cho má»¥c Ä‘Ã­ch khÃ¡c nhau (test, production, tÃ­nh toÃ¡n náº·ngâ€¦).

		Configuring the Instance

			Cáº¥u hÃ¬nh chi tiáº¿t cho instance:

				Cháº¡y bao nhiÃªu instance

				VPC/Subnet (máº¡ng nÃ o)

				Tá»± Ä‘á»™ng cáº¥p public IP hay khÃ´ng

				IAM Role (gáº¯n quyá»n Ä‘á»ƒ EC2 truy cáº­p dá»‹ch vá»¥ AWS khÃ¡c)

				User data (script tá»± cháº¡y khi khá»Ÿi Ä‘á»™ng instance)

		Adding Storage

			Chá»n dung lÆ°á»£ng á»• Ä‘Ä©a (EBS volume).

			CÃ³ thá»ƒ thÃªm nhiá»u volume, chá»n loáº¡i (SSD, HDD, provisioned IOPSâ€¦).

			VÃ­ dá»¥: 8GB gp2 SSD máº·c Ä‘á»‹nh.

		Adding Tags

			GÃ¡n nhÃ£n (key-value) Ä‘á»ƒ quáº£n lÃ½ dá»… hÆ¡n.

			VÃ­ dá»¥: Name=WebServer01, Environment=Dev.

			Tags giÃºp tÃ¬m kiáº¿m, phÃ¢n nhÃ³m, hoáº·c dÃ¹ng cho billing (chi phÃ­).

		Configure Security Group

			Security Group lÃ  tÆ°á»ng lá»­a áº£o cho EC2.

			á» Ä‘Ã¢y báº¡n Ä‘á»‹nh nghÄ©a inbound/outbound rules (cá»•ng nÃ o má»Ÿ cho IP nÃ o).

			VÃ­ dá»¥: má»Ÿ port 22 (SSH) cho IP cá»§a báº¡n, má»Ÿ port 80 (HTTP) cho má»i ngÆ°á»i.

		Review

			Xem láº¡i toÃ n bá»™ cáº¥u hÃ¬nh Ä‘Ã£ chá»n.

			Náº¿u á»•n, báº¡n nháº¥n "Launch" Ä‘á»ƒ khá»Ÿi táº¡o EC2.

			LÃºc nÃ y AWS sáº½ há»i báº¡n chá»n key pair (dÃ¹ng Ä‘á»ƒ SSH vÃ o mÃ¡y).

			Káº¿t quáº£: Báº¡n cÃ³ má»™t mÃ¡y áº£o EC2 cháº¡y trÃªn AWS, káº¿t ná»‘i qua SSH/HTTP/HTTPS tÃ¹y cáº¥u hÃ¬nh.

	Code trong Advanced details cá»§a EC2:

		Case 1:
		
			#!/bin/bash
			sudo yum install httpd -y
			sudo systemctl start httpd
			sudo systemctl enable httpd
			mkdir /tmp/test1
			
		Case 2:
		
			#!/bin/bash
			yum install httpd wget unzip -y
			systemctl start httpd
			systemctl enable httpd
			cd /tmp
			wget https://www.tooplate.com/zip-templates/2119_gymso_fitness.zip
			unzip -o 2119_gymso_fitness.zip
			cp -r 2119_gymso_fitness/* /var/www/html/
			systemctl restart httpd
			
		Case 3:
		
			#!/bin/bash

			# Variable Declaration
			#PACKAGE="httpd wget unzip"
			#SVC="httpd"
			URL='https://www.tooplate.com/zip-templates/2098_health.zip'
			ART_NAME='2098_health'
			TEMPDIR="/tmp/webfiles"

			yum --help &> /dev/null

			if [ $? -eq 0 ]
			then
			   # Set Variables for CentOS
			   PACKAGE="httpd wget unzip"
			   SVC="httpd"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo yum install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/

			else
				# Set Variables for Ubuntu
			   PACKAGE="apache2 wget unzip"
			   SVC="apache2"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo apt update
			   sudo apt install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/
			fi 

			
	Code trong vi /etc/fstab:

		Case 1:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1p1    /var/www/html/images ext4       defaults        0 0
			
		Case 2:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1  /var/lib/mysql ext4     defaults        0 0
			
		Case 3:
		
			fs-0001a83c45525fae6 /var/www/html/images efs _netdev,tls,accesspoint=fsap-04b21726d75c2e7eb 0 0

	
			
-- AWS Cloud For Project Set Up or Lift and Shift:

	Flow chÃ­nh:
	
		Luá»“ng hoáº¡t Ä‘á»™ng:

			User â†’ DNS (GoDaddy/Route53):
			
				NgÆ°á»i dÃ¹ng nháº­p domain vÃ o browser â†’ DNS phÃ¢n giáº£i domain â†’ IP cá»§a Load Balancer.

			DNS â†’ Application Load Balancer:
			
				Request tá»›i ALB (qua HTTPS/HTTP).

			ALB â†’ Tomcat Instances (ASG):
			
				ALB phÃ¢n phá»‘i request Ä‘áº¿n má»™t trong cÃ¡c EC2 Tomcat.

			Tomcat Instances â†’ Backend services:

				Náº¿u cáº§n dá»¯ liá»‡u â†’ Tomcat káº¿t ná»‘i tá»›i MySQL.

				Náº¿u cáº§n cache â†’ gá»i tá»›i Memcache.

				Náº¿u cáº§n message queue â†’ gá»i tá»›i RabbitMQ.

				Náº¿u cáº§n static file â†’ truy váº¥n tá»« S3 bucket.

				Domain ná»™i bá»™ (db01, mc01, rmq01) Ä‘Æ°á»£c phÃ¢n giáº£i nhá» Private DNS Zone (Route53).

			Response â†’ quay ngÆ°á»£c láº¡i cho user thÃ´ng qua Load Balancer.
	
		CÃ¡c thÃ nh pháº§n chÃ­nh:

			DNS Zones (GoDaddy / Route53):

				NgÆ°á»i dÃ¹ng (Users) truy cáº­p á»©ng dá»¥ng qua tÃªn miá»n (domain).

				DNS (vÃ­ dá»¥ GoDaddy hoáº·c Route53) phÃ¢n giáº£i tÃªn miá»n thÃ nh Ä‘á»‹a chá»‰ IP cá»§a Application Load Balancer.

			Application Load Balancer (ALB):

				Nháº­n request tá»« ngÆ°á»i dÃ¹ng qua HTTP/HTTPS.

				ALB cÃ³ Security Group riÃªng Ä‘á»ƒ kiá»ƒm soÃ¡t ai Ä‘Æ°á»£c phÃ©p truy cáº­p (vÃ­ dá»¥: má»Ÿ cá»•ng 80, 443).

				ALB phÃ¢n phá»‘i request Ä‘áº¿n cÃ¡c Tomcat Instances phÃ­a sau.

			Auto Scaling Group (ASG) + Tomcat Instances:

				CÃ¡c EC2 instances cháº¡y á»©ng dá»¥ng (Java/Tomcat).

				ASG Ä‘áº£m báº£o sá»‘ lÆ°á»£ng instances phÃ¹ há»£p (scale in/out).

				CÃ³ Security Group riÃªng, chá»‰ cho phÃ©p traffic Ä‘áº¿n tá»« ALB.

			Amazon S3 Bucket:

				LÆ°u trá»¯ static files (hÃ¬nh áº£nh, video, tÃ i liá»‡u).

				á»¨ng dá»¥ng Tomcat cÃ³ thá»ƒ Ä‘á»c/ghi file tá»« Ä‘Ã¢y.

			Amazon Route53 (Private DNS Zones):

				DÃ¹ng Ä‘á»ƒ phÃ¢n giáº£i ná»™i bá»™ trong VPC.

				VÃ­ dá»¥:

					db01 â†’ IP MySQL instance

					mc01 â†’ IP Memcache instance

					rmq01 â†’ IP RabbitMQ instance

				Äiá»u nÃ y giÃºp á»©ng dá»¥ng (Tomcat) gá»i service báº±ng tÃªn domain thay vÃ¬ IP cá»©ng.

			Backend Instances (cÃ³ Security Group riÃªng):

				MySQL Instances: Database server (cÃ³ thá»ƒ lÃ  RDS hoáº·c EC2 cÃ i MySQL).

				Memcache Instances: Cache Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½.

				RabbitMQ Instances: Message broker, dÃ¹ng Ä‘á»ƒ giao tiáº¿p phi Ä‘á»“ng bá»™ giá»¯a cÃ¡c service.
				
	Tá»« EC2 instance -> Táº¡o image -> Táº¡o launch template vÃ  thÃªm image má»›i táº¡o vÃ o -> tao auto scaling group
	vÃ  thÃªm launch template má»›i táº¡o vÃ o

	- Command:
	
		- cmd: mysql -u admin -padmin123 accounts
		
			mysql â†’ gá»i MySQL client (dÃ¹ng Ä‘á»ƒ káº¿t ná»‘i vÃ  lÃ m viá»‡c vá»›i MySQL server).

			-u admin â†’ chá»‰ Ä‘á»‹nh username Ä‘á»ƒ Ä‘Äƒng nháº­p vÃ o MySQL lÃ  admin.

			-padmin123

				-p dÃ¹ng Ä‘á»ƒ nháº­p máº­t kháº©u.

				á» Ä‘Ã¢y viáº¿t liá»n luÃ´n admin123 â†’ nghÄ©a lÃ  máº­t kháº©u lÃ  admin123.

				(Náº¿u chá»‰ viáº¿t -p thÃ´i thÃ¬ MySQL sáº½ há»i máº­t kháº©u sau khi enter).

			accounts â†’ Ä‘Ã¢y lÃ  tÃªn database muá»‘n truy cáº­p sau khi login.

				NghÄ©a lÃ  sau khi káº¿t ná»‘i thÃ nh cÃ´ng, báº¡n sáº½ á»Ÿ ngay trong database accounts.
		
			Káº¿t ná»‘i tá»›i MySQL server báº±ng user admin, password admin123, vÃ  chá»n sáºµn database accounts Ä‘á»ƒ lÃ m viá»‡c.
			
		- cmd: ping -c 4 db01.vprofile.in
		
			Cáº¥u trÃºc lá»‡nh:

				ping: cÃ´ng cá»¥ kiá»ƒm tra káº¿t ná»‘i máº¡ng (ICMP echo request/reply).

				-c 4: option -c (count) â†’ gá»­i 4 gÃ³i tin ICMP rá»“i dá»«ng (náº¿u khÃ´ng cÃ³, ping sáº½ cháº¡y vÃ´ háº¡n Ä‘áº¿n khi báº¡n báº¥m Ctrl+C).

				db01.vprofile.in: hostname / domain mÃ  báº¡n muá»‘n kiá»ƒm tra káº¿t ná»‘i.

			QuÃ¡ trÃ¬nh diá»…n ra

				DNS Resolution

					MÃ¡y cá»§a báº¡n sáº½ há»i DNS Ä‘á»ƒ láº¥y Ä‘á»‹a chá»‰ IP cá»§a db01.vprofile.in.

					Náº¿u trong Route 53 (hoáº·c DNS khÃ¡c) báº¡n cÃ³ record cho db01.vprofile.in, nÃ³ sáº½ tráº£ vá»
					IP (cÃ³ thá»ƒ lÃ  private hoáº·c public).

				Gá»­i ICMP Echo Request

					Ping sáº½ gá»­i 4 gÃ³i ICMP Ä‘áº¿n IP vá»«a resolve Ä‘Æ°á»£c.

				Nháº­n ICMP Echo Reply

					Náº¿u host tráº£ lá»i, báº¡n sáº½ tháº¥y thá»i gian pháº£n há»“i (time=xx ms) cho má»—i gÃ³i.

				Káº¿t quáº£ thá»‘ng kÃª

					Sau khi gá»­i 4 gÃ³i, ping sáº½ in thá»‘ng kÃª:

						Sá»‘ gÃ³i gá»­i/nháº­n

						Tá»· lá»‡ packet loss (%)

						Thá»i gian trung bÃ¬nh / min / max / stddev

			Ã nghÄ©a trong trÆ°á»ng há»£p nÃ y

				db01.vprofile.in thÆ°á»ng lÃ  record trong Route 53 hoáº·c DNS báº¡n tá»± quáº£n lÃ½.

				Náº¿u báº¡n Ä‘Ã£ táº¡o A record (vÃ­ dá»¥ db01.vprofile.in -> 10.0.1.5), thÃ¬ ping nÃ y sáº½ kiá»ƒm tra xem tá»« mÃ¡y báº¡n cÃ³ káº¿t
				ná»‘i Ä‘áº¿n server db01 khÃ´ng.

				Náº¿u record khÃ´ng tá»“n táº¡i hoáº·c khÃ´ng resolve Ä‘Æ°á»£c â†’ bÃ¡o lá»—i â€œunknown hostâ€.

				Náº¿u resolve Ä‘Æ°á»£c nhÆ°ng khÃ´ng ping Ä‘Æ°á»£c â†’ cÃ³ thá»ƒ bá»‹ firewall, security group, hoáº·c ICMP bá»‹ cháº·n.
		
		- cmd: $ aws configure
		
				AWS Access Key ID [****************ADE6]: AKIA5KBUVI7M7FM7RC5A
				AWS Secret Access Key [****************dUht]: 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
				Default region name [us-east-1]: us-east-1
				Default output format [json]: json
		
		- cmd: vim ~/.aws/credentials
		- cmd: vim ~/.aws/config
		- cmd: aws s3 cp target/vprofile-v2.war s3://vprofile-las-artifactss12311
		- cmd: aws s3 ls s3://vprofile-las-artifactss12311/
		- cmd: aws-cli --classic
		- cmd: aws s3 cp s3://vprofile-las-artifactss12311/vprofile-v2.war /tmp/
		- cmd: systemctl stop tomcat10
		- cmd: systemctl start tomcat10
		- cmd: ls /var/lib/tomcat10/webapps/
		- cmd: cp /tmp/vprofile-v2.war /var/lib/tomcat10/webapps/ROOT.war
		- cmd: ls /var/lib/tomcat10/webapps
		- cmd: sudo yum install -y telnet
		- cmd: telnet db01.vprofile.in 3306
		
		
	- Flow khi báº¡n mua domain á»Ÿ GoDaddy vÃ  muá»‘n dÃ¹ng vá»›i AWS Certificate Manager (ACM):
	
		1. Mua domain á»Ÿ GoDaddy

			Báº¡n Ä‘Äƒng kÃ½ domain táº¡i GoDaddy

			VÃ­ dá»¥ báº¡n mua: myawsdemo.com.

			Sau khi thanh toÃ¡n xong, domain nÃ y sáº½ Ä‘Æ°á»£c quáº£n lÃ½ trong GoDaddy DNS.

		2. Táº¡o Hosted Zone trong Route 53

			VÃ o AWS â†’ Route 53 â†’ Hosted Zones â†’ Create hosted zone.

			Nháº­p tÃªn domain cá»§a báº¡n (myawsdemo.com).

			Route 53 sáº½ sinh ra 4 record NS (Name Servers) vÃ  1 record SOA.

		3. Cáº­p nháº­t DNS á»Ÿ GoDaddy

			VÃ o GoDaddy â†’ My Domains â†’ DNS Management.

			Thay 4 NS máº·c Ä‘á»‹nh cá»§a GoDaddy báº±ng 4 NS tá»« AWS Route 53.

			Äiá»u nÃ y nghÄ©a lÃ : tá»« giá» má»i request tá»›i domain sáº½ Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi AWS.

			Thá»i gian cáº­p nháº­t cÃ³ thá»ƒ máº¥t 5 phÃºt â€“ 24h (thÆ°á»ng lÃ  vÃ i phÃºt).

		4. YÃªu cáº§u chá»©ng chá»‰ trong ACM

			VÃ o AWS Certificate Manager â†’ Request a certificate.

			Nháº­p domain cáº§n SSL:

			myawsdemo.com

			*.myawsdemo.com (náº¿u báº¡n muá»‘n wildcard cho subdomain).

			Chá»n DNS validation.

			ACM sáº½ Ä‘Æ°a ra 1 record CNAME.

		5. ThÃªm CNAME vÃ o Route 53

			VÃ o Route 53 â†’ Hosted Zone â†’ Create record.

			ThÃªm CNAME do ACM cung cáº¥p.

			Chá» AWS xÃ¡c thá»±c (vÃ i phÃºt â€“ 30 phÃºt).

			Khi tráº¡ng thÃ¡i certificate = Issued â†’ báº¡n Ä‘Ã£ cÃ³ chá»©ng chá»‰ há»£p lá»‡.

		6. Sá»­ dá»¥ng certificate

			CloudFront â†’ chá»n certificate trong pháº§n SSL.

			ALB (Application Load Balancer) â†’ add certificate vÃ o Listener 443.

			API Gateway â†’ add certificate cho Custom Domain.
				
	- TrÆ°á»ng há»£p táº¡o load blancer nhÆ°ng khÃ´ng mua domain:

		HTTPS á»Ÿ Load Balancer (ALB/NLB/CLB)

			Khi báº¡n táº¡o Load Balancer vÃ  muá»‘n láº¯ng nghe HTTPS (port 443), AWS sáº½ yÃªu cáº§u báº¡n chá»n:

				SSL certificate (tá»« AWS Certificate Manager â€“ ACM, hoáº·c import tá»« ngoÃ i).

			Táº¡i sao certificate láº¡i cáº§n domain?

				Certificate SSL/TLS luÃ´n gáº¯n vá»›i má»™t tÃªn miá»n (VD: myapp.com, *.myapp.com).

				TrÃ¬nh duyá»‡t khi ngÆ°á»i dÃ¹ng truy cáº­p https://myapp.com sáº½ kiá»ƒm tra:

				Domain trong URL cÃ³ khá»›p vá»›i domain trong certificate khÃ´ng?

				Náº¿u khá»›p â†’ trÃ¬nh duyá»‡t hiá»ƒn thá»‹ á»• khÃ³a xanh ğŸ”’.

				Náº¿u khÃ´ng khá»›p â†’ hiá»‡n warning â€œConnection not secureâ€.

				VÃ¬ váº­y, certificate khÃ´ng thá»ƒ cáº¥p cho DNS máº·c Ä‘á»‹nh cá»§a AWS ELB (xxxxx.elb.amazonaws.com), trá»«
				khi báº¡n cÃ³ chá»©ng chá»‰ wildcard chÃ­nh thá»©c bao trÃ¹m domain Ä‘Ã³ (cÃ¡i nÃ y báº¡n khÃ´ng sá»Ÿ há»¯u Ä‘Æ°á»£c).

			Khi khÃ´ng cÃ³ domain

				Báº¡n váº«n cÃ³ thá»ƒ táº¡o Load Balancer vá»›i listener HTTP (port 80) â†’ truy cáº­p bÃ¬nh thÆ°á»ng qua DNS máº·c Ä‘á»‹nh.

				NhÆ°ng náº¿u chá»n HTTPS (443), AWS sáº½ báº¯t buá»™c báº¡n chá»n certificate.

				Báº¡n sáº½ khÃ´ng cÃ³ certificate há»£p lá»‡ náº¿u khÃ´ng sá»Ÿ há»¯u domain â†’ khÃ´ng thá»ƒ hoÃ n táº¥t HTTPS chuáº©n.

				CÃ³ thá»ƒ import self-signed certificate Ä‘á»ƒ test, nhÆ°ng trÃ¬nh duyá»‡t sáº½ luÃ´n bÃ¡o lá»—i báº£o máº­t.

	- Táº¡i sao báº¡n táº¡o record trong Route 53 Hosted Zone trá» tá»›i private IP cá»§a EC2 vÃ  khi nÃ o nÃªn
	lÃ m váº­y, kÃ¨m vÃ­ dá»¥ vÃ  best-practice:
	
		Ã nghÄ©a / má»¥c Ä‘Ã­ch

			DNS ná»™i bá»™ (Private DNS): Khi báº¡n táº¡o Private Hosted Zone (liÃªn káº¿t vá»›i VPC) vÃ  thÃªm A record trá»
			tá»›i private IP cá»§a EC2, cÃ¡c instance trong VPC sáº½ cÃ³ thá»ƒ resolve tÃªn (vÃ­ dá»¥ db01.internal.example.com)
			thÃ nh IP ná»™i bá»™ (10.x.x.x). DÃ¹ng cho giao tiáº¿p ná»™i bá»™ giá»¯a service mÃ  khÃ´ng lá»™ ra Internet.

			Dá»… quáº£n lÃ½ hÆ¡n: Thay vÃ¬ hardcode IP vÃ o config á»©ng dá»¥ng báº¡n dÃ¹ng tÃªn dá»… Ä‘á»c/transfer: mysql -h db01.internal.example.com.
			Khi pháº£i thay Ä‘á»•i instance, báº¡n chá»‰ cáº§n update DNS record, khÃ´ng pháº£i edit má»i config.

			Service discovery Ä‘Æ¡n giáº£n: DÃ¹ng tÃªn cá»‘ Ä‘á»‹nh cho DB, cache, mq, v.v. Ä‘á»ƒ cÃ¡c app tÃ¬m tá»›i service Ä‘Ã³.

			Split-horizon / Private vs Public: Báº¡n cÃ³ thá»ƒ cÃ³ má»™t zone public (cho internet) vÃ  má»™t private (cho VPC); private chá»‰
			giáº£i quyáº¿t trong VPC â€” an toÃ n hÆ¡n.

			Há»— trá»£ failover / swap: Náº¿u báº¡n thay instance báº±ng instance má»›i, Ä‘á»•i record sang IP má»›i â†’ á»©ng dá»¥ng chuyá»ƒn sang server má»›i
			nhanh hÆ¡n.

			TÃ­ch há»£p vá»›i ALB/ENI/Elastic IP: Thay vÃ¬ trá» trá»±c tiáº¿p IP cá»§a instance, thÆ°á»ng tá»‘t hÆ¡n lÃ  trá» tá»›i internal ALB (dÃ¹ng Alias record)
			hoáº·c dÃ¹ng ENI cá»‘ Ä‘á»‹nh Ä‘á»ƒ trÃ¡nh pháº£i update DNS khi instance thay Ä‘á»•i.

		VÃ­ dá»¥ thá»±c táº¿

			Táº¡o Private Hosted Zone internal.example.com liÃªn káº¿t vá»›i VPC vpc-abc123.

			Táº¡o A record:

				db01.internal.example.com â†’ 10.0.1.5

				cache.internal.example.com â†’ 10.0.1.6

			TrÃªn EC2 trong cÃ¹ng VPC: ping db01.internal.example.com â†’ sáº½ resolve thÃ nh 10.0.1.5.

		CÃ¡ch táº¡o (tÃ³m táº¯t)

			AWS Console â†’ Route 53 â†’ Hosted zones â†’ Create hosted zone.

				Type: Private hosted zone for Amazon VPC

				Enter domain: internal.example.com

				Associate with one or more VPCs.

			VÃ o zone â†’ Create record â†’ Type: A â†’ Name: db01 â†’ Value: 10.0.1.5 â†’ Save.

		LÆ°u Ã½ & best practices

			Private IP cÃ³ thá»ƒ Ä‘á»•i náº¿u báº¡n terminate instance; stop/start thÆ°á»ng giá»¯ private IPv4 chÃ­nh nhÆ°ng khÃ´ng tuyá»‡t
			Ä‘á»‘i (náº¿u báº¡n detach ENI hay thay instance thÃ¬ thay Ä‘á»•i). Äá»ƒ á»•n Ä‘á»‹nh:

				GÃ¡n Elastic Network Interface (ENI) vá»›i private IP tÄ©nh, hoáº·c

				DÃ¹ng internal load balancer (ALB/NLB) vÃ  táº¡o Alias record trá» tá»›i ALB (khÃ´ng pháº£i IP) â€” ALB xá»­ lÃ½ backend thay Ä‘á»•i.

			TTL: Ä‘áº·t TTL tháº¥p (vÃ­ dá»¥ 60s) náº¿u báº¡n hay thay Ä‘á»•i record, nhÆ°ng TTL tháº¥p tÄƒng sá»‘ query DNS.

			Tá»± Ä‘á»™ng hÃ³a: Náº¿u báº¡n autoscale, update DNS thá»§ cÃ´ng sáº½ báº¥t tiá»‡n â€” dÃ¹ng AWS Cloud Map / Service Discovery hoáº·c tá»±
			Ä‘á»™ng Ä‘Äƒng kÃ½ qua userdata / script / automation (Lambda) Ä‘á»ƒ cáº­p nháº­t record khi instance khá»Ÿi táº¡o/terminate.

			Sá»­ dá»¥ng SRV / CNAME khi phÃ¹ há»£p (vÃ­ dá»¥ SRV cho service discovery cÃ³ port).

			Báº£o máº­t: Private hosted zone chá»‰ tráº£ lá»i tá»« VPC Ä‘Ã£ liÃªn káº¿t â€” an toÃ n ná»™i bá»™.

			KhÃ´ng trá» public records tá»›i private IP (sáº½ khÃ´ng truy cáº­p Ä‘Æ°á»£c tá»« Internet).

		Khi nÃ o KHÃ”NG nÃªn dÃ¹ng trá»±c tiáº¿p private IP record

			Náº¿u báº¡n cÃ³ autoscaling hoáº·c thÆ°á»ng xuyÃªn thay instance â†’ nÃªn dÃ¹ng internal ALB hoáº·c Cloud Map.

			Náº¿u service cáº§n load-balancing hoáº·c health checks â†’ dÃ¹ng ALB/NLB thay vÃ¬ trá» tháº³ng vÃ o má»™t instance.
		
		
--- Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:

	Flow chÃ­nh:
	
		Users (NgÆ°á»i dÃ¹ng)

			NgÆ°á»i dÃ¹ng gá»­i request (HTTP/HTTPS) tá»« trÃ¬nh duyá»‡t â†’ vÃ­ dá»¥: https://myapp.com.

		Amazon Route 53

			ÄÃ¢y lÃ  DNS service.

			NÃ³ sáº½ phÃ¢n giáº£i tÃªn miá»n (myapp.com) â†’ IP/public endpoint cá»§a há»‡ thá»‘ng.

		Amazon CloudFront (CDN)

			GiÃºp cache ná»™i dung tÄ©nh (áº£nh, CSS, JS, video) gáº§n vá»›i ngÆ°á»i dÃ¹ng nháº¥t.

			LÃ m á»©ng dá»¥ng nhanh hÆ¡n, giáº£m táº£i cho backend.

		Application Load Balancer (ALB)

			Äiá»u phá»‘i request Ä‘áº¿n cÃ¡c EC2 instances trong Elastic Beanstalk.

			Äáº£m báº£o high availability (HA), scale theo táº£i.

		Elastic Beanstalk

			Dá»‹ch vá»¥ tá»± Ä‘á»™ng deploy vÃ  quáº£n lÃ½ á»©ng dá»¥ng.

			Trong hÃ¬nh, Beanstalk quáº£n lÃ½ má»™t Auto Scaling Group chá»©a nhiá»u EC2 cháº¡y Apache Tomcat (á»©ng dá»¥ng Java).

			Khi táº£i cao â†’ tá»± Ä‘á»™ng thÃªm EC2, khi táº£i tháº¥p â†’ giáº£m EC2.

		Amazon CloudWatch

			Theo dÃµi metric (CPU, memory, request count...).

			DÃ¹ng Ä‘á»ƒ trigger auto scaling trong Elastic Beanstalk.

		Artifacts in S3 Bucket

			Code (WAR file, artifact) Ä‘Æ°á»£c upload vÃ o S3 bucket.

			Elastic Beanstalk láº¥y artifact nÃ y Ä‘á»ƒ deploy vÃ o Tomcat.

		Apache Tomcat

			NÆ¡i á»©ng dá»¥ng Java thá»±c sá»± cháº¡y (xá»­ lÃ½ business logic).

			VÃ­ dá»¥: Spring Boot app hoáº·c Java EE app.

		Amazon MQ

			Message broker (giá»‘ng ActiveMQ).

			DÃ¹ng khi há»‡ thá»‘ng cÃ³ asynchronous communication giá»¯a cÃ¡c service.

			VÃ­ dá»¥: app gá»­i message vÃ o queue, service khÃ¡c xá»­ lÃ½ sau.

		MySQL (Amazon RDS)

			Database chÃ­nh, lÆ°u dá»¯ liá»‡u á»©ng dá»¥ng.

		Memcached

			Cache server Ä‘á»ƒ tÄƒng tá»‘c truy váº¥n (giáº£m táº£i cho MySQL).

			LÆ°u trá»¯ táº¡m thá»i dá»¯ liá»‡u hay dÃ¹ng (session, query result).
		
	- Command:
	
		- cmd: apt update && apt install mysql-client git -y
		- cmd: mysql -h vprofile-rds-reach.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pxUScYu55cFzH5NxM8Zc5 accounts < src/main/resources/db_backup.sql
		
--- Continuous Integration with Jenkins:

	Continuous Integration with Jenkins - Flow of Continuous Integration Pipeline:
	
		Developer (Git)

			Developer viáº¿t code vÃ  push lÃªn GitHub repository.

			ÄÃ¢y lÃ  nÆ¡i lÆ°u trá»¯ source code chÃ­nh.

		GitHub â†’ Jenkins

			Khi cÃ³ commit má»›i, Jenkins sáº½ Ä‘Æ°á»£c trigger (cÃ³ thá»ƒ qua Webhook).

			Jenkins báº¯t Ä‘áº§u pipeline.

		Fetch Code (Git)

			Jenkins sá»­ dá»¥ng Git plugin Ä‘á»ƒ láº¥y code tá»« GitHub vá» mÃ´i trÆ°á»ng build.

		Build (Maven)

			Jenkins dÃ¹ng Maven Ä‘á»ƒ compile source code vÃ  build project.

			Maven sáº½ quáº£n lÃ½ dependencies (thÆ° viá»‡n bÃªn ngoÃ i) vÃ  Ä‘Ã³ng gÃ³i project (VD: file .jar hoáº·c .war).

		Unit Test (Maven)

			Sau khi build xong, Maven cháº¡y Unit Test Ä‘á»ƒ Ä‘áº£m báº£o code hoáº¡t Ä‘á»™ng Ä‘Ãºng.

			Náº¿u test fail â†’ pipeline sáº½ dá»«ng.

		Code Analysis (SonarQube)

			Code Ä‘Æ°á»£c gá»­i sang SonarQube Ä‘á»ƒ phÃ¢n tÃ­ch cháº¥t lÆ°á»£ng:

				Bugs, vulnerabilities, code smells, duplicated code.

			SonarQube cÃ³ Quality Gate: náº¿u code khÃ´ng Ä‘áº¡t yÃªu cáº§u, pipeline dá»«ng láº¡i.

		Upload Artifact (Nexus OSS)

			Náº¿u code pass SonarQube, Jenkins upload artifact (jar/war) lÃªn Nexus Repository Manager (Sonatype).

			Nexus OSS lÃ  nÆ¡i lÆ°u trá»¯ artifacts, Ä‘á»ƒ dÃ¹ng cho cÃ¡c mÃ´i trÆ°á»ng khÃ¡c (staging, production).
			
	ChÃº Ã½ lÃ  Ä‘á»ƒ cháº¡y Ä‘Æ°á»£c Sonar Qube cáº§n thÃªm authentication token cá»§a nÃ³ vÃ o Jenkins má»›i cháº¡y Ä‘Æ°á»£c
	
	Google search:
	
		sonar scanner pipiline script
		
		nexusartifactuploader
		
		slack login
		
		wget for git hash how to add more utilities
		
	Git Weebhook, Poll SCM
	
	LDAP
	
	
	
	
		
	Continuous Integration with Jenkins - Má»¥c Ä‘Ã­ch cá»§a Jenkins URL:

		NÃ³ Ä‘Æ°á»£c Jenkins (vÃ  plugin) dÃ¹ng lÃ m Ä‘á»‹a chá»‰ chÃ­nh Ä‘á»ƒ:

			Sinh link Ä‘áº§y Ä‘á»§ trong thÃ´ng bÃ¡o
			
				VÃ­ dá»¥ báº¡n gá»­i Slack, Email, hay Webhook thÃ¬ Jenkins sáº½ táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº¿n job/build cá»¥ thá»ƒ.

					Náº¿u khÃ´ng cÃ³ Jenkins URL, thÃ´ng bÃ¡o cÃ³ thá»ƒ chá»‰ chá»©a Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i (vÃ­ dá»¥ /job/vprofile-pipeline/23/) â†’ ngÆ°á»i
					ngoÃ i sáº½ khÃ´ng click Ä‘Æ°á»£c.

					Khi báº¡n cáº¥u hÃ¬nh Jenkins URL (vÃ­ dá»¥ http://jenkins.mycompany.com:8080/), Jenkins sáº½ táº¡o link Ä‘áº§y Ä‘á»§:

						http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/


			Jenkins agents (slave nodes) káº¿t ná»‘i vá» master
			
				Má»™t sá»‘ trÆ°á»ng há»£p Jenkins agent cáº§n biáº¿t chÃ­nh xÃ¡c URL cá»§a master Ä‘á»ƒ trao Ä‘á»•i thÃ´ng tin, táº£i config, hoáº·c trigger build.

			CÃ¡c plugin phá»¥ thuá»™c

				Slack, Email-ext plugin, GitHub plugin, Bitbucket plugin... cáº§n Jenkins URL Ä‘á»ƒ táº¡o link "Build result" trong notification.

				VÃ­ dá»¥ Slack message cÃ³:

					Build #23 failed. More info at: http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/

	- Commands:
	
		- cmd: sudo apt install fontconfig openjdk-21-jre
		- cmd: sudo wget -O /etc/apt/keyrings/jenkins-keyring.asc \
				https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
		- cmd: echo "deb [signed-by=/etc/apt/keyrings/jenkins-keyring.asc]" \
				  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
				  /etc/apt/sources.list.d/jenkins.list > /dev/null
		- cmd: sudo apt install jenkins
		- cmd: systemctl status jenkins
		- cmd: ls /var/lib/jenkins/
		- cmd: cat /var/lib/jenkins/secrets/initialAdminPassword
		- cmd: apt install openjdk-17-jdk -y
		- cmd: ls /usr/lib/jvm/
		- cmd: sudo apt update && sudo apt install maven -y
		- cmd: systemctl status nexus
		- cmd: ls /opt/nexus/
		- cmd: cat /opt/nexus/sonatype-work/nexus3/admin.password
		- cmd: cd /var/lib/jenkins/
		- cmd: cd /var/lib/jenkins/workspace
		- cmd: systemctl restart jenkins
		- cmd: sudo snap install aws-cli --classic
		- cmd:  # Add Docker's official GPG key:
				sudo apt-get update
				sudo apt-get install ca-certificates curl
				sudo install -m 0755 -d /etc/apt/keyrings
				sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
				sudo chmod a+r /etc/apt/keyrings/docker.asc

				# Add the repository to Apt sources:
				echo \
				  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
				  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
		- cmd: systemctl status docker
		- cmd: docker images
		- cmd: su - jenkins
		- cmd: usermod -a -G docker jenkins
		- cmd: id jenkins
		- cmd: reboot
		- cmd: ssh-keygen.exe
		- cmd: ls ~/.ssh/
		- cmd: git@github.com:ngoctuanqng/jenkinstriggers.git
		- cmd: choco install wget
		- cmd: wget -q --auth-no-challenge --user username --password password --output-document - 'http://JENNKINS_IP:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
			Example:
		
				wget -q --auth-no-challenge --user admin --password ngoctuan99@ --output-document - 'http://54.234.134.51:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
		- cmd: curl -I -X POST http://username:APItoken@Jenkins_IP:8080/job/JOB_NAME/build?token=TOKENNAME
				-H "Jenkins-Crumb:CRUMB"
				
			Example:
		
				curl -I -X POST http://admin:11cde1b5cb464093f62e8f88edfd14388d@54.234.134.51:8080/job/Build/build?token=mybuildtoken
				-H "Jenkins-Crumb:b05d9f0d30cc94463982e15ef5dea44b39cddc6ed3a5a9254fef0bc151afe76b"
		
		- cmd: adduser devops
		- cmd: chown devops.devops /opt/jenkins-slave -R
		- cmd: vim /etc/ssh/sshd_config
		- cmd: systemctl restart ssh
		- cmd: id devops
		


		
		
		
		
		
		
		
		

	- Jenkins: Manage Jenkins -> Tools -> JDK installations -> Add JDK
	- Jenkins: Manage Jenkins -> Tools -> Maven installations
	- Jenkins: Manage Jenkins -> Tools -> SonarQube Scanner installations
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> S3 publisher
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Build Timestamp
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Maven Integration
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Utility Steps
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Nexus Artifact Uploader
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> SonarQube Scanner
	- Jenkins: Manage Jenkins -> System -> Build Timestamp
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Add SonarQube
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Environment variables
	- Jenkins: Manage Jenkins -> System -> Slack
	- Jenkins: Manage Jenkins -> Credentials -> Stores scoped to Jenkins -> System -> Global credentials (unrestricted) -> Add Credentials
	- Jenkins: Manage Jenkins -> Nodes -> New node
	- Jenkins: Manage Jenkins -> Security -> Git Host Key Verification Configuration -> Accept first connection
	- Jenkins: New Item -> Freestyle project -> Source Code Management -> Git
	- Jenkins: New Item -> Freestyle project -> Build Steps -> Invoke top-level Maven targets
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Archive the artifacts
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Publish artifacts to S3 Bucket
	- Jenkins: New Item -> Freestyle project -> Copy from
	- Jenkins: New Item -> Pipeline -> Pipeline script
	- Jenkins: New Item -> Pipeline -> Pipeline script from SCM
	- Jenkins: New Item -> General -> This project is parameterized -> String parameter
	- Jenkins: Jenkins instance -> Build Now	
	- Jenkins: Jenkins instance -> Configure
	- Jenkins: Jenkins instance -> Workspace
	- Jenkins: Jenkins instance -> Stages
	- Jenkins: Jenkins result -> Workspaces	
	- AWS: EC2 -> Instances -> Instance state -> Reboot instance
	- AWS: Amazon Elastic Container Registry
	- AWS: Amazon Elastic Container Service -> Clusters -> Create cluster
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Create
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Update service
	- AWS: Amazon Elastic Container Service -> Task definitions -> Create new task definition
	- Nexus: Browse
	- SonarQube -> Project -> Project instance
	- SonarQube -> Project -> Project instance -> Project setting -> Quality gate
	- SonarQube -> Project -> Project instance -> Project setting -> Webhooks
	- SonarQube -> Quality Gates -> create
	- SonarQube -> Quality Gates -> Quality Gates instance -> Unlock editing -> Add condition
	- Nexus -> Configuration -> Repositories -> Create Repository -> maven2(hosted)
	- Nexus -> Browse -> instance
	- Github -> Settings -> SSH and GPG keys -> New SSH key
	- Github -> Project instance -> Settings -> Webhooks -> Add webhook
		
	- Code trong Execute Shell:
	
		Case 1:
		
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_ID.war
			
		Case 2:
		
			mkdir -p versions
			cp target/vprofile-v2.war version/vpro$VERSION.war

		Case 3:
					
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_TIMESTAMP.war
	
	- Code Jenkinsfile:

		Case 1:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Archiving artifact"
								archiveArtifacts artifacts: '**/*.war'
							}
						}
					}
				}
			}

		Case 2:
		
			def COLOR_MAP = [
				'SUCCESS': 'good',
				'FAILURE': 'danger',
			]

			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Now archiving it..."
								archiveArtifacts artifacts: '**/target/*.war'
							}
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage('Sonar Code Analysis') {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
							withSonarQubeEnv('sonarserver') {
								sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
								-Dsonar.projectName=vprofile \
								-Dsonar.projectVersion=1.0 \
								-Dsonar.sources=src/ \
								-Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
								-Dsonar.junit.reportsPath=target/surefire-reports/ \
								-Dsonar.jacoco.reportsPath=target/jacoco.exec \
								-Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
							}
						}
					}

					stage("Quality Gate") {
					  steps {
						timeout(time: 1, unit: 'HOURS') {
						  waitForQualityGate abortPipeline: true
						}
					  }
					}
					stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
								nexusVersion: 'nexus3',
								protocol: 'http',
								nexusUrl: '172.31.30.253:8081',
								groupId: 'QA',
								version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
								repository: 'vprofile-repo',
								credentialsId: 'nexuslogin',
								artifacts: [
									[artifactId: 'vproapp',
									classifier: '',
									file: 'target/vprofile-v2.war',
									type: 'war']
								]
							)
						}
					}
				}

				post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}
			}

		Case 3:
		
			def COLOR_MAP = [
				'SUCCESS': 'good', 
				'FAILURE': 'danger',
			]
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}

				stages {

					stage('test slack'){
						steps{
							sh 'NotARealCommand'

						}
					}
					stage('Fetch code') {
						steps {
						   git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar6.2'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					 stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
							  nexusVersion: 'nexus3',
							  protocol: 'http',
							  nexusUrl: '172.31.25.14:8081',
							  groupId: 'QA',
							  version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
							  repository: 'vprofile-repo',
							  credentialsId: 'nexuslogin',
							  artifacts: [
								[artifactId: 'vproapp',
								 classifier: '',
								 file: 'target/vprofile-v2.war',
								 type: 'war']
							  ]
							)
						}
					}


				}

			  post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}

			}
			
		Case 4:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					// stage('Remove Container Images'){
					//     steps{
					//         sh 'docker rmi -f $(docker images -a -q)'
					//     }
					// }

			  }
			}
			
		Case 5:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
					cluster = "vprofile"
					service = "vprofileappsvc"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					stage('Remove Container Images'){
						steps{
							sh 'docker rmi -f $(docker images -a -q)'
						}
					}


					stage('Deploy to ecs') {
					  steps {
						withAWS(credentials: 'awscreds', region: 'us-east-1') {
						sh 'aws ecs update-service --cluster ${cluster} --service ${service} --force-new-deployment'
						   }
					  }
					}

			  }
			}
			
	Dockerfile code:
	
		Case 1:
		
			FROM tomcat:10-jdk21
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webapps
			
			
--- Python:

	Google search:
	
		python jenkins library
		
		boto3 python

	- Commands:
	
		- cmd: python
		- cmd: ls -l first-python-code.py
		- cmd: ./first-python-code.py
		- cmd: python first-python-code.py
		- cmd: yum search python3
		- cmd: yum install python3 -y
		- cmd: wget https://bootstrap.pypa.io/get-pip.py
		- cmd: python3 get-pip.py
		- cmd: dnf install python3-pip -y
		- cmd: pip install 'fabric<2.0'
		- cmd: export PATH=$PATH:/usr/local/bin
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i ~/.ssh/id_rsa devops@web01
		- cmd: ssh devops@web01
		- cmd: fab -H web01 -u devops remote_exec
		- cmd: fab -H web01 -u devops web_setup:https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip,2136_kool_form_pack
		


		
		
		
		
		
		
	
	
	- Python code:
	
		Case 1:
		
			print("""
			xin chao
			cac ban
			""")
			
		Case 2:
		
			print('''
			xin chao
			cac ban
			''')
			
		Case 3:

			a = b = c = 65

			print(a)
			print(b)
			print(c)		
			
		Case 4:

			a = 65

			print("a: ",a)		
			
		Case 5:

			w, x, y, z = "alpha", "beta", 12, 5.4

			print("Variable w value is ", w)
			print("Variable w value is ", x)
			print("Variable w value is ", y)
			print("Variable w value is ", z)

			print(type(w))		
			
		Case 6:

			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_list = [str1, "DevOps", 47, num1, 1.5]
			print(first_list)		
			
			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_tuple = (str1, "DevOps", 47, num1, 1.5)
			print(first_tuple)			
			
		Case 7:
		
			user_skill = input("Enter your desired skill: ")
			print(user_skill)
			
		Case 8:
		
			import time
			time.sleep(2)
			print("test")
			
		Case 9:

			def time_activity(*args, **kwargs):
				print(args)
				print(kwargs)
			time_activity(10, 20, 10, hobby="Dance", sport="Boxing", fun="Driving", work="DevOps")
				
		Case 10:
		
			from fabric.api import *

			def greetings(msg):
				print("Good {}".format(msg))

			def system_info():
				print("Disk space.")
				local("df -h")

				print("Memory info.")
				local("free -m")

				print("System Uptime.")
				local("uptime")

			def remote_exec():
				run("hostname")
				run("uptime")
				run("df -h")
				run("free -m")
				
				sudo("yum install unzip zip wget -y")
				
			def web_setup(WEBURL, DIRNAME):
				print("###############################################")
				print("Installing dependencies")
				print("###############################################")
				sudo("yum install httpd wget unzip -y")

				print("###############################################")
				print("Start & enable service.")
				sudo("systemctl start httpd")
				sudo("systemctl enable httpd")

				print("###############################################")
				local("apt install zip unzip -y")

				print("###############################################")
				print("Downloading and pushing website to webservers.")
				print("###############################################")
				local("wget -O website.zip %s" % WEBURL)
				local("unzip -o website.zip")

				print("###############################################")
				with lcd(DIRNAME):
					local("zip -r tooplate.zip * ")
					put("tooplate.zip", "/var/www/html/", use_sudo=True)

				with cd("/var/www/html/"):
					sudo("unzip -o tooplate.zip")

				sudo("systemctl restart httpd")
				print("Website setup is done.")



	Code cá»§a file /etc/ssh/sshd_config:
	
		Case 1:
		
			PasswordAuthentication yes
			
	Code cá»§a visudo:
	
		Case 1:
		
			devops  ALL=(ALL)       NOPASSWD: ALL

			
--- Learn Terraform:

	- Commands:
	
		- cmd: choco install terraform
		- cmd: terraform --version
		- cmd: terraform fmt
		- cmd: terraform init
		- cmd: terraform validate
		- cmd: terraform plan
		- cmd: terraform apply
		- cmd: terraform destroy
		
	Amazon S3 -> Buckets -> instance -> Create folder
		
		
		
		
	- Google search:
	
		find ubuntu ami id aws
		
		terraform aws
		
		variable terraform
		
		terraform provisioners
		
	- CÃ i Ä‘áº·t VS Code extension:
	
		HashiCorp Terraform
		
	terraform.tfstate:
	
	File terraform.tfstate lÃ  trÃ¡i tim cá»§a Terraform â€” nÃ³ chá»©a toÃ n bá»™ tráº¡ng thÃ¡i thá»±c táº¿ cá»§a háº¡
	táº§ng mÃ  Terraform Ä‘ang quáº£n lÃ½.
	
	Äá»‹nh nghÄ©a cá»‘t lÃµi

		terraform.tfstate lÃ  file tráº¡ng thÃ¡i (state file) mÃ  Terraform dÃ¹ng Ä‘á»ƒ ghi nháº­n vÃ  theo dÃµi háº¡ táº§ng
		thá»±c táº¿ sau khi báº¡n cháº¡y terraform apply.

			NÃ³ mÃ´ táº£ má»i resource (mÃ¡y áº£o, VPC, subnet, load balancer, v.v.) mÃ  Terraform Ä‘Ã£ táº¡o,

			VÃ  liÃªn káº¿t chÃºng vá»›i code Terraform (.tf files).

		Terraform dÃ¹ng file nÃ y Ä‘á»ƒ:

			Biáº¿t hiá»‡n tráº¡ng háº¡ táº§ng Ä‘ang ra sao,

			So sÃ¡nh vá»›i mong muá»‘n (trong code),

			Tá»« Ä‘Ã³ quyáº¿t Ä‘á»‹nh táº¡o / sá»­a / xÃ³a gÃ¬ khi báº¡n cháº¡y terraform plan hoáº·c terraform apply.
			
	Báº£n cháº¥t & cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng
	
		Khi báº¡n cháº¡y terraform apply

			Terraform Ä‘á»c code .tf (infrastructure as code).

			NÃ³ kiá»ƒm tra file terraform.tfstate Ä‘á»ƒ biáº¿t Ä‘ang cÃ³ gÃ¬ ngoÃ i Ä‘á»i tháº­t.

			NÃ³ gá»i API lÃªn cloud (AWS, GCP, Azure, v.v.) Ä‘á»ƒ xÃ¡c minh.

			Sau khi thá»±c hiá»‡n xong thay Ä‘á»•i, Terraform ghi láº¡i káº¿t quáº£ má»›i vÃ o terraform.tfstate.
			
	ThÃ nh pháº§n chÃ­nh trong terraform.tfstate

		Má»™t file tfstate lÃ  JSON, gá»“m cÃ¡c pháº§n chÃ­nh:
		
			version	Version format cá»§a Terraform state
			
			terraform_version	PhiÃªn báº£n Terraform Ä‘Ã£ táº¡o file
			
			resources	Danh sÃ¡ch toÃ n bá»™ resource Ä‘ang quáº£n lÃ½
			
			outputs	CÃ¡c giÃ¡ trá»‹ output (náº¿u báº¡n cÃ³ output trong code)
			
			serial	Sá»‘ version ná»™i bá»™ Ä‘á»ƒ Terraform biáº¿t state Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t bao nhiÃªu láº§n
			
	Local state vs Remote state:
	
		Local state:

			Máº·c Ä‘á»‹nh terraform.tfstate náº±m ngay trong thÆ° má»¥c dá»± Ã¡n.

			Dá»… dÃ¹ng nhÆ°ng nguy hiá»ƒm trong team (vÃ¬ cÃ³ thá»ƒ bá»‹ ghi Ä‘Ã¨, máº¥t file, xung Ä‘á»™t...).

		Remote state (an toÃ n hÆ¡n):

			Terraform cho phÃ©p lÆ°u state tá»« xa (remote backend), vÃ­ dá»¥:

				AWS S3

				Azure Blob Storage

				Google Cloud Storage

				Terraform Cloud / Enterprise
				
	LÆ°u Ã½ báº£o máº­t cá»±c ká»³ quan trá»ng

		File terraform.tfstate cÃ³ thá»ƒ chá»©a thÃ´ng tin nháº¡y cáº£m, nhÆ°:

			Máº­t kháº©u, secret key, private IP, token, v.v.

		KhÃ´ng bao giá» commit terraform.tfstate lÃªn Git.
		
			HÃ£y thÃªm vÃ o .gitignore
		
	- Code file .tf:
	
		Case 1:
		
			data "aws_ami" "amiID" {
			  most_recent = true

			  filter {
				name   = "name"
				values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
			  }

			  filter {
				name   = "virtualization-type"
				values = ["hvm"]
			  }

			  owners = ["099720109477"]
			}
		
		Case 2:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = "us-east-1"
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "42.112.134.249/32"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
		Case 3:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = "dove-key"
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}

				resource "aws_ec2_instance_state" "web_state" {
					instance_id = aws_instance.web.id
					state       = "running"
				}
				
		Case 4:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:
			
				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					es-east-1 = "ami-0e2c8caa4b6378d8c"
				  }
				}
				
		Case 5:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }

				  provisioner "file" {
					source      = "web.sh"
					destination = "/tmp/web.sh"
				  }

				  connection {
					type        = "ssh"
					user        = var.webuser
					private_key = file("dovekey")
					host        = self.public_ip
				  }

				  provisioner "remote-exec" {

					inline = [
					  "chmod +x /tmp/web.sh",
					  "sudo /tmp/web.sh"
					]
				  }

				  provisioner "local-exec" {
					command = "echo ${self.private_ip} >> private_ips.txt"
				  }
				}

				resource "aws_ec2_instance_state" "web-state" {
				  instance_id = aws_instance.web.id
				  state       = "running"
				}

				output "WebPublicIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.public_ip
				}

				output "WebPrivateIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.private_ip
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}

			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "webuser" {
				  default = "ubuntu"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					us-east-1 = "ami-0360c520857e3138f"
				  }
				}
				
			web.sh:
			
				#!/bin/bash
				apt update
				apt install wget unzip apache2 -y
				systemctl start apache2
				systemctl enable apache2
				wget https://www.tooplate.com/zip-templates/2117_infinite_loop.zip
				unzip -o 2117_infinite_loop.zip
				cp -r 2117_infinite_loop/* /var/www/html/
				systemctl restart apache2
				
		Case 6:
		
			Giá»‘ng case 5 nhÆ°ng thÃªm file backend.tf
			
			backend.tf:
			
				terraform {
					backend "s3" {
						bucket = "terraformstate3245612112"
						key    = "terraform/backend"
						region = "us-east-1"
					}
				}
				
				
--- Ansible:

	- Commands:
	
		- cmd: ls ~/.ssh/known_hosts
		- cmd: cat ~/.ssh/known_hosts
		- cmd: cat /dev/null > ~/.ssh/known_hosts
		- cmd: sudo apt install software-properties-common
		- cmd: sudo add-apt-repository --yes --update ppa:ansible/ansible
		- cmd: sudo apt install ansible
		- cmd: ansible --version
		- cmd: ansible-config init --disable -t all > ansible.cfg
		- cmd: chmod 400 clientkey.pem
		- cmd: ansible web01 -m ping -i inventory
		- cmd: ansible all -m ping -i inventory
		- cmd: ansible '*' -m ping -i inventory
		- cmd: ansible 'web*' -m ping -i inventory
		- cmd: ansible web01 -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=absent" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.service -a "name=httpd state=started enabled=yes" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.copy -a "src=index.html dest=/var/www/html/index.html" -i inventory --become
		- cmd: ansible webservers -m yum -a "name=httpd state-absent" -i inventory --become
		- cmd: ansible-playbook -i inventory web-db.yaml
		- cmd: ansible-playbook -i inventory web-db.yaml -v
		- cmd: ansible-playbook -i inventory web-db.yaml -vv
		- cmd: ansible-playbook -i inventory web-db.yaml -vvvv
		- cmd: ansible-playbook -i inventory web-db.yaml --syntax-check
		- cmd: ansible-playbook -i inventory web-db.yaml -C
		- cmd: ssh -i clientkey.pem ec2-user@172.31.20.155
		- cmd: yum search python | grep -i mysql
		- cmd: ansible-galaxy collection install community.mysql
		- cmd: vim /etc/ansible/ansible.cfg
		- cmd: ansible-playbook db.yaml
		- cmd: sudo touch /var/log/ansible.log
		- cmd: sudo chown ubuntu.ubuntu /var/log/ansible.log
		- cmd: cat /var/log/ansible.log
		- cmd: ansible-playbook db.yaml -vv
		- cmd: ansible-playbook -e USRNM=cliuser -e COMM=cliuser vars_precedence.yaml
		- cmd: ansible -m setup web01
		- cmd: ansible-playbook print_facts.yaml
		- cmd: rm -rf print_facts.yaml vars_precedence.yaml
		- cmd: cat /etc/chrony.conf
		- cmd: cat /etc/ntpsec/ntp.conf
		- cmd: apt  install tree  # version 2.1.1-2
		- cmd: tree
		- cmd: ansible-galaxy init post-install
		- cmd: vim roles/post-install/vars/main.yml
		- cmd: vim roles/post-install/handlers/main.yml
		- cmd: vim roles/post-install/tasks/main.yml
		- cmd: ansible-galaxy role install geerlingguy.java
		- cmd: cd .ansible/
		- cmd: export AWS_ACCESS_KEY_ID='AK123'
		- cmd: export AWS_SECRET_ACCESS_KEY='abc123'
		- cmd: vim .bashrc
		- cmd: source .bashrc
		- cmd: sudo apt install python3-pip -y
		- cmd: ansible-galaxy collection install amazon.aws
		- cmd: ansible-galaxy collection install amazon.aws --force
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	- Google search:
	
		ansible installing
		
		ansible inventory
		
		ansible to ad hoc command
		
		ansible playbooks
		
		using ansible modules and plugins
		
		mysql ansible modules
		
		ansible configuration file
		
		ansible variables
		
		ansible when condition
		
		ansible loops
		
		ansible files module
		
		ntp server in oregon
		
		ansible handler
		
		ansible roles
		
		ntp servers in india
		
		ansible galaxy
		
		ansible amazon aws
		
		ansible modules
		
	- Code trong file vprofile/excercise1/inventory:
	
		Case 1:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				web02:
				  ansible_host: 172.31.16.152
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				db01:
				  ansible_host: 172.31.20.155
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem

			  children:
			    webservers:
				  hosts:
				    web01:
				    web02:
			    dbservers:
				  hosts:
				    db01:
			    dc_oregon:
				  children:
				    webservers:
				    dbservers:
					
		Case 2:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				web02:
				  ansible_host: 172.31.16.152
				db01:
				  ansible_host: 172.31.20.155

			  children:
				webservers:
				  hosts:
					web01:
					web02:
				dbservers:
				  hosts:
					db01:
				dc_oregon:
				  children:
					webservers:
					dbservers:
				  vars:
					ansible_user: ec2-user
					ansible_ssh_private_key_file: clientkey.pem




	- Ná»™i dung trong file clientkey.pem:

		Case 1:
		
			Chá»©a private key cá»§a web01
		
			-----BEGIN RSA PRIVATE KEY-----
			MIIEogIBAAKCAQEAreGWVicfl8jhOLqPUerGay/AThriECzXc8UcLLam4riCz05+
			TzZP+NeoX6dC83ZX4ssyLazcGj5OMCg2EhFqVZTuDABYsr+ucT/sdXPCP0bjf+mq
			SYT6iHpftoRfm1vk3Um61/HmSzp1vZ6YhE//x1GwWT5+4aOufGS4bCa5xPluEg+E
			IhaEZNRyNfgvc/ylblWVCM1KGkZgdhvenFUQDj8+icCWrvbFvykwW0L8D2WB+zFq
			iSDx1cKLL1zzVCQVgKfKofC3PPm+VMbwOL8HPw65g/OBnzWKrrSjrpWu5s4KKSM+
			3BXW5nRieoPI34ruAwwpYrJKUlR7t6/rBo6QgQIDAQABAoIBACLWvzN11UupMQ8X
			uh2Up7rUL3i2xDKveV+1z6ZZ1mg4xeTZek9Ot4lJVHAN6Ek1nfhP9DbYmqUbdLkL
			ZYILQT3ygBuheiQeacpBH5SM5A+fmXeIjtj6LuRneIPuU+Wh7OI1op0f15+dD/g1
			LaPdD4eVI3tOHUgCbrR3zcfFnpULfGl0+QdkZ7141FmMiAL+8tTjzfWbMYwbPWu2
			ZSIDWx7SZ/wycGDKBejmgJQHFyEn9Cwvexlt+Zh4IK/kTIXXe9XM0ZbVMF8am5D6
			BhSdD7JOB7OVWSghIZFNegtTdPS/+LI7wJj2y3O5KP2QJUGfSvjKMpdpBffoLwV0
			15soOu0CgYEA04abNpRYQbkd21R3F2sN5td2UTE5M31iaxnZXQrGyGCQUdIawb5n
			XNIvIVoH6dV3qFOrx9lmYQOV3B5kQNWPyfOI6q/tWXpsVV9WeUjXhki3TinRDGYq
			FaNlnRh5IvCtwbGIZpnfay4j8UCw8KgnpAqiz45TBQ/YqUR0MCZiPecCgYEA0nDB
			yhsFau6cpNUgcSjeGTCQpCTCckvFusa671x6gjOXC8gxBVzpCcSXxUzXlLANhkmU
			CZFaUG/hwh0FKqAItg0yEhNz7mnYv5Y/w0o5zycqF2QYlB1L6ph/LHh8z82OgQBN
			D0MQEHgkNjejLkaLf080KKYxqZE0d/oWucOxYVcCgYB4gT77oReGmceApGYUWVDa
			KfWl270SsGPZUCic8P6+OQT/GAtWRPrtznA7N+c6N/qrUr+SYzAIJNrDRC0pIoGA
			M9XUndVCHJSLLn09K1pdjh+f0ALgZXOkUCobjU21shfLOTDUAuVdUjP3xTsIX0P2
			GHkYdaSmRZjRFcZ7h+KAEQKBgFjvgGbarpp3h0n+LHzGab65kJdeVbMaJNF/xWb9
			bWTzSqWXEGiU0IPpSr7+b6mOEdkr5V15yXJvJjj0LMfL5IKT5xJOmFMs9oZZiE8P
			YokSoy5Jhj2qd/gIRM7ViOIFnHEWYHrPu81KCPvE3bjj5XaDUabQPfLMxCDkV5Bg
			jOl3AoGAP8u9/AnJsH+7k83fnBQvPHPkoEQ1PtMyclUnWjGbM5BUDdUi0beN1Kme
			PqUDarWVNfwXh4WsTmrmlUe41dInmnmHwf+Cfsn8f02eMV5YnxlUEDfcaOjnJ9+h
			VNXeYgsaoGFp6LiIj70I1X1WkegOT85sfuHuV5tYAZVlF8yzbxI=
			-----END RSA PRIVATE KEY-----
		
	- Ná»™i dung cá»§a file ansible.cfg:

		Case 1:
		
			host_key_checking=False
			
	- Ná»™i dung cá»§a file web-db.yaml:
	
		Case 1:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present
					
		Case 2:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
					
	- Ná»™i dung file db.yaml:
	
		Case 1:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present

		Case 2:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

		Case 3:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: vprofile
					password: 'admin943'
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 4:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 5:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
				    msg: " The dbname is {{dbname}}"

				- debug:
				    var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 6:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
					msg: " The dbname is {{dbname}}"

				- debug:
					var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
				  register: dbout

				- name: print dbout variable
				  debug:
					var: dbout


					
	- Ná»™i dung file ansible.cfg:
	
		Case 1:
		
			[defaults]
			host_key_checking=False
			inventory = ./inventory
			forks = 5
			log_path = /var/log/ansible.log

			[privilege_escalation]
			become=True
			become_method=sudo
			become_ask_pass=False

	- Ná»™i dung gile vars_precedence.yaml:
	
		Case 1:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"

		Case 2:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"
				  register: usrout

				- debug:
					var: usrout

		Case 3:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
					USRNM: playuser
					COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment
						
		Case 4:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  gather_facts: False
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment

						
	- Ná»™i dung file group_vars/all:
	
		Case 1:
		
			group_vars/all:
			
				dbname: sky
				dbuser: pilot
				dbpass: aircraft
				
			db.yaml:
			
				- name: DBserver setup
				  hosts: dbservers
				  become: yes
					#vars:
					#dbname: electric
					#dbuser: current
					#dbpass: tesla
				  tasks:
					- debug:
						msg: " The dbname is {{dbname}}"

					- debug:
						var: dbuser

					- name: Install mariadb-server
					  ansible.builtin.yum:
						name: mariadb-server
						state: present

					- name: Install pymysql
					  ansible.builtin.yum:
						name: python3-PyMySQL
						state: present

					- name: Start mariadb service
					  ansible.builtin.service:
						name: mariadb
						state: started
						enabled: yes

					- name: Create a new database with name 'accounts'
					  community.mysql.mysql_db:
						name: "{{dbname}}"
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock

					- name: Create database user with name 'vprofile'
					  community.mysql.mysql_user:
						name: "{{dbuser}}"
						password: "{{dbpass}}"
						priv: '*.*:ALL'
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock
					  register: dbout

					- name: print dbout variable
					  debug:
						var: dbout

		Case 2:
		
			group_vars/all:
			
				USRNM: commonuser
				COMM: variable from groupvars_all file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
		Case 3:
		

			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org


	
	- Ná»™i dung file group_vars/webservers:
	
		Case 1:
		
			group_vars/webservers:
			
				USRNM: webgroup
				COMM: variable from group_vars/webservers file

			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
	- Ná»™i dung file host_vars/web02:
	
		Case 1:
		
			host_vars/web02:
		
				USRNM: web02user
				COMM: variables from host_vars/web02 file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
		
	- Ná»™i dung file print_facts.yaml:
	
		Case 1:
		
			- name: Print facts
			  hosts: all
				#gather_facts: False
			  tasks:
				- name: Print OS name
				  debug:
					var: ansible_distribution
				- name: Print selinux mode
				  debug:
					var: ansible_selinux.mode
				- name: Print RAM memory
				  debug:
					var: ansible_memory_mb.real.free

				- name: Print Processor name
				  debug:
					var: ansible_processor[2]

	- Ná»™i dung file provisioning.yaml:
	
		Case 1:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
					
		Case 2:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd
					
		Case 3:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd


				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"

				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory
					
		Case 4:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

		Case 5:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  vars:
				mydir: /opt/dir22
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: "{{item}}"
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: "{{item}}"
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: "{{mydir}}"
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

				- name: Dump file
				  copy:
					files: files/myfile.txt
					dest: /tmp/myfile.txt

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"
				  
		Case 6:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- post-install
				
		Case 7:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org

		Case 8:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- geerlingguy.java
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org




	- Code file templates/ntpconf_centos:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst
			
	- Code file templates/ntpconf_ubuntu:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst

	- Code file roles/post-install/tasks/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# tasks file for post-install
			- name: Install ntp agent on centos
			  yum:
				name: "{{item}}"
				state: present
			  when: ansible_distribution == "CentOS"
			  loop:
				- chrony
				- wget
				- git
				- zip
				- unzip

			- name: Install ntp agent on Ubuntu
			  apt:
				name: "{{item}}"
				state: present
				update_cache: yes
			  when: ansible_distribution == "Ubuntu"
			  loop:
				- ntp
				- wget
				- git
				- zip
				- unzip


			- name: Start service on centos
			  service:
				name: chronyd
				state: started
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: Start service on ubuntu
			  service:
				name: ntp
				state: started
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

			- name: Banner file
			  copy:
				content: '# This server is managed by ansible. No manual changes please.'
				dest: /etc/motd

			- name: Create a folder
			  file:
				path: "{{mydir}}"
				state: directory

			- name: Deploy ntp agent conf on centos
			  template:
				src: ntpconf_centos.j2
				dest: /etc/chrony.conf
				backup: yes
			  when: ansible_distribution == "CentOS"
			  notify:
				- reStart service on centos

			- name: Deploy ntp agent conf on ubuntu
			  template:
				src: ntpconf_ubuntu.j2
				dest: /etc/ntpsec/ntp.conf
				backup: yes
			  when: ansible_distribution == "Ubuntu"
			  notify:
				- reStart service on ubuntu

			- name: Dump file
			  copy:
				src: myfile.txt
				dest: /tmp/myfile.txt

	- Code file roles/post-install/handlers/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# handlers file for post-install
			- name: reStart service on centos
			  service:
				name: chronyd
				state: restarted
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: reStart service on ubuntu
			  service:
				name: ntp
				state: restarted
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

	- Code file roles/post-install/vars/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# vars file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file roles/post-install/defaults/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# defaults file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file .bashrc:
	
		Case 1:
		
			if ! shopt -oq posix; then
			  if [ -f /usr/share/bash-completion/bash_completion ]; then
				. /usr/share/bash-completion/bash_completion
			  elif [ -f /etc/bash_completion ]; then
				. /etc/bash_completion
			  fi
			fi

			export AWS_ACCESS_KEY_ID='AK123'
			export AWS_SECRET_ACCESS_KEY='abc123'
			
	- Code file test-aws.yml:
	
		Case 1:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					
		Case 2:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout
					
		Case 3:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 4:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 5:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-west-2
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

				- name: start an instance
				  amazon.aws.ec2_instance:
					name: "public-compute-instance"
					key_name: "sample"
					  #vpc_subnet_id: subnet-5ca1ab1e
					instance_type: t3.micro
					security_group: default
					  #network_interfaces:
					  #- assign_public_ip: true
					image_id: ami-0caa91d6b7bee0ed0
					exact_count: 1
					region: us-west-2
					tags:
					  Environment: Testing


--- AWS Part -2:

	- VPC -> Your VPCs -> Create VPC
	- VPC -> Subnets -> Create subnet
	- VPC -> Subnets -> instance -> Route table
	- VPC -> Internet gateways -> Create internet gateway
	- VPC -> Internet gateways -> instance -> Actions -> Attach to VPC
	- VPC -> Route tables -> instance -> Routes
	- VPC -> Route tables -> instance -> Subnet associations -> Edit subnet associations
	- VPC -> Route tables -> instance -> Create route table
	- VPC -> Elastic IPs -> Allocate Elastic IP address
	- VPC -> NAT gateways -> Create NAT gateway
	- VPC -> Peering connections -> Create peering connection
	- VPC -> Peering connections -> Actions -> Accept request
	- IAM -> Roles -> Create role
	- EC2 -> Instances -> instance -> Actions -> Security -> Modify IAM role
	- CloudWatch -> Logs -> Log groups -> Log streams
	- CloudWatch -> Logs -> Metric filters -> Create metric filter
	
	
	
	- Google search:
	
		Terraform AWS VPC
		
		Terraform registry
		
		access logs loadbalancer
	
	- Commands:
	
		- cmd: scp -i vpro-bastion-key.pem web-key.pem ubuntu@54.177.221.62:/home/ubuntu/
		- cmd: chmod 400 web-key.pem
		- cmd: yum install httpd wget unzip -y
		- cmd: wget https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip
		- cmd: tail -f access_log
		- cmd: tar czvf wave-web01-httpdlogs19122020.tar.gz *
		- cmd: cat /dev/null > access_log error_log
		- cmd: ls -ltr /tmp/logs-wave/
		- cmd: yum install awscli -y
		- cmd: aws s3 ls
		- cmd: aws s3 sync /tmp/logs-wave/ s3://wave-web-logs-111222/
		- cmd: yum install awslogs -y
		- cmd: sudo yum install amazon-cloudwatch-agent -y
		- cmd: rm -rf .aws/credentials
		- cmd: systemctl restart awslogsd
		- cmd: systemctl enable awslogsd
		- cmd: sudo systemctl status awslogsd
		- cmd: sudo cat /etc/awslogs/awslogs.conf
		- cmd: service awslogsd restart
		- cmd: cat /var/log/awslogs.log
		- cmd: vim /etc/awslogs/awslogs.conf
		
		
		
		
		
		
		
		
	
	AWS Part -2 - MÃ´ hÃ¬nh mÃ´ phá»ng má»™t â€œCorporate Datacenterâ€ (trung tÃ¢m dá»¯ liá»‡u doanh nghiá»‡p
	truyá»n thá»‘ng), dÃ¹ng Ä‘á»ƒ so sÃ¡nh vá»›i Amazon VPC (Virtual Private Cloud) trong AWS:

		Tá»•ng quan

			HÃ¬nh nÃ y mÃ´ táº£ má»™t trung tÃ¢m dá»¯ liá»‡u váº­t lÃ½ (on-premises), nÆ¡i doanh nghiá»‡p quáº£n lÃ½ háº¡ táº§ng máº¡ng, mÃ¡y chá»§, router, switch,...

				ToÃ n bá»™ há»‡ thá»‘ng Ä‘Æ°á»£c chia nhá» thÃ nh nhiá»u subnet â€” tÆ°Æ¡ng tá»± nhÆ° cÃ¡ch AWS chia VPC thÃ nh subnet.

				CÃ¡c subnet nÃ y káº¿t ná»‘i vá»›i nhau qua router trung tÃ¢m.

				Network ACL (Access Control List) Ä‘Æ°á»£c Ä‘áº·t giá»¯a router vÃ  subnet Ä‘á»ƒ kiá»ƒm soÃ¡t lÆ°u lÆ°á»£ng.

		CÃ¡c thÃ nh pháº§n chÃ­nh trong sÆ¡ Ä‘á»“
		
			Router (bá»™ Ä‘á»‹nh tuyáº¿n)

				Náº±m á»Ÿ trÃªn cÃ¹ng.

				CÃ³ nhiá»‡m vá»¥ káº¿t ná»‘i cÃ¡c subnet khÃ¡c nhau trong cÃ¹ng máº¡ng ná»™i bá»™.

				CÅ©ng cÃ³ thá»ƒ káº¿t ná»‘i trung tÃ¢m dá»¯ liá»‡u nÃ y vá»›i Internet hoáº·c cÃ¡c máº¡ng khÃ¡c (WAN, VPN,...).

			Network ACL

				LÃ  bá»™ lá»c báº£o máº­t á»Ÿ má»©c subnet.

				Quyáº¿t Ä‘á»‹nh gÃ³i tin nÃ o Ä‘Æ°á»£c phÃ©p Ä‘i vÃ o hoáº·c Ä‘i ra subnet (theo IP, port, protocol,...).

				TÆ°Æ¡ng tá»± Network ACL trong AWS VPC, náº±m giá»¯a router vÃ  subnet.

			Subnet (máº¡ng con)

				Má»—i mÃ u trong hÃ¬nh lÃ  má»™t subnet:

					ğŸŸ§ 172.20.1.0/24

					ğŸŸ¦ 192.168.0.0/24

					ğŸŸ© 10.0.0.0/16

				Má»—i subnet Ä‘áº¡i diá»‡n cho má»™t phÃ¢n Ä‘oáº¡n máº¡ng riÃªng biá»‡t trong doanh nghiá»‡p.
				
					VÃ­ dá»¥:

						Má»™t subnet cho bá»™ pháº­n káº¿ toÃ¡n.

						Má»™t subnet cho bá»™ pháº­n ká»¹ thuáº­t.

						Má»™t subnet cho server ná»™i bá»™.

		BÃªn trong má»—i subnet

			Má»—i subnet cÃ³:

				Switch â€“ Ä‘á»ƒ káº¿t ná»‘i cÃ¡c mÃ¡y tÃ­nh ná»™i bá»™ trong cÃ¹ng subnet.

				Network of computers â€“ nhÃ³m mÃ¡y tráº¡m hoáº·c mÃ¡y chá»§.

				Mainframe â€“ cÃ¡c mÃ¡y chá»§ lá»›n, chá»‹u trÃ¡ch nhiá»‡m xá»­ lÃ½ dá»¯ liá»‡u trung tÃ¢m.

			Táº¥t cáº£ mÃ¡y trong cÃ¹ng subnet cÃ³ thá»ƒ giao tiáº¿p trá»±c tiáº¿p vá»›i nhau qua switch mÃ  khÃ´ng cáº§n Ä‘i qua router.
			
	AWS Part -2 - Kiáº¿n trÃºc máº¡ng cÆ¡ báº£n cá»§a má»™t há»‡ thá»‘ng cháº¡y trÃªn AWS VPC (Virtual Private Cloud):
	
		Tá»•ng quan: Amazon VPC (Virtual Private Cloud)

			VPC lÃ  má»™t máº¡ng riÃªng áº£o bÃªn trong AWS, nÆ¡i báº¡n cÃ³ thá»ƒ triá»ƒn khai cÃ¡c tÃ i nguyÃªn nhÆ° EC2, RDS, Load Balancer...
			VPC cho phÃ©p báº¡n kiá»ƒm soÃ¡t hoÃ n toÃ n máº¡ng lÆ°á»›i, gá»“m:

				Dáº£i IP (CIDR)

				Subnet

				Routing table

				Security Group vÃ  Network ACL
				
		CÃ¡c thÃ nh pháº§n trong hÃ¬nh
		
			Internet Gateway (IGW)

					LÃ  cá»•ng káº¿t ná»‘i giá»¯a VPC vÃ  Internet.

					Cho phÃ©p cÃ¡c instance trong public subnet cÃ³ thá»ƒ giao tiáº¿p ra/vÃ o Internet.

					IGW gáº¯n trá»±c tiáº¿p vÃ o VPC.

				Vai trÃ²:
				
					CÃ¡c web server muá»‘n Ä‘Æ°á»£c truy cáº­p cÃ´ng khai pháº£i Ä‘i qua IGW.
					
			Public Subnet

				ÄÃ¢y lÃ  subnet cÃ³ route ra Internet Gateway.

				Chá»©a cÃ¡c tÃ i nguyÃªn cáº§n cÃ´ng khai ra ngoÃ i, vÃ­ dá»¥:
				
					Web Server, NAT Gateway, Load Balancer, v.v.

				Trong hÃ¬nh:

					CÃ³ hai WEB SERVER trong public subnet.

					CÃ³ VPC NAT Gateway náº±m cÃ¹ng subnet.
					
			Private Subnet

				Subnet khÃ´ng cÃ³ route trá»±c tiáº¿p ra Internet.

				Chá»‰ cÃ³ thá»ƒ truy cáº­p Internet giÃ¡n tiáº¿p thÃ´ng qua NAT Gateway hoáº·c tá»« public subnet.

				DÃ¹ng cho cÃ¡c tÃ i nguyÃªn ná»™i bá»™ cáº§n báº£o máº­t cao:

					App Server

					Database Server

				Trong hÃ¬nh:

					CÃ³ 1 APP SERVER vÃ  2 DB SERVER trong private subnet.
					
			VPC NAT Gateway

					â€œNATâ€ = Network Address Translation.

					Náº±m trong public subnet vÃ  cÃ³ Elastic IP gáº¯n vÃ o.

					Cho phÃ©p cÃ¡c instance trong private subnet truy cáº­p Internet Ä‘á»ƒ cáº­p nháº­t package, táº£i dá»¯ liá»‡u, v.v.,
					nhÆ°ng Internet khÃ´ng thá»ƒ truy cáº­p ngÆ°á»£c láº¡i vÃ o private subnet.

				VÃ­ dá»¥:

					APP SERVER trong private subnet muá»‘n táº£i cáº­p nháº­t tá»« Internet â†’ Ä‘i qua NAT Gateway â†’ Internet.

					NhÆ°ng Internet khÃ´ng thá»ƒ vÃ o trá»±c tiáº¿p APP SERVER.
					
			Availability Zone (AZ)

				AWS chia má»—i region (vÃ­ dá»¥: us-east-1) thÃ nh nhiá»u AZ â€” cÃ¡c trung tÃ¢m dá»¯ liá»‡u váº­t lÃ½ riÃªng biá»‡t.

				Trong hÃ¬nh:

					Availability Zone A: chá»©a public subnet (WEB SERVER, NAT Gateway)

					Availability Zone B: chá»©a private subnet (APP SERVER, DB SERVER)

				ğŸ§­ Má»¥c tiÃªu: tÄƒng Ä‘á»™ tin cáº­y vÃ  tÃ­nh sáºµn sÃ ng (high availability).
				
		Luá»“ng hoáº¡t Ä‘á»™ng thá»±c táº¿

			NgÆ°á»i dÃ¹ng truy cáº­p web qua Internet â†’ Ä‘i vÃ o Internet Gateway â†’ Ä‘áº¿n WEB SERVER trong public subnet.

			WEB SERVER cÃ³ thá»ƒ:

				Gá»­i request Ä‘áº¿n APP SERVER trong private subnet.

				APP SERVER xá»­ lÃ½, truy váº¥n DB SERVER (cÅ©ng trong private subnet).

			Náº¿u APP SERVER cáº§n ra Internet (vÃ­ dá»¥ táº£i dependency) â†’ nÃ³ Ä‘i qua NAT Gateway trong public subnet â†’ Internet.
			
		Báº£o máº­t

			Security Group (SG): kiá»ƒm soÃ¡t traffic theo instance (firewall cáº¥p EC2).
			
				VÃ­ dá»¥:

					Web server SG cho phÃ©p inbound HTTP (80) vÃ  HTTPS (443).

					App/DB server SG chá»‰ cho phÃ©p inbound tá»« web server.

			Network ACL (NACL): kiá»ƒm soÃ¡t traffic theo subnet (firewall cáº¥p máº¡ng).
			
	AWS Part -2 - Kiáº¿n trÃºc máº¡ng VPC trong AWS:
	
		Má»¥c tiÃªu cá»§a sÆ¡ Ä‘á»“

			HÃ¬nh nÃ y cho ta tháº¥y cÃ¡ch AWS chia máº¡ng trong VPC thÃ nh hai pháº§n:

				Public Subnet: cho phÃ©p EC2 giao tiáº¿p trá»±c tiáº¿p vá»›i Internet

				Private Subnet: hoÃ n toÃ n cÃ¡ch ly, chá»‰ giao tiáº¿p ná»™i bá»™ hoáº·c qua VPN/NAT
				
		CÃ¡c thÃ nh pháº§n chÃ­nh trong hÃ¬nh
		
			Amazon VPC

				LÃ  â€œmáº¡ng áº£o riÃªngâ€ cá»§a báº¡n trong AWS (Virtual Private Cloud).

				Báº¡n cÃ³ thá»ƒ kiá»ƒm soÃ¡t toÃ n bá»™ máº¡ng, IP, routing, subnet, báº£o máº­tâ€¦

				Má»—i VPC náº±m trong má»™t region (vÃ­ dá»¥: us-east-1).
				
			Public Subnet

				Subnet nÃ y cÃ³ route ra Internet Gateway â†’ cho phÃ©p cÃ¡c instance bÃªn trong giao tiáº¿p trá»±c tiáº¿p vá»›i Internet.

				Trong hÃ¬nh:

					CÃ³ 1 EC2 instance

					Gáº¯n Security Group (báº£o máº­t cáº¥p instance)

					CÃ³ Network ACL (báº£o máº­t cáº¥p subnet)

					CÃ³ Route Table (Ä‘á»‹nh tuyáº¿n 172.16.0.0, 172.16.1.0, 172.16.2.0)

				TÃ¡c dá»¥ng:
				
					DÃ nh cho cÃ¡c tÃ i nguyÃªn cáº§n cÃ´ng khai, vÃ­ dá»¥:

						Web server

						Load balancer

						NAT Gateway
						
			Private Subnet

				Subnet nÃ y khÃ´ng cÃ³ route trá»±c tiáº¿p ra Internet Gateway, chá»‰ cÃ³ thá»ƒ káº¿t ná»‘i ná»™i bá»™.

				Trong hÃ¬nh:

					CÅ©ng cÃ³ EC2 instance

					CÃ³ Security Group, Network ACL, Route Table riÃªng

				TÃ¡c dá»¥ng:
				
					DÃ nh cho tÃ i nguyÃªn chá»‰ dÃ¹ng ná»™i bá»™, nhÆ°:

						App Server

						Database Server

						Internal API
						
			Route Table (Báº£ng Ä‘á»‹nh tuyáº¿n)

				Äiá»u khiá»ƒn â€œhÆ°á»›ng Ä‘iâ€ cá»§a traffic trong VPC.

				Má»—i subnet Ä‘Æ°á»£c gÃ¡n 1 Route Table.

					Public Subnet: cÃ³ route 0.0.0.0/0 â†’ Internet Gateway

					Private Subnet: khÃ´ng cÃ³ route Ä‘Ã³ â†’ cháº·n Internet trá»±c tiáº¿p

				Trong hÃ¬nh: Route Table liá»‡t kÃª cÃ¡c dáº£i IP ná»™i bá»™ (172.16.x.x) dÃ¹ng Ä‘á»ƒ Ä‘á»‹nh tuyáº¿n
				giá»¯a cÃ¡c subnet trong VPC.
				
			Network ACL (Access Control List)

				LÃ  tÆ°á»ng lá»­a cáº¥p subnet, kiá»ƒm soÃ¡t inbound/outbound traffic.

				CÃ³ thá»ƒ cho phÃ©p hoáº·c cháº·n dá»±a trÃªn rule theo IP, port, protocol.
				
			Security Group

				TÆ°á»ng lá»­a cáº¥p instance.

				Kiá»ƒm soÃ¡t traffic inbound/outbound cho tá»«ng EC2.

				Máº·c Ä‘á»‹nh: cháº·n má»i inbound, cho phÃ©p outbound.
				
			Router

				LÃ  router logic bÃªn trong VPC (AWS quáº£n lÃ½ tá»± Ä‘á»™ng).

				Káº¿t ná»‘i cÃ¡c subnet vá»›i nhau vÃ  vá»›i gateway (Internet hoáº·c VPN).

				Báº¡n khÃ´ng cáº¥u hÃ¬nh trá»±c tiáº¿p router, mÃ  Ä‘iá»u khiá»ƒn qua Route Table.
				
			Internet Gateway (IGW)

				Cá»•ng káº¿t ná»‘i VPC â†” Internet.

				Cáº§n thiáº¿t náº¿u EC2 trong public subnet muá»‘n cÃ³ IP cÃ´ng cá»™ng vÃ  truy cáº­p ra ngoÃ i.

				Chá»‰ hoáº¡t Ä‘á»™ng vá»›i subnet cÃ³ route tá»›i IGW.
				
			VPN Gateway

				Cho phÃ©p káº¿t ná»‘i VPC vá»›i datacenter on-premises (qua Ä‘Æ°á»ng VPN báº£o máº­t).

				DÃ nh cho mÃ´ hÃ¬nh hybrid cloud (vá»«a dÃ¹ng AWS vá»«a dÃ¹ng háº¡ táº§ng ná»™i bá»™).
				
		Luá»“ng hoáº¡t Ä‘á»™ng minh há»a

			EC2 trong Public Subnet cÃ³ thá»ƒ giao tiáº¿p Internet trá»±c tiáº¿p qua Internet Gateway.

			EC2 trong Private Subnet:

				KhÃ´ng thá»ƒ Ä‘i ra Internet trá»±c tiáº¿p.

				CÃ³ thá»ƒ Ä‘i qua VPN Gateway Ä‘á»ƒ káº¿t ná»‘i vá» há»‡ thá»‘ng ná»™i bá»™ (corporate network).

			Traffic ná»™i bá»™ giá»¯a cÃ¡c subnet (172.16.0.x â†” 172.16.2.x) Ä‘i qua Router cá»§a VPC, theo Route Table.
			
	AWS Part -2 - Thiáº¿t káº¿ kiáº¿n trÃºc máº¡ng (VPC Blueprint) trong AWS á»Ÿ region us-west-1, vá»›i Ä‘áº§y Ä‘á»§ subnet,
	route table, NAT, Internet Gateway, vÃ  VPC Peering:
	
		ThÃ´ng tin tá»•ng quan

			Region: us-west-1 (California)

			VPC Range: 172.20.0.0/16
			
				â†’ dáº£i IP tá»•ng cho toÃ n bá»™ VPC (65,536 Ä‘á»‹a chá»‰ IP kháº£ dá»¥ng).
				
				â†’ cÃ¡c subnet con sáº½ chia nhá» dáº£i nÃ y (theo /24).

		Cáº¥u trÃºc Subnet

			VPC nÃ y cÃ³ 4 subnet chia lÃ m 2 public vÃ  2 private, Ä‘áº·t á»Ÿ 2 Availability Zones (AZs) khÃ¡c nhau:

				Subnet 			CIDR				Loáº¡i		AZ	Ghi chÃº
				172.20.1.0/24	Public Subnet 1		us-west-1a	pub-sub-1
				172.20.2.0/24	Public Subnet 2		us-west-1b	pub-sub-2
				172.20.3.0/24	Private Subnet 1	us-west-1a	priv-sub-1
				172.20.4.0/24	Private Subnet 2	us-west-1b	priv-sub-2

			Má»¥c Ä‘Ã­ch:

				Public subnet: chá»©a cÃ¡c tÃ i nguyÃªn cáº§n truy cáº­p tá»« Internet (vÃ­ dá»¥: Bastion host, Load Balancer, NAT Gateway).

				Private subnet: chá»©a cÃ¡c á»©ng dá»¥ng ná»™i bá»™ (App Server, Database, Cacheâ€¦).

		Internet Gateway (IGW)

			CÃ³ 1 Internet Gateway gáº¯n vÃ o VPC nÃ y.

			Cho phÃ©p cÃ¡c instance trong public subnet giao tiáº¿p trá»±c tiáº¿p vá»›i Internet.
			
		NAT Gateway (2 cÃ¡i)

			Äáº·t trong public subnet, má»—i AZ má»™t cÃ¡i.

			DÃ¹ng Ä‘á»ƒ cho phÃ©p cÃ¡c instance trong private subnet truy cáº­p Internet ra ngoÃ i, nhÆ°ng Internet khÃ´ng
			truy cáº­p ngÆ°á»£c láº¡i Ä‘Æ°á»£c.

			VÃ­ dá»¥: private EC2 cÃ³ thá»ƒ táº£i gÃ³i update, patch, v.vâ€¦ mÃ  váº«n giá»¯ an toÃ n.
			
		Elastic IP (EIP)

			Má»—i NAT Gateway cáº§n má»™t Ä‘á»‹a chá»‰ IP tÄ©nh (EIP) Ä‘á»ƒ ra Internet.

			Do Ä‘Ã³, á»Ÿ Ä‘Ã¢y cÃ³ 1 EIP (hoáº·c 2 náº¿u má»—i NAT Gateway 1 IP).
			
		Route Tables (Báº£ng Ä‘á»‹nh tuyáº¿n)

			CÃ³ 2 Route Tables:

				TÃªn Route Table		DÃ nh cho		Äá»‹nh tuyáº¿n chÃ­nh
				Public Subnet RT	Public Subnet	0.0.0.0/0 â†’ Internet Gateway
				Private Subnet RT	Private Subnet	0.0.0.0/0 â†’ NAT Gateway

			Giáº£i thÃ­ch:

				Public subnet cÃ³ route trá»±c tiáº¿p ra IGW.

				Private subnet khÃ´ng cÃ³ route ra IGW, chá»‰ route qua NAT Gateway.
				
		Bastion Host

			1 Bastion host náº±m trong public subnet.

			LÃ  EC2 trung gian, dÃ¹ng Ä‘á»ƒ SSH vÃ o cÃ¡c instance trong private subnet.

			VÃ¬ private subnet khÃ´ng cÃ³ IP public, nÃªn báº¡n SSH vÃ o Bastion trÆ°á»›c â†’ rá»“i tá»« Ä‘Ã³ SSH
			tiáº¿p vÃ o cÃ¡c private EC2.
			
		Network ACL (NACL)

			CÃ³ nháº¯c Ä‘áº¿n â€œNACLâ€ á»Ÿ cuá»‘i â€” nghÄ©a lÃ  sáº½ cÃ³ Network ACL Ä‘á»ƒ kiá»ƒm soÃ¡t traffic á»Ÿ cáº¥p subnet.

			DÃ¹ng Ä‘á»ƒ tÄƒng thÃªm lá»›p báº£o máº­t, thÆ°á»ng cáº¥u hÃ¬nh song song vá»›i Security Group.
			
		VPC Peering

			DÃ²ng cuá»‘i â€œ1 more VPC â†’ VPC Peeringâ€ nghÄ©a lÃ :

				CÃ³ má»™t VPC khÃ¡c trong cÃ¹ng hoáº·c khÃ¡c region.

				Sáº½ táº¡o VPC Peering Ä‘á»ƒ hai VPC giao tiáº¿p vá»›i nhau qua máº¡ng riÃªng (khÃ´ng qua Internet).

			á»¨ng dá»¥ng:

			DÃ¹ng Ä‘á»ƒ káº¿t ná»‘i VPC cá»§a team khÃ¡c, mÃ´i trÆ°á»ng dev/staging, hoáº·c há»‡ thá»‘ng on-premises (khi khÃ´ng dÃ¹ng VPN).
			
	- Google search:
	
		online subnet calculator
		
	- Code trong file /etc/awslogs/awslogs.conf:
	
		Case 1:
		
		
			[/var/log/messages]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/messages
			buffer_duration = 5000
			log_stream_name = web01-sys-logs
			initial_position = start_of_file
			log_group_name = wave-web

			[/var/log/httpd/access_log]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/httpd/access_log
			buffer_duration = 5000
			log_stream_name = web01-httpd-access
			initial_position = start_of_file
			log_group_name = wave-web

	- Code trong s3-policy.json:
	
		Case 1:
		
			{
			  "Version":"2012-10-17",		 	 	 
			  "Statement": [
				{
				  "Effect": "Allow",
				  "Principal": {
					"AWS": "arn:aws:iam::127311923021:root"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*"
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*",
				  "Condition": {
					"StringEquals": {
						"s3:x-amz-acl": "bucket-owner-full-control"
					}
				  }
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:GetBucketAcl",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222"
				}
			  ]
			}


--- AWS CI CD Project:

	- Elastic Beanstalk -> Create application
	- Codebuild -> Getting started -> Create project
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Start build
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Phase details
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Edit
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Build logs
	- Codebuild -> Pipeline -> Pipelines -> Create pipeline
	- Bitbucket -> Settings -> Personal Bitbucket settings -> SSH keys -> Add key
	- Bitbucket -> Settings -> Personal Bitbucket settings -> App passwords
	
	- Google search:
	
		bitbucket
		
		codebuild buildspec file documentation

	- Commands:
	
		- cmd: chmod 400 vprobeankey.pem
		- cmd: dnf search mysql
		- cmd: dnf install mariadb1011 -y
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts
		- cmd: wget https://raw.githubusercontent.com/hkhcoder/vprofile-project/refs/heads/aws-ci/src/main/resources/db_backup.sql
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts < db_backup.sql
		- cmd: ls .ssh/
		- cmd: cat vprobit_rsa.pub
		- cmd: ssh -T git@bitbucket.org
		- cmd: ls -ltr
		- cmd: cat .git/config
		- cmd: git fetch --tags
		- cmd: git remote rm origin
		- cmd: git remote add origin git@bitbucket.org:devopscicd1122/vproapp.git
		- cmd: git push origin --all
		
		
		
		
	- Code trong file config:
	
		Case 1:
		
			# bitbucket.org
			Host bitbucket.org
			  PreferredAuthentications publickey
			  IdentityFile ~/.ssh/vprobit_rsa
			  
--- Docker:

	- Commands:
	
		- cmd: docker images
		- cmd: sudo vim /etc/group
		- cmd: sudo usermod -aG docker ubuntu
		- cmd: id ubuntu
		- cmd: docker run hello-world
		- cmd: docker ps
		- cmd: docker ps -a
		- cmd: docker pull nginx:stable-alpine3.21-perl
		- cmd: docker pull nginx
		- cmd: docker run --name myweb -p 7090:80 -d nginx
		- cmd: docker stop 2c0ebbbbc6cc
		- cmd: docker start myweb
		- cmd: ps -ef
		- cmd: cd /var/lib/docker/
		- cmd: root@ip-172-31-35-202:/var/lib/docker# cd containers/
		- cmd: du -sh 6e18faf905a703aad2a20e13e47edcd43a8e3fa8463dc22a76ef653133f04fe5
		- cmd: root@ip-172-31-35-202:/var/lib/docker# ls image/overlay2/
		- cmd: docker exec myweb ls /
		- cmd: docker exec myweb /bin/bash
		- cmd: docker exec -it myweb /bin/bash
		- cmd: apt install procps -y
		- cmd: docker rmi nginx:stable-alpine3.21-perl
		- cmd: docker rm 2c0ebbbbc6cc
		- cmd: docker pull ubuntu
		- cmd: docker run ubuntu
		- cmd: docker run -it ubuntu /bin/bash
		- cmd: docker rm sleepy_clarke amazing_ganguly tender_kilby
		- cmd: docker rmi ubuntu hello-world
		- cmd: docker inspect nginx
		- cmd: docker run -d -P nginx
		- cmd: docker logs peaceful_einstein
		- cmd: docker run -P nginx
		- cmd: docker run -d -P mysql:5.7
		- cmd: docker run -d -P -e MYSQL_ROOT_PASSWORD=mypass mysql:5.7
		- cmd: docker pull mysql:5.7
		- cmd: docker inspect mysql:5.7
		- cmd: docker exec -it vprodb /bin/bash
		- cmd: bash-4.2# cd /var/lib/mysql
		- cmd: docker stop vprodb
		- cmd: docker rm vprodb
		- cmd: docker volume
		- cmd: docker volume create mydbdata
		- cmd: docker volume ls
		- cmd: docker run --name vprodb -d -e MYSQL_ROOT_PASSWORD=secretpass -p 3030:3306 -v mydbdata:/var/lib/mysql mysql:5.7
		- cmd: ls /var/lib/docker/volumes/
		- cmd: ls /var/lib/docker/volumes/mydbdata/_data/
		- cmd: 	sudo apt update
				sudo apt install -y mysql-client
		- cmd: sudo apt install unzip -y
		- cmd: tar czvf nano.tar.gz *
		- cmd: mv nano.tar.gz ../
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg
		- cmd: docker build -t nanoimg:V2 .
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg:V2
		- cmd: docker build --no-cache -t nanoimg:V2 .
		
			Cháº¡y Ä‘á»ƒ trÃ¡nh nÃ³ cháº¡y vá»›i cache cÅ©
			
		- cmd: docker login
		- cmd: docker build -t ngoctuan99/nanoimg:V2 .
		- cmd: docker push ngoctuan99/nanoimg:V2
		- cmd: ubuntu@ip-172-31-35-202:~/EntryCMD$ docker build -t printer:v1 -f cmd/Dockerfile .
		- cmd: docker build -t printer:v2 entry/
		- cmd: docker-compose up
		- cmd: curl http://localhost:8000
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose top
		- cmd: git clone -b docker https://github.com/devopshydclub/vprofile-project.git
		
		
		
		
			
		
		
	

		
		
		
		
	
	- Google search:
	
		docker ubuntu install
		
		dockerhub official images
		
		hub docker
		
		docker compose install
		
		docker compose get started
		
	- Code trong Dockerfile:
	
		Case 1:
		
			FROM ubuntu:latest
			LABEL "Author"="Imran"
			LABEL "Project"="nano"
			ENV DEBIAN_FRONTEND=noninteractive
			RUN apt update && apt install git -y
			RUN apt install apache2 -y
			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			EXPOSE 80
			WORKDIR /var/www/html
			VOLUME /var/log/apache2
			ADD nano.tar.gz /var/www/html
			#COPY nano.tar.gz /var/www/html
			
		Case 2:
		
			# syntax=docker/dockerfile:1
			FROM python:3.10-alpine
			WORKDIR /code
			ENV FLASK_APP=app.py
			ENV FLASK_RUN_HOST=0.0.0.0
			RUN apk add --no-cache gcc musl-dev linux-headers
			COPY requirements.txt requirements.txt
			RUN pip install -r requirements.txt
			EXPOSE 5000
			COPY . .
			CMD ["flask", "run"]
			
		Case 3:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
				volumes:
				  - .:/code
				environment:
				  FLASK_ENV: development
			  redis:
				image: "redis:alpine"

		Case 4:
		
			FROM ubuntu:latest
			CMD ["echo", "hello"]
			
		Case 5:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			
		Case 6:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			cmd ["hello"]
			
		Case 7:
		
			FROM tomcat:8-jre11
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webappsubuntu@ip-172-31-35-202:~/composetest/vprofile-project/Docker-files/app$
			
		Case 8:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11

			RUN rm -rf /usr/local/tomcat/webapps/*

			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]



	-  Code trong app.py:
	
		Case 1:
		
			import time

			import redis
			from flask import Flask

			app = Flask(__name__)
			cache = redis.Redis(host='redis', port=6379)

			def get_hit_count():
				retries = 5
				while True:
					try:
						return cache.incr('hits')
					except redis.exceptions.ConnectionError as exc:
						if retries == 0:
							raise exc
						retries -= 1
						time.sleep(0.5)

			@app.route('/')
			def hello():
				count = get_hit_count()
				return 'Hello World! I have been seen {} times.\n'.format(count)
				
	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
			  redis:
				image: "redis:alpine"

	- Code trong file requirements.txt:
	
		Case 1:
		
			flask
			redis
			
--- Containerization:

	Git bash khÃ´ng thá»ƒ cháº¡y nhiá»u lá»‡nh 1 lÃºc Ä‘Æ°á»£cA

	- Google search:
	
		Docker ubuntu install
		
		Dockerfile reference
		
	Containerization -  Sá»± khÃ¡c nhau giá»¯a / vÃ  /root:
	
		Trong Ubuntu (vÃ  Linux nÃ³i chung), hai thÆ° má»¥c / vÃ  /root hoÃ n toÃ n khÃ¡c nhau vá» vai trÃ² vÃ  quyá»n háº¡n.
		
		/ (root directory)

			/ Ä‘Æ°á»£c gá»i lÃ  thÆ° má»¥c gá»‘c cá»§a toÃ n bá»™ há»‡ thá»‘ng táº­p tin (root filesystem).

			Táº¥t cáº£ má»i thá»© trong Linux â€” file há»‡ thá»‘ng, ngÆ°á»i dÃ¹ng, thiáº¿t bá»‹, á»©ng dá»¥ng, v.v. â€” Ä‘á»u náº±m dÆ°á»›i thÆ° má»¥c nÃ y.

			NÃ³ giá»‘ng nhÆ° á»• C: trong Windows, nhÆ°ng rá»™ng hÆ¡n, vÃ¬ nÃ³ chá»©a luÃ´n cÃ¡c â€œá»• Ä‘Ä©aâ€ vÃ  mount khÃ¡c.

			Cáº¥u trÃºc thÆ°á»ng tháº¥y trong /:
	
				/
				â”œâ”€â”€ bin        â†’ Chá»©a lá»‡nh cÆ¡ báº£n (ls, cp, mv, cat, v.v.)
				â”œâ”€â”€ boot       â†’ Chá»©a file khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng (kernel, grub)
				â”œâ”€â”€ dev        â†’ Thiáº¿t bá»‹ (á»• cá»©ng, USB, tty,...)
				â”œâ”€â”€ etc        â†’ File cáº¥u hÃ¬nh há»‡ thá»‘ng
				â”œâ”€â”€ home       â†’ ThÆ° má»¥c ngÆ°á»i dÃ¹ng bÃ¬nh thÆ°á»ng (vd: /home/nhuan)
				â”œâ”€â”€ lib        â†’ ThÆ° viá»‡n há»‡ thá»‘ng
				â”œâ”€â”€ root       â†’ ThÆ° má»¥c cÃ¡ nhÃ¢n cá»§a tÃ i khoáº£n root
				â”œâ”€â”€ tmp        â†’ File táº¡m thá»i
				â”œâ”€â”€ usr        â†’ ChÆ°Æ¡ng trÃ¬nh, thÆ° viá»‡n dÃ¹ng chung
				â”œâ”€â”€ var        â†’ Log, mail, cache,...
				
			/root

				/root lÃ  thÆ° má»¥c cÃ¡ nhÃ¢n (home directory) cá»§a ngÆ°á»i dÃ¹ng root (superuser).

				Khi báº¡n Ä‘Äƒng nháº­p báº±ng tÃ i khoáº£n root, Ä‘Ã¢y lÃ  nÆ¡i máº·c Ä‘á»‹nh báº¡n sáº½ â€œÄ‘á»©ngâ€ (cd).

				TÆ°Æ¡ng tá»± nhÆ° /home/username cá»§a ngÆ°á»i dÃ¹ng thÆ°á»ng, nhÆ°ng Ä‘Æ°á»£c Ä‘áº·t á»Ÿ Ä‘Ã¢y vÃ¬:

				Root cáº§n truy cáº­p Ä‘Æ°á»£c ngay cáº£ khi /home chÆ°a mount (vÃ­ dá»¥ trong cháº¿ Ä‘á»™ khÃ´i phá»¥c).

				Báº£o máº­t vÃ  tÃ¡ch biá»‡t vá»›i user thÃ´ng thÆ°á»ng.

	- Commands:
	
		- cmd: usermod -aG docker vagrant
		- cmd: id vagrant
		- cmd: docker compose up -d
		- cmd: docker compose down
		- cmd: docker volume ls
		- cmd: docker volume rm vagrant_vproappdata vagrant_vprodbdata
		- cmd: docker volume prune
		- cmd: docker system prune -a
		- cmd: code .
		- cmd: id
		- cmd: docker-compose --version
		- cmd: docker-compose build
		
		
		
		
		
	
	- Code trong file Dockerfile:
	
		Case 1:
		
			FROM maven:3.9.9-eclipse-temurin-21-jammy AS BUILD_IMAGE
			RUN git clone https://github.com/hkhcoder/vprofile-project.git
			RUN cd vprofile-project && git checkout containers && mvn install

			FROM tomcat:10-jdk21
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 2:
		
			FROM mysql:8.0.33
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			ENV MYSQL_ROOT_PASSWORD="vprodbpass"
			ENV MYSQL_DATABASE="accounts"

			ADD db_backup.sql docker-entrypoint-initdb.d/db_backup.sql
			
		Case 3:
		
			FROM nginx
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /etc/nginx/conf.d/default.conf
			COPY nginvproapp.conf /etc/nginx/conf.d/vproapp.conf
			
		Case 4:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			#ADD ../../vprofile-project
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 5:
		
			FROM node:14 AS web-build
			WORKDIR /usr/src/app
			COPY ./ ./client
			RUN cd client && npm install && npm run build --prod

			# Use official nginx image as the base image
			FROM nginx:latest

			# Copy the build output to replace the default nginx contents.
			COPY --from=web-build /usr/src/app/client/dist/client/ /usr/share/nginx/html
			COPY nginx.conf /etc/nginx/conf.d/default.conf

			# Expose port 4200
			EXPOSE 4200
			
		Case 6:
		
			FROM node:14 AS nodeapi-build
			WORKDIR /usr/src/app
			COPY ./ ./nodeapi/
			RUN cd nodeapi && npm install

			FROM node:14
			WORKDIR /usr/src/app/
			COPY --from=nodeapi-build /usr/src/app/nodeapi/ ./
			RUN ls
			EXPOSE 5000
			CMD ["/bin/sh", "-c", "cd /usr/src/app/ && npm start"]
			# Test3

		Case 7:
		
			FROM openjdk:8 AS BUILD_IMAGE
			WORKDIR /usr/src/app/
			RUN apt update && apt install maven -y
			COPY ./ /usr/src/app/
			RUN mvn install -DskipTests

			FROM openjdk:8

			WORKDIR /usr/src/app/
			COPY --from=BUILD_IMAGE /usr/src/app/target/book-work-0.0.1-SNAPSHOT.jar ./book-work-0.0.1.jar

			EXPOSE 9000
			ENTRYPOINT ["java","-jar","book-work-0.0.1.jar"]
			# Test


			
	- Code trong file nginvproapp.conf:
	
		Case 1:
		
			upstream vproapp {
			 server vproapp:8080;
			}
			server {
			  listen 80;
			location / {
			  proxy_pass http://vproapp;
			}
			}

	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
				vprodb:
				  build:
					context: ./Docker-files/db
				  image: ngoctuan99/vprofiledb
				  container_name: vprodb
				  ports:
					- "3306:3306"
				  volumes:
					- vprodbdata:/var/lib/mysql
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vprocache01:
				  image: memcached
				  container_name: vprocache01
				  ports:
					- "11211:11211"

				vpromq01:
				  image: rabbitmq
				  container_name: vpromq01
				  ports:
					- "5672:5672"
				  environment:
					- RABBITMQ_DEFAULT_USER=guest
					- RABBITMQ_DEFAULT_PASS=guest

				vproapp:
				  build:
					context: ./Docker-files/app
				  image: ngoctuan99/vprofileapp
				  container_name: vproapp
				  ports:
					- "8080:8080"
				  volumes:
					- vproappdata:/usr/local/tomcat/webapps
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vproweb:
				  build:
					context: ./Docker-files/web
				  image: ngoctuan99/vprofileweb
				  container_name: vproweb
				  ports:
					- "80:80"

			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books

			  
	- Code trong file nginx.conf:
	
		Case 1:
		
			server {
				listen       4200;
				listen  [::]:4200;
				server_name  localhost;

				#access_log  /var/log/nginx/host.access.log  main;

				location / {
					root   /usr/share/nginx/html;
					index  index.html index.htm;
				}

				#error_page  404              /404.html;

				# redirect server error pages to the static page /50x.html
				#
				error_page   500 502 503 504  /50x.html;
				location = /50x.html {
					root   /usr/share/nginx/html;
				}

				# proxy the PHP scripts to Apache listening on 127.0.0.1:80
				#
				#location ~ \.php$ {
				#    proxy_pass   http://127.0.0.1;
				#}

				# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
				#
				#location ~ \.php$ {
				#    root           html;
				#    fastcgi_pass   127.0.0.1:9000;
				#    fastcgi_index  index.php;
				#    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
				#    include        fastcgi_params;
				#}

				# deny access to .htaccess files, if Apache's document root
				# concurs with nginx's one
				#
				#location ~ /\.ht {
				#    deny  all;
				#}
			}

	- Code trong file default.conf:
	
		Case 1:
		
			#upstream api {
			#    server api:5000; 
			#}
			#upstream webapi {
			#    server webapi:9000;
			#}
			upstream client {
				server client:4200;
			}
			server {
				listen 80;
				location / {
					proxy_set_header Host $host;
					proxy_set_header X-Real-IP $remote_addr;
					proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
					proxy_set_header X-Forwarded-Proto $scheme; 

					proxy_http_version 1.1;
					proxy_set_header Upgrade $http_upgrade;
					proxy_set_header Connection "upgrade";
					proxy_pass http://client/;
				}
				location /api {
			#        rewrite ^/api/(.*) /$1 break; # works for both /api and /api/
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			#        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme; 
			#        proxy_http_version 1.1;

					proxy_pass http://api:5000;
				}
				location /webapi {
			#        rewrite ^/webapi/(.*) /$1 break;
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			##        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme;
					proxy_pass http://webapi:9000;
				}
			}

	- Code trong Advanced detials cá»§a EC2:
	
		Case 1:
		
			#!/bin/bash

			# Install docker on Ubuntu
			sudo apt-get update
				sudo apt-get install \
				 ca-certificates \
				 curl \
				 gnupg \
				 lsb-release -y
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
				echo \
				"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
				$(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

			# Install docker-compose
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io -y
				sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
				sudo chmod +x /usr/local/bin/docker-compose

			# Add ubuntu user into docker group
				sudo usermod -a -G docker ubuntu

--- Kubernetes:

	- Route 53 -> Dashboard -> Create hosted zone
	- Amazon Elastic Kubernetes Service (Amazon EKS)

	- Google search:
	
		kubernetes docs
		
		minikube get started
		
		install kops
		
		install kubectl on linux
		
		kubernetes pod
		
		kubernetes deployment
		
		install kubectl on window
		
		kubernetes replicaset
		
		kubernetes deployment
		
		dockerfile reference
		
		define a command and arguments for a container
		
		kubernetes volume
		
		kubernetes configmaps
		
		kubernetes secrets
		
		kubernetes ingress
		
		kubectl cheatsheet
		
		kubernetes taint
		
		kubernetes limit
		
		kubernetes jobs
		
		kubernetes cronjob
		
		kubernetes daemonset
		
		k8slens.dev
		
		terraform modules
		
		
		
		
		
	Cáº§n check code trong source tá»« git clone (emartapp)
		
		
		
		

	- Commands:
	
		- cmd: kops export kubecfg kubevpro.vnstudio.info --state=s3://kopsstate112211 --admin
		- cmd: choco install minikube kubernetes-cli -y
		- cmd: minikube.exe --help
		- cmd: minikube start
		- cmd: minikube start --no-vtx-check
		- cmd: minikube delete
		- cmd: cat .kube/config
		- cmd: kubectl.exe get nodes
		- cmd: kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.0
		- cmd: kubectl.exe get pod
		- cmd: kubectl.exe get deploy
		- cmd: kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0
		- cmd: minikube service hello-minikube --url
		- cmd: minikube service hello-minikube
		- cmd: kubectl.exe delete deploy hello-minikube
		- cmd: minikube.exe stop
		- cmd: minikube.exe delete
		- cmd: chmod +x kops
		- cmd: sudo mv kops /usr/local/bin/kops
		- cmd: kubectl version --client
		- cmd: cat .kube/config
		- cmd:
		
				kops create cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate956
				--zones=us-east-1a,us-east-1b --node-count=2 --node-size=t3.small --control-plane-size=t3.medium
				--dns-zone=kubevpro.vnstudio.info --node-volume-size=12 --control-plane-volume-size=12
				--ssh-public-key ~/.ssh/id_ed25519.pub
				
		- cmd: kops update cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes --admin
		- cmd: kubectl get nodes
		- cmd: kubectl config view
		- cmd: kubectl get ns
		- cmd: kubectl get all
		- cmd: kubectl get all --all-namespaces
		- cmd: kubectl get svc -n kube-system
		- cmd: kubectl create ns kubekart
		- cmd: kubectl run nginx1 --image=nginx -n kubekart
		- cmd: kubectl apply -f pod1.yaml
		- cmd: kubectl get pod -n kubekart
		- cmd: kubectl delete ns kubekart
		- cmd: kubectl create -f pod-setup.yml
		- cmd: kubectl describe pod webapp-pod
		- cmd: kubectl get pod webapp-pod -o yaml
		- cmd: kubectl get pod webapp-pod -o yaml > webpod-definition.yml
		- cmd: kubectl edit pod webapp-pod
		- cmd: kubectl delete pod nginx12
		- cmd: kubectl logs web2
		- cmd: history | grep test47
		- cmd: kubectl run nginx12 --image=nginx
		- cmd: kubectl get svc
		- cmd: kubectl describe svc helloworld-service
		- cmd: kubectl describe pod | grep IP
		- cmd: kubectl delete svc helloworld-services
		- cmd: kubectl get rs
		- cmd: kubectl delete pod frontend-z2qch frontend-zr67k
		- cmd: kubectl scale --replicas=1 rs/frontend
		- cmd: kubectl edit rs frontend
		- cmd: kubectl get deploy
		- cmd: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		- cmd: kubectl rollout undo deployment/nginx-deployment
		- cmd: kubectl describe pod nginx-deployment-647677fc66-62mt4 | grep Image
		- cmd: kubectl rollout history deployment/nginx-deployment
		- cmd: kubectl delete deploy nginx-deployment
		- cmd: kubectl apply -f samplecm.yaml
		- cmd: kubectl get cm game-demo -o yaml
		
		- cmd:
		
			ubuntu@ip-172-31-36-50:~$ kubectl exec --stdin --tty configmap-demo-pod -- /bin/sh
			/ # ls /config/
			game.properties            user-interface.properties
			/ # cd /config/
			/config # cat game.properties
			enemy.types=aliens,monsters
			player.maximum-lives=5
			/config # cat user-interface.properties
			color.good=purple
			color.bad=yellow
			allow.textmode=true
			/config # echo $PLAYER_INITIAL_LIVES
			3
			/config # echo $UI_PROPERTIES_FILE_NAME
			user-interface.properties
			/config # exit
			
		- cmd: echo -n "admin" | base64
		- cmd: echo -n "mysecretpass" | base64
		- cmd: echo 'c2VjcmV0cGFzcw==' | base64 --decode
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl get all -n ingress-nginx
		- cmd:
		
			ubuntu@ip-172-31-36-50:~/vprofile$ # Controller
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReated DNS Cname Record for LB
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReate Ingress
			
		- cmd: kubectl delete ingress vpro-ingress
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl config view
		- cmd: kubectl run nginxpod --image=nginx --dry-run=client -o yaml > ngpod.yaml
		- cmd: cat ngpod.yaml
		- cmd: kubectl create deployment ngdep --image=nginx --dry-run=client -o yaml > ngdep.yaml
		- cmd: cat ngdep.yaml
		- cmd: kubectl get ds -A
		- cmd: kubectl get pod -n kube-system
		- cmd: choco uninstall terraform
		- cmd: choco install terraform -y
		- cmd: terraform init -upgrade
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vpro-eks
		- cmd: cat ~/.kube/config
		- cmd: choco install kubernetes-cli -y
		- cmd: kubectl get pods
		
		
		
		
		
		
		
		
		
		
		
		

		
		

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	
		

	Kubernetes - SÆ¡ Ä‘á»“ kiáº¿n trÃºc cá»§a Kubernetes (K8s):
	
		Tá»•ng quan vá» Kubernetes Architecture

			Kubernetes chia há»‡ thá»‘ng thÃ nh hai pháº§n chÃ­nh:

				Master Node (Control Plane) â€“ quáº£n lÃ½ toÃ n bá»™ cluster

				Worker Nodes â€“ nÆ¡i tháº­t sá»± cháº¡y cÃ¡c container (á»©ng dá»¥ng cá»§a báº¡n)
				
		Master Node (Control Plane)

			ÄÃ¢y lÃ  bá»™ nÃ£o cá»§a Kubernetes, chá»‹u trÃ¡ch nhiá»‡m Ä‘iá»u phá»‘i vÃ  quáº£n lÃ½ toÃ n bá»™ cluster.
			Trong hÃ¬nh, báº¡n tháº¥y cÃ¡c thÃ nh pháº§n chÃ­nh sau:

			API Server

				LÃ  trung tÃ¢m giao tiáº¿p cá»§a Kubernetes.

				Má»i lá»‡nh (kubectl apply, kubectl get pods, â€¦) Ä‘á»u Ä‘i qua API Server.

				NÃ³ nháº­n file YAML (bÃªn trÃ¡i hÃ¬nh), chá»©a Ä‘á»‹nh nghÄ©a Pod, Service, Deployment, v.v.

				Sau Ä‘Ã³ ghi thÃ´ng tin Ä‘Ã³ vÃ o etcd (database).

				Hiá»ƒu Ä‘Æ¡n giáº£n: API Server lÃ  â€œcá»­a chÃ­nhâ€ cá»§a cluster.

			etcd

				LÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¢n tÃ¡n (key-value store) cá»§a Kubernetes.

				LÆ°u toÃ n bá»™ tráº¡ng thÃ¡i cá»§a cluster: pods, nodes, cáº¥u hÃ¬nh máº¡ng, secrets, â€¦

				Náº¿u máº¥t etcd, báº¡n máº¥t toÃ n bá»™ â€œtrÃ­ nhá»›â€ cá»§a há»‡ thá»‘ng.

			âš™Controller Manager

				Theo dÃµi tráº¡ng thÃ¡i cluster qua etcd.

				Náº¿u phÃ¡t hiá»‡n sá»± khÃ¡c biá»‡t giá»¯a tráº¡ng thÃ¡i mong muá»‘n vÃ  tráº¡ng thÃ¡i thá»±c táº¿, nÃ³ sáº½ hÃ nh Ä‘á»™ng Ä‘á»ƒ khÃ´i phá»¥c láº¡i.

				VÃ­ dá»¥:

					YAML khai bÃ¡o cáº§n 3 pods, nhÆ°ng thá»±c táº¿ chá»‰ cÃ³ 2 pods cháº¡y â†’ Controller Manager sáº½ táº¡o thÃªm 1 pod má»›i.

			Scheduler

				Quyáº¿t Ä‘á»‹nh pod sáº½ Ä‘Æ°á»£c cháº¡y á»Ÿ node nÃ o.

				Dá»±a trÃªn tÃ i nguyÃªn cÃ²n trá»‘ng (CPU, RAM), cÃ¡c rÃ ng buá»™c (nodeSelector, affinity, v.v.)

				Sau khi chá»n node, Scheduler bÃ¡o cho kubelet á»Ÿ node Ä‘Ã³ Ä‘á»ƒ triá»ƒn khai pod.
				
		Worker Nodes

			Má»—i Worker Node lÃ  má»™t mÃ¡y (hoáº·c VM) tháº­t nÆ¡i container thá»±c sá»± cháº¡y.
			
			Trong hÃ¬nh báº¡n tháº¥y ba Worker nodes, má»—i node gá»“m:

				Kubelet

					Agent cháº¡y trÃªn má»—i node, nháº­n lá»‡nh tá»« API Server.

					Khi Scheduler quyáº¿t Ä‘á»‹nh gÃ¡n Pod vÃ o node nÃ y, Kubelet sáº½:

						Gá»i Docker (hoáº·c container runtime khÃ¡c nhÆ° containerd)

						KÃ©o image vá»

						Táº¡o vÃ  khá»Ÿi Ä‘á»™ng container

						BÃ¡o láº¡i tráº¡ng thÃ¡i pod vá» API Server

				Kube Proxy

					Quáº£n lÃ½ máº¡ng vÃ  load balancing giá»¯a cÃ¡c Pods.

					GiÃºp cÃ¡c Pods á»Ÿ cÃ¡c node khÃ¡c nhau cÃ³ thá»ƒ liÃªn láº¡c vá»›i nhau.

					Duy trÃ¬ service IP vÃ  routing rules trong há»‡ thá»‘ng.

				Docker (hoáº·c container runtime)

					LÃ  cÃ´ng cá»¥ tháº­t sá»± cháº¡y container.

					Kubernetes chá»‰ â€œra lá»‡nhâ€, Docker thá»±c thi viá»‡c táº¡o vÃ  quáº£n lÃ½ container.

				Pods

					ÄÆ¡n vá»‹ triá»ƒn khai nhá» nháº¥t trong Kubernetes.

					Má»™t Pod cÃ³ thá»ƒ chá»©a má»™t hoáº·c nhiá»u container, cháº¡y cÃ¹ng trÃªn má»™t node.

					Náº¿u Pod cháº¿t, Kubernetes sáº½ tá»± táº¡o láº¡i Pod má»›i theo YAML Ä‘á»‹nh nghÄ©a (báº¡n tháº¥y dáº¥u â€œX Ä‘á»â€ á»Ÿ node
					cuá»‘i hÃ¬nh â†’ má»™t Pod cháº¿t, K8s sáº½ khá»Ÿi táº¡o láº¡i).
					
		Luá»“ng hoáº¡t Ä‘á»™ng tá»•ng quÃ¡t

			Dev gá»­i file YAML Ä‘áº¿n API Server (kubectl apply -f deployment.yaml)

			API Server lÆ°u cáº¥u hÃ¬nh vÃ o etcd

			Controller Manager kiá»ƒm tra vÃ  ra lá»‡nh táº¡o Pod

			Scheduler chá»n node phÃ¹ há»£p

			Kubelet á»Ÿ node Ä‘Ã³ nháº­n lá»‡nh â†’ cháº¡y Pod báº±ng Docker

			Kube Proxy Ä‘áº£m báº£o Pod cÃ³ thá»ƒ giao tiáº¿p vá»›i cÃ¡c Pod khÃ¡c
			
	- Code trong file pod1.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: nginx12
			  namespace: kubekart
			spec:
			  containers:
			  - name: nginx
				image: nginx:1.14.2
				ports:
				- containerPort: 80

	- Code trong file vproapppod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: vproapp
			  labels:
				app: vproapp
			spec:
			  containers:
				- name: appcontainer
				  image: imranvisualpath/freshtomapp:V7
				  ports:
					- name: vproapp-port
					  containerPort: 8080
	- Code trong file vproapp-nodeport.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 8090
				nodePort: 30001
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: NodePort

	- Code trong file vproapp-loadbalancer.yml
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 80
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: LoadBalancer

	- Code trong file replset.yaml
	
		Case 1:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 3
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
		Case 2:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 5
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
	- Code trong file deployment.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: nginx-deployment
			  labels:
				app: nginx
			spec:
			  replicas: 3
			  selector:
				matchLabels:
				  app: nginx
			  template:
				metadata:
				  labels:
					app: nginx
				spec:
				  containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80

	- Code trong file com.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: command-demo
			  labels:
				purpose: demonstrate-command
			spec:
			  containers:
			  - name: command-demo-container
				image: debian
				command: ["printenv"]
				args: ["HOSTNAME", "KUBERNETES_PORT"]
			  restartPolicy: OnFailure
			  
	- Code trong file mysqlpod.yaml:
	
		Case 1: cháº¡y gáº·p lá»—i vÃ  Ä‘Æ°á»£c fix nhÆ° á»Ÿ Case 2
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: dbpod
			spec:
			  containers:
			  - image: mysql:5.7
				name: mysql
				volumeMounts:
				- mountPath: /var/lib/mysql
				  name: dbvol
			  volumes:
			  - name: dbvol
				hostPath:
				  # directory location on host
				  path: /data
				  # this field is optional
				  type: DirectoryOrCreate
				  
		Case 2:
		
			apiVersion: v1

			kind: Pod

			metadata:

			  name: dbpod

			spec:

			  containers:

			  - image: mysql:5.7

				name: mysql

				env:

				- name: MYSQL_ROOT_PASSWORD

				  value: examplepassword

				- name: MYSQL_DATABASE

				  value: exampledb

				- name: MYSQL_USER

				  value: exampleuser

				- name: MYSQL_PASSWORD

				  value: examplepassword

				volumeMounts:

				- mountPath: /var/lib/mysql

				  name: dbvol

			  volumes:

			  - name: dbvol

				hostPath:

				  # directory location on host

				  path: /data

				  # this field is optional

				  type: DirectoryOrCreate
				  
	- Code trong file samplecm.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: ConfigMap
			metadata:
			  name: game-demo
			data:
			  # property-like keys; each key maps to a simple value
			  player_initial_lives: "3"
			  ui_properties_file_name: "user-interface.properties"

			  # file-like keys
			  game.properties: |
				enemy.types=aliens,monsters
				player.maximum-lives=5    
			  user-interface.properties: |
				color.good=purple
				color.bad=yellow
				allow.textmode=true    

	- Code trong file readcmpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: configmap-demo-pod
			spec:
			  containers:
				- name: demo
				  image: alpine
				  command: ["sleep", "3600"]
				  env:
					# Define the environment variable
					- name: PLAYER_INITIAL_LIVES # Notice that the case is different here
												 # from the key name in the ConfigMap.
					  valueFrom:
						configMapKeyRef:
						  name: game-demo           # The ConfigMap this value comes from.
						  key: player_initial_lives # The key to fetch.
					- name: UI_PROPERTIES_FILE_NAME
					  valueFrom:
						configMapKeyRef:
						  name: game-demo
						  key: ui_properties_file_name
				  volumeMounts:
				  - name: config
					mountPath: "/config"
					readOnly: true
			  volumes:
			  # You set volumes at the Pod level, then mount them into containers inside that Pod
			  - name: config
				configMap:
				  # Provide the name of the ConfigMap you want to mount.
				  name: game-demo
				  # An array of keys from the ConfigMap to create as files
				  items:
				  - key: "game.properties"
					path: "game.properties"
				  - key: "user-interface.properties"
					path: "user-interface.properties"
					
	- Code trong file mysecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			
	- Code trong file readsecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			ubuntu@ip-172-31-36-50:~$ cat readsecret.yaml
			apiVersion: v1
			kind: Pod
			metadata:
			  name: secret-env-pod
			spec:
			  containers:
				- name: mycontainer
				  image: redis
				  env:
					- name: SECRET_USERNAME
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: username
						  optional: false  # same as default; "mysecret" must exist and include a key named "username"

					- name: SECRET_PASSWORD
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: password
						  optional: false  # same as default; "mysecret" must exist and include a key named "password"

			  restartPolicy: Never
			  
	- Code trong file vprodep.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080

	- Code trong file vprosvc.yaml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080
			ubuntu@ip-172-31-36-50:~/vprofile$ cat vprosvc.yaml
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-app
			spec:
			  ports:
			  - port: 8080
				protocol: TCP
				targetPort: 8080
			  selector:
				run: my-app
			  type: ClusterIP

	- Code trong file vprosvc.yaml:

		apiVersion: v1
		kind: Service
		metadata:
		  name: my-app
		spec:
		  ports:
		  - port: 8080
			protocol: TCP
			targetPort: 8080
		  selector:
			run: my-app
		  type: ClusterIP
	- Code trong file vproingress.yaml:
	
		Case 1:
		
			apiVersion: networking.k8s.io/v1
			kind: Ingress
			metadata:
			  name: vpro-ingress
			  annotations:
				nginx.ingress.kubernetes.io/use-regex: "true"
			spec:
			  ingressClassName: nginx
			  rules:
			  - host: vprofile.vnstudio.info
				http:
				  paths:
				  - path: /
					pathType: Prefix
					backend:
					  service:
						name: my-app
						port:
						  number: 8080
						  
	- Code trong file ngpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  labels:
				run: nginxpod
			  name: nginxpod
			spec:
			  containers:
			  - image: nginx
				name: nginxpod

	- Code trong file sampleds.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: DaemonSet
			metadata:
			  name: fluentd-elasticsearch
			  namespace: kube-system
			  labels:
				k8s-app: fluentd-logging
			spec:
			  selector:
				matchLabels:
				  name: fluentd-elasticsearch
			  template:
				metadata:
				  labels:
					name: fluentd-elasticsearch
				spec:
				  tolerations:
				  # these tolerations are to have the daemonset runnable on control plane nodes
				  # remove them if your control plane nodes should not run pods
				  - key: node-role.kubernetes.io/control-plane
					operator: Exists
					effect: NoSchedule
				  - key: node-role.kubernetes.io/master
					operator: Exists
					effect: NoSchedule
				  containers:
				  - name: fluentd-elasticsearch
					image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
					resources:
					  limits:
						memory: 200Mi
					  requests:
						cpu: 100m
						memory: 200Mi
					volumeMounts:
					- name: varlog
					  mountPath: /var/log
				  # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
				  # preempts running Pods
				  # priorityClassName: important
				  terminationGracePeriodSeconds: 30
				  volumes:
				  - name: varlog
					hostPath:
					  path: /var/log

	- Code trong file eks-cluster.tf:
	
		Case 1:
		
			module "eks" {
			  source = "terraform-aws-modules/eks/aws"
			  version = "19.0.4"

			  cluster_name = local.cluster_name
			  cluster_version = "1.30"

			  vpc_id = module.vpc.vpc_id
			  subnet_ids = module.vpc.private_subnets
			  cluster_endpoint_public_access = true

			  eks_managed_node_group_defaults = {
				ami_type = "AL2_x86_64"

			  }

			  eks_managed_node_groups = {
				one = {
				  name = "node-group-1"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 3
				  desired_size = 2
				}

				two = {
				  name = "node-group-2"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 2
				  desired_size = 1
				}
			  }
			}

	- Code trong file main.tf:
	
		Case 1:
		
			provider "kubernetes" {
			  host = module.eks.cluster_endpoint
			  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
			}

			provider "aws" {
			  region = var.region
			}

			data "aws_availability_zones" "available" {}

			locals {
			  cluster_name = var.clusterName
			}

	- Code trong file outputs.tf:
	
		Case 1:
		
			output "cluster_name" {
			  description = "Amazon Web Service EKS Cluster Name"
			  value = module.eks.cluster_name
			}

			output "cluster_endpoint" {
			  description = "Endpoint for Amazon Web Service EKS "
			  value = module.eks.cluster_endpoint
			}

			output "region" {
			  description = "Amazon Web Service EKS Cluster region"
			  value = var.region
			}


			output "cluster_security_group_id" {
			  description = "Security group ID for the Amazon Web Service EKS Cluster "
			  value = module.eks.cluster_security_group_id
			}

	- Code trong file terraform.tf:
	
		Case 1:
		
			terraform {
			  required_providers {
				aws = {
				  source = "hashicorp/aws"
				  version = "~> 4.46.0"
				}

				random = {
				  source = "hashicorp/random"
				  version = "~> 3.4.3"
				}

				tls = {
				  source = "hashicorp/tls"
				  version = "~> 4.0.4"
				}

				cloudinit = {
				  source = "hashicorp/cloudinit"
				  version = "~> 2.2.0"
				}

				kubernetes = {
				  source = "hashicorp/kubernetes"
				  version = "~> 2.16.1"
				}
			  }

			  backend "s3" {
				bucket         	   = "terraform-eks112211"
				key              	   = "state/terraform.tfstate"
				region         	   = "us-east-1"
			  }

			  required_version = "~> 1.3"
			}
			
	- Code trong file variables.tf:
	
		Case 1:
		
			variable "region" {
			  description = "AWS region"
			 type = string
			 default = "us-east-1"
			}

			variable "clusterName" {
			  description = "Name of the EKS cluster"
			 type = string
			 default = "vpro-eks"
			}
			
	- Code trong file vpc.tf:
	
		Case 1:
		
			module "vpc" {
			  source = "terraform-aws-modules/vpc/aws"
			  version = "3.14.2"

			  name = "vprofile-eks"

			  cidr = "172.20.0.0/16"
			  azs = slice(data.aws_availability_zones.available.names, 0, 3)

			  private_subnets = ["172.20.1.0/24", "172.20.2.0/24", "172.20.3.0/24"]
			  public_subnets = ["172.20.4.0/24", "172.20.5.0/24", "172.20.6.0/24"]

			  enable_nat_gateway = true
			  single_nat_gateway = true
			  enable_dns_hostnames = true

			  public_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/elb" = 1
			  }

			  private_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/internal-elb" = 1
			  }
			}


--- App Deployment on Kubernetes Cluster:

	- Google search:
	
		kubernetes persistent volume claim
		
		kubernetes deploy yaml
		
		https://github.com/marketplace?type=
		
		helm docs install helm
		
		

	- Commands:
	
		- cmd: echo -n "vprodbpass" | base64
		- cmd: echo -n "guest" | base64
		- cmd: kubectl create -f .
		- cmd: kubectl delete -f .
		- cmd: kubectl get sc
		- cmd: kubectl get pods -n kube-system | grep ingress
		- cmd: kubectl describe ingress vpro-ingress
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
		- cmd: kops delete cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes
		- cmd: choco install kubernetes-helm
		- cmd: helm create vprofilecharts
		- cmd: aws eks list-clusters --region us-east-1
		
		
		
--- GitOps Project:

	- Amazon Container Registry
	- Gibhub -> Project instance -> Setting -> Branches -> Add branch protection rules -> Require a pull request before merging
	- Gibhub -> Project instance -> Pull requests -> New pull request
	- SonarCloud -> New... -> Create new organization -> create one manually
	- SonarCloud -> My Account -> Security -> Generate Token
	- SonarCloud -> My organization -> instance -> Quality Gates -> Create -> Add Condition
	- SonarCloud -> My Projects -> instance -> Administration -> Quality Gate -> Use a specific quality gate
	
	
	- Google search:
	
		terraform aws modules vpc
		
		


	- Command:
	
		- cmd: cd ~/.ssh
		- cmd: export GIT_SSH_COMMAND="ssh -i ~/.ssh/actions"
		- cmd: git config core.sshCommand "ssh -i ~/.ssh/actions -F /dev/null"
		- cmd: git config --global user.name ngoctuan99
		- cmd: git config --global user.email ngoctuanqng1@gmail.com
		- cmd: git fetch upstream
		- cmd: git merge stage
		- cmd: git fetch origin stage
		- cmd: git checkout main
		- cmd: git merge origin/stage
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vprofile-eks
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: helm list
		- cmd: helm uninstall vprofile-stack
		- cmd: terraform init -backend-config="bucket=vprofileactions112211"
		
		
		
	GitOps Project - SÆ¡ Ä‘á»“ tá»•ng thá»ƒ cá»§a GitOps CI/CD pipeline trong mÃ´i trÆ°á»ng DevOps, káº¿t há»£p GitHub,
	Terraform, EKS, Docker, Maven, Helm, SonarCloud:
	
		Visual Studio Code (VS Code):
		
			LÃ  mÃ´i trÆ°á»ng local IDE cá»§a developer.

			Dev sáº½ code, commit vÃ  push lÃªn GitHub.

			Repo nÃ y cÃ³ thá»ƒ gá»“m:

				main branch (sáº£n xuáº¥t)

				stage branch (mÃ´i trÆ°á»ng staging/test)

			ğŸ’¡ Dev lÃ m viá»‡c chá»§ yáº¿u á»Ÿ stage, sau khi test xong má»›i merge vá» main.
			
		GitHub (Git + Actions):
		
			ÄÃ¢y lÃ  trung tÃ¢m Ä‘iá»u phá»‘i CI/CD.

			Gá»“m 2 workflow chÃ­nh:
			
				Terraform Workflow (bÃªn trÃ¡i):

					DÃ nh cho háº¡ táº§ng (Infrastructure as Code).

					Khi cÃ³ thay Ä‘á»•i trong repo â€œIaCâ€ (infrastructure code), GitHub Actions sáº½:

						Fetch stage branch â†’ Láº¥y code háº¡ táº§ng tá»« nhÃ¡nh stage.

						Terraform Plan/Test â†’ Kiá»ƒm tra xem viá»‡c apply cÃ³ an toÃ n khÃ´ng, háº¡ táº§ng cÃ³ thay Ä‘á»•i gÃ¬.

						Khi PR Ä‘Æ°á»£c merge vÃ o main, thÃ¬ GitHub Actions cháº¡y:

							terraform apply


								Ä‘á»ƒ triá»ƒn khai (apply) thay Ä‘á»•i lÃªn AWS.
		
				Build, Test & Deploy Workflow (bÃªn pháº£i):

					DÃ nh cho á»©ng dá»¥ng (Application Code).

					Khi code app thay Ä‘á»•i, GitHub Actions sáº½:

						Fetch code tá»« repo (branch tÆ°Æ¡ng á»©ng).

						Maven â†’ build vÃ  cháº¡y unit test (Java project).

						SonarCloud â†’ quÃ©t cháº¥t lÆ°á»£ng code (code quality & security scan).

						Docker â†’ build image vÃ  push lÃªn Amazon ECR (Elastic Container Registry).

						Helm Charts â†’ triá»ƒn khai image má»›i lÃªn Amazon EKS (Kubernetes cluster).		
		
		AWS Cloud (pháº§n trung tÃ¢m)

			ÄÃ¢y lÃ  nÆ¡i toÃ n bá»™ háº¡ táº§ng vÃ  app cháº¡y.

			CÃ¡c thÃ nh pháº§n chÃ­nh:

				Amazon ECR: nÆ¡i lÆ°u trá»¯ Docker image (container registry cá»§a AWS).

				Amazon EKS: Kubernetes cluster â€” nÆ¡i á»©ng dá»¥ng thá»±c táº¿ Ä‘Æ°á»£c deploy.

				VPC Subnet: máº¡ng ná»™i bá»™ AWS, chá»©a EKS cluster, ECR, security groups, subnet, etc.

			Má»‘i quan há»‡:

				Terraform quáº£n lÃ½ háº¡ táº§ng AWS (EKS, VPC, ECR,...).

				GitHub Actions quáº£n lÃ½ pipeline build vÃ  deploy app vÃ o Ä‘Ã³.
				
		Quy trÃ¬nh tá»•ng thá»ƒ:
		
			Giai Ä‘oáº¡n	HÃ nh Ä‘á»™ng												CÃ´ng cá»¥
			Dev Code	Code vÃ  push lÃªn GitHub (stage)							VS Code, Git
			IaC Test	GitHub Actions cháº¡y terraform plan						Terraform
			Merge		Khi PR tá»« stage â†’ main, GitHub cháº¡y terraform apply		Terraform
			Build		Build app (Maven), test code (SonarCloud)				GitHub Actions
			Image		Docker build â†’ push lÃªn ECR								Docker, ECR
			Deploy		Helm deploy image lÃªn EKS								Helm, Kubernetes
			Run	App 	cháº¡y thá»±c táº¿ trÃªn AWS									AWS EKS
		
		
		
	- Code trong file terraform.yml:

		Case 1:
		
			name: "Vprofile IAC"
			on:
			  push:
				  branches:
					  - main
					  - stage
				  paths:
					  - terraform/**
			  pull_request:
				  branches:
					  - main
				  paths:
					  - terraform/**

			env:
			  # Credentials for deployment to AWS
			  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
			  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
			  # S3 bucket for the Terraform state
			  BUCKET_TF_STATE: ${{ secrets.BUCKET_TF_STATE }}
			  AWS_REGION: us-east-1
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  terraform:
				name: "Apply terraform code changes"
				runs-on: ubuntu-latest
				defaults:
				  run:
					shell: bash
					working-directory: ./terraform
				steps:
				  - name: Checkout source code
					uses: actions/checkout@v2
				  - name: Setup Terraform with specified version on the runner
					uses: hashicorp/setup-terraform@v2
					with:
					  terraform_version: 1.6.3
				  - name: Terraform init
					id: init
					run: terraform init -backend-config="bucket=$BUCKET_TF_STATE"
				  - name: Terraform format
					id: fmt
					run: terraform fmt -check
				  - name: Terraform validate
					id: validate
					run: terraform validate
				  - name: Terraform plan
					id: plan
					run: terraform plan -no-color -input=false -out planfile
				  - name: Terraform plan status
					if: steps.plan.outcome == 'failure'
					run: exit 1

				  - name: Terraform Apply
					id: apple
					if: github.ref == 'refs/heads/main' && github.event_name == 'push'
					run: terraform apply -auto-approve -input=false -parallelism=1 planfile

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					id: getconfig
					if: steps.apple.outcome == 'success'
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Install Ingress controller
					if: steps.apple.outcome == 'success' && steps.getconfig.outcome == 'success'
					run: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		
	- Code trong file vproappdep.yml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: vproapp
			  labels: 
				app: vproapp
			spec:
			  replicas: 1
			  selector:
				matchLabels:
				  app: vproapp
			  template:
				metadata:
				  labels:
					app: vproapp
				spec:
				  containers:
				  - name: vproapp
					image: "{{ .Values.appimage }}:{{ .Values.apptag }}"
					ports:
					- name: vproapp-port
					  containerPort: 8080
				  initContainers:
				  - name: init-mydb
					image: busybox
					command: ['sh', '-c', 'until nslookup vprodb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']
				  - name: init-memcache
					image: busybox
					command: ['sh', '-c', 'until nslookup vprocache01.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']

	- Code trong file main.yml:

		Case 1:
		
			name: vprofile actions
			on: workflow_dispatch
			env:
			  AWS_REGION: us-east-1
			  ECR_REPOSITORY: vprofileapp
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  Testing:
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Maven test
					run: mvn test

				  - name: Checkstyle
					run: mvn checkstyle:checkstyle

				  # Setup java 11 to be default (sonar-scanner requirement as of 5.x)
				  - name: Set Java 11
					uses: actions/setup-java@v3
					with:
					  distribution: 'temurin' # See 'supported distributions' for availabel
					  java-version: '11'

				  # Setup sonar-scanner
				  - name: Setup SonarQube
					uses: warchant/setup-sonar-scanner@v7

				  # Run sonar-scanner
				  - name: SonarQube Scan
					run: sonar-scanner
						-Dsonar.host.url=${{ secrets.SONAR_URL }}
						-Dsonar.login=${{ secrets.SONAR_TOKEN }}
						-Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }}
						-Dsonar.projectKey=${{ secrets.SONAR_PROJECT_KEY }}
						-Dsonar.sources=src/
						-Dsonar.junit.reportsPath=target/surefire-reports/
						-Dsonar.jacoco.reportsPath=target/jacoco.exec
						-Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml
						-Dsonar.java.binaries=target/test-classes/com/visualpathit/account

				  # Check the Quality Gate status.
				  - name: SonarQube Quality Gate check
					id: sonarqube-quality-gate-check
					uses: sonarsource/sonarqube-quality-gate-action@master
					# Force to fail step after specific time.
					timeout-minutes: 5
					env:
					  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
					  SONAR_HOST_URL: ${{ secrets.SONAR_URL }} # OPTIONAL

			  BUILD_AND_PUBLISH:
				needs: Testing
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Build & Upload image to ECR
					uses: appleboy/docker-ecr-action@master
					with:
						access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
						secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
						registry: ${{ secrets.REGISTRY }}
						repo: ${{ env.ECR_REPOSITORY }}
						region: ${{ env.AWS_REGION }}
						tags: latest,${{ github.run_number }}
						daemon_off: false
						dockerfile: ./Dockerfile
						context: ./

			  DeployToEKS:
				needs: BUILD_AND_PUBLISH
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Print config file
					run: cat ~/.kube/config

				  - name: Login to ECR
					run: |
					  kubectl delete secret regcred --ignore-not-found
					  kubectl create secret docker-registry regcred --docker-server=${{ secrets.REGISTRY }} --docker-username=AWS --docker-password=$(aws ecr get-login-password)

				  - name: Deploy Helm
					uses: bitovi/github-actions-deploy-eks-helm@v1.2.8
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ secrets.AWS_REGION }}
					  cluster-name: ${{ env.EKS_CLUSTER }}
					  #configfiles: .github/values/dev.yaml
					  chart-path: helm/vprofilecharts
					  namespace: default
					  value: appimage=${{ secrets.REGISTRY }}/${{ env.ECR_REPOSITORY }},apptag=${{ github.run_number }}
					  name: vprofile-stack
		
		
		
		
		