- Danh sách chưa làm được:

	Cần đọc hiểu phần VPC trong AWS part 2

	AWS Part 1:
	
		EC2 Load Balancer
		
	AWS Cloud For Project Set Up | Lift & Shift:
	
		Chưa đăng nhập web thành công với user, password là admin_vp
		
	Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:
	
		Build & Deploy Artifact:
		
			Phần target group kiểm tra vẫn unhealthy
			
	Jenkins:
	
		Jenkins Master and Slave từ chỗ này về sau, xem xét chụp ảnh mới và làm lại
		
	Terraform:
	
		Chưa upload được file lên s3 ở phần cuối
		
	Ansible:
	
		Chỗ roles có phần chạy bị lỗi
		
	AWS Part 2:
	
		Bị lỗi tại Website in VPC
		
	Docker:
	
		Building images
		
			Chưa thể show đúng trang web
			
	Containerization:
	
		Phần cuối chạy theo source git của tác giả chưa được
		
	App Deployment on Kubernetes Cluster:
	
		Deploy App on K8s Cluster:
		
			Ingress chưa có address
			
	GitOps Project:
	
		Deploy to EKS:
		
			Chưa chạy lên trang web được


- Iaas, Paas
- ORACLE VM VIRTUAL BOX
- AWS Command line Interface











- GIT BASH

	Git Bash là một ứng dụng giả lập dòng lệnh (terminal) dành cho Windows.

	Nó cung cấp:

		Git CLI (Command Line Interface) → để chạy các lệnh Git (git init, git clone, git commit, …).

		Bash shell → một môi trường giống Linux/Unix để chạy các lệnh bash (ls, pwd, cd, rm, touch, …).

	Nói đơn giản: Git Bash giúp người dùng Windows có trải nghiệm giống như đang làm việc trên Linux khi dùng Git.
	
	Tại sao cần Git Bash?

		Trên Linux/Mac, Git thường đi kèm với Bash sẵn có.

		Nhưng Windows không có Bash gốc → vì vậy Git for Windows ra đời, kèm theo Git Bash để:

			Dùng lệnh Git thuận tiện.

			Dùng các lệnh bash cơ bản (thay vì chỉ Command Prompt hay PowerShell).
			
	Các thành phần chính

		Git → công cụ quản lý phiên bản.

		Bash → shell dựa trên MSYS2 (một môi trường giúp mang Linux tools sang Windows).

		Unix tools → bạn có thể dùng nhiều lệnh quen thuộc trong Linux (ssh, scp, cat, nano, vim, …).

- VAGRANT

	Vagrant là một công cụ mã nguồn mở (open-source) dùng để tạo, quản lý và tự động hóa môi trường máy ảo (VMs).

	Nó giúp lập trình viên, tester, DevOps… có thể dựng môi trường phát triển giống nhau chỉ với một file cấu
	hình (Vagrantfile).

	Vagrant thường chạy trên VirtualBox, VMware, Hyper-V hoặc Docker.
	
	Ưu điểm của Vagrant

		Tái tạo môi trường dễ dàng

		Chỉ cần 1 file Vagrantfile, bất kỳ ai cũng có thể tạo ra môi trường y hệt.

		Tránh lỗi kiểu: "Máy anh chạy được, máy em không chạy".

		Tích hợp với nhiều provider

		Hỗ trợ VirtualBox, VMware, Docker, AWS EC2...

		Tự động hóa

		Có thể cài đặt phần mềm, cấu hình hệ điều hành thông qua provisioning tools (Shell script, Ansible, Puppet, Chef...).

		Dễ dùng

		Lệnh chính thường chỉ có vagrant up, vagrant halt, vagrant destroy.
		
	Cách hoạt động

		Bạn cài Vagrant + một provider (ví dụ VirtualBox).

		Tạo file Vagrantfile mô tả môi trường (OS, RAM, CPU, mạng, phần mềm cần cài).
		
		Chạy lệnh:
		
			vagrant up
			
				Vagrant sẽ tải box (giống như 1 template OS) từ Vagrant Cloud, tạo máy ảo, cấu hình mạng, cài phần mềm.

- CHOCOLATEY/BREW
- SUBLIME TEXT EDITOR
- AWS CLI

- IAM WITH MFA
- Homebrew
- SonarSource:

	SonarSource là một công ty công nghệ chuyên phát triển các công cụ giúp phân tích chất
	lượng mã nguồn và phát hiện vấn đề bảo mật trong phần mềm.
	
- SonarQube:

	Công cụ phân tích code phổ biến nhất của SonarSource.

	Hỗ trợ hơn 25 ngôn ngữ lập trình (Java, C#, Python, JavaScript, C/C++, Go…).

	Tích hợp vào CI/CD để tự động kiểm tra code mỗi khi build.

	Giúp phát hiện:

		Bug (lỗi có thể gây crash hoặc kết quả sai).

		Code Smell (đoạn code khó bảo trì, không tối ưu).

		Security Vulnerability (lỗ hổng bảo mật).

- SONARCLOUD:

	Dịch vụ SaaS trên cloud (không cần cài SonarQube server).

	Dùng để phân tích code của dự án public hoặc private trên GitHub, GitLab, Bitbucket, Azure DevOps.
	
	

	Homebrew là một trình quản lý gói (package manager) phổ biến trên macOS (và Linux).

	Nói đơn giản, Homebrew giúp bạn cài đặt, quản lý và cập nhật phần mềm bằng các
	lệnh trong terminal, thay vì phải tải file .dmg hoặc .pkg và cài thủ công.
	
- choco
- MFA device

	MFA (Multi-Factor Authentication) = Xác thực đa yếu tố.

	Khi bật MFA, ngoài username + password, bạn cần thêm mã OTP từ thiết bị MFA thì mới đăng nhập được.

	Giúp tăng cường bảo mật cho tài khoản AWS (nhất là với root user hoặc IAM user có quyền cao).
	
	MFA Device (thiết bị MFA) trong AWS:
	
		MFA Device = nguồn phát mã OTP để đăng nhập AWS.
		
		AWS hỗ trợ nhiều loại thiết bị MFA:

		Virtual MFA device (phổ biến nhất):

			Dùng app trên điện thoại, ví dụ:

				Google Authenticator

				Authy

				Microsoft Authenticator

				LastPass Authenticator

			Khi đăng nhập AWS, bạn mở app → nhập mã OTP hiển thị (thay đổi mỗi 30 giây).

		Hardware MFA device (phần cứng):

			Thiết bị vật lý nhỏ (giống USB token).

			Ví dụ: Gemalto, Yubikey.

			Phù hợp cho môi trường cần bảo mật cao.

		U2F security key (khóa bảo mật chuẩn FIDO):

			Dùng USB/NFC key (như Yubico YubiKey).

			Chỉ cần cắm vào máy hoặc chạm vào khi AWS yêu cầu xác thực.

		SMS MFA (ít dùng):

			AWS gửi mã OTP qua tin nhắn SMS.

			Tuy nhiên kém an toàn (dễ bị hack SIM), nên ít được khuyên dùng.
			
	Cách hoạt động:
	
		Đăng nhập vào AWS Console bằng username & password.

		AWS yêu cầu MFA code.

		Bạn nhập mã OTP từ MFA device (app hoặc hardware).

		Nếu đúng → vào được AWS Console.
		
	Tại sao nên dùng MFA?
	
		Ngăn hacker truy cập ngay cả khi mật khẩu bị lộ.

		Bảo vệ root account (tài khoản quản trị tối cao trong AWS).

		Tăng cường tuân thủ bảo mật (PCI-DSS, ISO, SOC2…).
		
	AWS MFA Device = thiết bị phát mã OTP (virtual app, USB key, hardware token…) để đăng nhập AWS an toàn hơn.

- .csv file AWS
- Cloudwatch AWS
- SNS topic AWS
- EC2 AWS

	EC2 (Elastic Compute Cloud) là dịch vụ máy chủ ảo (virtual server) trên nền tảng AWS (Amazon Web Services).

	Nói nôm na: EC2 giống như bạn thuê một cái máy tính trong đám mây để cài đặt hệ điều hành,
	phần mềm, web server, database… và chạy ứng dụng của bạn.
	
	Khi tạo một EC2 Instance, bạn cần chọn:
	
		AMI (Amazon Machine Image) → giống như chọn hệ điều hành (Ubuntu, Amazon Linux, Windows…).

		Instance type → cấu hình phần cứng (t2.micro, m5.large, …).

		Storage (EBS) → dung lượng ổ cứng.

		Security Group → giống firewall, cho phép mở port (ví dụ: 22 cho SSH, 80 cho HTTP, 443 cho HTTPS).

		Key Pair → để đăng nhập vào server qua SSH.
		
	EC2 dùng để làm gì?
	
		Host website hoặc API.

		Chạy database (MySQL, PostgreSQL, MongoDB…).

		Chạy ứng dụng microservices, container (Docker, Kubernetes).

		Làm máy chủ dev/test cho lập trình viên.

		Xử lý big data, AI/ML, game server.
		
	EC2 = Máy chủ ảo trong AWS → bạn muốn chạy app/web gì thì chỉ cần tạo một EC2 instance rồi cài đặt như trên máy tính thật.

- AWS Certificate Manager:

	ACM là dịch vụ của AWS để cấp phát, quản lý và triển khai chứng chỉ SSL/TLS cho ứng dụng, website và dịch vụ AWS.

	Nó giúp bạn bảo mật kết nối HTTPS mà không cần tự mua hoặc tự quản lý certificate thủ công.

	Nói ngắn gọn: ACM = dịch vụ quản lý chứng chỉ SSL/TLS trong AWS.
	
	Tính năng chính

		Cấp phát (Provision)

			ACM có thể cấp chứng chỉ SSL/TLS miễn phí cho domain của bạn.

			Ví dụ: www.myapp.com có thể xin chứng chỉ từ ACM thay vì mua ngoài.

		Triển khai (Deploy)

			Dùng trực tiếp với các dịch vụ AWS:

				Elastic Load Balancer (ELB)

				CloudFront (CDN)

				API Gateway

				App Runner / Elastic Beanstalk

		Gia hạn tự động (Auto-renewal)

			Chứng chỉ do ACM cấp được tự động gia hạn trước khi hết hạn → không lo downtime vì quên renew SSL.

		Import Certificate (Nhập chứng chỉ)

			Nếu bạn đã có chứng chỉ từ CA khác (Ví dụ: DigiCert, GoDaddy, Let’s Encrypt), bạn có thể import vào ACM để quản lý.

		Quản lý tập trung

			Lưu trữ và phân phối chứng chỉ an toàn.

			Không cần copy file .crt và .key thủ công.

- RSA 2048 AWS
- VMware
- Host OS
- Guest OS
- VM
- Snapshot
- Hypervisor
- CentOS VM, Ubuntu VM

	CentOS VM

		CentOS (Community ENTerprise Operating System) là một bản phân phối Linux dựa trên Red Hat Enterprise Linux (RHEL).

		Thường dùng trong doanh nghiệp, server production vì:

			Ổn định, ít thay đổi.

			Chu kỳ phát hành dài (7–10 năm).

			Tương thích với phần mềm viết cho RHEL.

		Package Manager: yum (CentOS 7), dnf (CentOS 8+).

		Thích hợp: Web server (Apache, Nginx), Database server (MySQL, PostgreSQL), môi trường Enterprise.

		Tuy nhiên: CentOS Linux chính thức bị ngừng phát triển từ 2021, thay thế bằng CentOS Stream (rolling release, cập nhật nhanh hơn).
		
	Ubuntu VM

		Ubuntu là bản phân phối Linux dựa trên Debian.

		Thân thiện hơn với người dùng, tài liệu và cộng đồng hỗ trợ cực nhiều.

		Package Manager: apt (Advanced Package Tool).

		Có nhiều phiên bản:

			Ubuntu Server → chạy trên cloud, server.

			Ubuntu Desktop → dùng làm hệ điều hành cá nhân.

		Phù hợp: Development, AI/ML, Cloud (AWS, Azure, GCP), container (Docker, Kubernetes).
		
- RHEL (Red Hat Enterprise Linux) và Debian:

	RHEL (Red Hat Enterprise Linux)

		Nguồn gốc: Dựa trên Linux kernel, phát triển bởi Red Hat.

		Định hướng: Hệ điều hành thương mại, enterprise, dùng trong doanh nghiệp và data center.

		Hỗ trợ: Có hợp đồng support từ Red Hat (SLA, cập nhật bảo mật, bug fix).

		Package manager: rpm (Red Hat Package Manager) + yum/dnf.

		Ưu điểm:

			Ổn định, chu kỳ release lâu dài (7–10 năm).

			Được chứng nhận để chạy nhiều phần mềm enterprise (Oracle DB, SAP, VMware...).

			Hỗ trợ tốt cho server vật lý và máy ảo.

		Nhược điểm:

			Phải trả phí để dùng bản chính thức và nhận cập nhật.

			Tài liệu cộng đồng ít hơn so với Ubuntu/Debian.

		Bản miễn phí, cộng đồng hóa của RHEL: CentOS, Rocky Linux, AlmaLinux.
		
	Debian

		Nguồn gốc: Một trong những bản Linux lâu đời nhất, phát triển bởi cộng đồng (ra đời 1993).

		Định hướng: Hệ điều hành miễn phí, mã nguồn mở, tập trung vào sự ổn định và tự do phần mềm.

		Package manager: dpkg + apt.

		Ưu điểm:

			Rất ổn định, uy tín lâu đời.

			Là nền tảng cho nhiều distro phổ biến (như Ubuntu, Linux Mint...).

			Hoàn toàn miễn phí.

			Cộng đồng hỗ trợ rộng rãi.

		Nhược điểm:

			Không có hỗ trợ thương mại chính thức (trừ khi thông qua bên thứ 3).

			Chu kỳ phát hành hơi chậm → ít cập nhật tính năng mới so với Ubuntu.

- Linux distros

	Linux Distro là gì?

		Linux Distro (Linux Distribution) = bản phân phối Linux.

		Linux chỉ có kernel (nhân hệ điều hành). Muốn dùng được, người ta phải thêm:

			Trình quản lý gói (package manager)

			Thư viện hệ thống

			Công cụ quản trị

			Môi trường desktop (GUI) (nếu cần)

			Ứng dụng mặc định

		Khi kết hợp tất cả những thành phần này lại, ta có một Linux Distribution – tức là một "phiên bản Linux" hoàn chỉnh.
		
	Ví dụ các Linux Distro phổ biến:

		Dựa trên Debian: Ubuntu, Linux Mint, Kali Linux.

		Dựa trên RHEL: CentOS, Rocky Linux, AlmaLinux, Fedora.

		Dành cho bảo mật: Kali Linux, Parrot OS.

		Dành cho người mới: Ubuntu, Linux Mint, Zorin OS.

		Dành cho enterprise: RHEL, SUSE, Oracle Linux.

		Dành cho developer/hacker thích tuỳ biến: Arch Linux, Gentoo.

- Putty
- Oracle VM Virtualbox, Virtualbox
- ISO file
- Vagrant
- BIOS

	BIOS = Basic Input/Output System.

	Là một phần mềm hệ thống cấp thấp được lưu trong chip nhớ ROM/Flash trên mainboard.

	Khi bạn bật máy tính, BIOS sẽ là chương trình chạy đầu tiên, trước cả hệ điều hành (Windows, Linux, macOS…).
	
	Chức năng chính của BIOS

		POST (Power-On Self-Test)

			Kiểm tra phần cứng: RAM, CPU, bàn phím, ổ cứng… có hoạt động bình thường không.

		Khởi tạo phần cứng

			Gắn driver cơ bản cho bàn phím, chuột, card màn hình…

		Chọn thiết bị khởi động (Bootloader)

			Quyết định máy sẽ boot từ HDD/SSD, USB, CD/DVD hay Network.

		Cung cấp giao diện cấu hình (BIOS Setup Utility)

			Cho phép bạn chỉnh thời gian, thứ tự boot, mật khẩu BIOS, ép xung CPU/RAM, bật/tắt thiết bị…
			
	BIOS = phần mềm chạy ngay khi bật máy, nhiệm vụ là kiểm tra phần cứng và nạp hệ điều hành.

	Ngày nay, phần lớn máy tính mới đã dùng UEFI, nhưng người ta vẫn hay gọi chung là "BIOS".

- Vtx
- Reboot
- Ethernet (emp0s8), Ethernet (emp0s3)
- ACPI shutdown trong Oracle VM
- SSH
- NAT trong Oracle VM
- Vagrantfile
- Linux distros
- Desktop Linux OS

	Desktop Linux OS (hay còn gọi là Linux Desktop Operating System) là một hệ điều hành Linux được thiết kế
	để sử dụng trên máy tính cá nhân, laptop hoặc workstation với giao diện đồ họa (GUI) thân thiện, giống
	như Windows hay macOS.
	
	Thành phần chính của Desktop Linux OS

		Kernel (Linux kernel): lõi của hệ điều hành, quản lý phần cứng.

		Distro (bản phân phối): ví dụ Ubuntu, Fedora, Linux Mint, Manjaro… → cung cấp bộ công cụ, ứng dụng, package manager.

		Desktop Environment (DE): giao diện đồ họa người dùng, như:

			GNOME (Ubuntu)

			KDE Plasma (Kubuntu, openSUSE)

			XFCE, LXQt (nhẹ, tiết kiệm tài nguyên)

		Ứng dụng người dùng: trình duyệt (Firefox, Chrome), bộ office (LibreOffice), media player, email client...
		
	Đặc điểm của Desktop Linux OS

		Miễn phí & mã nguồn mở.

		Tùy biến cao (có thể thay đổi giao diện, DE, theme).

		Ổn định, bảo mật (ít virus hơn Windows).

		Hỗ trợ phần cứng khá rộng, nhưng vẫn còn hạn chế với một số driver (ví dụ card đồ họa, máy in đời mới).

		Nhiều bản phân phối khác nhau phục vụ nhu cầu khác nhau:

			Thân thiện cho người mới: Ubuntu, Linux Mint, Zorin OS

			Dành cho dev/pro: Fedora, openSUSE, Debian

			Siêu nhẹ: Lubuntu, Puppy Linux
			
	Desktop Linux OS là phiên bản Linux được đóng gói sẵn với giao diện đồ họa và ứng dụng cần thiết cho người
	dùng cá nhân, giống như Windows/macOS, nhưng miễn phí và tùy biến mạnh mẽ.

- Server Linux OS

	Server Linux OS (Linux Server Operating System) là một hệ điều hành Linux được tối ưu để chạy trên máy
	chủ (server) thay vì máy tính cá nhân. Nó cung cấp môi trường ổn định, bảo mật, và hiệu suất cao để chạy
	các dịch vụ mạng (web, database, mail, file server, container...).
	
	Đặc điểm của Server Linux OS

		Không cần giao diện đồ họa (GUI) → thường chỉ chạy ở chế độ command line (CLI) để tiết kiệm tài nguyên.

		Tối ưu cho hiệu suất và bảo mật → chạy lâu dài, ít cần reboot.

		Hỗ trợ nhiều dịch vụ server: web server (Apache, Nginx), database (MySQL, PostgreSQL), container (Docker, Kubernetes)...

		Cập nhật ổn định, vòng đời dài (LTS – Long Term Support).

		Khả năng mở rộng (scalability) để phục vụ nhiều người dùng cùng lúc.
		
	Các bản phân phối (distro) Server Linux phổ biến

		Ubuntu Server (dễ dùng, cộng đồng lớn, hỗ trợ LTS).

		CentOS Stream (tiếp nối CentOS, gần với Red Hat Enterprise Linux).

		Red Hat Enterprise Linux (RHEL) (bản thương mại, hỗ trợ doanh nghiệp).

		Debian Server (ổn định, ít thay đổi, rất phổ biến).

		SUSE Linux Enterprise Server (SLES) (doanh nghiệp, hỗ trợ mạnh mẽ).
		
	Server Linux OS là Linux được tinh chỉnh cho môi trường server, chú trọng hiệu suất, bảo mật, độ ổn định.

	Desktop Linux OS thì thiên về trải nghiệm người dùng cuối với GUI và ứng dụng văn phòng/giải trí.
	
	Khi nào cần dùng Linux Server?

		Bạn sẽ dùng Linux Server khi:

			Cần một máy chạy dịch vụ cho nhiều người khác truy cập (web server, database server, mail server, file server...).

			Cần hệ thống ổn định, chạy 24/7 (không tắt máy như PC cá nhân).

			Cần một môi trường bảo mật, nhẹ, ít tốn tài nguyên (thường không có GUI, chỉ dùng dòng lệnh).

			Triển khai trong data center, cloud (AWS, Azure, GCP) hoặc trên máy chủ vật lý.

		Trường hợp deploy web bằng AWS thì sao?

			Khi bạn deploy website lên AWS, bạn thường sẽ chạy trên một dịch vụ như EC2 (máy ảo trong AWS).

			Máy EC2 này thường được cài một distro Linux Server (ví dụ: Ubuntu Server, Amazon Linux, CentOS Stream, Debian Server...).

			Sau đó bạn sẽ cài nginx, Apache (httpd), Node.js, Java, MySQL, v.v. trên đó để chạy ứng dụng.

- RPM based, Debian based trong Linux

	Trong thế giới Linux, khi người ta nói “RPM-based” và “Debian-based”, họ đang nói về hệ quản lý
	gói (package management system) mà bản phân phối (distro) đó sử dụng.
	
	Debian-based:
	
		Nguồn gốc: từ Debian.

		Trình quản lý gói: dpkg (Debian Package).

		Định dạng gói: .deb.

		Công cụ cao hơn: apt-get, apt, aptitude (dùng để cài đặt, update từ repository).

		Đặc điểm:

			Kho phần mềm rất lớn, cộng đồng mạnh.

			Thân thiện cho người mới (ví dụ Ubuntu).

		Ví dụ distro Debian-based:

			Debian

			Ubuntu (và các biến thể: Kubuntu, Xubuntu, …)

			Linux Mint

			Kali Linux

			Pop!_OS
			
	RPM-based:

		Nguồn gốc: từ Red Hat.

		Trình quản lý gói: rpm (Red Hat Package Manager).

		Định dạng gói: .rpm.

		Công cụ cao hơn:

			yum (Yellowdog Updater, Modified – dùng trong CentOS, RHEL cũ).

			dnf (Dandified YUM – thay thế yum trong Fedora, RHEL 8+).

		Đặc điểm:

			Ổn định, nhiều bản thương mại (RHEL, SUSE).

			Thường dùng trong môi trường doanh nghiệp.

		Ví dụ distro RPM-based:

			Red Hat Enterprise Linux (RHEL)

			CentOS, CentOS Stream

			Fedora

			openSUSE, SUSE Linux Enterprise

- ftp
- telnet
- jenkin
- tar
- bitbucket
- codecommit
- yum
- vagrant cloud

	Vagrant Cloud là một dịch vụ online do HashiCorp cung cấp, đóng vai trò như
	một “kho lưu trữ box” (Vagrant boxes) cho cộng đồng và doanh nghiệp.

	Bạn có thể xem nó giống như Docker Hub nhưng dành cho Vagrant.
	
	Các chức năng chính của Vagrant Cloud

		Chia sẻ box (Vagrant Boxes)

			Tải về các box có sẵn (Ubuntu, CentOS, Debian, Windows, …).

			Ví dụ:	

				vagrant init hashicorp/bionic64
				vagrant up

		Đăng tải box của bạn

			Nếu bạn tạo một máy ảo Vagrant với cấu hình đặc biệt, bạn có thể upload lên
			Vagrant Cloud để chia sẻ cho đồng đội hoặc cộng đồng.

		Quản lý box private/public

			Public box: ai cũng có thể tải.

			Private box: chỉ nhóm/bạn bè hoặc công ty của bạn có quyền truy cập.
			
		Tích hợp với Vagrant CLI

			Bạn có thể login bằng CLI:
			
				vagrant login
				
		Team & Enterprise

			Cho phép tạo tổ chức, nhóm, phân quyền để quản lý box trong nội bộ công ty.

- daemon
- hostmanager
- memcache
- apache tomcat
- nginx
- EPEL repository
- systemctl command
- OSI
- DNS & DHCP
- .ppk va .pem trong key pair aws
- AWS EC2
- AWS AMI
- tag trong EC2
- security group trong AMI
- AWS IAM

	IAM (Identity and Access Management) là dịch vụ của AWS dùng để quản lý người dùng, nhóm và quyền truy cập đến các tài nguyên AWS.

	Nó giúp bạn kiểm soát ai có thể truy cập vào dịch vụ nào và được làm gì trong AWS.

	Nói ngắn gọn: IAM = Hệ thống phân quyền & bảo mật trong AWS.
	
	Thành phần chính trong IAM:
	
		User (Người dùng)

			Đại diện cho một cá nhân (developer, admin…) hoặc một ứng dụng.

			Mỗi user có credentials (username/password hoặc access key/secret key).

		Group (Nhóm)

			Tập hợp nhiều user.

			Quyền được gán cho group → tất cả user trong group đều có quyền đó.

		Role (Vai trò)

			Dùng cho dịch vụ AWS hoặc ứng dụng để có quyền truy cập tài nguyên.

			Ví dụ: EC2 có thể assume một role để đọc dữ liệu từ S3.

		Policy (Chính sách)

			Tài liệu JSON định nghĩa quyền truy cập (cho phép hay từ chối).

			Ví dụ: policy cho phép user đọc file trong S3 bucket.
			
	Chức năng chính của IAM:
	
		Quản lý người dùng & nhóm (user, group).

		Cấp quyền chi tiết bằng policy (theo nguyên tắc least privilege – chỉ cấp quyền tối thiểu cần thiết).

		Hỗ trợ MFA (Multi-Factor Authentication) để tăng bảo mật.

		Tích hợp với dịch vụ khác như EC2, Lambda, S3, RDS…
		
	Ví dụ thực tế:
	
		Dev A chỉ cần đọc dữ liệu trong S3 → tạo IAM user + policy s3:GetObject.

		Dev B cần quyền quản lý EC2 → gán policy AmazonEC2FullAccess.

		EC2 server cần quyền truy cập DynamoDB → tạo IAM Role cho EC2.
		
	AWS IAM = Quản lý danh tính & quyền truy cập trong AWS, giúp đảm bảo an toàn và kiểm soát chính xác ai được
	làm gì trên tài nguyên AWS.
	
- AWS IAM security credentials

	Trong AWS IAM, security credentials là các thông tin xác thực mà một IAM user hoặc role dùng để truy cập tài nguyên AWS.

	Có 2 cách chính để truy cập AWS:

		AWS Management Console (giao diện web) → dùng username + password (+ MFA nếu bật).

		AWS CLI / SDK / API → dùng Access keys (Access key ID + Secret access key).
		
	Các loại Security Credentials trong IAM:
	
		Password (Console access)

			Dùng để đăng nhập vào AWS Management Console.

			Có thể yêu cầu user đổi mật khẩu khi lần đầu đăng nhập.
			
		Access Keys (Programmatic access)

			Gồm 2 phần:

				Access key ID → giống username.

				Secret access key → giống mật khẩu.

			Dùng để truy cập AWS qua CLI, SDK, API.

			Ví dụ khi cấu hình AWS CLI:
			
				aws configure
				AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
				AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
				Default region name [None]: ap-southeast-1
				Default output format [None]: json
				
		SSH Keys (cho EC2 Instance Connect)

			Dùng để kết nối SSH vào các máy EC2.	

		Server Certificates

			Dùng cho AWS services như Elastic Load Balancing (ELB) hoặc CloudFront để hỗ trợ HTTPS (SSL/TLS).
			
		MFA Devices

			Virtual (Google Authenticator, Authy), Hardware (YubiKey…), SMS.

			Dùng để tăng bảo mật cho login.
			
		X.509 Certificates (ít dùng hiện nay)

			Dùng cho các dịch vụ cũ hoặc tích hợp đặc biệt.
			
	Best Practices (khuyến nghị bảo mật)

		Không bao giờ dùng Access Key của root user 🚫.

		Tạo IAM user/role riêng cho từng ứng dụng.

		Bật MFA cho root user và user quan trọng.

		Dùng IAM Role thay vì access key nếu app chạy trên AWS (VD: EC2 → S3).

		Rotate access key thường xuyên (xóa key cũ, tạo key mới).

		Chỉ cấp quyền theo nguyên tắc least privilege (tối thiểu cần thiết).
		
	Security Credentials trong AWS IAM = tập hợp thông tin xác thực (password, access keys,
	SSH keys, MFA…) mà user/role dùng để truy cập AWS một cách an toàn.
	
- Console Access:

	Console Access nghĩa là một IAM User (hoặc root user) có thể đăng nhập vào AWS Management Console (giao diện web của AWS) bằng:

		Username/Password

		(MFA code nếu đã bật Multi-Factor Authentication

	Nói ngắn gọn: Console Access = quyền truy cập AWS qua giao diện web (console.aws.amazon.com).
	
- AWS Billing:

	AWS Billing là hệ thống quản lý chi phí, hóa đơn và thanh toán trong tài khoản AWS.
	
	Nó cho phép bạn:

		Xem chi phí sử dụng dịch vụ AWS (theo ngày, theo dịch vụ, theo region).

		Quản lý hóa đơn (invoice) và phương thức thanh toán.

		Thiết lập cảnh báo chi phí (budget alerts) để tránh vượt quá ngân sách.

		Phân tích chi phí theo từng dự án, team, hoặc môi trường (prod/dev/test).
		
	Các thành phần chính trong AWS Billing

		Bills (Hóa đơn)

			Hiển thị chi tiết chi phí theo từng dịch vụ (EC2, S3, RDS, Lambda…).

			Có thể tải về dạng PDF hoặc CSV.

		Payment Methods (Phương thức thanh toán)

			AWS chấp nhận thẻ tín dụng/ghi nợ (Visa, Mastercard, AmEx…) và một số phương thức khác.

		Budgets (Ngân sách)

			Đặt ngân sách chi phí (ví dụ $50/tháng).

			Nhận cảnh báo qua email hoặc SNS nếu vượt mức.

		Cost Explorer

			Công cụ trực quan hóa chi phí (biểu đồ, báo cáo).

			Cho phép phân tích xu hướng chi tiêu, dự đoán chi phí trong tương lai.

		Free Tier Usage

			Hiển thị mức sử dụng AWS Free Tier (miễn phí 12 tháng hoặc vĩnh viễn cho một số dịch vụ).

			Giúp tránh vượt giới hạn free mà phát sinh phí ngoài ý muốn.

		Consolidated Billing (cho AWS Organizations)

			Nếu bạn có nhiều tài khoản AWS trong một Organization, có thể gộp hóa đơn → nhận giảm giá volume.
			
	Best Practices về Billing

		Luôn bật Billing Alerts để theo dõi chi phí.

		Dùng AWS Budgets để cảnh báo vượt mức ngân sách.

		Gắn Tags cho tài nguyên (ví dụ: Project: Ecommerce, Env: Dev) để phân tích chi phí theo dự án.

		Xóa tài nguyên không dùng (EC2, EBS, Elastic IP, Load Balancer…) vì AWS vẫn tính phí nếu còn tồn tại.
		
	AWS Billing = hệ thống quản lý chi phí, hóa đơn và thanh toán trong AWS, giúp bạn kiểm soát và tối
	ưu chi tiêu khi dùng dịch vụ AWS.
	
- AWS CloudWatch:

	Amazon CloudWatch là dịch vụ giám sát (monitoring) và quan sát (observability) của AWS.
	
	Nó giúp bạn theo dõi:

		Metrics (chỉ số: CPU, RAM, Network, Disk I/O…).

		Logs (nhật ký ứng dụng, hệ điều hành, dịch vụ AWS).

		Events (sự kiện hệ thống, thay đổi trạng thái).

		Alarms (cảnh báo khi vượt ngưỡng).
		
	Thành phần chính của CloudWatch

		Metrics:

			Thu thập số liệu về tài nguyên AWS.

			Ví dụ:

			EC2: CPUUtilization, NetworkIn/Out.

			RDS: FreeStorageSpace, DatabaseConnections.

			Lambda: Invocations, Duration, Errors.

		Alarms:

			Đặt ngưỡng cảnh báo dựa trên metrics.

			Ví dụ: CPU EC2 > 80% trong 5 phút → gửi cảnh báo qua SNS (email/SMS) hoặc tự động scale up.

		Logs:

			Lưu và phân tích log từ:

			Ứng dụng (Java, Python, NodeJS…).

			Hệ điều hành (EC2).

			Lambda (log tự động vào CloudWatch Logs).

		Events (hoặc EventBridge):

			Theo dõi sự kiện hệ thống AWS (ví dụ: EC2 stop/start).

			Có thể tự động kích hoạt Lambda function, SNS notification, hoặc Step Functions.

		Dashboards:

			Tạo bảng điều khiển tùy chỉnh với biểu đồ metrics, logs.

			Ví dụ: giám sát toàn bộ hệ thống microservices trên 1 dashboard.
			
	Ví dụ thực tế

		Bạn có một EC2 chạy Spring Boot:

		CloudWatch Metrics theo dõi CPU, RAM, Disk.

		CloudWatch Logs lưu lại log ứng dụng (/var/log/...).

		CloudWatch Alarm cảnh báo khi CPU > 70%.

		CloudWatch Event kích hoạt Auto Scaling để thêm EC2 mới khi tải cao.
		
	Lợi ích

		Giúp tự động giám sát hệ thống → không cần kiểm tra thủ công.

		Dễ dàng tìm lỗi nhờ log tập trung.

		Hỗ trợ tối ưu chi phí (theo dõi sử dụng tài nguyên).

		Kết hợp với Auto Scaling, Lambda, SNS để phản ứng tự động.
		
	AWS CloudWatch = công cụ giám sát metrics, logs, events và tạo cảnh báo cho hệ thống AWS.

	Nó giống như Prometheus + Grafana + ELK, nhưng được tích hợp sẵn trong AWS.

- AWS IAM access key
- AWS Elastic Block Storage
- Volumn in configure storage of IAM S3
- AWS EC2 Image Builder
- AWS EC2 launch templates
- AWS Cloud Watch
- AWS Route53
- AWS RDS
- AWS cloudfront
- AWS Amazon S3
- AWS Amazon SNS
- Amazon Elastic File System
- AWS EFS
- AWS NFS
- AWS Auto Scaling
- AWS RDS
- public access trong RDS
- AWS S3
- Elastic Beanstalk
- Amazon ElastiCache
- Amazon Elastic Container Registry
- Poll SCM
- Webhooks
- CRUMB
- event-driven
- boto3 python
- python fabric
- Terraform
- Ansible
- ad hoc command
- ansible palybook
- Amazon VPC
- AWS CloudWatch Logs
- Bitbucket
- AWS CodeBuild
- AWS CodePipeline
- docker pull nginx:mainline-alpine-perl
- docker engine
- minikube
- Kops
- key pair va security group trong ec2 instance
- curl
- kubectl binary
- kubernetes cli
- kubernetes namespace
- kubernetes taints and tolerations
- Amazon Elastic Kubernetes Service
- kubernetes ingress
- git fork
- SSH key
- AWS Elastic Container Registry
- kubernetes cluster
- QA, BA trong doanh nghiep
- Nesus
- file TAR
- helm chart
- Server vật lý (physical server) và Server ảo (virtual server / VM):

	Server vật lý (physical server)

		Là máy chủ thật đặt trong data center (có CPU, RAM, ổ cứng, card mạng).

		Bạn có thể cài hệ điều hành lên nó: Linux Server (Ubuntu Server, CentOS, Debian Server, …)
		hoặc Windows Server.

		Sau khi cài xong, bạn thao tác quản lý bằng SSH (nếu Linux) hoặc RDP (nếu Windows).

	Server ảo (virtual server / VM)

		Là máy chủ ảo hóa tạo ra từ phần mềm như VMware, VirtualBox, KVM, hoặc dịch vụ Cloud
		như AWS EC2, Azure VM, Google Compute Engine.

		Bên trong server ảo đó, bạn cũng cài Linux Server (hoặc Windows Server).

		Khi truy cập và thao tác, trải nghiệm không khác gì server vật lý.

	Nghĩa là:

		Dù bạn dùng server vật lý hay server ảo, nếu hệ điều hành cài bên trong là Linux Server, thì bạn vẫn
		sẽ thao tác bằng dòng lệnh Linux Server (CLI qua SSH).



Missing in Jenkin lession:
	- Agent/Node/Slave in Jenkins
	- Using Agent/Note/Slave
Missing Build tools lession


--- Vòng đời DevOps:

	1. CODE

	Lập trình viên viết code (Java, HTML, CSS, JS, …).

	Đây là nguồn gốc của mọi thứ trong pipeline.

	2. CODE BUILD

	Code được build thành gói chạy được (JAR, WAR, Docker image, …).

	Công cụ thường dùng: Maven, Gradle, Jenkins, GitLab CI, GitHub Actions.

	3. CODE TEST

	Chạy test tự động (unit test, integration test).

	Đảm bảo code không bị lỗi logic.

	4. CODE ANALYSIS

	Phân tích chất lượng code (coding style, security scan, bug detection).

	Công cụ: SonarQube, PMD, Checkstyle.

	5. DELIVERY

	Chuẩn bị phần mềm để đưa qua môi trường staging hoặc production.

	Bao gồm artifact repository (Nexus, Artifactory).

	6. DB / Security / OS CHANGES

	Các thay đổi liên quan tới database migration, cấu hình hệ điều hành, bảo mật.

	Thường được tự động hóa bằng Ansible, Terraform, Liquibase, Flyway.

	7. SOFTWARE TESTING

	Test ở mức hệ thống (system testing, performance testing, user acceptance testing).

	Mục tiêu: kiểm thử toàn bộ ứng dụng trong môi trường staging.

	8. DEPLOY TO PROD

	Deploy ứng dụng sang production environment.

	Công cụ: Docker, Kubernetes, Jenkins, ArgoCD.

	9. GO LIVE

	Ứng dụng bắt đầu chạy cho người dùng thật.

	Đòi hỏi monitoring (Prometheus, Grafana, ELK stack) để theo dõi hoạt động.

	10. USER APPROVAL

	Người dùng/khách hàng kiểm tra và xác nhận ứng dụng đáp ứng yêu cầu.

	Feedback được đưa lại cho team Dev, tạo vòng lặp mới.


--- Các lệnh cmd:

	- cmd: choco list
	- cmd: choco install virtualbox --version=7.0.8 -y
	- cmd: choco install vagrant --version=2.3.7 -y
	- cmd: choco install corretto17jdk -y
	- cmd: choco install maven -y
	- cmd: choco install awscli -y
	- cmd: choco install intellijidea-community -y
	- cmd: choco install vscode -y
	- cmd: choco install sublimetext3 -y
	
	
--- VM setup:

	- cmd: ip addr show
	
		Đây là lệnh trong Linux dùng để hiển thị thông tin các địa chỉ IP trên máy. Nó thuộc bộ công
		cụ iproute2 (thay thế dần ifconfig).
		
		Kết quả khi chạy:
		
			[root@localhost apache-tomcat-10.1.46]# ip addr show
			1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
				link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
				inet 127.0.0.1/8 scope host lo
				   valid_lft forever preferred_lft forever
				inet6 ::1/128 scope host
				   valid_lft forever preferred_lft forever
			2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:e4:ec:85 brd ff:ff:ff:ff:ff:ff
				inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute enp0s3
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fee4:ec85/64 scope link noprefixroute
				   valid_lft forever preferred_lft forever
			3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:00:b4:c3 brd ff:ff:ff:ff:ff:ff
				inet 192.168.56.10/24 brd 192.168.56.255 scope global noprefixroute enp0s8
				   valid_lft forever preferred_lft forever
				inet6 fe80::a00:27ff:fe00:b4c3/64 scope link
				   valid_lft forever preferred_lft forever
			4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
				link/ether 08:00:27:3c:20:80 brd ff:ff:ff:ff:ff:ff
				inet 192.168.1.25/24 brd 192.168.1.255 scope global dynamic noprefixroute enp0s9
				   valid_lft 85216sec preferred_lft 85216sec
				inet6 fe80::a00:27ff:fe3c:2080/64 scope link
				   valid_lft forever preferred_lft forever
				   
			Loopback (lo):
			
				lo: card loopback, dùng để giao tiếp nội bộ trong máy (localhost).

				127.0.0.1: địa chỉ IP đặc biệt để tham chiếu chính máy đó.

				mtu 65536: kích thước gói tin tối đa.

				IPv6 tương ứng: ::1.
				
			enp0s3 (NAT interface):
			
				Đây là card mạng NAT do VirtualBox tạo.

				10.0.2.15/24: IP gán tự động trong mạng NAT của VirtualBox.

				dynamic: IP được cấp từ DHCP.

				Dùng để máy ảo có thể truy cập internet thông qua host.

			enp0s8 (Host-Only Adapter):
			
				Đây là card Host-Only Network.

				IP 192.168.56.10: cho phép giao tiếp giữa máy ảo và host, nhưng không ra internet.

				Thường dùng khi bạn cấu hình private_network trong Vagrant (192.168.56.x).
				
			enp0s9 (Bridge Adapter):
			
				Đây là card Bridge (nối thẳng vào mạng LAN/WiFi thực tế của bạn).

				IP 192.168.1.25: được cấp từ DHCP của router WiFi/LAN.

				Cho phép máy ảo hoạt động như một máy riêng trong mạng thật, các máy khác trong mạng LAN có thể
				truy cập trực tiếp.


	- cmd: ssh ngoctuanqng1@192.168.1.17
	
		Đây là lệnh kết nối đến một máy tính khác qua giao thức SSH (Secure Shell).	

		Phân tích từng phần:

			ssh: chương trình dùng để đăng nhập từ xa an toàn, cho phép bạn chạy lệnh trên một máy khác qua mạng.

			ngoctuanqng1: tên user mà bạn muốn đăng nhập vào máy đích.

			@: phân tách giữa user và địa chỉ máy.

			192.168.1.17: địa chỉ IP của máy đích trong mạng LAN (ở đây là máy bạn muốn SSH vào).
			
		Quy trình khi chạy lệnh:

			Máy bạn mở kết nối SSH đến địa chỉ 192.168.1.17 qua cổng 22 (mặc định).

			Server SSH trên máy 192.168.1.17 phản hồi, yêu cầu xác thực.

			Bạn nhập password (hoặc dùng SSH key) cho user ngoctuanqng1.

			Nếu đăng nhập thành công, bạn sẽ có một terminal shell của máy 192.168.1.17 và có thể chạy lệnh từ xa.
			
		Bây giờ bạn đang ở trong máy 192.168.1.17 với quyền user ngoctuanqng1.

		Một số tuỳ chọn hay dùng:

			Chỉ định cổng khác (nếu SSH server không chạy trên port 22):

				ssh -p 2222 ngoctuanqng1@192.168.1.17
				
			Chạy 1 lệnh từ xa mà không cần login interactive:
			
				ssh ngoctuanqng1@192.168.1.17 "hostname"
				
				Sẽ in ra hostname của máy đích rồi thoát.
	
	- cmd: hostname
	
		Đây là lệnh trong Linux/Unix dùng để hiển thị hoặc đặt tên của máy tính (hostname).	
			
		Khi chạy đơn giản hostname
			
			Nó sẽ in ra tên máy (hostname) hiện tại.
			
				$ hostname
				localhost.localdomain
				
				localhost → tên ngắn (short hostname).

				localhost.localdomain → tên đầy đủ (FQDN – Fully Qualified Domain Name).
				
			Đặt lại hostname (cần quyền sudo)
			
				sudo hostname myserver
				
				hostname tạm thời đổi thành myserver (chỉ có hiệu lực đến khi reboot).
				
		Kiểm tra các tùy chọn khác:

			hostname -i → in ra địa chỉ IP gắn với hostname.

			hostname -f → in ra FQDN (tên đầy đủ với domain).

			hostname -s → in ra tên ngắn (short hostname).
			
		Liên hệ thực tế:

			Hostname giúp phân biệt các máy trong mạng nội bộ hoặc hệ thống server.

			Khi bạn SSH vào máy khác, dòng prompt thường hiển thị user@hostname để biết mình đang ở đâu.
	
	- cmd: pwd
	
		Đây là lệnh trong Linux/Unix dùng để hiển thị đường dẫn thư mục hiện tại (thư mục mà bạn đang đứng trong terminal).

		pwd = print working directory.	
	
	- cmd: mkdir /f/vagrant-vms
	
		mkdir (make directory): lệnh dùng để tạo thư mục mới trong Linux/Unix.

		/f/vagrant-vms: đường dẫn tới thư mục muốn tạo.

		Nghĩa là bạn đang muốn tạo một thư mục có tên vagrant-vms bên trong thư mục /f.

		Một vài lưu ý quan trọng

			Trong Linux, đường dẫn /f/... có nghĩa là trong thư mục gốc / có một thư mục con tên là f.
			
			Trong Windows + Git Bash / WSL, cú pháp /f/... thường ám chỉ ổ đĩa F: được mount vào Linux environment.

				Ví dụ:

					/c/... → ổ đĩa C:

					/d/... → ổ đĩa D:

					/f/... → ổ đĩa F:
					
					=> Nghĩa là bạn đang tạo thư mục vagrant-vms trong ổ đĩa F:.
	
	- cmd: vagrant init eurolinux-vagrant/centos-stream-9
	
		Đây là lệnh của Vagrant dùng để khởi tạo (init) một môi trường máy ảo mới dựa trên box có
		tên eurolinux-vagrant/centos-stream-9.
		
		Giải thích chi tiết từng phần:

			vagrant: công cụ quản lý môi trường ảo (VM) dùng chung với VirtualBox, VMware, Hyper-V, Docker...

			init: khởi tạo project Vagrant mới. Lệnh này sẽ tạo ra một file Vagrantfile trong thư mục hiện tại.

			eurolinux-vagrant/centos-stream-9: tên của box (hình ảnh hệ điều hành template) được lấy từ Vagrant Cloud.

				eurolinux-vagrant → publisher/organization đã tạo box.

				centos-stream-9 → tên box (ở đây là CentOS Stream 9).
				
		Kết quả khi chạy lệnh:

			Tạo file Vagrantfile trong thư mục hiện tại (nếu chưa có).
			
			Ví dụ nội dung cơ bản:

				Vagrant.configure("2") do |config|
				  config.vm.box = "eurolinux-vagrant/centos-stream-9"
				end

			File này định nghĩa máy ảo mà Vagrant sẽ quản lý.
	
	
	- cmd: vagrant init ubuntu/jammy64
	- cmd: cat Vagrantfile
	- cmd: vagrant up
	- cmd: vagrant box list
	- cmd: vagrant status
	- cmd: vagrant ssh
	- cmd: whoami
	- cmd: exit
	- cmd: vagrant halt
	- cmd: vagrant reload
	- cmd: vagrant destroy
	- cmd: vagrant global-status
	- cmd: vagrant global-status --prune
	- cmd: sudo -i
	- cmd: history

	-- Manual:
	
		Oracle VM Virtualbox (Hypervisor)
		
		ISO file (CentOS & Ubuntu)
		
		Login tool (Git Bash & Putty)
		
		Tải file iso của centos và ubuntu để import vào Oracle VM Virtualbox
		
	-- Automated:
	
		VirtualBox (Hypervisor)
		
		Vagrant (Creates vms with Vagrantfile)
		
		commands (vagrant up)
		
		Vào vagrant cloud để tìm lệnh của vagrant để kéo file về centos 9
		(eurolinux-vagrant/centos-stream-9) và ubuntu jammy (ubuntu/jammy64)
		
	Tải các file iso rồi import vào storage của từng vm
	
	-- Oracle VM VirtualBox là gì?

		Là phần mềm ảo hóa (virtualization), cho phép bạn chạy nhiều hệ điều hành (OS)
		khác nhau trên cùng một máy tính thật.

		Ví dụ: máy thật của bạn chạy Windows, nhưng bạn có thể cài thêm một máy ảo Ubuntu
		Linux trong VirtualBox để học, test mà không ảnh hưởng Windows.
		
		Tại sao cần dùng VirtualBox?
		
			Học tập & Thử nghiệm

				Bạn có thể cài nhiều hệ điều hành (Ubuntu, CentOS, Kali, Windows Server, …)
				để học IT, mạng, bảo mật, DevOps, lập trình.

				Thử nghiệm phần mềm, cấu hình hệ thống, cài package mà không sợ làm hỏng máy thật.

			Mạng & Server

				Giả lập môi trường server: cài Apache/Nginx, MySQL, Kafka, RabbitMQ, Docker…

				Dễ dàng cấu hình mạng ảo (NAT, Bridged, Host-only) → học cách các server giao
				tiếp với nhau.

			Phát triển phần mềm

				Tạo môi trường riêng biệt để test code, deploy ứng dụng.

				Có thể snapshot (lưu trạng thái máy ảo) → nếu hỏng thì rollback lại ngay.

			An toàn & Bảo mật

				Chạy thử phần mềm lạ, file nghi ngờ virus trong máy ảo.

				Máy ảo cách ly với máy thật, hạn chế rủi ro.

			Phổ biến trong DevOps / Cloud

				Trước khi triển khai thật trên server (AWS, GCP, Azure), bạn có thể mô phỏng môi
				trường cloud ngay trên máy tính cá nhân bằng VirtualBox.

				Kết hợp với Vagrant để tự động tạo môi trường dev/test.
				
		Khi nào không cần VirtualBox?

			Nếu bạn chỉ muốn chạy Linux thường xuyên → có thể dual
			boot (cài song song Windows + Linux).

			Nếu bạn chỉ cần môi trường dev nhanh → có thể dùng WSL
			(Windows Subsystem for Linux) thay cho VM.
	
	-- File ISO là gì?

		ISO file = một bản sao toàn bộ (image) của đĩa CD/DVD.

		Nó chứa đầy đủ hệ điều hành hoặc phần mềm, giống như bạn cầm một cái đĩa cài đặt.

		Trong VirtualBox, file ISO được dùng như đĩa cài đặt hệ điều hành cho máy ảo (VM).

		Ví dụ:

		Nếu bạn muốn cài Ubuntu trên VirtualBox → bạn cần tải file ubuntu-22.04.iso.

		Nếu bạn muốn cài Windows 10 → bạn cần file Win10.iso.

		Nếu bạn muốn cài CentOS/RHEL → tải file CentOS-Stream-9.iso hoặc rhel.iso.
		
	-- Setting trong Oracle VM Virtualbox:
	
		General: Thông tin chung của VM (tên, phiên bản OS, mô tả).

		System: CPU, RAM, Boot order.

		Display: cấu hình card màn hình, độ phân giải.

		Storage: gắn ổ cứng ảo (VDI, VMDK) hoặc file ISO cài hệ điều hành.

		Audio: bật/tắt âm thanh.

		Network: cấu hình mạng cho máy ảo.

		USB, Shared Folders, User Interface: cấu hình USB, thư mục chia sẻ, giao diện người dùng.
		
	-- Controller trong VirtualBox là gì?
	
		Controller là bộ điều khiển (controller) giả lập để quản lý các thiết bị lưu trữ (Storage Devices) gắn vào máy ảo.

		Nó hoạt động giống như card điều khiển ổ cứng/ổ đĩa trong máy thật.

		Các controller phổ biến: IDE, SATA, SCSI, SAS, NVMe, Floppy.
		
		Controller: IDE

			IDE (Integrated Drive Electronics) là chuẩn cũ, thường dùng cho ổ đĩa CD/DVD và ổ cứng thế hệ cũ.

			Trong VirtualBox:

			IDE thường được dùng để mount file ISO (như đĩa cài CentOS, Ubuntu).

			Ở ảnh của bạn:

				Controller: IDE có 1 thiết bị gắn là Empty (ổ CD/DVD trống).

				Khi bạn chọn và gắn file ISO → VirtualBox coi như bạn bỏ "đĩa CD ảo" vào ổ IDE này để boot và cài OS.

		Controller: SATA

			SATA (Serial ATA) là chuẩn mới hơn, nhanh hơn IDE, thường dùng cho ổ cứng hiện đại.

			Trong VirtualBox:

				Controller: SATA đang chứa centosvm.vdi → đây chính là ổ cứng ảo (Virtual Disk Image) của máy CentOS.

				Hệ điều hành (CentOS) sau khi cài sẽ nằm trong ổ này.
		
	ACPI là gì?

		ACPI (Advanced Configuration and Power Interface) là một chuẩn do Intel, Microsoft và Toshiba phát triển.

		Nó cho phép hệ điều hành quản lý nguồn điện của phần cứng (power management).

		Nhờ ACPI, OS có thể:

			Tắt/mở máy bằng phần mềm.

			Đưa máy vào chế độ Sleep/Hibernate.

			Giảm tốc độ CPU khi không cần thiết để tiết kiệm điện.

		ACPI Shutdown trong VirtualBox là gì?

			Khi bạn chọn ACPI Shutdown trong VirtualBox (hoặc trong các hypervisor khác), nó sẽ gửi tín hiệu
			ACPI đến hệ điều hành trong máy ảo.

			Hành động này tương tự như nhấn nút Power trên máy tính thật (nhưng không phải "tắt cứng").

			Hệ điều hành trong VM sẽ nhận tín hiệu và tự thực hiện quá trình shutdown an toàn:

				Đóng ứng dụng.

				Ghi dữ liệu còn trong RAM xuống đĩa.

				Sau đó tắt máy.

			Khác với Power Off

				ACPI Shutdown: Tắt an toàn, giống như bạn bấm nút Shutdown trong Windows/Linux.

				Power Off (Hard Power Off): Tắt cứng, giống như rút dây nguồn → có thể gây mất dữ liệu hoặc hỏng
				file hệ thống.
				
				
--- Linux:

	Command line:

		- cmd: cat /etc/os-release
		- cmd: cd
		
			cd (không tham số) → quay về thư mục home của user.
			
		- cmd: cd /tmp/
		
			/tmp/ có dấu / ở đầu → đây là đường dẫn tuyệt đối.

			Nghĩa là: đi thẳng vào thư mục /tmp nằm ở gốc của hệ thống (/).

			Dù bạn đang đứng ở đâu thì kết quả vẫn giống nhau.		
		
		- cmd: uptime
		- cmd: free -m
		- cmd: mkdir ops bakupdir
		
			mkdir = make directory, dùng để tạo thư mục mới.

				ops = tên thư mục thứ nhất bạn muốn tạo.

				bakupdir = tên thư mục thứ hai bạn muốn tạo.
				
			Khi viết hai tên thư mục như vậy, mkdir sẽ tạo cùng lúc 2 thư mục trong thư mục hiện tại	
		
		- cmd: touch devopsfile{1..10}.txt
		- cmd: cp devopsfile1.txt dev/
		- cmd: ls dev/
		- cmd: ls /home/vagrant/dev/
		- cmd: cp --help
		- cmd: mv devopsfile3.txt ops/
		- cmd: mv testfile1.txt testfile22.txt
		
			Đây là lệnh di chuyển (move) hoặc đổi tên (rename) file/thư mục trong Linux/Unix.
			
			Vì cả hai tên đều ở cùng một thư mục → lệnh này đổi tên file từ testfile1.txt thành testfile22.txt.
			
		- cmd: touch testfile1.txt
		- cmd: mv *.txt textdir/
		- cmd: rm devopsfile10.txt
		- cmd: rm -r mobile
		- cmd: mkdir testdir{1..5}
		- cmd: rm -rf *
		- cmd: history
		- cmd: cat /etc/os-release
		
			Đây là lệnh để xem thông tin về hệ điều hành Linux đang chạy.
			
		- cmd: sudo yum install vim -y
		- cmd: vim firstfile.txt
		- cmd: ls -l
		- cmd: file anaconda-ks.cfg
		- cmd: file yum
		- cmd: file /bin/pwd
		- cmd: mkdir -p /opt/dev/ops/devops/test
		- cmd: ln -s /opt/dev/ops/devops/test/command.txt cmds
		- cmd: unlink cmds
		- cmd: ls -lt
		- cmd: ls -ltr
		- cmd: ls -ltr /etc/
		- cmd: hostname centos7.devops. in
		- cmd: ls -ltr /etc/
		- cmd: grep firewall anaconda-ks.cfg
		- cmd: grep Firewall anaconda-ks.cfg
		- cmd: grep -i Firewall anaconda-ks.cfg
		- cmd: grep -i firewall < anaconda-ks.cfg
		- cmd: grep -i firewall *
		- cmd: grep -iR firewall *
		- cmd: grep -R SELINUX /etc/*
		- cmd: grep -vi firewall anaconda-ks.cfg
		- cmd: less anaconda-ks.cfg
		- cmd: more anaconda-ks.cfg
		- cmd: head anaconda-ks.cfg
		- cmd: head -20 anaconda-ks.cfg
		- cmd: tail anaconda-ks.cfg
		- cmd: tail -2 anaconda-ks.cfg
		- cmd: tail -f anaconda-ks.cfg
		- cmd: tail -f yum.log
		- cmd: cut -d: -f1 /etc/passwd
		- cmd: cut -d: -f3 /etc/passwd
		- cmd: awk -F': ' ' {print $1}' /etc/passwd
		- cmd: sed 's/coronavirus/covid19/g' samplefile.txt
		- cmd: sed -i 's/covid19/nothing/g' samplefile.txt
		- cmd: sed -i 's/coronavirus/covid19/g' samplefile.txt
		- cmd: uptime > /tmp/sysinfo.txt
		- cmd: ls > /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m
		- cmd: df -h
		- cmd: echo "Good Morning"
		- cmd: echo "#############################" > /tmp/sysinfo.txt
		- cmd: date > /tmp/sysinfo.txt
		- cmd: echo "#############################" >> /tmp/sysinfo.txt
		- cmd: uptime >> /tmp/sysinfo.txt
		- cmd: free -m >> /tmp/sysinfo.txt
		- cmd: df -h >> /tmp/sysinfo.txt
		- cmd: yum install vim -y > /dev/null
		- cmd: cat /dev/null > /tmp/sysinfo.txt
		- cmd: free -m > /dev/null
		- cmd: freeeee -m 2>> /tmp//error.log
		- cmd: free -m 1>> /tmp//error.log
		- cmd: free -m &>> /tmp//error.log
		- cmd: freesdsd -m &>> /tmp//error.log
		- cmd: wc -l /etc/passwd
		- cmd: wc -l < /etc/passwd
		- cmd: ls | wc -l
		- cmd: ls | grep host*
		- cmd: ls | grep host
		- cmd: tail /var/log/messages | grep -i vagrant
		- cmd: tail -20 /var/log/messages | grep -i vagrant
		- cmd: free -m | grep Mem
		- cmd: ls -l | tail
		- cmd: ls -l | head
		- cmd: find /etc -name host*
		- cmd: find / -name host*
		- cmd: yum install mlocate -y
		- cmd: updatedb
		- cmd: locate host
		- cmd: head -1 /etc/passwd
		- cmd: grep vagrant /etc/passwd
		- cmd: id vagrant
		- cmd: useradd ansible
		- cmd: useradd jenkins
		- cmd: useradd aws
		- cmd: tail -4 /etc/passwd
		- cmd: tail -4 /etc/group
		- cmd: id ansible
		- cmd: groupadd devops
		- cmd: ls -l
		- cmd: ls -ld /opt/devopsdir
		- cmd: chown -R ansible:devops /opt/devopsdir
		- cmd: ls -ld /opt/devopsdir
		- cmd: chmod o-x /opt/devopsdir
		- cmd: chmod g+w /opt/devopsdir
		- cmd: su - miles
		- cmd: su - aws
		- cmd: chown aws.devops /opt/webdata
		- cmd: chmod -R 770 /opt/webdata
		- cmd: chmod -R 754 /opt/webdata/
		- cmd: sudo yum install git -y
		- cmd: sudo useradd test
		- cmd: passwd ansible
		- cmd: su - ansible
		- cmd: sudo useradd test12
		- cmd: visudo
		- cmd: ls -l /etc/sudoers
		- cmd: cd /etc/sudoers.d/
		- cmd: cat vagrant
		- cmd: cat *
		- cmd: rpm -qa
		- cmd: telnet
		- cmd: arch
		- cmd: uname -m
		- cmd: curl https://rpmfind.net/linux/RPM/centos-stream/9/appstream/x86_64/telnet-0.17-85.el9.x86_64.html
		- cmd: curl https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/telnet-0.17-85.el9.x86_64.rpm -o telnet-0.17-85.el9.x86_64.rpm
		- cmd: rpm -ivh telnet-0.17-85.el9.x86_64.rpm
		- cmd: telnet
		- cmd: rpm -qa | grep telnet
		- cmd: rpm -e telnet-0.17-85.el9.x86_64
		- cmd: wget https://rpmfind.net/linux/centos-stream/9-stream/AppStream/x86_64/os/Packages/httpd-2.4.57-8.el9.x86_64.rpm
		- cmd: cd /etc/yum.repos.d/
		- cmd: yum search httpd
		- cmd: yum install httpd
		- cmd: dnf remove httpd
		- cmd: dnf install httpd -y
		- cmd: dnf --help
		- cmd: yum upgrade
		- cmd: yum install jenkins
		- cmd: sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
		- cmd: sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
		- cmd: sudo yum upgrade
		- cmd: dnf repolist
		- cmd: dnf install epel-release -y
		- cmd: dnf history
		- cmd: systemctl status httpd
		- cmd: systemctl start httpd
		- cmd: sudo reboot
		- cmd: systemctl enable httpd
		- cmd: systemctl status sshd
		- cmd: systemctl is-active httpd
		- cmd: systemctl is-enabled httpd
		- cmd: cat /etc/systemd/system/multi-user.target.wants/httpd.service
		- cmd: top
		- cmd: ps aux
		- cmd: ps -ef
		- cmd: ps -ef | grep httpd | grep -v 'grep'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}'
		- cmd: ps -ef | grep httpd | grep -v 'grep' | awk '{print $2}' | xargs kill -9
		- cmd: file jenkins_06122020.tar.gz
		- cmd: tar -czvf kenkins_06122020.tar.gz jenkins
		- cmd: tar -xzvf jenkins_06122020.tar.gz
		- cmd: tar -xzvf jenkins_06122020.tar.gz -C/pop/
		- cmd: tar --help
		- cmd: yum install zip unzip -y
		- cmd: zip -r jenkins_06122020.zip jenkins
		- cmd: ls -ltr jenkins*
		- cmd: rm -rf jenkins
		- cmd: unzip jenkins_06122020.zip
		- cmd: userdel -r devops
		- cmd: export EDITOR=vim
		- cmd: wget http://archive.ubuntu.com/ubuntu/pool/universe/t/tree/tree_2.0.2-1_amd64.deb
		- cmd: dpkg -i tree_2.0.2-1_amd64.deb
		- cmd: dpkg -l
		- cmd: dpkg -l | grep tree
		- cmd: dpkg -r tree
		- cmd: apt update
		- cmd: apt search tree
		- cmd: apt install tree
		- cmd: apt install apache2
		- cmd: systemctl status apache2
		- cmd: apt upgrade
		- cmd: apt remove apache2
		- cmd: apt purge apache2
		
	
	Trên host: bạn không có sudo → lỗi.

	Sau khi vagrant ssh: bạn đang ở trong máy ảo, nơi sudo đã được cài và cấu hình sẵn → chạy được sudo -i.
	
	$ vagrant ssh
	
		A Vagrant environment or target machine is required to run this
		command. Run `vagrant init` to create a new Vagrant environment. Or,
		get an ID of a target machine from `vagrant global-status` to run
		this command on. A final option is to change to a directory with a
		Vagrantfile and to try again.
		
		Có nghĩa là bạn đang chạy vagrant ssh nhưng Vagrant không biết bạn muốn SSH vào máy ảo nào.
		
		Bạn chưa tạo máy ảo bằng vagrant init + vagrant up.

		Hoặc bạn đang không đứng trong thư mục có chứa file Vagrantfile.

		Hoặc chưa có máy ảo nào đang chạy.
		
	Cách thoát ra khỏi vim:

		Nhấn phím Esc (để chắc chắn không còn ở chế độ nhập văn bản nữa).

		Sau đó gõ một trong các lệnh sau rồi bấm Enter:

		:q → thoát nếu chưa chỉnh sửa gì.

		:q! → thoát bỏ qua thay đổi (không lưu).

		:wq → lưu rồi thoát.

		:x → tương tự :wq (lưu rồi thoát).

	Vim:
	
		Lệnh Shift + g
		
			Di chuyển con trỏ xuống dòng cuối cùng của file.
			
		Lệnh yy

			yy = yank line (sao chép 1 dòng hiện tại).

			Khi bạn đang ở chế độ Normal mode (bấm Esc để chắc chắn):

			yy sẽ copy nguyên cả dòng nơi con trỏ đang đứng vào bộ nhớ tạm (clipboard của Vim).

			Ví dụ:
			
				Con trỏ đang ở dòng số 5, gõ yy → dòng số 5 được copy.
				
		Lệnh yyyy

		Lệnh p

			p = put (dán sau con trỏ).

			Sau khi copy bằng yy, bạn có thể dán dòng đó xuống ngay dưới con trỏ bằng p.

			Ví dụ:

				Bạn đứng ở dòng số 5, gõ yy.

				Sau đó xuống dòng số 10, gõ p.

				Dòng số 5 sẽ được chèn vào sau dòng số 10.
				
		Lệnh dd
		
		Lệnh ddddd
		
		Lệnh u
		
		Lệnh /
		
		Lệnh :%s/coronavirus/covid19/g
		
			Thay toàn bộ tất cả các chữ "coronavirus" trong file thành "covid19".
			
			Giải thích từng phần:
		
				: → vào command mode trong Vim (gõ lệnh).

				% → phạm vi áp dụng: toàn bộ file.

				Nếu chỉ viết :s/.../.../ thì nó chỉ thay trong dòng hiện tại.

				% nghĩa là từ dòng 1 đến dòng cuối.

				s → viết tắt của substitute (thay thế).

				coronavirus → là chuỗi tìm kiếm (cái cần thay).

				covid19 → là chuỗi thay thế.

				g → global trong dòng → thay tất cả các lần xuất hiện trong mỗi dòng.

				Nếu bỏ g, thì trên mỗi dòng chỉ thay lần xuất hiện đầu tiên.
		
		Lệnh :%s/covid19//g
		
			Xóa toàn bộ tất cả các chữ covid19 trong toàn bộ file.
			
			Giải thích ngắn gọn:

				:% → áp dụng cho toàn bộ file.

				s → substitute (thay thế).

				covid19 → chuỗi cần tìm.

				// → thay thế bằng chuỗi rỗng (nghĩa là xóa đi).

				g → thay tất cả các lần xuất hiện trên mỗi dòng.
		
	Grep:
	
		grep trong Linux là một lệnh dùng để tìm kiếm chuỗi (string/pattern) trong file hoặc
		trong output của lệnh khác. Nó là viết tắt của Global Regular Expression Print.
		
		Cách hoạt động

			grep sẽ quét qua từng dòng dữ liệu.

			Nếu dòng nào khớp với chuỗi hoặc biểu thức chính quy (regex) mà bạn chỉ định → nó sẽ in dòng đó ra màn hình.
			
	dnf:
	
		Trong Linux (đặc biệt là CentOS, RHEL, Fedora), dnf là một trình quản lý gói (package manager).
		
		Giải thích ngắn gọn

			dnf viết tắt của Dandified YUM.

			Nó là phiên bản mới thay thế cho yum từ RHEL/CentOS 8 trở đi.

			Dùng để:

			Cài đặt phần mềm.

			Gỡ bỏ phần mềm.

			Cập nhật hệ thống.

			Quản lý kho (repository).
			
	yum:
	
		Trong Linux (đặc biệt là các bản CentOS, RHEL, Fedora cũ), yum là một trình quản lý gói (package manager).
		
		Giải thích ngắn gọn

			yum viết tắt của Yellowdog Updater, Modified.

			Nó giúp bạn cài đặt, cập nhật, gỡ bỏ và quản lý phần mềm trên hệ thống Linux dựa trên gói RPM.

			yum làm việc với các repository (kho phần mềm) để tự động tải về và xử lý phụ thuộc giữa các gói (dependencies).

		Quan hệ giữa yum và dnf

			yum là công cụ cũ (dùng trên CentOS 7, RHEL 7 trở xuống).

			Từ CentOS/RHEL 8 trở đi, yum đã được thay thế bởi dnf.

			Tuy nhiên, để tránh gây rối cho người dùng quen yum, nhiều hệ thống mới vẫn để lệnh yum tồn tại, nhưng thực
			chất nó chỉ là symlink (liên kết) trỏ tới dnf.
			
	apt:
	
		apt (Advanced Package Tool) là trình quản lý gói (package manager) dùng trong các hệ điều hành Linux
		thuộc họ Debian (như Ubuntu, Debian, Linux Mint).

		Nó giúp bạn cài đặt, nâng cấp, gỡ bỏ và quản lý phần mềm một cách dễ dàng, thay vì phải tải thủ công từng file .deb.
		
		
		
--- GIT:

	- Commands:
	
		- cmd: git init
		- cmd: ls -a
		- cmd: touch saturn{1..10}.py
		- cmd: git status
		- cmd: git add .
		- cmd: git commit -m "new files commited"
		- cmd: git config --global user.email "ngoctuanqng1@gmail.com"
		- cmd: git config --global user.name  "ngoctuanqng1"
		- cmd: git remote add origin https://github.com/ngoctuanqng/gitpractice.git
		- cmd: cat .git/config
		- cmd: git branch -m main
		- cmd: git push -u origin main
		- cmd: git add satural.py
		- cmd: git push origin main
		- cmd: git log
		- cmd: git log --oneline
		- cmd: git show 625b034
		- cmd: git pull
		- cmd: git branch -c sprintl
		- cmd: git branch -a
		- cmd: git checkout sprintl
		- cmd: git rm saturn6.py saturn7.py saturn8.py
		- cmd: git mv saturn1.py saturn11.py
		- cmd: touch jupiter{1..4}.rb
		- cmd: git push origin sprintl
		- cmd: touch sun earth venus mercury
		- cmd: git switch sprint1
		- cmd: git merge sprintl
		- cmd: git clone https://github.com/ngoctuanqng/gitpractice.git
		- cmd: git checkout jupiter1.rb
		- cmd: git diff
		- cmd: git diff --cached
		- cmd: git restore --staged jupiter1.rb
		- cmd: git diff 8dff644..cbb01a8
		- cmd: git revert HEAD
		- cmd: git reset --hard 8dff644
		- cmd: cat .git/config
		- cmd: rm -rf .ssh/*
		- cmd: cat .ssh/id_rsa.pub
		- cmd: git tag
		- cmd: git show v2.0.0
		- cmd: git tag -a v3.5.3 -m "Release 3.5.3"
		- cmd: systemctl restart httpd
		
		
		
		
		
	Ta có thể tạo ở local trước, sau đó tạo ở remote, dùng lệnh git remote add origin để đẩy local vào remote
	
	touch a và touch a/ là khác nhau, touch a sẽ tạo file a, còn touch a/ sẽ tạo folder tên là a
	

--- Vagrant and Linux Servers:

	- Command:
	
		- cmd: vagrant destroy --force
		- cmd: ls ~/.vagrant.d/
		- cmd: cat /proc/cpuinfo
		- cmd: vagrant reload --provision
		- cmd: vi /etc/hostname
		- cmd: hostname finance
		- cmd: yum install httpd wget vim unzip zip -y
		- cmd: unzip 2138_aqua_nova.zip
		- cmd: rm -r 2138_aqua_nova
		- cmd: cp -r * /var/www/html/
		- cmd: systemctl status firewalld
		- cmd: systemctl stop firewalld
		- cmd: systemctl disable firewalld
		- cmd: vagrant init ubuntu/focal64
		- cmd: sudo apt update
		
		- cmd: sudo apt install apache2 \
                 ghostscript \
                 libapache2-mod-php \
                 mysql-server \
                 php \
                 php-bcmath \
                 php-curl \
                 php-imagick \
                 php-intl \
                 php-json \
                 php-mbstring \
                 php-mysql \
                 php-xml \
                 php-zip
				 
		- cmd: sudo mkdir -p /srv/www
		- cmd: sudo chown www-data: /srv/www
		- cmd: curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www
		- cmd: vim /etc/apache2/sites-available/wordpress.conf
		- cmd: sudo a2ensite wordpress
		- cmd: sudo a2enmod rewrite
		- cmd: sudo a2dissite 000-default
		- cmd: sudo service apache2 reload
		- cmd: sudo mysql -u root
		- cmd: CREATE DATABASE wordpress;
		- cmd: show databases;
		- cmd: CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
		- cmd: GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
		- cmd: FLUSH PRIVILEGES;
		- cmd: sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php
		- cmd: vim /srv/www/wordpress/wp-config.php
		- cmd: sudo -u www-data vim /srv/www/wordpress/wp-config.php
		- cmd: vagrant ssh web02
		- cmd: vagrant destroy web01
		- cmd: vagrant up web03
		- cmd: ls /usr/lib/systemd/system/
		- cmd: cat /usr/lib/systemd/system/httpd.service
		- cmd: ls /etc/httpd/
		- cmd: ls /var/log/httpd/
		- cmd: wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.46/bin/apache-tomcat-10.1.46.tar.gz
		- cmd: tar xzvf apache-tomcat-10.1.46.tar.gz
		- cmd: dnf install java-17-openjdk -y
		- cmd: [root@localhost apache-tomcat-10.1.46]# bin/startup.sh
		- cmd: ps -ef | grep tomcat
		- cmd: kill 9886
		- cmd: [root@localhost ~]# useradd --home-dir /opt/tomcat --shell /sbin/nologin tomcat
		- cmd: [root@localhost ~]# cp -r apache-tomcat-10.1.46/* /opt/tomcat
		- cmd: [root@localhost ~]# chown -R tomcat.tomcat /opt/tomcat/
		- cmd: vim /etc/systemd/system/tomcat.service
		
			[Unit]
			Description=Tomcat
			After=network.target

			[Service]
			Type=forking

			User=tomcat
			Group=tomcat

			workingDirectory=/opt/tomcat

			Environment=JAVA_HOME=/usr/lib/jvm/jre

			Environment=CATALINA_HOME=/opt/tomcat
			Environment=CATALINE_BASE=/opt/tomcat

			ExecStart=/opt/tomcat/bin/startup.sh
			ExecStop=/opt/tomcat/bin/shutdown.sh

			[Install]
			WantedBy=multi-user.target

		- cmd: systemctl daemon-reload
		- cmd: [root@localhost ~]# systemctl start tomcat
		- cmd: [root@localhost ~]# systemctl status tomcat
		- cmd: [root@localhost ~]# systemctl enable tomcat
		
		
		
		
		
	
		
		
		
	Gán các nội dung của trang web vào /var/www/html/ để có thể chạy lên trang web mong muốn
	
	Download tomcat 10
	
	Truy cập Tomcat:
	
		Tomcat theo mặc định sẽ nghe trên port 8080, trừ khi bạn đã sửa file server.xml.
		
		Trong output ip addr show, card enp0s8 có địa chỉ: inet 192.168.1.11/24 brd 192.168.1.255
		
		Đây là IP để bạn truy cập từ host hoặc các máy khác trong cùng mạng 192.168.1.x.
		
		Bạn cần mở trình duyệt và nhập:
		
			http://192.168.1.11:8080/
	
	Intall and configure WordPress:
	
		https://ubuntu.com/tutorials/install-and-configure-wordpress#7-configure-wordpress
		
			WordPress là gì?

			WordPress là một CMS (Content Management System) – hệ thống quản trị nội dung.

			Nó giúp bạn tạo website/blog mà không cần viết code nhiều.

			Rất phổ biến để làm: blog, website tin tức, website công ty, thậm chí thương mại điện
			tử (với plugin như WooCommerce).

		“Install and Configure WordPress” (cài đặt và cấu hình WordPress) để làm gì?

			Install (cài đặt):

				Tải mã nguồn WordPress về máy chủ (server Linux).

				Đặt nó vào thư mục web server (Apache/nginx).

				Kết nối với cơ sở dữ liệu (MySQL/MariaDB).

				Sau bước này, server của bạn có WordPress, nhưng nó chưa được cấu hình để chạy.

			Configure (cấu hình):

				Tạo file wp-config.php để WordPress biết phải kết nối tới database nào, username/password là gì.

				Thiết lập các thông số như ngôn ngữ, timezone, theme, plugin.

				Truy cập qua trình duyệt để hoàn tất wizard (bạn đặt tên site, tài khoản admin, mật khẩu…).

				Sau bước này, bạn có một website WordPress chạy được, sẵn sàng dùng.

		Tóm lại

			Install WordPress = đưa WordPress vào server.

			Configure WordPress = kết nối WordPress với database + thiết lập để nó chạy thành một website hoàn chỉnh.
			
	Promt để tạo multivm:
	
		Multivm vagrantfile web01 ubuntu20, web02 with ubuntu20 & db01 with centos 7. Private IP for all the
		vms. Provisioning for db01. Set hostname also
		
		
		
		
		
	Nội dung file Vagrantfile khi mở một số comment:
	
		Case 1 (ubuntu):

			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end
			
		Case 2 (ubuntu):
		
			Vagrant.configure("2") do |config|
			   config.vm.box = "ubuntu/jammy64"
			   config.vm.network "private_network", ip: "192.168.33.10"
			   config.vm.network "public_network"
			   config.vm.synced_folder "E:\\devops_practice\\scripts\\shellscripts", "/opt/scripts"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			end			

		Case 3 (centos):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.10"
			  config.vm.network "public_network"
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip git -y
				mkdir /opt/devopsdir
				free -m
				uptime
			  SHELL
			end
			
		Case 4 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.33.10"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end

		Case 5 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/jammy64"
			  config.vm.network "private_network", ip: "192.168.56.14"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "1600"
				 vb.cpus = "2"
			   end
			  config.vm.provision "shell", inline: <<-SHELL
				apt-get update
				apt-get install -y apache2
			  SHELL
			end
			
			Chú thể truy cập vào link 192.168.56.14
			
		Case 6 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.22"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			end
			
		Case 7 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.26"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			end
			
		Case 8 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "eurolinux-vagrant/centos-stream-9"
			  config.vm.network "private_network", ip: "192.168.56.28"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1024"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
				yum install httpd wget unzip vim -y
				systemctl start httpd
				systemctl enabled httpd
				mkdir -p /tmp/finance
				cd /tmp/finance
				wget https://www.tooplate.com/zip-templates/2138_aqua_nova.zip
				unzip -o 2138_aqua_nova.zip
				cp -r 2138_aqua_nova/* /var/www/html/
				systemctl restart httpd
				cd /tmp/
				rm -rf /tmp/finance
			  SHELL
			end
			
		Case 9 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.30"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				vb.memory = "1600"
			  end
			  config.vm.provision "shell", inline: <<-SHELL
			  sudo apt update -y
			  sudo apt install -y apache2 \
							  ghostscript \
							  libapache2-mod-php \
							  mysql-server \
							  php \
							  php-bcmath \
							  php-curl \
							  php-imagick \
							  php-intl \
							  php-json \
							  php-mbstring \
							  php-mysql \
							  php-xml \
							  php-zip

			  sudo mkdir -p /srv/www
			  sudo chown www-data: /srv/www
			  curl https://wordpress.org/latest.tar.gz | sudo -u www-data tar zx -C /srv/www

			  cat > /etc/apache2/sites-available/wordpress.conf <<EOF
			<VirtualHost *:80>
				DocumentRoot /srv/www/wordpress
				<Directory /srv/www/wordpress>
					Options FollowSymLinks
					AllowOverride Limit Options FileInfo
					DirectoryIndex index.php
					Require all granted
				</Directory>
				<Directory /srv/www/wordpress/wp-content>
					Options FollowSymLinks
					Require all granted
				</Directory>
			</VirtualHost>
			EOF

			  sudo a2ensite wordpress
			  sudo a2enmod rewrite
			  sudo a2dissite 000-default

			  mysql -u root -e CREATE DATABASE wordpress;
			  mysql -u root -e CREATE USER wordpress@localhost IDENTIFIED BY 'admin123';
			  mysql -u root -e GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON wordpress.* TO wordpress@localhost;
			  mysql -u root -e FLUSH PRIVILEGES;

			  sudo -u www-data cp /srv/www/wordpress/wp-config-sample.php /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/database_name_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/username_here/wordpress/' /srv/www/wordpress/wp-config.php
			  sudo -u www-data sed -i 's/password_here/admin123/' /srv/www/wordpress/wp-config.php

			  systemctl restart mysql
			  systemctl restart apache2
			  SHELL
			end
			
			Lúc đầu là sudo apt update và sudo apt install apache2 nhưng sau đó thêm -y để nó tạo đồng ý, nếu
			không có nó sẽ absort rồi gây ra lỗi

		Case 10 (ubuntu):
		
			Vagrant.configure("2") do |config|
			  # ---------- Web01 ----------
			  config.vm.define "web01" do |web01|
				web01.vm.box = "ubuntu/focal64"
				web01.vm.hostname = "web01"
				web01.vm.network "private_network", ip: "192.168.56.41"
			  end

			  # ---------- Web02 ----------
			  config.vm.define "web02" do |web02|
				web02.vm.box = "ubuntu/focal64"
				web02.vm.hostname = "web02"
				web02.vm.network "private_network", ip: "192.168.56.42"
			  end

			  # ---------- DB01 ----------
			  config.vm.define "db01" do |db01|
				db01.vm.box = "centos/7"
				db01.vm.hostname = "db01"
				db01.vm.network "private_network", ip: "192.168.56.43"
				# Provisioning script for DB01
				db01.vm.provision "shell", inline: <<-SHELL
				  yum install -y wget unzip mariadb-server -y
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL
			  end
			end

			Cái này sẽ khởi tạo nhiều vm 1 lúc
			
			Có thể thao tác với từng vm theo các lệnh sau:
			
				vagrant ssh web02
				
				vagrant destroy web01
				
				vagrant destroy -f web01
				
				vagrant up web03
				
				vagrant destroy --force
				
					Xóa tất cả các vm
				
			Code trên centos 7 cũ nên đã bị EOL nên sẽ được thêm với dòng code sau để fix:

				db01.vm.provision "shell", inline: <<-SHELL
				  # --- Fix CentOS 7 EOL repo ---
				  sed -i 's|mirrorlist=|#mirrorlist=|g' /etc/yum.repos.d/CentOS-Base.repo
				  sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-Base.repo
				  yum clean all
				  yum makecache

				  # --- Install MariaDB ---
				  yum install -y wget unzip mariadb-server
				  systemctl start mariadb
				  systemctl enable mariadb
				SHELL

	httpd:
	
		httpd trong Linux thường dùng để chỉ Apache HTTP Server – một trong những web server phổ
		biến và lâu đời nhất trên thế giới.
		
		Giải thích

		httpd = HTTP Daemon (daemon nghĩa là dịch vụ chạy ngầm trên hệ thống).

		Khi cài Apache trên Linux (ví dụ CentOS, RHEL, Fedora), gói dịch vụ sẽ được đặt tên là httpd.

		Nhiệm vụ chính:

			Nhận request HTTP/HTTPS từ client (trình duyệt, curl, API…).

			Xử lý và trả về response (HTML, CSS, JS, JSON, file tĩnh, dữ liệu động qua PHP/Python/Java…).

		httpd có thể mở rộng bằng module (ví dụ: mod_ssl cho HTTPS, mod_rewrite cho rewrite URL, mod_php để chạy PHP).
		
	Lệnh để in nội dung của file html ra màn hình:

		[root@Finance ~]# cd /var/www/html/
		[root@Finance html]# ls
		[root@Finance html]# vim index.html
		index.html
		[root@Finance html]# systemctl restart httpd

	Kiểm tra xem có tất cả cái nào đang chạy:
	
		vagrant global-status
		
		
--- Variables, JSON and YAML:

	- Commands:
	
		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ skill="DevOps"

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo $skill
		DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo skill
		skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo "I am learning $skill"
		I am learning DevOps

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ echo 'I am learning $skill'
		I am learning $skill

		PC@DESKTOP-GNVB183 MINGW64 /e/devops practice/VM_setup/vagrant/vagrant-vms/centos
		$ Num=123
		
	Python edittors online
	
	Json editor
	
	Yaml editor
	
--- Networking:

	- Commands:
	
		- cmd: sudo apt update
		- cmd: sudo apt install -y net-tools
		- cmd: ifconfig
		- cmd:
		
				vagrant@web01:~$ ping 192.168.56.41
				PING 192.168.56.41 (192.168.56.41) 56(84) bytes of data.
				64 bytes from 192.168.56.41: icmp_seq=1 ttl=64 time=0.021 ms
				64 bytes from 192.168.56.41: icmp_seq=2 ttl=64 time=0.037 ms
				64 bytes from 192.168.56.41: icmp_seq=3 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=4 ttl=64 time=0.027 ms
				64 bytes from 192.168.56.41: icmp_seq=5 ttl=64 time=0.042 ms

		- cmd: root@web01:~# vi /etc/hosts
		
				127.0.0.1       localhost

				# The following lines are desirable for IPv6 capable hosts
				::1     ip6-localhost   ip6-loopback
				fe00::0 ip6-localnet
				ff00::0 ip6-mcastprefix
				ff02::1 ip6-allnodes
				ff02::2 ip6-allrouters
				ff02::3 ip6-allhosts
				127.0.1.1       ubuntu-focal    ubuntu-focal

				127.0.2.1 web01 web01

				192.168.56.41 web01
				
		- cmd: ping web01
		- cmd: logout
		- cmd: $ tracert www.google.in

			   Tracing route to www.google.in [2404:6800:4003:c11::5e]
			   over a maximum of 30 hops:
			   
			     1     1 ms     9 ms     1 ms  2405:4803:d75f:ab0:5e3a:45ff:fe2e:8f78
			     2     *        *        *     Request timed out.
			     3     *        *        *     Request timed out.
			     4    73 ms     3 ms     3 ms  2405:4802:f500::37d
			     5    71 ms     *        *     2405:4800::1117:5118:a
			     6     *        *        *     Request timed out.
			     7    49 ms    48 ms    47 ms  2405:4800::101:1112:1
			     8    48 ms    45 ms    46 ms  2001:4860:0:1::5bc2
			     9    49 ms    51 ms    51 ms  2001:4860::c:4000:db82
			    10   157 ms    82 ms    82 ms  2001:4860::c:4003:1c94
			    11     *        *        *     Request timed out.
			    12     *        *        *     Request timed out.
			    13     *        *        *     Request timed out.
			    14     *        *        *     Request timed out.
			    15     *        *        *     Request timed out.
			    16     *        *        *     Request timed out.
			    17     *        *        *     Request timed out.
			    18     *        *        *     Request timed out.
			    19     *        *        *     Request timed out.
			    20    82 ms   166 ms    82 ms  se-in-f94.1e100.net [2404:6800:4003:c11::5e]
			   
			   Trace complete.
		
		- cmd: vagrant@web01:~$ netstat -antp
		
			   (Not all processes could be identified, non-owned process info
			    will not be shown, you would have to be root to see it all.)
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED -
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED -
			   tcp6       0      0 :::22                   :::*                    LISTEN      -
			   
		- cmd: root@web01:~# netstat -antp
		
			   Active Internet connections (servers and established)
			   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
			   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      598/systemd-resolve
			   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      856/sshd: /usr/sbin
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53802          ESTABLISHED 3245/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53904          ESTABLISHED 3358/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53428          ESTABLISHED 2250/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:53696          ESTABLISHED 3146/sshd: vagrant
			   tcp        0      0 10.0.2.15:22            10.0.2.2:54814          ESTABLISHED 3492/sshd: vagrant
			   tcp6       0      0 :::22                   :::*                    LISTEN      856/sshd: /usr/sbin


		- cmd:  root@web01:~# ps -ef | grep apache2
				
				root        3602    3589  0 05:53 pts/4    00:00:00 grep --color=auto apache2
				
		- cmd:  root@web01:~# ss -tunlp
		
				Netid          State           Recv-Q          Send-Q                      Local Address:Port                   Peer Address:Port          Process
				udp            UNCONN          0               0                           127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=12))
				udp            UNCONN          0               0                        10.0.2.15%enp0s3:68                          0.0.0.0:*              users:(("systemd-network",pid=1932,fd=20))
				tcp            LISTEN          0               4096                        127.0.0.53%lo:53                          0.0.0.0:*              users:(("systemd-resolve",pid=598,fd=13))
				tcp            LISTEN          0               128                               0.0.0.0:22                          0.0.0.0:*              users:(("sshd",pid=856,fd=3))
				tcp            LISTEN          0               128                                  [::]:22                             [::]:*              users:(("sshd",pid=856,fd=4))
				
		- cmd: apt install nmap -y
		- cmd: nmap
		- cmd:  root@web01:~# nmap localhost
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 05:56 UTC
				Nmap scan report for localhost (127.0.0.1)
				Host is up (0.0000030s latency).
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.44 seconds
				
		- cmd:  root@web01:~# nmap web01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 06:47 UTC
				Nmap scan report for web01 (127.0.2.1)
				Host is up (0.0000040s latency).
				Other addresses for web01 (not scanned): 192.168.56.41
				Not shown: 999 closed ports
				PORT   STATE SERVICE
				22/tcp open  ssh

				Nmap done: 1 IP address (1 host up) scanned in 0.12 seconds

		- cmd:  root@web01:~# dig www.google.com

				; <<>> DiG 9.18.30-0ubuntu0.20.04.2-Ubuntu <<>> www.google.com
				;; global options: +cmd
				;; Got answer:
				;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 54039
				;; flags: qr rd ra; QUERY: 1, ANSWER: 6, AUTHORITY: 0, ADDITIONAL: 1

				;; OPT PSEUDOSECTION:
				; EDNS: version: 0, flags:; udp: 65494
				;; QUESTION SECTION:
				;www.google.com.                        IN      A

				;; ANSWER SECTION:
				www.google.com.         285     IN      A       142.251.12.99
				www.google.com.         285     IN      A       142.251.12.147
				www.google.com.         285     IN      A       142.251.12.104
				www.google.com.         285     IN      A       142.251.12.106
				www.google.com.         285     IN      A       142.251.12.105
				www.google.com.         285     IN      A       142.251.12.103

				;; Query time: 36 msec
				;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)
				;; WHEN: Sun Sep 21 06:49:44 UTC 2025
				;; MSG SIZE  rcvd: 139
				
		- cmd:  root@web01:~# nslookup www.google.com
		
				Server:         127.0.0.53
				Address:        127.0.0.53#53

				Non-authoritative answer:
				Name:   www.google.com
				Address: 142.251.12.103
				Name:   www.google.com
				Address: 142.251.12.105
				Name:   www.google.com
				Address: 142.251.12.106
				Name:   www.google.com
				Address: 142.251.12.104
				Name:   www.google.com
				Address: 142.251.12.147
				Name:   www.google.com
				Address: 142.251.12.99
				Name:   www.google.com
				Address: 2404:6800:4003:c11::63
				Name:   www.google.com
				Address: 2404:6800:4003:c11::67
				Name:   www.google.com
				Address: 2404:6800:4003:c11::69
				Name:   www.google.com
				Address: 2404:6800:4003:c11::68
				
		- cmd:  root@web01:~# route -n
		
				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				10.0.2.2        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# route

				Kernel IP routing table
				Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
				default         _gateway        0.0.0.0         UG    100    0        0 enp0s3
				10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
				_gateway        0.0.0.0         255.255.255.255 UH    100    0        0 enp0s3
				192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
				
		- cmd:  root@web01:~# arp
		
				Address                  HWtype  HWaddress           Flags Mask            Iface
				_gateway                 ether   52:54:00:12:35:02   C                     enp0s3
				10.0.2.3                 ether   52:54:00:12:35:03   C                     enp0s3
				192.168.56.42                    (incomplete)                              enp0s8
				
		- cmd: mtr
		- cmd: mtr www.google.com
		
		- cmd:  root@web01:~# nmap db01
		
				Starting Nmap 7.80 ( https://nmap.org ) at 2025-09-21 07:06 UTC
				Failed to resolve "db01".
				WARNING: No targets were specified, so 0 hosts scanned.
				Nmap done: 0 IP addresses (0 hosts up) scanned in 0.06 seconds
				
		- cmd:  root@web01:~# telnet 192.168.56.43 3306
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				HHost '192.168.56.41' is not allowed to connect to this MariaDB serverConnection closed by foreign host.
				
		- cmd: sudo yum install -y net-tools
		
		- cmd:  root@web01:~# telnet 192.168.56.43 22
		
				Trying 192.168.56.43...
				Connected to 192.168.56.43.
				Escape character is '^]'.
				SSH-2.0-OpenSSH_7.4



	OSI model
	
	Switch
	
	Router
	
	Firewall
	
	Gateway IP
	
	Wireless Access Point
	
	Port
	
	TCP, UDP
	
--- Introducing Containers:

	- Commands:
	
		- cmd: sudo install -m 0755 -d /etc/apt/keyrings
		- cmd: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
		- cmd: echo \
			  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
		- cmd: sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
		
		- cmd:  root@ubuntu-focal:~# systemctl status docker
		
				● docker.service - Docker Application Container Engine
					 Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
					 Active: active (running) since Sun 2025-09-21 07:47:33 UTC; 1min 12s ago
				TriggeredBy: ● docker.socket
					   Docs: https://docs.docker.com
				   Main PID: 3868 (dockerd)
					  Tasks: 9
					 Memory: 21.3M
					 CGroup: /system.slice/docker.service
							 └─3868 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

				Sep 21 07:47:33 ubuntu-focal systemd[1]: Started Docker Application Container Engine.
				
		- cmd: docker run hello-world
		- cmd:  root@ubuntu-focal:~# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
				hello-world   latest    1b44b5a3e06a   6 weeks ago   10.1kB
				
		- cmd:  root@ubuntu-focal:~# docker ps
		
				CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
				
		- cmd:  root@ubuntu-focal:~# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND    CREATED              STATUS                          PORTS     NAMES
				00001e808420   hello-world   "/hello"   About a minute ago   Exited (0) About a minute ago             cool_agnesi
				
		- cmd: docker run --name web01 -d -p 9080:80 nginx
		- cmd: docker inspect web01
		- cmd:  root@ubuntu-focal:~# curl http://172.17.0.2:80
		
				<!DOCTYPE html>
				<html>
				<head>
				<title>Welcome to nginx!</title>
				<style>
				html { color-scheme: light dark; }
				body { width: 35em; margin: 0 auto;
				font-family: Tahoma, Verdana, Arial, sans-serif; }
				</style>
				</head>
				<body>
				<h1>Welcome to nginx!</h1>
				<p>If you see this page, the nginx web server is successfully installed and
				working. Further configuration is required.</p>

				<p>For online documentation and support please refer to
				<a href="http://nginx.org/">nginx.org</a>.<br/>
				Commercial support is available at
				<a href="http://nginx.com/">nginx.com</a>.</p>

				<p><em>Thank you for using nginx.</em></p>
				</body>
				</html>
				
		- cmd: docker build -t tesimg .
		- cmd: docker run -d -P tesimg
		- cmd:  root@ubuntu-focal:~/images# docker ps
		
				CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                       NAMES
				f41b4289fc35   tesimg    "/usr/sbin/apache2ct…"   2 minutes ago    Up 2 minutes    0.0.0.0:32768->80/tcp, [::]:32768->80/tcp   elegant_moore
				44fde054c460   nginx     "/docker-entrypoint.…"   25 minutes ago   Up 25 minutes   0.0.0.0:9080->80/tcp, [::]:9080->80/tcp     web01
				
				Con số 32768 trong output docker ps này chính là cổng ngẫu nhiên trên máy host mà Docker ánh
				xạ đến cổng 80 trong container.

		- cmd: docker stop web01 elegant_moore
		
		- cmd:  root@ubuntu-focal:~/images# docker ps -a
		
				CONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS                        PORTS     NAMES
				f41b4289fc35   tesimg        "/usr/sbin/apache2ct…"   6 minutes ago    Exited (137) 28 seconds ago             elegant_moore
				44fde054c460   nginx         "/docker-entrypoint.…"   29 minutes ago   Exited (0) 38 seconds ago               web01
				00001e808420   hello-world   "/hello"                 32 minutes ago   Exited (0) 32 minutes ago               cool_agnesi
				
		- cmd: docker rm elegant_moore web01 cool_agnesi
		
		- cmd:  root@ubuntu-focal:~/images# docker images
		
				REPOSITORY    TAG       IMAGE ID       CREATED          SIZE
				tesimg        latest    c197410a4d04   13 minutes ago   271MB
				nginx         latest    41f689c20910   5 weeks ago      192MB
				hello-world   latest    1b44b5a3e06a   6 weeks ago      10.1kB
				
		- cmd: docker rmi c197410a4d04 41f689c20910 1b44b5a3e06a
		- cmd: wget https://raw.githubusercontent.com/devopshydclub/vprofile-project/refs/heads/docker/compose/docker-compose.yml
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose down
		- cmd: docker system prune -a
		- cmd: git clone https://github.com/devopshydclub/emartapp.git
		
		
		
		
		
	Khi bạn copy–paste vào vim thì lỗi format YAML thường xảy ra do:

		Dính tab thay vì space.

		Paste vào vim ở chế độ autoindent gây lệch dòng.

		Hoặc paste từ Windows → Linux thì dính ký tự ẩn (CRLF).

		Khắc phục:
		
			Trước khi paste, gõ: :set paste
			
	Docker compose là để run các image chứ không phải để build các image
				

		



		

	Nội dung file Vagrantfile khi mở một số comment:
	
		Case 1:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			  config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
			   sudo apt-get update
			   sudo apt-get install \
				ca-certificates \
				curl \
				gnupg -y

			   sudo install -m 0755 -d /etc/apt/keyrings
			   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
			   sudo chmod a+r /etc/apt/keyrings/docker.gpg
			   echo \
			     "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
			     "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
			     sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
			   sudo apt-get update
			   sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
			  SHELL
			end
			
		Case 2:
		
			FROM ubuntu:latest AS BUILD_IMAGE
			RUN apt update && apt install wget unzip -y
			RUN wget https://www.tooplate.com/zip-templates/2128_tween_agency.zip
			RUN unzip 2128_tween_agency.zip && cd 2128_tween_agency && tar -czf tween.tgz * && mv tween.tgz /root/tween.tgz

			FROM ubuntu:latest
			LABEL "project"="Marketing"
			ENV DEBIAN_FRONTEND=noninteractive

			RUN apt update && apt install apache2 git wget -y
			COPY --from=BUILD_IMAGE /root/tween.tgz /var/www/html/
			RUN cd /var/www/html/ && tar xzf tween.tgz

			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			VOLUME /var/log/apache2
			WORKDIR /var/www/html
			EXPOSE 80

		Case 3:
		
			Vagrant.configure("2") do |config|
			  config.vm.box = "ubuntu/focal64"
			  config.vm.network "private_network", ip: "192.168.56.82"
			  config.vm.network "public_network"
			   config.vm.provider "virtualbox" do |vb|
				 vb.memory = "2048"
			   end
			   config.vm.provision "shell", inline: <<-SHELL
				sudo apt-get update
				sudo apt-get install \
					ca-certificates \
					curl \
					gnupg -y

				sudo install -m 0755 -d /etc/apt/keyrings
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
				sudo chmod a+r /etc/apt/keyrings/docker.gpg
				echo \
				  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
				  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
				sudo curl -L "https://github.com/docker/compose/releases/download/v2.1.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

				chmod +x /usr/local/bin/docker-compose
			   SHELL
			end
			
	Nội dung file docker-compose.yml:
	
		Case 1:
		
			version: '3.8'
			services:
			  vprodb:
				image: vprocontainers/vprofiledb
				ports:
				  - "3306:3306"
				volumes:
				  - vprodbdata:/var/lib/mysql
				environment:
				  - MYSQL_ROOT_PASSWORD=vprodbpass

			  vprocache01:
				image: memcached
				ports:
				  - "11211:11211"

			  vpromq01:
				image: rabbitmq
				ports:
				  - "15672:15672"
				environment:
				  - RABBITMQ_DEFAULT_USER=guest
				  - RABBITMQ_DEFAULT_PASS=guest

			  vproapp:
				image: vprocontainers/vprofileapp
				ports:
				  - "8080:8080"
				volumes: 
				  - vproappdata:/usr/local/tomcat/webapps

			  vproweb:
				image: vprocontainers/vprofileweb
				ports:
				  - "80:80"
			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books
		  
--- Bash Scripting:

	- Commands:
		
		- cmd: vim /etc/hostname
		
			File này để chuyển đổi tên hostname
			
		- cmd: hostname scriptbox
		- cmd: hostname
		- cmd: yum install vim -y
		- cmd: chmod +x firstscript.sh
		- cmd: chmod +x dismantle.sh
		- cmd: w
		- cmd: who
		- cmd: free -m | grep Mem
		- cmd: free -m | grep Mem | awk '{print $4}'
		- cmd: source .bashrc
		- cmd: ip addr show | grep -v LOOPBACK | grep -ic mtu
		- cmd: cat /var/run/httpd/httpd.pid
		- cmd: ssh-copy-id vagrant@192.168.10.13
		- cmd: ssh vagrant@web01
		- cmd: passwd devops
		- cmd: vim /etc/ssh/sshd_config
		- cmd: ssh devops@web01 uptime
		- cmd: ssh-keygen
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i .ssh/id_rsa devops@web01 uptime
		- cmd: cat .ssh/id_rsa
		- cmd: cat .ssh/id_rsa.pub
		- cmd: for host in `cat remhosts`; do echo $host;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host hostname;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host uptime;done
		- cmd: for host in `cat remhosts`; do ssh devops@$host sudo yum install git -y;done
		- cmd: echo "testfile" >testfile.txt
		- cmd: scp testfile.txt devops@web01:/tmp/
		- cmd: scp testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/root/
		- cmd: scp -i ~/.ssh/id_rsa testfile.txt devops@web01:/home/devops/
		
		
		
		
	Varantfile:

		Vagrant.configure("2") do |config|

		  config.vm.define "scriptbox" do |scriptbox|
			scriptbox.vm.box = "eurolinux-vagrant/centos-stream-9"
				scriptbox.vm.network "private_network", ip: "192.168.10.12"
				scriptbox.vm.provider "virtualbox" do |vb|
			 vb.memory = "1024"
		   end
		  end

		  config.vm.define "web01" do |web01|
			web01.vm.box = "eurolinux-vagrant/centos-stream-9"
				web01.vm.network "private_network", ip: "192.168.10.13"
		  end

		  config.vm.define "web02" do |web02|
			web02.vm.box = "eurolinux-vagrant/centos-stream-9"
				web02.vm.network "private_network", ip: "192.168.10.14"
		  end

		   config.vm.define "web03" do |web03|
			web03.vm.box = "ubuntu/bionic64"
				web03.vm.network "private_network", ip: "192.168.10.15"
		  end
		end
	
		
		
		
	Nội dung file script:
	
		Case 1:
		
			#!/bin/bash

			echo "Welcome to bash script."
			echo

			echo "The uptime of the system is: "
			uptime

			echo "Memory Utilization"
			free -m

			echo "Disk Utilization"
			df -h
			
		Case 2:
		
			#!/bin/bash

			### This script prints system info ###

			echo "Welcome to bash script."
			echo

			#checking system uptime
			echo "#################################"
			echo "The uptime of the system is: "
			uptime

			# Memory Utilization
			echo "#################################"
			echo "Memory Utilization"
			free -m

			# Disk Utilization
			echo "#################################"
			echo "Disk Utilization"
			df -h
			
		Case 3:
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip
			unzip 2098_health.zip
			sudo cp -r 2098_health.zip/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo
			
		Case 4:
		
			Cái này có thể chạy lên trang web
		
			#!/bin/bash

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install wget unzip httpd -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start httpd
			sudo systemctl enable httpd
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p /tmp/webfiles
			cd /tmp/webfiles
			echo

			wget https://www.tooplate.com/zip-templates/2098_health.zip > /dev/null
			unzip 2098_health.zip > /dev/null
			sudo cp -r 2098_health/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart httpd
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf /tmp/webfiles
			echo

			sudo systemctl status httpd
			ls /var/www/html/
			
		Case 5:
		
			[root@scriptbox ~]# SKILL="DevOps"
			[root@scriptbox ~]# echo $SKILL
			DevOps
			[root@scriptbox ~]# echo SKILL
			SKILL
			[root@scriptbox ~]# PACKAGE="httpd wget unzip"
			[root@scriptbox ~]# yum install $PACKAGE -y
			Last metadata expiration check: 0:12:59 ago on Sun 21 Sep 2025 11:19:55 AM UTC.
			Package httpd-2.4.62-7.el9.x86_64 is already installed.
			Package wget-1.21.1-8.el9.x86_64 is already installed.
			Package unzip-6.0-59.el9.x86_64 is already installed.
			Dependencies resolved.
			Nothing to do.
			Complete!
			
		Case 6:
		
			[root@scriptbox scripts]# ls
			firstscript.sh  websetup.sh
			[root@scriptbox scripts]# mv firstscript.sh 1_firstscript.sh
			[root@scriptbox scripts]# mv websetup.sh 2_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh
			[root@scriptbox scripts]# cp 2_websetup.sh 3_vars_websetup.sh
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh
		
		Case 7:
		
			#!/bin/bash
			ART_NAME="2098_health"
			SVC="httpd"
			URL="https://www.tooplate.com/zip-templates/2098_health.zip"
			ART_NAME="2098_health"
			TEMPDIR="/tmp/webfiles"

			#Installing Dependencies
			echo "###################################"
			echo "Installing packages"
			echo "###################################"
			sudo yum install $PACKAGE -y > /dev/null
			echo

			# Start & Enable Service
			echo "###################################"
			echo "Start & Enabel HTTPD Service"
			echo "###################################"
			sudo systemctl start $SVC
			sudo systemctl enable $SVC
			echo

			# Creating Temp Directory
			echo "###################################"
			echo "Starting Artifact Deployment"
			echo "###################################"
			mkdir -p $TEMPDIR
			cd $TEMPDIR
			echo

			wget $URL > /dev/null
			unzip $ART_NAME.zip > /dev/null
			sudo cp -r $ART_NAME/* /var/www/html/
			echo

			# Bounce Service
			echo "###################################"
			echo "Restarting HTTPD Service"
			echo "###################################"
			systemctl restart $SVC
			echo

			# Clean Up
			echo "###################################"
			echo "Removing Temporary Files"
			echo "###################################"
			rm -rf $TEMPDIR
			echo

			sudo systemctl status $SVC
			ls /var/www/html/
			
		Case 8:
		
			#!/bin/bash
			sudo systemctl stop httpd
			sudo rm -rf /var/www/html/*
			sudo yum remove httpd wget unzip -y
			
		Case 9:
		
			#!/bin/bash

			echo "Value of 0 is "
			echo $0

			echo "Value of 1"
			echo $1

			echo "Value of 2"
			echo $2

			echo "Value of 3"
			echo $3
			
				[root@scriptbox scripts]# ./4_args.sh Linux AWS Ansible Jenkins
				Value of 0 is
				./4_args.sh
				Value of 1
				Linux
				Value of 2
				AWS
				Value of 3
				Ansible
				
		Case 10:
		
			[root@scriptbox scripts]# echo $USER
			root
			[root@scriptbox scripts]# echo $HOSTNAME
			scriptbox
			[root@scriptbox scripts]# echo $RANDOM
			11889
			[root@scriptbox scripts]# SKILL="DevOps"
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# SKILL='DevOps'
			[root@scriptbox scripts]# echo $SKILL
			DevOps
			[root@scriptbox scripts]# echo "Ihave got $SKILL skill."
			Ihave got DevOps skill.
			[root@scriptbox scripts]# echo 'Ihave got $SKILL skill.'
			Ihave got $SKILL skill.
			[root@scriptbox scripts]# echo "Ihave got \$SKILL skill."
			Ihave got $SKILL skill.

			[root@scriptbox scripts]# uptime
			 13:25:12 up  3:03,  1 user,  load average: 0.02, 0.04, 0.03
			[root@scriptbox scripts]# UP="uptime"
			[root@scriptbox scripts]# echo $UP
			uptime
			[root@scriptbox scripts]# UP=`uptime`
			[root@scriptbox scripts]# echo $UP
			13:25:42 up 3:03, 1 user, load average: 0.01, 0.03, 0.03
			
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-21 10:44 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         330         155           3         414         436
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         330         155           3         414         436
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			155
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 155 mb.
			
		Case 11:
		
			#!/bin/bash

			echo "Welcome $USER on $HOSTNAME."
			echo "##############################################"

			FREERAM=$(free -m | grep Mem | awk '{print $4}')
			LOAD=`uptime | awk '{print $9}'`
			ROOTFREE=$(df -h | grep '/dev/sdal' | awk '{print $4}')

			echo "##############################################"
			echo "Available free RAM is $FREERAM MB"
			echo "##############################################"
			echo "Current Load Average $LOAD"
			echo "##############################################"
			echo "Free ROOT partition size is $ROOTFREE"
			
		Case 12:
		
			[root@scriptbox scripts]# CURRENT_USER=$(who)
			[root@scriptbox scripts]# echo CURRENT_USER
			CURRENT_USER
			[root@scriptbox scripts]# echo $CURRENT_USER
			vagrant pts/0 2025-09-22 13:05 (10.0.2.2)
			[root@scriptbox scripts]# free -m
						   total        used        free      shared  buff/cache   available
			Mem:             767         282         383           3         229         485
			Swap:           1023           0        1023
			[root@scriptbox scripts]# free -m | grep Mem
			Mem:             767         282         383           3         229         485
			[root@scriptbox scripts]# free -m | grep Mem | awk '{print $4}'
			382
			[root@scriptbox scripts]# FREE_RAM=`free -m | grep Mem | awk '{print $4}'`
			[root@scriptbox scripts]# echo "Free RAM is $FREE_RAM mb."
			Free RAM is 382 mb.
			[root@scriptbox scripts]# ./6_command_subs.sh
			Welcome root on scriptbox.
			##############################################
			##############################################
			Available free RAM is 382 MB
			##############################################
			Current Load Average 0.04,
			##############################################
			Free ROOT partition size is
			
		Case 13:
		
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo SEASON
			SEASON
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON

			[root@scriptbox ~]# cd /opt/scripts/

		Case 14:
		
			#!/bin/bash
			echo "The $SEASON season is more than expected, this time."
			
			[root@scriptbox scripts]# chmod +x testvars.sh
			[root@scriptbox scripts]# SEASON="Monsoon"
			[root@scriptbox scripts]# echo $SEASON
			Monsoon
			[root@scriptbox scripts]# ./testvars.sh
			The  season is more than expected, this time.
			[root@scriptbox scripts]# export SEASON
			[root@scriptbox scripts]# ./testvars.sh
			The Monsoon season is more than expected, this time.
			
		Case 15:
		
			# .bashrc
			# User specific aliases and functions
			alias rm='rm -i'
			alias cp='cp -i'
			alias mv='mv -i'
			# Source global definitions
			if [ -f /etc/bashrc ]; then
					. /etc/bashrc
			fi
			export SEASON="Monsoon"


			[root@scriptbox ~]# echo $SEASON
			[root@scriptbox ~]# exit
			logout
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# vi .bashrc
			[root@scriptbox ~]# sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
		
		Case 16:
			
			#!/bin/bash
			export SEASON='Winter'
			
			[vagrant@scriptbox ~]$ echo $SEASON
			Winter
			[vagrant@scriptbox ~]$ sudo -i
			[root@scriptbox ~]# echo $SEASON
			Monsoon
			
		Case 17:
		
			#!/bin/bash
			echo "Enter youe skills:"
			read SKILL
			echo "Your $SKILL skill is in high Demand in the IT Industry."
			read -p 'Username: ' USR
			read -sp 'Password: ' pass
			echo
			echo "Login Successfull: Welcome USER $USR,"

			[root@scriptbox scripts]# chmod +x 7_userInput.sh
			[root@scriptbox scripts]# ./7_userInput.sh
			Enter youe skills:
			CloudComputing
			Your CloudComputing skill is in high Demand in the IT Industry.
			Username: ngoctuanqng1
			Password:
			Login Successfull: Welcome USER ngoctuanqng1,

		Case 18:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# chmod +x 8if1.sh
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 120
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:40:30 PM UTC 2025
			Script execution completed successfully.
			[root@scriptbox scripts]# ./8if1.sh
			Enter a number: 50
			Script execution completed successfully.
		
		Case 19:
		
			#!/bin/bash
			read -p "Enter a number: " NUM
			echo
			if [ $NUM -gt 100 ]
			then
					echo "We have entered in IF block."
					sleep 3
					echo "Your Number is greater than 100"
					echo
					date
			else
					echo "You have entered number less than 100."
			fi
			echo "Script execution completed successfully."
			
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 60
			You have entered number less than 100.
			Script execution completed successfully.
			[root@scriptbox scripts]# ./9_if1.sh
			Enter a number: 110
			We have entered in IF block.
			Your Number is greater than 100
			Mon Sep 22 03:43:41 PM UTC 2025
			Script execution completed successfully.
			
		Case 20:
		
			#!/bin/bash
			value=$(ip addr show | grep -v LOOPBACK | grep -ic mtu)
			if [ $value -eq 1 ]
			then
					echo "1 Active Network Interface found."
			elif [ $value -gt 1 ]
			then
					echo "Found Multiple active Interface."
			else
					echo "No Active interface found."
			fi
			
			[root@scriptbox ~]# ./9_ifelif.sh
			Found Multiple active Interface.
			[root@scriptbox ~]# ls
			9_ifelif.sh  anaconda-ks.cfg  original-ks.cfg
			[root@scriptbox ~]# ls /opt/scripts/
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  dismantle.sh  testvars.sh
			[root@scriptbox ~]# mv ./9_ifelif.sh /opt/scripts/
			[root@scriptbox ~]# cd /opt/scripts/
			[root@scriptbox scripts]# ls
			1_firstscript.sh  2_websetup.sh  3_vars_websetup.sh  4_args.sh  5_args_websetup.sh  6_command_subs.sh  7_userInput.sh  8if1.sh  9_if1.sh  9_ifelif.sh  dismantle.sh  testvars.sh
			
		Case 21:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null
			if [ $? -eq 0 ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
			[root@scriptbox scripts]# ./11_monit.sh
			#######################################################
			Mon Sep 22 04:12:59 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 22:
		
			#!/bin/bash
			# mm hh dom MM DOW COMMAND
			# mm hh dom MM DOW COMMAND
			# 30 20 * * 1-5 COMMAND
			* * * * * /opt/scripts/11_monit.sh &>> /var/log/monit_httpd.log

			[root@scriptbox scripts]# crontab -e
			no crontab for root - using an empty one
			crontab: installing new crontab
			[root@scriptbox scripts]# systemctl stop httpd
			[root@scriptbox scripts]# ls /var/log/monit_httpd.log
			/var/log/monit_httpd.log
			[root@scriptbox scripts]# cat /var/log/monit_httpd.log
			#######################################################
			Mon Sep 22 04:20:08 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################

			#######################################################
			Mon Sep 22 04:21:03 PM UTC 2025
			Httpd process is NOT Running.
			Starting the process
			Process started successfully.
			#######################################################
			
		Case 23:
		
			#!/bin/bash
			echo "#######################################################"
			date
			ls /var/run/httpd/httpd.id &> /dev/null

			if [ -f /var/run/httpd/httpd.pid ]
			then
					echo "Httpd process is running."
			else
					echo "Httpd process is NOT Running."
					echo "Starting the process"
					systemctl start httpd
					if [ $? -eq 0 ]
					then
							echo "Process started successfully."
					else
							echo "Process Starting Failed, contact the admin."
					fi
			fi
			echo "#######################################################"
			echo
			
		Case 24:
		
			#!/bin/bash
			for VAR1 in java .net python ruby php
			do
					echo "Looping...."
					sleep 1
					echo "#####################################"
					echo "Value of VAR1 is $VAR1."
					echo "#####################################"
					date
			done
			
			[root@scriptbox scripts]# chmod +x 13_for.sh
			[root@scriptbox scripts]# ./13_for.sh
			Looping....
			#####################################
			Value of VAR1 is java.
			#####################################
			Mon Sep 22 04:28:16 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is .net.
			#####################################
			Mon Sep 22 04:28:18 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is python.
			#####################################
			Mon Sep 22 04:28:19 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is ruby.
			#####################################
			Mon Sep 22 04:28:20 PM UTC 2025
			Looping....
			#####################################
			Value of VAR1 is php.
			#####################################
			Mon Sep 22 04:28:21 PM UTC 2025
			
		Case 25:
		
			#!/bin/bash
			MYUSERS="alpha beta gamma"
			for usr in $MYUSERS
			do
					echo "Adding user $usr."
					useradd $usr
					id $usr
					echo "##############################"
			done
			
			[root@scriptbox scripts]# chmod +x 14_for.sh
			[root@scriptbox scripts]# ./14_for.sh
			Adding user alpha.
			uid=1001(alpha) gid=1001(alpha) groups=1001(alpha)
			##############################
			Adding user beta.
			uid=1002(beta) gid=1002(beta) groups=1002(beta)
			##############################
			Adding user gamma.
			uid=1003(gamma) gid=1003(gamma) groups=1003(gamma)
			##############################
	
		Case 26:
		
			#!/bin/bash
			counter=0
			while [ $counter -lt 5 ]
			do
					echo "Looping...."
					echo "Value of counter is $counter."
					counter=$(( $counter + 1))
					sleep 1
			done
			echo "Out of the loop"
			
			[root@scriptbox ~]# chmod +x 15_while.sh
			[root@scriptbox ~]# ./15_while.sh
			Looping....
			Value of counter is 0.
			Looping....
			Value of counter is 1.
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 3.
			Looping....
			Value of counter is 4.
			Out of the loop
			
		Case 27:
		
            #!/bin/bash
            counter=2
            while true
            do
                echo "Looping...."
                echo "Value of counter is $counter."
                counter=$(( $counter * 2))
                sleep 1
            done
            echo "Out of the loop"
			
			[root@scriptbox ~]# ./16_while.sh
			Looping....
			Value of counter is 2.
			Looping....
			Value of counter is 4.
			Looping....
			Value of counter is 8.
			Looping....
			Value of counter is 16.
			Looping....
			Value of counter is 32.
			Looping....
			Value of counter is 64.
			Looping....
			Value of counter is 128.
			...
			
		Case 28:
		
			127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
			::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
			192.168.10.13 web01
			192.168.10.14 web02
			192.168.10.15 web03
			
			[root@localhost ~]# ping web01
			PING web01 (192.168.10.13) 56(84) bytes of data.
			64 bytes from web01 (192.168.10.13): icmp_seq=1 ttl=64 time=3.83 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=2 ttl=64 time=2.57 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=3 ttl=64 time=1.44 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=4 ttl=64 time=3.46 ms
			64 bytes from web01 (192.168.10.13): icmp_seq=5 ttl=64 time=2.18 ms

		Case 29:
		
			web01: visudo
		
				## Allow root to run any commands anywhere
				root    ALL=(ALL)       ALL
				devops ALL=(ALL)        NOPASSWD: ALL
				
			web03: /etc/ssh/sshd_config

				PasswordAuthentication yes
				ChallengeResponseAuthentication no
			
		Case 30:
		
			multios_websetup.sh:
		
				#!/bin/bash

				# Variable Declaration
				#PACKAGE="httpd wget unzip"
				#SVC="httpd"
				URL='https://www.tooplate.com/zip-templates/2098_health.zip'
				ART_NAME='2098_health'
				TEMPDIR="/tmp/webfiles"

				yum --help &> /dev/null

				if [ $? -eq 0 ]
				then
				   # Set Variables for CentOS
				   PACKAGE="httpd wget unzip"
				   SVC="httpd"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo yum install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/

				else
					# Set Variables for Ubuntu
				   PACKAGE="apache2 wget unzip"
				   SVC="apache2"

				   echo "Running Setup on CentOS"
				   # Installing Dependencies
				   echo "########################################"
				   echo "Installing packages."
				   echo "########################################"
				   sudo apt update
				   sudo apt install $PACKAGE -y > /dev/null
				   echo

				   # Start & Enable Service
				   echo "########################################"
				   echo "Start & Enable HTTPD Service"
				   echo "########################################"
				   sudo systemctl start $SVC
				   sudo systemctl enable $SVC
				   echo

				   # Creating Temp Directory
				   echo "########################################"
				   echo "Starting Artifact Deployment"
				   echo "########################################"
				   mkdir -p $TEMPDIR
				   cd $TEMPDIR
				   echo

				   wget $URL > /dev/null
				   unzip $ART_NAME.zip > /dev/null
				   sudo cp -r $ART_NAME/* /var/www/html/
				   echo

				   # Bounce Service
				   echo "########################################"
				   echo "Restarting HTTPD service"
				   echo "########################################"
				   systemctl restart $SVC
				   echo

				   # Clean Up
				   echo "########################################"
				   echo "Removing Temporary Files"
				   echo "########################################"
				   rm -rf $TEMPDIR
				   echo

				   sudo systemctl status $SVC
				   ls /var/www/html/
				fi 

		Case 31:
		
			#!/bin/bash
			USR='devops'
			for host in `cat remhosts`
			do
					echo
					echo "#######################################"
					echo "Connecting to $host"
					echo "Pushing Script to $host"
					scp multios_websetup.sh $USR@$host:/tmp/
					echo "Executing Script on $host"
					ssh $USR@$host sudo /tmp/multios_websetup.sh
					ssh $USR@$host sudo rm -rf /tmp/multios_websetup.sh
					echo "#######################################"
					echo
			done

--- AWS Part-1:

	Setting EC2
	
	Khi tạo inbound rules trong security group chú ý đến port, phải đúng port mới chạy ra được trang web
	
	Chú ý khi truy cập ec2 với ssh key pair thì cần import đúng file key pair, nếu không sẽ bị denied
	
	My IP mà bạn thường thấy khi cấu hình Inbound rule trong Security Group của AWS nghĩa là:

		AWS tự động lấy địa chỉ IP public hiện tại của máy tính bạn đang dùng
		(máy laptop/PC/điện thoại đang truy cập vào AWS Console).

		Khi bạn chọn My IP, rule sẽ chỉ cho phép địa chỉ IP đó được truy cập vào EC2 qua cổng
		bạn chọn (ví dụ SSH port 22, hoặc HTTP port 80).

	Chưa chạy được mkfs

		Tuy nhiên lại chạy được lệnh mkfs.t4 /dev/nvme1n1p1
	
	
	
	
	
	
	
	
	

	AWS Part-1 - Command:
	
		- cmd: ssh -i "web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ssh -i "E:/devops learning download/AWS_Part-1/web-dev-key.pem" ec2-user@ec2-3-90-88-184.compute-1.amazonaws.com
		- cmd: ss -tunlp | grep 80
		- cmd: curl http://localhost
		- cmd: apt update && apt install apache2 -y
		- cmd: apt install unzip -y
		- cmd: systemctl restart apache2
		- cmd: ps -ef | grep apache2
		- cmd: ss -tunlp | grep 2668
		- cmd: choco install awscli -y
		- cmd: aws --version
		- cmd: aws configure
		- cmd: ls ~/.aws/
		- cmd: cat ~/.aws/config
		- cmd: cat ~/.aws/credentials
		- cmd: aws sts get-caller-identity
		- cmd: aws ec2 describe-instances

		- cmd: sudo amazon-linux-extras install epel -y
		- cmd: sudo yum install stress -y
		- cmd: nohup stress -c 4 -t 300 &
		- cmd: top
		
			Lệnh top trong Linux là một công cụ theo dõi tiến trình (process monitoring) chạy trong real-time.		
		
			Công dụng chính

				Hiển thị tình trạng hệ thống (CPU, RAM, uptime, load average).

				Liệt kê danh sách tiến trình (process) đang chạy, sắp xếp theo mức độ tiêu thụ tài nguyên.

				Giúp bạn nhanh chóng biết tiến trình nào đang chiếm CPU hoặc RAM nhiều nhất.

			Một số phím tắt hữu ích trong top

				P → sắp xếp theo CPU usage.

				M → sắp xếp theo RAM usage.

				k → kill process (nhập PID).

				q → thoát khỏi top.
				
			top = Task Manager trong Linux, giúp bạn giám sát CPU, RAM, và tiến trình theo thời gian thực.
		
		- cmd: fdisk -l	
		
			Lệnh fdisk -l trong Linux được dùng để liệt kê (list) tất cả các phân vùng (partitions)
			và ổ đĩa hiện có trên hệ thống.
			
			fdisk: công cụ quản lý phân vùng trên Linux (tạo, xóa, thay đổi partition).

			-l (list): hiển thị danh sách các thiết bị lưu trữ và thông tin chi tiết.
		
		- cmd: df -h
		
			Lệnh df -h trong Linux dùng để hiển thị dung lượng ổ đĩa và các phân vùng đang được
			mount theo cách dễ đọc (human-readable).
			
			df = disk filesystem (thông tin hệ thống tệp).

			-h = human readable → hiển thị dung lượng bằng đơn vị KB, MB, GB, TB thay vì block.	

			Khác với fdisk -l:

				df -h → xem dung lượng đang sử dụng của phân vùng đã mount.

				fdisk -l → xem cấu trúc phân vùng của ổ đĩa (dù chưa mount).			
		
		- cmd: mount /dev/nvme1n1p1 /var/www/html/images
		
			Ý nghĩa:

				mount: gắn (mount) một phân vùng/ổ đĩa vào một thư mục trên hệ thống.

				/dev/nvme1n1p1: thiết bị vật lý/phân vùng (ở đây là ổ NVMe, partition số 1).

				/var/www/html/images: thư mục mount point (nơi nội dung của phân vùng sẽ xuất hiện).
				
			Quy trình hoạt động:

				Sau khi chạy lệnh, toàn bộ nội dung gốc trong /var/www/html/images (nếu có) sẽ bị ẩn
				đi (không mất, chỉ bị che khuất) và được thay thế bởi nội dung của phân vùng /dev/nvme1n1p1.

				Khi unmount (umount /var/www/html/images), dữ liệu gốc trong thư mục lại hiện ra.
				
			Các lưu ý:
			
				Thư mục mount point phải tồn tại trước

				Thiết bị phải có hệ thống tệp (filesystem) hợp lệ (ext4, xfs, vfat...).

				Nếu muốn mount tự động khi khởi động lại, cần chỉnh file /etc/fstab.
				
		- cmd: mount -a
		
			Ý nghĩa:

				mount: gắn (mount) phân vùng/thiết bị vào hệ thống.

				-a (all): mount tất cả các mục được khai báo trong file cấu
				hình /etc/fstab (ngoại trừ những dòng có option noauto).
				
			Khi nào dùng:

				Sau khi bạn chỉnh sửa file /etc/fstab để thêm mới phân vùng cần mount tự
				động khi khởi động lại.

				Thay vì phải reboot, bạn có thể chạy mount -a để kiểm tra ngay.

		- cmd: umount /var/www/html/images
		
			Ý nghĩa:

				umount (không có chữ n ở đầu) dùng để tháo (unmount) một phân vùng khỏi hệ thống.

				/var/www/html/images là mount point mà trước đó bạn đã mount thiết bị vào.
				
			Kết quả khi chạy:

				Sau khi umount, dữ liệu trên phân vùng /dev/nvme1n1p1 sẽ không còn xuất
				hiện ở /var/www/html/images.

				Nội dung gốc của thư mục /var/www/html/images (nếu có trước đó) sẽ hiện ra lại.
		
		- cmd: vi /etc/fstab
		- cmd: systemctl daemon-reload
		
		
		- cmd: fdisk /dev/nvme1n1
		
			Thư mục sẽ được lấy theo lệnh giá trị của Disk theo lệnh sau:
			
				[ec2-user@ip-172-31-16-206 ~]$ fdisk -l
				fdisk: cannot open /dev/nvme0n1: Permission denied
				fdisk: cannot open /dev/nvme1n1: Permission denied
				[ec2-user@ip-172-31-16-206 ~]$ sudo -i
				[root@ip-172-31-16-206 ~]# fdisk -l
				Disk /dev/nvme0n1: 8 GiB, 8589934592 bytes, 16777216 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				Disklabel type: gpt
				Disk identifier: 7129DF59-1D55-477D-A6E9-19E29F26FE4F

				Device           Start      End  Sectors  Size Type
				/dev/nvme0n1p1    2048     6143     4096    2M BIOS boot
				/dev/nvme0n1p2    6144  1030143  1024000  500M EFI System
				/dev/nvme0n1p3 1030144  3078143  2048000 1000M Linux extended boot
				/dev/nvme0n1p4 3078144 16777182 13699039  6.5G Linux root (x86-64)


				Disk /dev/nvme1n1: 5 GiB, 5368709120 bytes, 10485760 sectors
				Disk model: Amazon Elastic Block Store
				Units: sectors of 1 * 512 = 512 bytes
				Sector size (logical/physical): 512 bytes / 512 bytes
				I/O size (minimum/optimal): 4096 bytes / 4096 bytes
				
			Ý nghĩa

				fdisk: công cụ quản lý phân vùng (partition) trên ổ đĩa.

				/dev/nvme1n1: thiết bị NVMe (ổ đĩa vật lý, chưa chọn phân vùng cụ thể).

				Khi chạy lệnh này, bạn sẽ mở trình tương tác (interactive mode) để thao tác với bảng phân
				vùng của ổ đĩa đó.
				
			Lệnh con	Chức năng
			m			Hiển thị menu trợ giúp (help).
			p			In (print) bảng phân vùng hiện tại.
			n			Tạo (new) phân vùng mới.
			d			Xóa (delete) một phân vùng.
			t			Thay đổi loại phân vùng (partition type).
			w			Lưu thay đổi (write) xuống đĩa và thoát.
			q			Thoát mà không lưu gì (quit).
				
			Lưu ý quan trọng

				Rất nguy hiểm: nếu bạn d (xóa) rồi w, dữ liệu sẽ mất.
				
		- cmd: lsof /var/www/html/images
		
			Ý nghĩa

				lsof = list open files → liệt kê tất cả file đang được mở trên hệ thống.

				/var/www/html/images = đường dẫn bạn muốn kiểm tra.

			Khi chạy, nó sẽ hiển thị các process (tiến trình) nào đang dùng thư mục hoặc file trong thư mục này.		
		
		- cmd: yum install lsof -y
		- cmd: yum install mariadb-server -y
		- cmd: systemctl start mariadb
		- cmd: systemctl status mariadb
		- cmd: systemctl stop mariadb
		- cmd: stress-ng --cpu 2 --timeout 60s
		- cmd: stress-ng --cpu $(nproc) --timeout 300s
		- cmd: stress-ng --cpu $(nproc) --cpu-load 60 --timeout 600s
		- cmd: sudo yum install -y amazon-efs-utils
		- cmd: mount -fav
		- cmd: apt install mysql-client -y
		
		
	AWS Part-1 - Amazon EFS (Elastic File System):	

		Amazon EFS (Elastic File System) là dịch vụ lưu trữ file được AWS cung cấp, dùng để chia sẻ dữ liệu giữa
		nhiều EC2 instance hoặc container một cách dễ dàng.
		
		Đặc điểm chính của EFS:

			Lưu trữ dạng file (NFS – Network File System), không phải block như EBS và không phải object như S3.

			Tự động mở rộng: dung lượng tăng/giảm theo nhu cầu, bạn chỉ trả tiền cho dung lượng dùng thực tế.

			Chia sẻ giữa nhiều EC2: nhiều máy ảo (EC2) có thể mount vào cùng một EFS và đọc/ghi dữ liệu chung.

			High availability: dữ liệu được lưu trữ tự động trong nhiều AZ (Availability Zone) trong cùng 1 region.

			Hiệu năng cao, có thể dùng cho web server, big data, container, CI/CD.
		
	AWS Part-1 - Security group:	
		
		0.0.0.0/0 → toàn bộ IPv4 address space (mọi địa chỉ IPv4 đều truy cập được).

		::/0 → toàn bộ IPv6 address space (mọi địa chỉ IPv6 đều truy cập được).
		
		Chú ý khi tạo load balancer thì trong nó sẽ có 1 security group ví dụ như sg-b, trong instance sẽ có 1 security group ví
		dụ như a-sg, thì trong a-sg phải add thêm b-sg để nó chấp nhận truy cập từ load balancer
		
		Chú ý khi gán public IP của máy vào security group, chúng ta tắt máy rồi mở lại thì sẽ có public API mới,
		lúc này cần cập nhật nó trong security group để có public IP mới nhất
		
	AWS Part-1 - Security group NFS:
	
		Trong Security Group của AWS, khi bạn thấy NFS thì nó là viết tắt của:

		NFS (Network File System): giao thức chia sẻ file qua mạng, thường dùng bởi Amazon EFS (Elastic File System).
		
		Thông tin cơ bản về NFS trong Security Group:

			Protocol: TCP

			Port: 2049 (cổng mặc định của NFS)

			Ý nghĩa: Cho phép EC2 (hoặc các máy khác trong VPC) kết nối đến EFS để đọc/ghi file.
			
		Ví dụ Security Group cho EFS:

			Để EC2 có thể mount EFS:

				Security Group của EFS phải mở inbound rule:

				Type: NFS

				Protocol: TCP

				Port Range: 2049

				Source: Security Group của EC2 (hoặc CIDR của VPC, ví dụ 10.0.0.0/16)
		
	AWS Part-1 - EBS:
	
		EBS trong AWS là viết tắt của Amazon Elastic Block Store. Đây là dịch vụ cung cấp lưu
		trữ dạng block (block storage) cho các EC2 instance.		
		
		Đặc điểm chính của Amazon EBS:

			Block storage:

				Dữ liệu được lưu thành các khối (block), tương tự như ổ cứng gắn vào máy tính.

				Có thể format thành hệ thống file (ext4, xfs, NTFS, …) hoặc dùng trực tiếp cho
				database, ứng dụng cần truy cập I/O nhanh.

			Gắn với EC2 Instance:

				Một volume EBS giống như ổ cứng ảo gắn vào EC2.

				Có thể gắn một volume cho nhiều instance (chế độ read-only) hoặc cho 1 instance (read/write).

			Tính đàn hồi (Elastic):

				Có thể tăng/giảm dung lượng mà không cần dừng instance.

				Hỗ trợ thay đổi loại volume (ví dụ từ HDD sang SSD).

			Tính bền vững (Durability):

				EBS được lưu trữ trong Availability Zone (AZ), có khả năng replicate trong AZ để tránh
				mất dữ liệu khi có lỗi phần cứng.

			Snapshot:

				Có thể tạo bản sao (snapshot) của volume và lưu trên Amazon S3.

				Snapshot dùng để backup hoặc tạo volume mới từ bản copy.

	AWS Part-1 - EBS Snapshot:
	
		Trong Amazon EBS, snapshot chính là cơ chế lưu lại dữ liệu của một volume tại một
		thời điểm cụ thể (point-in-time).
					
		Cụ thể hơn:

			Một EBS volume giống như ổ cứng gắn vào EC2 instance.

			Khi bạn tạo snapshot:

				AWS sẽ "chụp lại" toàn bộ dữ liệu của volume tại thời điểm đó.

				Snapshot này được lưu trong S3 nội bộ của AWS (bạn không trực tiếp thấy trong
				bucket S3, mà quản lý qua EC2 console/CLI).
				
		Mục đích chính của snapshot:

			Sao lưu dữ liệu (backup)

				Để phòng sự cố mất dữ liệu, hỏng volume, hoặc EC2 bị terminate.

			Phục hồi (restore)

				Bạn có thể tạo lại một volume mới từ snapshot → có dữ liệu giống hệt volume ban đầu tại thời điểm chụp.

			Tạo bản copy (clone)

				Dùng snapshot để tạo volume mới cho môi trường test/dev mà không ảnh hưởng đến volume gốc.

			Di chuyển dữ liệu giữa region

				Snapshot có thể copy sang region khác, sau đó tạo volume ở đó.
			
		Lưu ý quan trọng:

			Snapshot EBS là incremental:

				Snapshot đầu tiên = toàn bộ dữ liệu.

				Snapshot tiếp theo = chỉ lưu phần dữ liệu thay đổi từ lần trước → tiết kiệm chi phí.

			Nếu xóa 1 snapshot, dữ liệu của snapshot đó được giữ lại trong các snapshot
			sau (AWS đảm bảo không mất dữ liệu miễn là còn ít nhất 1 snapshot trong chuỗi).
			
	WS Part-1 - EC2 Launch Template:
	
		Launch Template là một “mẫu cấu hình” (template) chứa sẵn thông tin cần thiết để tạo EC2 instance.

		Nó giúp bạn chuẩn hóa và tái sử dụng cấu hình khi khởi tạo nhiều EC2.

		Có thể coi như là một "blueprint" cho instance.

		Thông tin lưu trong Launch Template:

			Khi tạo Launch Template, bạn có thể định nghĩa sẵn:

				AMI (Amazon Machine Image) → hệ điều hành, phần mềm cài sẵn.

				Instance type (t2.micro, t3.small, v.v.).

				Key pair (dùng để SSH).

				Security group.

				VPC + Subnet.

				EBS volume mapping (ổ cứng gắn vào instance).

				IAM role cho EC2.

				User data (script chạy khi instance khởi động).

				Network settings (private/public IP).
				
		Tính năng chính:

			Tái sử dụng cấu hình:

				Không phải nhập tay lại từng thông tin khi launch EC2 mới.

			Versioning (phiên bản hóa):

				Mỗi khi bạn thay đổi Launch Template → sẽ tạo ra một version mới.

				Có thể quay lại version cũ hoặc chọn version mặc định.

			Tích hợp với Auto Scaling và Spot Fleet:

				Khi bạn tạo Auto Scaling Group hoặc EC2 Spot Fleet, thay vì cấu hình phức tạp → chỉ cần trỏ đến Launch Template.

			So với Launch Configuration (LC):

				Launch Template là phiên bản cải tiến của Launch Configuration.

				Hỗ trợ nhiều tính năng hơn (versioning, multiple instance types, T2/T3 Unlimited, placement groups...).

				AWS khuyên dùng Launch Template thay vì Launch Configuration.
				
		Ví dụ thực tế

			Bạn muốn triển khai Auto Scaling Group cho web server:

			Tạo Launch Template với cấu hình:

				AMI Ubuntu 22.04

				Instance type: t3.micro

				Security Group mở cổng 80/443

				User Data: cài Nginx

			Khi Auto Scaling cần tạo thêm EC2 → nó sẽ dựa vào Launch Template này, đảm bảo tất cả instance đều giống nhau.
			
	AWS Part-1 - Load Balancing:
	
		Load Balancing = kỹ thuật phân phối lưu lượng (traffic) đến nhiều server (targets) thay vì dồn hết vào một server.

		Mục tiêu:

			Tăng tính sẵn sàng (High Availability) → nếu 1 server hỏng, traffic tự động chuyển sang server khác.

			Tăng khả năng mở rộng (Scalability) → thêm server mới để gánh thêm tải.

			Cân bằng hiệu năng (Performance) → tránh 1 server bị quá tải trong khi server khác rảnh.

			Trong AWS, Elastic Load Balancer (ELB) chính là dịch vụ thực hiện load balancing cho EC2 và
			các dịch vụ backend khác.
			
		Thành phần chính trong Load Balancing (EC2 + ELB):

			Load Balancer (ALB, NLB, GWLB, CLB):

				Thành phần đứng trước client.

				Nhận request từ user → phân phối đến backend.

			Listener:

				Quy định protocol & port (VD: HTTP:80, HTTPS:443, TCP:3306).

				Có thể có nhiều listener trong một Load Balancer.

			Target Group:

				Tập hợp backend (EC2 instances, IPs, Lambda, ECS tasks).

				Load Balancer sẽ gửi request đến Target Group theo rule.

			Health Check:

				Load Balancer kiểm tra trạng thái (healthy/unhealthy) của từng target.

				Chỉ gửi request đến target còn "healthy".
				
		Các loại Load Balancer trong AWS

			Application Load Balancer (ALB)

				Dùng cho HTTP/HTTPS.

				Có tính năng routing theo path, host, header (Layer 7).

				Ví dụ: /api/* vào backend API, /img/* vào backend static server.

			Network Load Balancer (NLB)

				Dùng cho TCP/UDP (Layer 4).

				Hiệu năng cực cao, độ trễ thấp, xử lý hàng triệu request/s.

			Gateway Load Balancer (GWLB)

				Dùng để phân phối traffic qua firewall/appliance (Layer 3).

				Classic Load Balancer (CLB)

				Dịch vụ đời cũ, ít dùng cho hệ thống mới.
				
		Ví dụ thực tế:

			Bạn có 3 EC2 chạy web server:

			Cấu hình 1 Application Load Balancer (ALB) với listener HTTP:80.

			Gắn 3 EC2 này vào Target Group.

			Khi người dùng truy cập website:

				Request đến ALB.

				ALB chọn 1 EC2 trong target group (theo Round Robin hoặc rule khác).

				Nếu 1 EC2 bị down → ALB ngừng gửi traffic đến nó.
			
	AWS Part-1 - Target Group của Load Balancing:
	
		Target Group là tập hợp các backend (targets) – tức là nơi nhận request từ Load Balancer.

		Các target có thể là:

			EC2 instances

			IP addresses (private IP)

			Lambda functions

			ECS tasks (trong case dùng với container)

		Nói đơn giản: Target Group = danh sách các server/ứng dụng mà Load Balancer sẽ phân phối traffic đến.
		
		Cách hoạt động

			Người dùng gửi request → Load Balancer nhận request.

			Load Balancer sẽ chuyển request đến một Target Group đã gắn với nó.

			Trong Target Group, LB phân phối request đến các target cụ thể (EC2, IP, ECS task...) dựa
			theo policy (Round Robin, Least Outstanding Request, …).

			Target Group có health check để xác định target nào còn “sống” (healthy). LB chỉ gửi traffic đến các target healthy.
			
		Các loại Target Group (tùy loại Load Balancer):

			Application Load Balancer (ALB): target group có thể chứa EC2, IP, ECS tasks, Lambda.

			Network Load Balancer (NLB): target group thường chứa EC2 hoặc IP.

			Gateway Load Balancer (GWLB): target group chứa appliances (ví dụ firewall ảo).
			
		Health Check:

			Mỗi Target Group có thể cấu hình health check riêng (HTTP, HTTPS, TCP).

			Nếu target fail health check → LB tạm dừng gửi request đến đó.
			
		Ví dụ thực tế:

			Bạn có 2 EC2 web server chạy Nginx:

				Tạo Target Group web-servers.

				Add 2 EC2 instance (10.0.1.10, 10.0.1.11) vào Target Group.

				Cấu hình ALB rule: nếu request đến / → forward đến Target Group web-servers.

				Khi ALB nhận request, nó sẽ forward sang 1 trong 2 EC2 đó (theo Round Robin).
				
	AWS Part-1 - EC2 Auto Scaling:
	
		Trong AWS, Auto Scaling (thường gọi là EC2 Auto Scaling) là dịch vụ giúp bạn tự động điều chỉnh
		số lượng tài nguyên (EC2 instances) để đáp ứng nhu cầu tải, mà không cần can thiệp thủ công.
		
		Các thành phần chính trong AWS Auto Scaling

		Launch Template / Launch Configuration:

			Định nghĩa cách EC2 sẽ được tạo: AMI, instance type, key pair, security group, user data,…

		Auto Scaling Group (ASG):

			Là nhóm EC2 được quản lý bởi Auto Scaling.

			Bạn cấu hình:

				Min size: số lượng EC2 tối thiểu.

				Max size: số lượng EC2 tối đa.

				Desired capacity: số lượng EC2 mong muốn (ASG sẽ duy trì con số này).

		Scaling Policies:

			Quy tắc để scale in (giảm) hoặc scale out (tăng).

			Ví dụ:

				Nếu CPU > 70% trong 5 phút → scale out +1 instance.

				Nếu CPU < 30% trong 10 phút → scale in -1 instance.

		Health Checks:

			ASG sẽ kiểm tra tình trạng EC2. Nếu một instance unhealthy, nó sẽ terminate và khởi tạo lại instance mới.

		Integration với Load Balancer

			ASG thường gắn với Target Group của một Elastic Load Balancer (ALB/NLB).

			Khi EC2 scale in/out, chúng sẽ tự động được đăng ký/dỡ khỏi target group.
			
	AWS Part-1 - Route 53:
	
		Amazon Route 53 là một dịch vụ DNS (Domain Name System) và quản lý tên miền của AWS.

		Các chức năng chính của Route 53:

			Domain Registration (Đăng ký tên miền)

				Bạn có thể mua và quản lý domain trực tiếp trên AWS (ví dụ: mywebsite.com).

			DNS Service (Hệ thống phân giải tên miền)

				Biến tên miền dễ nhớ (mywebsite.com) thành địa chỉ IP thật (192.0.2.1).

				Quản lý các bản ghi DNS như:

					A (IPv4 address)

					AAAA (IPv6 address)

					CNAME (alias)

					MX (mail server)

					TXT (text records, thường dùng cho xác thực email, SSL, …).

			Traffic Routing (Điều hướng lưu lượng thông minh)

				Route 53 hỗ trợ nhiều kiểu điều hướng:

					Simple routing → trỏ domain đến 1 server duy nhất.

					Weighted routing → chia tải theo tỉ lệ % giữa nhiều server.

					Latency-based routing → gửi user đến server có độ trễ thấp nhất.

					Failover routing → tự động chuyển sang server khác khi server chính bị down.

			Health Checks & Monitoring (Giám sát dịch vụ)

				Route 53 có thể kiểm tra tình trạng endpoint (web server, API, …).

				Nếu server không phản hồi, Route 53 sẽ tự động chuyển traffic sang server backup.

			Ứng dụng thực tế

				Khi bạn có website chạy trên EC2 / S3 / Load Balancer, bạn dùng Route 53 để:

					Trỏ domain (myapp.com) → Elastic Load Balancer.

					Tự động điều hướng người dùng đến server gần nhất (multi-region).

					Đảm bảo hệ thống vẫn online khi một server bị lỗi (failover).
		
	AWS Part-1 - Kiến thức:	
		
		- EC2 Key pair name: RSA, ED25519
		- EC2 prite key format: .pem, .ppk
		- EC2 network settings: VPC
		- EC2 network setting: Inbound Security Group Rules
		- EC2 instances -> connect -> ssh client
		- EC2 instance -> security -> security group -> edit inbound rules
		- EC2 -> key pair
		- EC2 network interface
		- EC2 security group: My IP
		- EC2 -> Launch an instance -> Resource types -> Intances, Volumes
		- EC2 -> Instance -> Actions -> Instance settings -> Change instance type
		- EC2 -> Instance -> Actions -> Image and templates -> Create image
		- EC2 -> Images -> AMIs
		- EC2 -> Images -> Action -> Copy AMI
		- EC2 -> Images -> Action -> Edit AMI permissions
		- EC2 -> Images -> Action -> Launch instance from AMI
		- EC2 -> Instance -> Actions -> Security -> Change security groups
		- EC2 -> Instance -> Actions -> Monitor and troubleshoot -> Get system log
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Dissociate Elastic IP address
		- EC2 -> Instance -> Network & Security -> Elastic IPs -> Actions -> Release Elastic IP addresses
		- EC2 -> Instances -> Storage -> Block devices
		- EC2 -> Instances -> Launch Templates -> Actions -> Launch instance from template
		- EC2 -> Instances -> Create image
		- EC2 -> Elastic Block Store -> Volumes
		- EC2 -> Elastic Block Store -> Volumes -> Action -> Attach Volume
		- EC2 -> Elastic Block Store ->  Actions -> Detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Force detach volume
		- EC2 -> Elastic Block Store ->  Actions -> Create snapshot
		- EC2 -> Load Balancing -> Target groups -> Create target group
		- EC2 -> Load Balancing -> Load Balancers -> Create load balancer
		- EC2 -> AMIs -> Launch instance from AMI
		- EC2 -> Auto Scaling groups
		- EC2 -> Instances -> Vao instance -> Actions -> Security -> Modify IAM role
		- CloudWatch -> All alarms -> Create alarm
		- Amazon EFS -> Access points
		- Amazon S3 -> Buckets
		- Amazon S3 -> Buckets -> Objects
		- Amazon S3 -> Buckets -> Properties
		- Amazon S3 -> Buckets -> Permission
		- Amazon S3 -> Buckets -> Manegement -> Lifecycle rules -> Create lifecycle rule
		- Amazon S3 -> Buckets -> Manegement -> Replication rules -> Create replication rules
		- Aurora and RDS
		- Aurora and RDS -> Snapshots		
		- Aurora and RDS -> Databases -> Actions -> Migrate snapshot
		- Aurora and RDS -> Parameter group
		- Aurora and RDS -> Subnet groups
		- Route 53 -> Dashboard -> Create hosted zone
		- IAM -> Access management -> Users -> Add users
		- IAM -> Access management -> Users -> Security credentials -> Access keys -> Create access key
		- IAM -> Access management -> Roles -> Create role
		- IAM -> Users -> Create user
		- IAM -> User -> Vào user -> Security credentials -> Create access key
		- Certificate Manager
		- Amazon AlasticCache -> Parameter groups -> Create parameter group
		- Amazon AlasticCache -> Subnet groups -> Create subnet group
		- Amazon AlasticCache -> Dashboard -> Create cache -> Create Memcache
		- Amazon MQ -> Broker -> Create brokers
		- Amazon Elastic Beanstalk -> Environment -> Upload and deploy
		- Cloudfront
		
		
		
		

	
	Trong bước Set permissions khi tạo IAM User trên AWS, bạn thấy có 3 tùy chọn cấp quyền:

		1. Add user to group

			Thêm user vào một nhóm (IAM Group).

			Nhóm này đã có sẵn các policy (quyền hạn).

			Tất cả các user trong nhóm sẽ kế thừa quyền từ nhóm đó.

			Đây là cách quản lý tập trung, dễ duy trì khi có nhiều user. Ví dụ: tạo group Developers
			với quyền AmazonEC2FullAccess, mọi user trong group sẽ có quyền đó.

		2. Copy permissions

			Sao chép toàn bộ quyền của một user khác sang user mới.

			Bao gồm group memberships, attached managed policies, và inline policies.

			Tiện khi bạn có một user cũ làm chuẩn, và muốn user mới có quyền giống hệt.

		3. Attach policies directly

			Gắn trực tiếp policy (quyền hạn) vào user.

			Bạn chọn các AWS managed policy (AWS tạo sẵn, ví dụ: AmazonS3ReadOnlyAccess) hoặc customer
			managed policy (do bạn tự tạo).

			Linh hoạt cho từng user, nhưng không khuyến khích nếu có nhiều user vì sẽ khó quản lý về lâu dài.	
		
		
			
			

		Quy trình tạo một Amazon EC2 Instance (máy ảo chạy trên AWS). Flow gồm các bước tuần tự như sau:

			Choose an AMI (Amazon Machine Image)

				AMI là "bản mẫu" hệ điều hành + phần mềm (Ubuntu, Amazon Linux, Windows Server, …).

				Bạn chọn AMI để quyết định máy ảo của bạn sẽ chạy hệ điều hành nào, có cài sẵn phần mềm gì.

			Choose an Instance Type

				Ở bước này bạn chọn loại EC2 instance (tính năng giống như chọn cấu hình phần cứng).

				Ví dụ: t2.micro (1 vCPU, 1GB RAM – miễn phí trong Free Tier), m5.large, c5.xlarge, …

				Mỗi loại khác nhau về CPU, RAM, khả năng mạng, dùng cho mục đích khác nhau (test, production, tính toán nặng…).

		Configuring the Instance

			Cấu hình chi tiết cho instance:

				Chạy bao nhiêu instance

				VPC/Subnet (mạng nào)

				Tự động cấp public IP hay không

				IAM Role (gắn quyền để EC2 truy cập dịch vụ AWS khác)

				User data (script tự chạy khi khởi động instance)

		Adding Storage

			Chọn dung lượng ổ đĩa (EBS volume).

			Có thể thêm nhiều volume, chọn loại (SSD, HDD, provisioned IOPS…).

			Ví dụ: 8GB gp2 SSD mặc định.

		Adding Tags

			Gán nhãn (key-value) để quản lý dễ hơn.

			Ví dụ: Name=WebServer01, Environment=Dev.

			Tags giúp tìm kiếm, phân nhóm, hoặc dùng cho billing (chi phí).

		Configure Security Group

			Security Group là tường lửa ảo cho EC2.

			Ở đây bạn định nghĩa inbound/outbound rules (cổng nào mở cho IP nào).

			Ví dụ: mở port 22 (SSH) cho IP của bạn, mở port 80 (HTTP) cho mọi người.

		Review

			Xem lại toàn bộ cấu hình đã chọn.

			Nếu ổn, bạn nhấn "Launch" để khởi tạo EC2.

			Lúc này AWS sẽ hỏi bạn chọn key pair (dùng để SSH vào máy).

			Kết quả: Bạn có một máy ảo EC2 chạy trên AWS, kết nối qua SSH/HTTP/HTTPS tùy cấu hình.

	Code trong Advanced details của EC2:

		Case 1:
		
			#!/bin/bash
			sudo yum install httpd -y
			sudo systemctl start httpd
			sudo systemctl enable httpd
			mkdir /tmp/test1
			
		Case 2:
		
			#!/bin/bash
			yum install httpd wget unzip -y
			systemctl start httpd
			systemctl enable httpd
			cd /tmp
			wget https://www.tooplate.com/zip-templates/2119_gymso_fitness.zip
			unzip -o 2119_gymso_fitness.zip
			cp -r 2119_gymso_fitness/* /var/www/html/
			systemctl restart httpd
			
		Case 3:
		
			#!/bin/bash

			# Variable Declaration
			#PACKAGE="httpd wget unzip"
			#SVC="httpd"
			URL='https://www.tooplate.com/zip-templates/2098_health.zip'
			ART_NAME='2098_health'
			TEMPDIR="/tmp/webfiles"

			yum --help &> /dev/null

			if [ $? -eq 0 ]
			then
			   # Set Variables for CentOS
			   PACKAGE="httpd wget unzip"
			   SVC="httpd"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo yum install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/

			else
				# Set Variables for Ubuntu
			   PACKAGE="apache2 wget unzip"
			   SVC="apache2"

			   echo "Running Setup on CentOS"
			   # Installing Dependencies
			   echo "########################################"
			   echo "Installing packages."
			   echo "########################################"
			   sudo apt update
			   sudo apt install $PACKAGE -y > /dev/null
			   echo

			   # Start & Enable Service
			   echo "########################################"
			   echo "Start & Enable HTTPD Service"
			   echo "########################################"
			   sudo systemctl start $SVC
			   sudo systemctl enable $SVC
			   echo

			   # Creating Temp Directory
			   echo "########################################"
			   echo "Starting Artifact Deployment"
			   echo "########################################"
			   mkdir -p $TEMPDIR
			   cd $TEMPDIR
			   echo

			   wget $URL > /dev/null
			   unzip $ART_NAME.zip > /dev/null
			   sudo cp -r $ART_NAME/* /var/www/html/
			   echo

			   # Bounce Service
			   echo "########################################"
			   echo "Restarting HTTPD service"
			   echo "########################################"
			   systemctl restart $SVC
			   echo

			   # Clean Up
			   echo "########################################"
			   echo "Removing Temporary Files"
			   echo "########################################"
			   rm -rf $TEMPDIR
			   echo

			   sudo systemctl status $SVC
			   ls /var/www/html/
			fi 

			
	Code trong vi /etc/fstab:

		Case 1:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1p1    /var/www/html/images ext4       defaults        0 0
			
		Case 2:
		
			UUID=596343d2-9dab-4c86-9dd0-4a537eb34256 / xfs defaults 0 1
			UUID=5c6d2636-1ce3-4997-bf2f-1d032b4db44e /boot ext4 defaults 0 0
			UUID=DCE7-EF83 /boot/efi vfat defaults,umask=0077,shortname=winnt 0 0
			/dev/nvme1n1  /var/lib/mysql ext4     defaults        0 0
			
		Case 3:
		
			fs-0001a83c45525fae6 /var/www/html/images efs _netdev,tls,accesspoint=fsap-04b21726d75c2e7eb 0 0

	
			
-- AWS Cloud For Project Set Up or Lift and Shift:

	Flow chính:
	
		Luồng hoạt động:

			User → DNS (GoDaddy/Route53):
			
				Người dùng nhập domain vào browser → DNS phân giải domain → IP của Load Balancer.

			DNS → Application Load Balancer:
			
				Request tới ALB (qua HTTPS/HTTP).

			ALB → Tomcat Instances (ASG):
			
				ALB phân phối request đến một trong các EC2 Tomcat.

			Tomcat Instances → Backend services:

				Nếu cần dữ liệu → Tomcat kết nối tới MySQL.

				Nếu cần cache → gọi tới Memcache.

				Nếu cần message queue → gọi tới RabbitMQ.

				Nếu cần static file → truy vấn từ S3 bucket.

				Domain nội bộ (db01, mc01, rmq01) được phân giải nhờ Private DNS Zone (Route53).

			Response → quay ngược lại cho user thông qua Load Balancer.
	
		Các thành phần chính:

			DNS Zones (GoDaddy / Route53):

				Người dùng (Users) truy cập ứng dụng qua tên miền (domain).

				DNS (ví dụ GoDaddy hoặc Route53) phân giải tên miền thành địa chỉ IP của Application Load Balancer.

			Application Load Balancer (ALB):

				Nhận request từ người dùng qua HTTP/HTTPS.

				ALB có Security Group riêng để kiểm soát ai được phép truy cập (ví dụ: mở cổng 80, 443).

				ALB phân phối request đến các Tomcat Instances phía sau.

			Auto Scaling Group (ASG) + Tomcat Instances:

				Các EC2 instances chạy ứng dụng (Java/Tomcat).

				ASG đảm bảo số lượng instances phù hợp (scale in/out).

				Có Security Group riêng, chỉ cho phép traffic đến từ ALB.

			Amazon S3 Bucket:

				Lưu trữ static files (hình ảnh, video, tài liệu).

				Ứng dụng Tomcat có thể đọc/ghi file từ đây.

			Amazon Route53 (Private DNS Zones):

				Dùng để phân giải nội bộ trong VPC.

				Ví dụ:

					db01 → IP MySQL instance

					mc01 → IP Memcache instance

					rmq01 → IP RabbitMQ instance

				Điều này giúp ứng dụng (Tomcat) gọi service bằng tên domain thay vì IP cứng.

			Backend Instances (có Security Group riêng):

				MySQL Instances: Database server (có thể là RDS hoặc EC2 cài MySQL).

				Memcache Instances: Cache để tăng tốc độ xử lý.

				RabbitMQ Instances: Message broker, dùng để giao tiếp phi đồng bộ giữa các service.
				
	Từ EC2 instance -> Tạo image -> Tạo launch template và thêm image mới tạo vào -> tao auto scaling group
	và thêm launch template mới tạo vào

	- Command:
	
		- cmd: mysql -u admin -padmin123 accounts
		
			mysql → gọi MySQL client (dùng để kết nối và làm việc với MySQL server).

			-u admin → chỉ định username để đăng nhập vào MySQL là admin.

			-padmin123

				-p dùng để nhập mật khẩu.

				Ở đây viết liền luôn admin123 → nghĩa là mật khẩu là admin123.

				(Nếu chỉ viết -p thôi thì MySQL sẽ hỏi mật khẩu sau khi enter).

			accounts → đây là tên database muốn truy cập sau khi login.

				Nghĩa là sau khi kết nối thành công, bạn sẽ ở ngay trong database accounts.
		
			Kết nối tới MySQL server bằng user admin, password admin123, và chọn sẵn database accounts để làm việc.
			
		- cmd: ping -c 4 db01.vprofile.in
		
			Cấu trúc lệnh:

				ping: công cụ kiểm tra kết nối mạng (ICMP echo request/reply).

				-c 4: option -c (count) → gửi 4 gói tin ICMP rồi dừng (nếu không có, ping sẽ chạy vô hạn đến khi bạn bấm Ctrl+C).

				db01.vprofile.in: hostname / domain mà bạn muốn kiểm tra kết nối.

			Quá trình diễn ra

				DNS Resolution

					Máy của bạn sẽ hỏi DNS để lấy địa chỉ IP của db01.vprofile.in.

					Nếu trong Route 53 (hoặc DNS khác) bạn có record cho db01.vprofile.in, nó sẽ trả về
					IP (có thể là private hoặc public).

				Gửi ICMP Echo Request

					Ping sẽ gửi 4 gói ICMP đến IP vừa resolve được.

				Nhận ICMP Echo Reply

					Nếu host trả lời, bạn sẽ thấy thời gian phản hồi (time=xx ms) cho mỗi gói.

				Kết quả thống kê

					Sau khi gửi 4 gói, ping sẽ in thống kê:

						Số gói gửi/nhận

						Tỷ lệ packet loss (%)

						Thời gian trung bình / min / max / stddev

			Ý nghĩa trong trường hợp này

				db01.vprofile.in thường là record trong Route 53 hoặc DNS bạn tự quản lý.

				Nếu bạn đã tạo A record (ví dụ db01.vprofile.in -> 10.0.1.5), thì ping này sẽ kiểm tra xem từ máy bạn có kết
				nối đến server db01 không.

				Nếu record không tồn tại hoặc không resolve được → báo lỗi “unknown host”.

				Nếu resolve được nhưng không ping được → có thể bị firewall, security group, hoặc ICMP bị chặn.
		
		- cmd: $ aws configure
		
				AWS Access Key ID [****************ADE6]: AKIA5KBUVI7M7FM7RC5A
				AWS Secret Access Key [****************dUht]: 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
				Default region name [us-east-1]: us-east-1
				Default output format [json]: json
		
		- cmd: vim ~/.aws/credentials
		- cmd: vim ~/.aws/config
		- cmd: aws s3 cp target/vprofile-v2.war s3://vprofile-las-artifactss12311
		- cmd: aws s3 ls s3://vprofile-las-artifactss12311/
		- cmd: aws-cli --classic
		- cmd: aws s3 cp s3://vprofile-las-artifactss12311/vprofile-v2.war /tmp/
		- cmd: systemctl stop tomcat10
		- cmd: systemctl start tomcat10
		- cmd: ls /var/lib/tomcat10/webapps/
		- cmd: cp /tmp/vprofile-v2.war /var/lib/tomcat10/webapps/ROOT.war
		- cmd: ls /var/lib/tomcat10/webapps
		- cmd: sudo yum install -y telnet
		- cmd: telnet db01.vprofile.in 3306
		
		
	- Flow khi bạn mua domain ở GoDaddy và muốn dùng với AWS Certificate Manager (ACM):
	
		1. Mua domain ở GoDaddy

			Bạn đăng ký domain tại GoDaddy

			Ví dụ bạn mua: myawsdemo.com.

			Sau khi thanh toán xong, domain này sẽ được quản lý trong GoDaddy DNS.

		2. Tạo Hosted Zone trong Route 53

			Vào AWS → Route 53 → Hosted Zones → Create hosted zone.

			Nhập tên domain của bạn (myawsdemo.com).

			Route 53 sẽ sinh ra 4 record NS (Name Servers) và 1 record SOA.

		3. Cập nhật DNS ở GoDaddy

			Vào GoDaddy → My Domains → DNS Management.

			Thay 4 NS mặc định của GoDaddy bằng 4 NS từ AWS Route 53.

			Điều này nghĩa là: từ giờ mọi request tới domain sẽ được quản lý bởi AWS.

			Thời gian cập nhật có thể mất 5 phút – 24h (thường là vài phút).

		4. Yêu cầu chứng chỉ trong ACM

			Vào AWS Certificate Manager → Request a certificate.

			Nhập domain cần SSL:

			myawsdemo.com

			*.myawsdemo.com (nếu bạn muốn wildcard cho subdomain).

			Chọn DNS validation.

			ACM sẽ đưa ra 1 record CNAME.

		5. Thêm CNAME vào Route 53

			Vào Route 53 → Hosted Zone → Create record.

			Thêm CNAME do ACM cung cấp.

			Chờ AWS xác thực (vài phút – 30 phút).

			Khi trạng thái certificate = Issued → bạn đã có chứng chỉ hợp lệ.

		6. Sử dụng certificate

			CloudFront → chọn certificate trong phần SSL.

			ALB (Application Load Balancer) → add certificate vào Listener 443.

			API Gateway → add certificate cho Custom Domain.
				
	- Trường hợp tạo load blancer nhưng không mua domain:

		HTTPS ở Load Balancer (ALB/NLB/CLB)

			Khi bạn tạo Load Balancer và muốn lắng nghe HTTPS (port 443), AWS sẽ yêu cầu bạn chọn:

				SSL certificate (từ AWS Certificate Manager – ACM, hoặc import từ ngoài).

			Tại sao certificate lại cần domain?

				Certificate SSL/TLS luôn gắn với một tên miền (VD: myapp.com, *.myapp.com).

				Trình duyệt khi người dùng truy cập https://myapp.com sẽ kiểm tra:

				Domain trong URL có khớp với domain trong certificate không?

				Nếu khớp → trình duyệt hiển thị ổ khóa xanh 🔒.

				Nếu không khớp → hiện warning “Connection not secure”.

				Vì vậy, certificate không thể cấp cho DNS mặc định của AWS ELB (xxxxx.elb.amazonaws.com), trừ
				khi bạn có chứng chỉ wildcard chính thức bao trùm domain đó (cái này bạn không sở hữu được).

			Khi không có domain

				Bạn vẫn có thể tạo Load Balancer với listener HTTP (port 80) → truy cập bình thường qua DNS mặc định.

				Nhưng nếu chọn HTTPS (443), AWS sẽ bắt buộc bạn chọn certificate.

				Bạn sẽ không có certificate hợp lệ nếu không sở hữu domain → không thể hoàn tất HTTPS chuẩn.

				Có thể import self-signed certificate để test, nhưng trình duyệt sẽ luôn báo lỗi bảo mật.

	- Tại sao bạn tạo record trong Route 53 Hosted Zone trỏ tới private IP của EC2 và khi nào nên
	làm vậy, kèm ví dụ và best-practice:
	
		Ý nghĩa / mục đích

			DNS nội bộ (Private DNS): Khi bạn tạo Private Hosted Zone (liên kết với VPC) và thêm A record trỏ
			tới private IP của EC2, các instance trong VPC sẽ có thể resolve tên (ví dụ db01.internal.example.com)
			thành IP nội bộ (10.x.x.x). Dùng cho giao tiếp nội bộ giữa service mà không lộ ra Internet.

			Dễ quản lý hơn: Thay vì hardcode IP vào config ứng dụng bạn dùng tên dễ đọc/transfer: mysql -h db01.internal.example.com.
			Khi phải thay đổi instance, bạn chỉ cần update DNS record, không phải edit mọi config.

			Service discovery đơn giản: Dùng tên cố định cho DB, cache, mq, v.v. để các app tìm tới service đó.

			Split-horizon / Private vs Public: Bạn có thể có một zone public (cho internet) và một private (cho VPC); private chỉ
			giải quyết trong VPC — an toàn hơn.

			Hỗ trợ failover / swap: Nếu bạn thay instance bằng instance mới, đổi record sang IP mới → ứng dụng chuyển sang server mới
			nhanh hơn.

			Tích hợp với ALB/ENI/Elastic IP: Thay vì trỏ trực tiếp IP của instance, thường tốt hơn là trỏ tới internal ALB (dùng Alias record)
			hoặc dùng ENI cố định để tránh phải update DNS khi instance thay đổi.

		Ví dụ thực tế

			Tạo Private Hosted Zone internal.example.com liên kết với VPC vpc-abc123.

			Tạo A record:

				db01.internal.example.com → 10.0.1.5

				cache.internal.example.com → 10.0.1.6

			Trên EC2 trong cùng VPC: ping db01.internal.example.com → sẽ resolve thành 10.0.1.5.

		Cách tạo (tóm tắt)

			AWS Console → Route 53 → Hosted zones → Create hosted zone.

				Type: Private hosted zone for Amazon VPC

				Enter domain: internal.example.com

				Associate with one or more VPCs.

			Vào zone → Create record → Type: A → Name: db01 → Value: 10.0.1.5 → Save.

		Lưu ý & best practices

			Private IP có thể đổi nếu bạn terminate instance; stop/start thường giữ private IPv4 chính nhưng không tuyệt
			đối (nếu bạn detach ENI hay thay instance thì thay đổi). Để ổn định:

				Gán Elastic Network Interface (ENI) với private IP tĩnh, hoặc

				Dùng internal load balancer (ALB/NLB) và tạo Alias record trỏ tới ALB (không phải IP) — ALB xử lý backend thay đổi.

			TTL: đặt TTL thấp (ví dụ 60s) nếu bạn hay thay đổi record, nhưng TTL thấp tăng số query DNS.

			Tự động hóa: Nếu bạn autoscale, update DNS thủ công sẽ bất tiện — dùng AWS Cloud Map / Service Discovery hoặc tự
			động đăng ký qua userdata / script / automation (Lambda) để cập nhật record khi instance khởi tạo/terminate.

			Sử dụng SRV / CNAME khi phù hợp (ví dụ SRV cho service discovery có port).

			Bảo mật: Private hosted zone chỉ trả lời từ VPC đã liên kết — an toàn nội bộ.

			Không trỏ public records tới private IP (sẽ không truy cập được từ Internet).

		Khi nào KHÔNG nên dùng trực tiếp private IP record

			Nếu bạn có autoscaling hoặc thường xuyên thay instance → nên dùng internal ALB hoặc Cloud Map.

			Nếu service cần load-balancing hoặc health checks → dùng ALB/NLB thay vì trỏ thẳng vào một instance.
		
		
--- Re-Architecting Web App on AWS Cloud [PAAS & SAAS]:

	Flow chính:
	
		Users (Người dùng)

			Người dùng gửi request (HTTP/HTTPS) từ trình duyệt → ví dụ: https://myapp.com.

		Amazon Route 53

			Đây là DNS service.

			Nó sẽ phân giải tên miền (myapp.com) → IP/public endpoint của hệ thống.

		Amazon CloudFront (CDN)

			Giúp cache nội dung tĩnh (ảnh, CSS, JS, video) gần với người dùng nhất.

			Làm ứng dụng nhanh hơn, giảm tải cho backend.

		Application Load Balancer (ALB)

			Điều phối request đến các EC2 instances trong Elastic Beanstalk.

			Đảm bảo high availability (HA), scale theo tải.

		Elastic Beanstalk

			Dịch vụ tự động deploy và quản lý ứng dụng.

			Trong hình, Beanstalk quản lý một Auto Scaling Group chứa nhiều EC2 chạy Apache Tomcat (ứng dụng Java).

			Khi tải cao → tự động thêm EC2, khi tải thấp → giảm EC2.

		Amazon CloudWatch

			Theo dõi metric (CPU, memory, request count...).

			Dùng để trigger auto scaling trong Elastic Beanstalk.

		Artifacts in S3 Bucket

			Code (WAR file, artifact) được upload vào S3 bucket.

			Elastic Beanstalk lấy artifact này để deploy vào Tomcat.

		Apache Tomcat

			Nơi ứng dụng Java thực sự chạy (xử lý business logic).

			Ví dụ: Spring Boot app hoặc Java EE app.

		Amazon MQ

			Message broker (giống ActiveMQ).

			Dùng khi hệ thống có asynchronous communication giữa các service.

			Ví dụ: app gửi message vào queue, service khác xử lý sau.

		MySQL (Amazon RDS)

			Database chính, lưu dữ liệu ứng dụng.

		Memcached

			Cache server để tăng tốc truy vấn (giảm tải cho MySQL).

			Lưu trữ tạm thời dữ liệu hay dùng (session, query result).
		
	- Command:
	
		- cmd: apt update && apt install mysql-client git -y
		- cmd: mysql -h vprofile-rds-reach.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pxUScYu55cFzH5NxM8Zc5 accounts < src/main/resources/db_backup.sql
		
--- Continuous Integration with Jenkins:

	Continuous Integration with Jenkins - Flow of Continuous Integration Pipeline:
	
		Developer (Git)

			Developer viết code và push lên GitHub repository.

			Đây là nơi lưu trữ source code chính.

		GitHub → Jenkins

			Khi có commit mới, Jenkins sẽ được trigger (có thể qua Webhook).

			Jenkins bắt đầu pipeline.

		Fetch Code (Git)

			Jenkins sử dụng Git plugin để lấy code từ GitHub về môi trường build.

		Build (Maven)

			Jenkins dùng Maven để compile source code và build project.

			Maven sẽ quản lý dependencies (thư viện bên ngoài) và đóng gói project (VD: file .jar hoặc .war).

		Unit Test (Maven)

			Sau khi build xong, Maven chạy Unit Test để đảm bảo code hoạt động đúng.

			Nếu test fail → pipeline sẽ dừng.

		Code Analysis (SonarQube)

			Code được gửi sang SonarQube để phân tích chất lượng:

				Bugs, vulnerabilities, code smells, duplicated code.

			SonarQube có Quality Gate: nếu code không đạt yêu cầu, pipeline dừng lại.

		Upload Artifact (Nexus OSS)

			Nếu code pass SonarQube, Jenkins upload artifact (jar/war) lên Nexus Repository Manager (Sonatype).

			Nexus OSS là nơi lưu trữ artifacts, để dùng cho các môi trường khác (staging, production).
			
	Chú ý là để chạy được Sonar Qube cần thêm authentication token của nó vào Jenkins mới chạy được
	
	Google search:
	
		sonar scanner pipiline script
		
		nexusartifactuploader
		
		slack login
		
		wget for git hash how to add more utilities
		
	Git Weebhook, Poll SCM
	
	LDAP
	
	
	
	
		
	Continuous Integration with Jenkins - Mục đích của Jenkins URL:

		Nó được Jenkins (và plugin) dùng làm địa chỉ chính để:

			Sinh link đầy đủ trong thông báo
			
				Ví dụ bạn gửi Slack, Email, hay Webhook thì Jenkins sẽ tạo đường dẫn đến job/build cụ thể.

					Nếu không có Jenkins URL, thông báo có thể chỉ chứa đường dẫn tương đối (ví dụ /job/vprofile-pipeline/23/) → người
					ngoài sẽ không click được.

					Khi bạn cấu hình Jenkins URL (ví dụ http://jenkins.mycompany.com:8080/), Jenkins sẽ tạo link đầy đủ:

						http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/


			Jenkins agents (slave nodes) kết nối về master
			
				Một số trường hợp Jenkins agent cần biết chính xác URL của master để trao đổi thông tin, tải config, hoặc trigger build.

			Các plugin phụ thuộc

				Slack, Email-ext plugin, GitHub plugin, Bitbucket plugin... cần Jenkins URL để tạo link "Build result" trong notification.

				Ví dụ Slack message có:

					Build #23 failed. More info at: http://jenkins.mycompany.com:8080/job/vprofile-pipeline/23/

	- Commands:
	
		- cmd: sudo apt install fontconfig openjdk-21-jre
		- cmd: sudo wget -O /etc/apt/keyrings/jenkins-keyring.asc \
				https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
		- cmd: echo "deb [signed-by=/etc/apt/keyrings/jenkins-keyring.asc]" \
				  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
				  /etc/apt/sources.list.d/jenkins.list > /dev/null
		- cmd: sudo apt install jenkins
		- cmd: systemctl status jenkins
		- cmd: ls /var/lib/jenkins/
		- cmd: cat /var/lib/jenkins/secrets/initialAdminPassword
		- cmd: apt install openjdk-17-jdk -y
		- cmd: ls /usr/lib/jvm/
		- cmd: sudo apt update && sudo apt install maven -y
		- cmd: systemctl status nexus
		- cmd: ls /opt/nexus/
		- cmd: cat /opt/nexus/sonatype-work/nexus3/admin.password
		- cmd: cd /var/lib/jenkins/
		- cmd: cd /var/lib/jenkins/workspace
		- cmd: systemctl restart jenkins
		- cmd: sudo snap install aws-cli --classic
		- cmd:  # Add Docker's official GPG key:
				sudo apt-get update
				sudo apt-get install ca-certificates curl
				sudo install -m 0755 -d /etc/apt/keyrings
				sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
				sudo chmod a+r /etc/apt/keyrings/docker.asc

				# Add the repository to Apt sources:
				echo \
				  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
				  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
				  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
				sudo apt-get update
		- cmd: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
		- cmd: systemctl status docker
		- cmd: docker images
		- cmd: su - jenkins
		- cmd: usermod -a -G docker jenkins
		- cmd: id jenkins
		- cmd: reboot
		- cmd: ssh-keygen.exe
		- cmd: ls ~/.ssh/
		- cmd: git@github.com:ngoctuanqng/jenkinstriggers.git
		- cmd: choco install wget
		- cmd: wget -q --auth-no-challenge --user username --password password --output-document - 'http://JENNKINS_IP:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
			Example:
		
				wget -q --auth-no-challenge --user admin --password ngoctuan99@ --output-document - 'http://54.234.134.51:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
		
		- cmd: curl -I -X POST http://username:APItoken@Jenkins_IP:8080/job/JOB_NAME/build?token=TOKENNAME
				-H "Jenkins-Crumb:CRUMB"
				
			Example:
		
				curl -I -X POST http://admin:11cde1b5cb464093f62e8f88edfd14388d@54.234.134.51:8080/job/Build/build?token=mybuildtoken
				-H "Jenkins-Crumb:b05d9f0d30cc94463982e15ef5dea44b39cddc6ed3a5a9254fef0bc151afe76b"
		
		- cmd: adduser devops
		- cmd: chown devops.devops /opt/jenkins-slave -R
		- cmd: vim /etc/ssh/sshd_config
		- cmd: systemctl restart ssh
		- cmd: id devops
		


		
		
		
		
		
		
		
		

	- Jenkins: Manage Jenkins -> Tools -> JDK installations -> Add JDK
	- Jenkins: Manage Jenkins -> Tools -> Maven installations
	- Jenkins: Manage Jenkins -> Tools -> SonarQube Scanner installations
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> S3 publisher
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Build Timestamp
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Maven Integration
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Pipeline Utility Steps
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> Nexus Artifact Uploader
	- Jenkins: Manage Jenkins -> Plugins -> Available plugins -> SonarQube Scanner
	- Jenkins: Manage Jenkins -> System -> Build Timestamp
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Add SonarQube
	- Jenkins: Manage Jenkins -> System -> SonarQube servers -> Environment variables
	- Jenkins: Manage Jenkins -> System -> Slack
	- Jenkins: Manage Jenkins -> Credentials -> Stores scoped to Jenkins -> System -> Global credentials (unrestricted) -> Add Credentials
	- Jenkins: Manage Jenkins -> Nodes -> New node
	- Jenkins: Manage Jenkins -> Security -> Git Host Key Verification Configuration -> Accept first connection
	- Jenkins: New Item -> Freestyle project -> Source Code Management -> Git
	- Jenkins: New Item -> Freestyle project -> Build Steps -> Invoke top-level Maven targets
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Archive the artifacts
	- Jenkins: New Item -> Freestyle project -> Post-build Actions -> Publish artifacts to S3 Bucket
	- Jenkins: New Item -> Freestyle project -> Copy from
	- Jenkins: New Item -> Pipeline -> Pipeline script
	- Jenkins: New Item -> Pipeline -> Pipeline script from SCM
	- Jenkins: New Item -> General -> This project is parameterized -> String parameter
	- Jenkins: Jenkins instance -> Build Now	
	- Jenkins: Jenkins instance -> Configure
	- Jenkins: Jenkins instance -> Workspace
	- Jenkins: Jenkins instance -> Stages
	- Jenkins: Jenkins result -> Workspaces	
	- AWS: EC2 -> Instances -> Instance state -> Reboot instance
	- AWS: Amazon Elastic Container Registry
	- AWS: Amazon Elastic Container Service -> Clusters -> Create cluster
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Create
	- AWS: Amazon Elastic Container Service -> Clusters -> instance -> Services -> Update service
	- AWS: Amazon Elastic Container Service -> Task definitions -> Create new task definition
	- Nexus: Browse
	- SonarQube -> Project -> Project instance
	- SonarQube -> Project -> Project instance -> Project setting -> Quality gate
	- SonarQube -> Project -> Project instance -> Project setting -> Webhooks
	- SonarQube -> Quality Gates -> create
	- SonarQube -> Quality Gates -> Quality Gates instance -> Unlock editing -> Add condition
	- Nexus -> Configuration -> Repositories -> Create Repository -> maven2(hosted)
	- Nexus -> Browse -> instance
	- Github -> Settings -> SSH and GPG keys -> New SSH key
	- Github -> Project instance -> Settings -> Webhooks -> Add webhook
		
	- Code trong Execute Shell:
	
		Case 1:
		
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_ID.war
			
		Case 2:
		
			mkdir -p versions
			cp target/vprofile-v2.war version/vpro$VERSION.war

		Case 3:
					
			mkdir -p versions
			cp target/vprofile-v2.war versions/vpro$BUILD_TIMESTAMP.war
	
	- Code Jenkinsfile:

		Case 1:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Archiving artifact"
								archiveArtifacts artifacts: '**/*.war'
							}
						}
					}
				}
			}

		Case 2:
		
			def COLOR_MAP = [
				'SUCCESS': 'good',
				'FAILURE': 'danger',
			]

			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}
				stages {
					stage('Fetch code') {
						steps{
							git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}
					}

					stage('Unit Test') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Build') {
						steps{
							sh 'mvn install -DskipTests'
						}
						post {
							success {
								echo "Now archiving it..."
								archiveArtifacts artifacts: '**/target/*.war'
							}
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage('Sonar Code Analysis') {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
							withSonarQubeEnv('sonarserver') {
								sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
								-Dsonar.projectName=vprofile \
								-Dsonar.projectVersion=1.0 \
								-Dsonar.sources=src/ \
								-Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
								-Dsonar.junit.reportsPath=target/surefire-reports/ \
								-Dsonar.jacoco.reportsPath=target/jacoco.exec \
								-Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
							}
						}
					}

					stage("Quality Gate") {
					  steps {
						timeout(time: 1, unit: 'HOURS') {
						  waitForQualityGate abortPipeline: true
						}
					  }
					}
					stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
								nexusVersion: 'nexus3',
								protocol: 'http',
								nexusUrl: '172.31.30.253:8081',
								groupId: 'QA',
								version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
								repository: 'vprofile-repo',
								credentialsId: 'nexuslogin',
								artifacts: [
									[artifactId: 'vproapp',
									classifier: '',
									file: 'target/vprofile-v2.war',
									type: 'war']
								]
							)
						}
					}
				}

				post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}
			}

		Case 3:
		
			def COLOR_MAP = [
				'SUCCESS': 'good', 
				'FAILURE': 'danger',
			]
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}

				stages {

					stage('test slack'){
						steps{
							sh 'NotARealCommand'

						}
					}
					stage('Fetch code') {
						steps {
						   git branch: 'atom', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar6.2'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					 stage("UploadArtifact"){
						steps{
							nexusArtifactUploader(
							  nexusVersion: 'nexus3',
							  protocol: 'http',
							  nexusUrl: '172.31.25.14:8081',
							  groupId: 'QA',
							  version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
							  repository: 'vprofile-repo',
							  credentialsId: 'nexuslogin',
							  artifacts: [
								[artifactId: 'vproapp',
								 classifier: '',
								 file: 'target/vprofile-v2.war',
								 type: 'war']
							  ]
							)
						}
					}


				}

			  post {
					always {
						echo 'Slack Notifications.'
						slackSend channel: '#devopscicd',
							color: COLOR_MAP[currentBuild.currentResult],
							message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
					}
				}

			}
			
		Case 4:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					// stage('Remove Container Images'){
					//     steps{
					//         sh 'docker rmi -f $(docker images -a -q)'
					//     }
					// }

			  }
			}
			
		Case 5:
		
			pipeline {
				agent any
				tools {
					maven "MAVEN3.9"
					jdk "JDK17"
				}


				environment {
					registryCredential = 'ecr:us-east-1:awscreds'
					appRegistry = "195524911861.dkr.ecr.us-east-1.amazonaws.com/vprofileappimg"
					vprofileRegistry = "https://195524911861.dkr.ecr.us-east-1.amazonaws.com"
					cluster = "vprofile"
					service = "vprofileappsvc"
				}
			  stages {
			   
					stage('Fetch code') {
						steps {
						   git branch: 'docker', url: 'https://github.com/hkhcoder/vprofile-project.git'
						}

					}


					stage('Build'){
						steps{
						   sh 'mvn install -DskipTests'
						}

						post {
						   success {
							  echo 'Now Archiving it...'
							  archiveArtifacts artifacts: '**/target/*.war'
						   }
						}
					}

					stage('UNIT TEST') {
						steps{
							sh 'mvn test'
						}
					}

					stage('Checkstyle Analysis') {
						steps{
							sh 'mvn checkstyle:checkstyle'
						}
					}

					stage("Sonar Code Analysis") {
						environment {
							scannerHome = tool 'sonar7.3'
						}
						steps {
						  withSonarQubeEnv('sonarserver') {
							sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
							   -Dsonar.projectName=vprofile \
							   -Dsonar.projectVersion=1.0 \
							   -Dsonar.sources=src/ \
							   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
							   -Dsonar.junit.reportsPath=target/surefire-reports/ \
							   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
							   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
						  }
						}
					}

					stage("Quality Gate") {
						steps {
						  timeout(time: 1, unit: 'HOURS') {
							waitForQualityGate abortPipeline: true
						  }
						}
					  }

					stage('Build App Image') {
					  steps {
				   
						script {
							dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
							}
					  }
				
					}

					stage('Upload App Image') {
					  steps{
						script {
						  docker.withRegistry( vprofileRegistry, registryCredential ) {
							dockerImage.push("$BUILD_NUMBER")
							dockerImage.push('latest')
						  }
						}
					  }
					}

					stage('Remove Container Images'){
						steps{
							sh 'docker rmi -f $(docker images -a -q)'
						}
					}


					stage('Deploy to ecs') {
					  steps {
						withAWS(credentials: 'awscreds', region: 'us-east-1') {
						sh 'aws ecs update-service --cluster ${cluster} --service ${service} --force-new-deployment'
						   }
					  }
					}

			  }
			}
			
	Dockerfile code:
	
		Case 1:
		
			FROM tomcat:10-jdk21
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webapps
			
			
--- Python:

	Google search:
	
		python jenkins library
		
		boto3 python

	- Commands:
	
		- cmd: python
		- cmd: ls -l first-python-code.py
		- cmd: ./first-python-code.py
		- cmd: python first-python-code.py
		- cmd: yum search python3
		- cmd: yum install python3 -y
		- cmd: wget https://bootstrap.pypa.io/get-pip.py
		- cmd: python3 get-pip.py
		- cmd: dnf install python3-pip -y
		- cmd: pip install 'fabric<2.0'
		- cmd: export PATH=$PATH:/usr/local/bin
		- cmd: ssh-copy-id devops@web01
		- cmd: ssh -i ~/.ssh/id_rsa devops@web01
		- cmd: ssh devops@web01
		- cmd: fab -H web01 -u devops remote_exec
		- cmd: fab -H web01 -u devops web_setup:https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip,2136_kool_form_pack
		


		
		
		
		
		
		
	
	
	- Python code:
	
		Case 1:
		
			print("""
			xin chao
			cac ban
			""")
			
		Case 2:
		
			print('''
			xin chao
			cac ban
			''')
			
		Case 3:

			a = b = c = 65

			print(a)
			print(b)
			print(c)		
			
		Case 4:

			a = 65

			print("a: ",a)		
			
		Case 5:

			w, x, y, z = "alpha", "beta", 12, 5.4

			print("Variable w value is ", w)
			print("Variable w value is ", x)
			print("Variable w value is ", y)
			print("Variable w value is ", z)

			print(type(w))		
			
		Case 6:

			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_list = [str1, "DevOps", 47, num1, 1.5]
			print(first_list)		
			
			str1 = "alpha"
			str2 = 'beta'
			str3 = '''gamma string'''
			str4 = """delta string"""
			num1 = 123
			first_tuple = (str1, "DevOps", 47, num1, 1.5)
			print(first_tuple)			
			
		Case 7:
		
			user_skill = input("Enter your desired skill: ")
			print(user_skill)
			
		Case 8:
		
			import time
			time.sleep(2)
			print("test")
			
		Case 9:

			def time_activity(*args, **kwargs):
				print(args)
				print(kwargs)
			time_activity(10, 20, 10, hobby="Dance", sport="Boxing", fun="Driving", work="DevOps")
				
		Case 10:
		
			from fabric.api import *

			def greetings(msg):
				print("Good {}".format(msg))

			def system_info():
				print("Disk space.")
				local("df -h")

				print("Memory info.")
				local("free -m")

				print("System Uptime.")
				local("uptime")

			def remote_exec():
				run("hostname")
				run("uptime")
				run("df -h")
				run("free -m")
				
				sudo("yum install unzip zip wget -y")
				
			def web_setup(WEBURL, DIRNAME):
				print("###############################################")
				print("Installing dependencies")
				print("###############################################")
				sudo("yum install httpd wget unzip -y")

				print("###############################################")
				print("Start & enable service.")
				sudo("systemctl start httpd")
				sudo("systemctl enable httpd")

				print("###############################################")
				local("apt install zip unzip -y")

				print("###############################################")
				print("Downloading and pushing website to webservers.")
				print("###############################################")
				local("wget -O website.zip %s" % WEBURL)
				local("unzip -o website.zip")

				print("###############################################")
				with lcd(DIRNAME):
					local("zip -r tooplate.zip * ")
					put("tooplate.zip", "/var/www/html/", use_sudo=True)

				with cd("/var/www/html/"):
					sudo("unzip -o tooplate.zip")

				sudo("systemctl restart httpd")
				print("Website setup is done.")



	Code của file /etc/ssh/sshd_config:
	
		Case 1:
		
			PasswordAuthentication yes
			
	Code của visudo:
	
		Case 1:
		
			devops  ALL=(ALL)       NOPASSWD: ALL

			
--- Learn Terraform:

	- Commands:
	
		- cmd: choco install terraform
		- cmd: terraform --version
		- cmd: terraform fmt
		- cmd: terraform init
		- cmd: terraform validate
		- cmd: terraform plan
		- cmd: terraform apply
		- cmd: terraform destroy
		
	Amazon S3 -> Buckets -> instance -> Create folder
		
		
		
		
	- Google search:
	
		find ubuntu ami id aws
		
		terraform aws
		
		variable terraform
		
		terraform provisioners
		
	- Cài đặt VS Code extension:
	
		HashiCorp Terraform
		
	terraform.tfstate:
	
	File terraform.tfstate là trái tim của Terraform — nó chứa toàn bộ trạng thái thực tế của hạ
	tầng mà Terraform đang quản lý.
	
	Định nghĩa cốt lõi

		terraform.tfstate là file trạng thái (state file) mà Terraform dùng để ghi nhận và theo dõi hạ tầng
		thực tế sau khi bạn chạy terraform apply.

			Nó mô tả mọi resource (máy ảo, VPC, subnet, load balancer, v.v.) mà Terraform đã tạo,

			Và liên kết chúng với code Terraform (.tf files).

		Terraform dùng file này để:

			Biết hiện trạng hạ tầng đang ra sao,

			So sánh với mong muốn (trong code),

			Từ đó quyết định tạo / sửa / xóa gì khi bạn chạy terraform plan hoặc terraform apply.
			
	Bản chất & cơ chế hoạt động
	
		Khi bạn chạy terraform apply

			Terraform đọc code .tf (infrastructure as code).

			Nó kiểm tra file terraform.tfstate để biết đang có gì ngoài đời thật.

			Nó gọi API lên cloud (AWS, GCP, Azure, v.v.) để xác minh.

			Sau khi thực hiện xong thay đổi, Terraform ghi lại kết quả mới vào terraform.tfstate.
			
	Thành phần chính trong terraform.tfstate

		Một file tfstate là JSON, gồm các phần chính:
		
			version	Version format của Terraform state
			
			terraform_version	Phiên bản Terraform đã tạo file
			
			resources	Danh sách toàn bộ resource đang quản lý
			
			outputs	Các giá trị output (nếu bạn có output trong code)
			
			serial	Số version nội bộ để Terraform biết state đã được cập nhật bao nhiêu lần
			
	Local state vs Remote state:
	
		Local state:

			Mặc định terraform.tfstate nằm ngay trong thư mục dự án.

			Dễ dùng nhưng nguy hiểm trong team (vì có thể bị ghi đè, mất file, xung đột...).

		Remote state (an toàn hơn):

			Terraform cho phép lưu state từ xa (remote backend), ví dụ:

				AWS S3

				Azure Blob Storage

				Google Cloud Storage

				Terraform Cloud / Enterprise
				
	Lưu ý bảo mật cực kỳ quan trọng

		File terraform.tfstate có thể chứa thông tin nhạy cảm, như:

			Mật khẩu, secret key, private IP, token, v.v.

		Không bao giờ commit terraform.tfstate lên Git.
		
			Hãy thêm vào .gitignore
		
	- Code file .tf:
	
		Case 1:
		
			data "aws_ami" "amiID" {
			  most_recent = true

			  filter {
				name   = "name"
				values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
			  }

			  filter {
				name   = "virtualization-type"
				values = ["hvm"]
			  }

			  owners = ["099720109477"]
			}
		
		Case 2:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = "us-east-1"
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "42.112.134.249/32"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
		Case 3:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = data.aws_ami.amiID.id
				  instance_type          = "t3.micro"
				  key_name               = "dove-key"
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = "us-east-1a"

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}

				resource "aws_ec2_instance_state" "web_state" {
					instance_id = aws_instance.web.id
					state       = "running"
				}
				
		Case 4:
		
			Instance.tf:
			
				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}
				
			Keypair.tf:
			
				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					es-east-1 = "ami-0e2c8caa4b6378d8c"
				  }
				}
				
		Case 5:
		
			Instance.tf:

				resource "aws_instance" "web" {
				  ami                    = var.amiID[var.region]
				  instance_type          = "t3.micro"
				  key_name               = aws_key_pair.dove-key.key_name
				  vpc_security_group_ids = [aws_security_group.dove-sg.id]
				  availability_zone      = var.zone1

				  tags = {
					Name    = "Dove-web"
					Project = "Dove"
				  }

				  provisioner "file" {
					source      = "web.sh"
					destination = "/tmp/web.sh"
				  }

				  connection {
					type        = "ssh"
					user        = var.webuser
					private_key = file("dovekey")
					host        = self.public_ip
				  }

				  provisioner "remote-exec" {

					inline = [
					  "chmod +x /tmp/web.sh",
					  "sudo /tmp/web.sh"
					]
				  }

				  provisioner "local-exec" {
					command = "echo ${self.private_ip} >> private_ips.txt"
				  }
				}

				resource "aws_ec2_instance_state" "web-state" {
				  instance_id = aws_instance.web.id
				  state       = "running"
				}

				output "WebPublicIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.public_ip
				}

				output "WebPrivateIP" {
				  description = "AMI ID of Ubuntu instance"
				  value       = aws_instance.web.private_ip
				}
				
			InstlD.tf:
			
				data "aws_ami" "amiID" {
				  most_recent = true

				  filter {
					name   = "name"
					values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
				  }

				  filter {
					name   = "virtualization-type"
					values = ["hvm"]
				  }

				  owners = ["099720109477"]
				}

				output "instance_id" {
				  description = "AMI ID of Ubuntu instance"
				  value       = data.aws_ami.amiID.id
				}

			Keypair.tf:

				resource "aws_key_pair" "dove-key" {
				  key_name   = "dove-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJ/Zj2tR4kZImO7zhNmUtfIJxNZvGgx8bwPdcCfyLbn0 PC@DESKTOP-GNVB183"
				}
				resource "aws_key_pair" "test-key" {
				  key_name   = "test-key"
				  public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIRshwBBa6viR7dT1P4aSc1fYbDhuqBJtbbWLBVhWzj2 PC@DESKTOP-GNVB183"
				}
				
			provider.tf:
			
				provider "aws" {
				  region = var.region
				}
				
			SecGrp.tf:
			
				resource "aws_security_group" "dove-sg" {
				  name        = "dove-sg"
				  description = "dove-sg"
				  tags = {
					Name = "dove-sg"
				  }
				}

				resource "aws_vpc_security_group_ingress_rule" "sshfromyIP" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 22
				  ip_protocol       = "tcp"
				  to_port           = 22
				}

				resource "aws_vpc_security_group_ingress_rule" "allow_http" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  from_port         = 80
				  ip_protocol       = "tcp"
				  to_port           = 80
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv4" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv4         = "0.0.0.0/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}

				resource "aws_vpc_security_group_egress_rule" "allowAllOutbound_ipv6" {
				  security_group_id = aws_security_group.dove-sg.id
				  cidr_ipv6         = "::/0"
				  ip_protocol       = "-1" # semantically equivalent to all ports
				}
				
			vars.tf:
			
				variable "region" {
				  default = "us-east-1"
				}

				variable "zone1" {
				  default = "us-east-1a"
				}

				variable "webuser" {
				  default = "ubuntu"
				}

				variable "amiID" {
				  type = map(any)
				  default = {
					us-east-2 = "ami-036841078a4b68e14"
					us-east-1 = "ami-0360c520857e3138f"
				  }
				}
				
			web.sh:
			
				#!/bin/bash
				apt update
				apt install wget unzip apache2 -y
				systemctl start apache2
				systemctl enable apache2
				wget https://www.tooplate.com/zip-templates/2117_infinite_loop.zip
				unzip -o 2117_infinite_loop.zip
				cp -r 2117_infinite_loop/* /var/www/html/
				systemctl restart apache2
				
		Case 6:
		
			Giống case 5 nhưng thêm file backend.tf
			
			backend.tf:
			
				terraform {
					backend "s3" {
						bucket = "terraformstate3245612112"
						key    = "terraform/backend"
						region = "us-east-1"
					}
				}
				
				
--- Ansible:

	- Commands:
	
		- cmd: ls ~/.ssh/known_hosts
		- cmd: cat ~/.ssh/known_hosts
		- cmd: cat /dev/null > ~/.ssh/known_hosts
		- cmd: sudo apt install software-properties-common
		- cmd: sudo add-apt-repository --yes --update ppa:ansible/ansible
		- cmd: sudo apt install ansible
		- cmd: ansible --version
		- cmd: ansible-config init --disable -t all > ansible.cfg
		- cmd: chmod 400 clientkey.pem
		- cmd: ansible web01 -m ping -i inventory
		- cmd: ansible all -m ping -i inventory
		- cmd: ansible '*' -m ping -i inventory
		- cmd: ansible 'web*' -m ping -i inventory
		- cmd: ansible web01 -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.yum -a "name=httpd state=absent" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.service -a "name=httpd state=started enabled=yes" -i inventory --become
		- cmd: ansible webservers -m ansible.builtin.copy -a "src=index.html dest=/var/www/html/index.html" -i inventory --become
		- cmd: ansible webservers -m yum -a "name=httpd state-absent" -i inventory --become
		- cmd: ansible-playbook -i inventory web-db.yaml
		- cmd: ansible-playbook -i inventory web-db.yaml -v
		- cmd: ansible-playbook -i inventory web-db.yaml -vv
		- cmd: ansible-playbook -i inventory web-db.yaml -vvvv
		- cmd: ansible-playbook -i inventory web-db.yaml --syntax-check
		- cmd: ansible-playbook -i inventory web-db.yaml -C
		- cmd: ssh -i clientkey.pem ec2-user@172.31.20.155
		- cmd: yum search python | grep -i mysql
		- cmd: ansible-galaxy collection install community.mysql
		- cmd: vim /etc/ansible/ansible.cfg
		- cmd: ansible-playbook db.yaml
		- cmd: sudo touch /var/log/ansible.log
		- cmd: sudo chown ubuntu.ubuntu /var/log/ansible.log
		- cmd: cat /var/log/ansible.log
		- cmd: ansible-playbook db.yaml -vv
		- cmd: ansible-playbook -e USRNM=cliuser -e COMM=cliuser vars_precedence.yaml
		- cmd: ansible -m setup web01
		- cmd: ansible-playbook print_facts.yaml
		- cmd: rm -rf print_facts.yaml vars_precedence.yaml
		- cmd: cat /etc/chrony.conf
		- cmd: cat /etc/ntpsec/ntp.conf
		- cmd: apt  install tree  # version 2.1.1-2
		- cmd: tree
		- cmd: ansible-galaxy init post-install
		- cmd: vim roles/post-install/vars/main.yml
		- cmd: vim roles/post-install/handlers/main.yml
		- cmd: vim roles/post-install/tasks/main.yml
		- cmd: ansible-galaxy role install geerlingguy.java
		- cmd: cd .ansible/
		- cmd: export AWS_ACCESS_KEY_ID='AK123'
		- cmd: export AWS_SECRET_ACCESS_KEY='abc123'
		- cmd: vim .bashrc
		- cmd: source .bashrc
		- cmd: sudo apt install python3-pip -y
		- cmd: ansible-galaxy collection install amazon.aws
		- cmd: ansible-galaxy collection install amazon.aws --force
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	- Google search:
	
		ansible installing
		
		ansible inventory
		
		ansible to ad hoc command
		
		ansible playbooks
		
		using ansible modules and plugins
		
		mysql ansible modules
		
		ansible configuration file
		
		ansible variables
		
		ansible when condition
		
		ansible loops
		
		ansible files module
		
		ntp server in oregon
		
		ansible handler
		
		ansible roles
		
		ntp servers in india
		
		ansible galaxy
		
		ansible amazon aws
		
		ansible modules
		
	- Code trong file vprofile/excercise1/inventory:
	
		Case 1:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				web02:
				  ansible_host: 172.31.16.152
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem
				db01:
				  ansible_host: 172.31.20.155
				  ansible_user: ec2-user
				  ansible_ssh_private_key_file: clientkey.pem

			  children:
			    webservers:
				  hosts:
				    web01:
				    web02:
			    dbservers:
				  hosts:
				    db01:
			    dc_oregon:
				  children:
				    webservers:
				    dbservers:
					
		Case 2:
		
			all:
			  hosts:
				web01:
				  ansible_host: 172.31.31.108
				web02:
				  ansible_host: 172.31.16.152
				db01:
				  ansible_host: 172.31.20.155

			  children:
				webservers:
				  hosts:
					web01:
					web02:
				dbservers:
				  hosts:
					db01:
				dc_oregon:
				  children:
					webservers:
					dbservers:
				  vars:
					ansible_user: ec2-user
					ansible_ssh_private_key_file: clientkey.pem




	- Nội dung trong file clientkey.pem:

		Case 1:
		
			Chứa private key của web01
		
			-----BEGIN RSA PRIVATE KEY-----
			MIIEogIBAAKCAQEAreGWVicfl8jhOLqPUerGay/AThriECzXc8UcLLam4riCz05+
			TzZP+NeoX6dC83ZX4ssyLazcGj5OMCg2EhFqVZTuDABYsr+ucT/sdXPCP0bjf+mq
			SYT6iHpftoRfm1vk3Um61/HmSzp1vZ6YhE//x1GwWT5+4aOufGS4bCa5xPluEg+E
			IhaEZNRyNfgvc/ylblWVCM1KGkZgdhvenFUQDj8+icCWrvbFvykwW0L8D2WB+zFq
			iSDx1cKLL1zzVCQVgKfKofC3PPm+VMbwOL8HPw65g/OBnzWKrrSjrpWu5s4KKSM+
			3BXW5nRieoPI34ruAwwpYrJKUlR7t6/rBo6QgQIDAQABAoIBACLWvzN11UupMQ8X
			uh2Up7rUL3i2xDKveV+1z6ZZ1mg4xeTZek9Ot4lJVHAN6Ek1nfhP9DbYmqUbdLkL
			ZYILQT3ygBuheiQeacpBH5SM5A+fmXeIjtj6LuRneIPuU+Wh7OI1op0f15+dD/g1
			LaPdD4eVI3tOHUgCbrR3zcfFnpULfGl0+QdkZ7141FmMiAL+8tTjzfWbMYwbPWu2
			ZSIDWx7SZ/wycGDKBejmgJQHFyEn9Cwvexlt+Zh4IK/kTIXXe9XM0ZbVMF8am5D6
			BhSdD7JOB7OVWSghIZFNegtTdPS/+LI7wJj2y3O5KP2QJUGfSvjKMpdpBffoLwV0
			15soOu0CgYEA04abNpRYQbkd21R3F2sN5td2UTE5M31iaxnZXQrGyGCQUdIawb5n
			XNIvIVoH6dV3qFOrx9lmYQOV3B5kQNWPyfOI6q/tWXpsVV9WeUjXhki3TinRDGYq
			FaNlnRh5IvCtwbGIZpnfay4j8UCw8KgnpAqiz45TBQ/YqUR0MCZiPecCgYEA0nDB
			yhsFau6cpNUgcSjeGTCQpCTCckvFusa671x6gjOXC8gxBVzpCcSXxUzXlLANhkmU
			CZFaUG/hwh0FKqAItg0yEhNz7mnYv5Y/w0o5zycqF2QYlB1L6ph/LHh8z82OgQBN
			D0MQEHgkNjejLkaLf080KKYxqZE0d/oWucOxYVcCgYB4gT77oReGmceApGYUWVDa
			KfWl270SsGPZUCic8P6+OQT/GAtWRPrtznA7N+c6N/qrUr+SYzAIJNrDRC0pIoGA
			M9XUndVCHJSLLn09K1pdjh+f0ALgZXOkUCobjU21shfLOTDUAuVdUjP3xTsIX0P2
			GHkYdaSmRZjRFcZ7h+KAEQKBgFjvgGbarpp3h0n+LHzGab65kJdeVbMaJNF/xWb9
			bWTzSqWXEGiU0IPpSr7+b6mOEdkr5V15yXJvJjj0LMfL5IKT5xJOmFMs9oZZiE8P
			YokSoy5Jhj2qd/gIRM7ViOIFnHEWYHrPu81KCPvE3bjj5XaDUabQPfLMxCDkV5Bg
			jOl3AoGAP8u9/AnJsH+7k83fnBQvPHPkoEQ1PtMyclUnWjGbM5BUDdUi0beN1Kme
			PqUDarWVNfwXh4WsTmrmlUe41dInmnmHwf+Cfsn8f02eMV5YnxlUEDfcaOjnJ9+h
			VNXeYgsaoGFp6LiIj70I1X1WkegOT85sfuHuV5tYAZVlF8yzbxI=
			-----END RSA PRIVATE KEY-----
		
	- Nội dung của file ansible.cfg:

		Case 1:
		
			host_key_checking=False
			
	- Nội dung của file web-db.yaml:
	
		Case 1:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present
					
		Case 2:
		
			- name: Webserver setup
			  hosts: webservers
			  become: yes
			  tasks:
				- name: Install httpd
				  ansible.builtin.yum:
					name: httpd
					state: present

				- name: Start service
				  ansible.builtin.service:
					name: httpd
					state: started
					enabled: yes
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
					
	- Nội dung file db.yaml:
	
		Case 1:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present

		Case 2:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes
				- name: Create a new database with name 'accounts'
				  mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

		Case 3:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: accounts
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: vprofile
					password: 'admin943'
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 4:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 5:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
				    msg: " The dbname is {{dbname}}"

				- debug:
				    var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
					
		Case 6:
		
			- name: DBserver setup
			  hosts: dbservers
			  become: yes
			  vars:
				dbname: electric
				dbuser: current
				dbpass: tesla
			  tasks:
				- debug:
					msg: " The dbname is {{dbname}}"

				- debug:
					var: dbuser

				- name: Install mariadb-server
				  ansible.builtin.yum:
					name: mariadb-server
					state: present

				- name: Install pymysql
				  ansible.builtin.yum:
					name: python3-PyMySQL
					state: present

				- name: Start mariadb service
				  ansible.builtin.service:
					name: mariadb
					state: started
					enabled: yes

				- name: Create a new database with name 'accounts'
				  community.mysql.mysql_db:
					name: "{{dbname}}"
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock

				- name: Create database user with name 'vprofile'
				  community.mysql.mysql_user:
					name: "{{dbuser}}"
					password: "{{dbpass}}"
					priv: '*.*:ALL'
					state: present
					login_unix_socket: /var/lib/mysql/mysql.sock
				  register: dbout

				- name: print dbout variable
				  debug:
					var: dbout


					
	- Nội dung file ansible.cfg:
	
		Case 1:
		
			[defaults]
			host_key_checking=False
			inventory = ./inventory
			forks = 5
			log_path = /var/log/ansible.log

			[privilege_escalation]
			become=True
			become_method=sudo
			become_ask_pass=False

	- Nội dung gile vars_precedence.yaml:
	
		Case 1:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"

		Case 2:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
				- name: create user
				  ansible.builtin.user:
					name: "{{USRNM}}"
					comment: "{{COMM}}"
				  register: usrout

				- debug:
					var: usrout

		Case 3:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  vars:
					USRNM: playuser
					COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment
						
		Case 4:
		
			- name: Understanding vars
			  hosts: all
			  become: yes
			  gather_facts: False
			  vars:
				USRNM: playuser
				COMM: variable from playbook
			  tasks:
					- name: create user
					  ansible.builtin.user:
							name: "{{USRNM}}"
							comment: "{{COMM}}"
					  register: usrout

					- debug:
							var: usrout.name

					- debug:
						var: usrout.comment

						
	- Nội dung file group_vars/all:
	
		Case 1:
		
			group_vars/all:
			
				dbname: sky
				dbuser: pilot
				dbpass: aircraft
				
			db.yaml:
			
				- name: DBserver setup
				  hosts: dbservers
				  become: yes
					#vars:
					#dbname: electric
					#dbuser: current
					#dbpass: tesla
				  tasks:
					- debug:
						msg: " The dbname is {{dbname}}"

					- debug:
						var: dbuser

					- name: Install mariadb-server
					  ansible.builtin.yum:
						name: mariadb-server
						state: present

					- name: Install pymysql
					  ansible.builtin.yum:
						name: python3-PyMySQL
						state: present

					- name: Start mariadb service
					  ansible.builtin.service:
						name: mariadb
						state: started
						enabled: yes

					- name: Create a new database with name 'accounts'
					  community.mysql.mysql_db:
						name: "{{dbname}}"
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock

					- name: Create database user with name 'vprofile'
					  community.mysql.mysql_user:
						name: "{{dbuser}}"
						password: "{{dbpass}}"
						priv: '*.*:ALL'
						state: present
						login_unix_socket: /var/lib/mysql/mysql.sock
					  register: dbout

					- name: print dbout variable
					  debug:
						var: dbout

		Case 2:
		
			group_vars/all:
			
				USRNM: commonuser
				COMM: variable from groupvars_all file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
		Case 3:
		

			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org


	
	- Nội dung file group_vars/webservers:
	
		Case 1:
		
			group_vars/webservers:
			
				USRNM: webgroup
				COMM: variable from group_vars/webservers file

			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
							
	- Nội dung file host_vars/web02:
	
		Case 1:
		
			host_vars/web02:
		
				USRNM: web02user
				COMM: variables from host_vars/web02 file
				
			vars_precedence.yaml:
			
				- name: Understanding vars
				  hosts: all
				  become: yes
					#vars:
					#USRNM: playuser
					#COMM: variable from playbook
				  tasks:
						- name: create user
						  ansible.builtin.user:
								name: "{{USRNM}}"
								comment: "{{COMM}}"
						  register: usrout

						- debug:
								var: usrout.name

						- debug:
							var: usrout.comment
		
	- Nội dung file print_facts.yaml:
	
		Case 1:
		
			- name: Print facts
			  hosts: all
				#gather_facts: False
			  tasks:
				- name: Print OS name
				  debug:
					var: ansible_distribution
				- name: Print selinux mode
				  debug:
					var: ansible_selinux.mode
				- name: Print RAM memory
				  debug:
					var: ansible_memory_mb.real.free

				- name: Print Processor name
				  debug:
					var: ansible_processor[2]

	- Nội dung file provisioning.yaml:
	
		Case 1:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
					
		Case 2:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd
					
		Case 3:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd


				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"

				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory
					
		Case 4:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: chrony
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: ntp
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: /opt/test21
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

		Case 5:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  vars:
				mydir: /opt/dir22
			  tasks:
				- name: Install ntp agent on centos
				  yum:
					name: "{{item}}"
					state: present
				  when: ansible_distribution == "CentOS"
				  loop:
					- chrony
					- wget
					- git
					- zip
					- unzip

				- name: Install ntp agent on Ubuntu
				  apt:
					name: "{{item}}"
					state: present
					update_cache: yes
				  when: ansible_distribution == "Ubuntu"
				  loop:
					- ntp
					- wget
					- git
					- zip
					- unzip


				- name: Start service on centos
				  service:
					name: chronyd
					state: started
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: Start service on ubuntu
				  service:
					name: ntp
					state: started
					enabled: yes
				  when: ansible_distribution == "Ubuntu"

				- name: Banner file
				  copy:
					content: '# This server is managed by ansible. No manual changes please.'
					dest: /etc/motd

				- name: Create a folder
				  file:
					path: "{{mydir}}"
					state: directory

				- name: Deploy ntp agent conf on centos
				  template:
					src: templates/ntpconf_centos
					dest: /etc/chrony.conf
					backup: yes
				  when: ansible_distribution == "CentOS"
				  notify:
					- reStart service on centos

				- name: Deploy ntp agent conf on ubuntu
				  template:
					src: templates/ntpconf_ubuntu
					dest: /etc/ntpsec/ntp.conf
					backup: yes
				  when: ansible_distribution == "Ubuntu"
				  notify:
					- reStart service on ubuntu

				- name: Dump file
				  copy:
					files: files/myfile.txt
					dest: /tmp/myfile.txt

			  handlers:
				- name: reStart service on centos
				  service:
					name: chronyd
					state: restarted
					enabled: yes
				  when: ansible_distribution == "CentOS"

				- name: reStart service on ubuntu
				  service:
					name: ntp
					state: restarted
					enabled: yes
				  when: ansible_distribution == "Ubuntu"
				  
		Case 6:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- post-install
				
		Case 7:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org

		Case 8:
		
			- name: Provisioning servers
			  hosts: all
			  become: yes
			  roles:
				- geerlingguy.java
				- role: post-install
				  vars:
					ntp0: 0.in.pool.ntp.org
					ntp1: 1.in.pool.ntp.org
					ntp2: 2.in.pool.ntp.org
					ntp3: 3.in.pool.ntp.org




	- Code file templates/ntpconf_centos:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst
			
	- Code file templates/ntpconf_ubuntu:
	
		Case 1:
		
			pool "{{ntp0}}" iburst
			pool "{{ntp1}}" iburst
			pool "{{ntp2}}" iburst
			pool "{{ntp3}}" iburst

	- Code file roles/post-install/tasks/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# tasks file for post-install
			- name: Install ntp agent on centos
			  yum:
				name: "{{item}}"
				state: present
			  when: ansible_distribution == "CentOS"
			  loop:
				- chrony
				- wget
				- git
				- zip
				- unzip

			- name: Install ntp agent on Ubuntu
			  apt:
				name: "{{item}}"
				state: present
				update_cache: yes
			  when: ansible_distribution == "Ubuntu"
			  loop:
				- ntp
				- wget
				- git
				- zip
				- unzip


			- name: Start service on centos
			  service:
				name: chronyd
				state: started
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: Start service on ubuntu
			  service:
				name: ntp
				state: started
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

			- name: Banner file
			  copy:
				content: '# This server is managed by ansible. No manual changes please.'
				dest: /etc/motd

			- name: Create a folder
			  file:
				path: "{{mydir}}"
				state: directory

			- name: Deploy ntp agent conf on centos
			  template:
				src: ntpconf_centos.j2
				dest: /etc/chrony.conf
				backup: yes
			  when: ansible_distribution == "CentOS"
			  notify:
				- reStart service on centos

			- name: Deploy ntp agent conf on ubuntu
			  template:
				src: ntpconf_ubuntu.j2
				dest: /etc/ntpsec/ntp.conf
				backup: yes
			  when: ansible_distribution == "Ubuntu"
			  notify:
				- reStart service on ubuntu

			- name: Dump file
			  copy:
				src: myfile.txt
				dest: /tmp/myfile.txt

	- Code file roles/post-install/handlers/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# handlers file for post-install
			- name: reStart service on centos
			  service:
				name: chronyd
				state: restarted
				enabled: yes
			  when: ansible_distribution == "CentOS"

			- name: reStart service on ubuntu
			  service:
				name: ntp
				state: restarted
				enabled: yes
			  when: ansible_distribution == "Ubuntu"

	- Code file roles/post-install/vars/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# vars file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file roles/post-install/defaults/main.yml:
	
		Case 1:
		
			#SPDX-License-Identifier: MIT-0
			---
			# defaults file for post-install
			USRNM: commonuser
			COMM: variable from groupvars_all file
			ntp0: 0.north-america.pool.ntp.org
			ntp1: 1.north-america.pool.ntp.org
			ntp2: 2.north-america.pool.ntp.org
			ntp3: 3.north-america.pool.ntp.org
			mydir: /opt/dir22

	- Code file .bashrc:
	
		Case 1:
		
			if ! shopt -oq posix; then
			  if [ -f /usr/share/bash-completion/bash_completion ]; then
				. /usr/share/bash-completion/bash_completion
			  elif [ -f /etc/bash_completion ]; then
				. /etc/bash_completion
			  fi
			fi

			export AWS_ACCESS_KEY_ID='AK123'
			export AWS_SECRET_ACCESS_KEY='abc123'
			
	- Code file test-aws.yml:
	
		Case 1:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					
		Case 2:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout
					
		Case 3:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

				- name: print key
				  debug:
					var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 4:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-east-1
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

		Case 5:
		
			- hosts: localhost
			  gather_facts: False
			  tasks:
				- name: Create key pair
				  ec2_key:
					name: sample
					region: us-west-2
				  register: keyout

					#- name: print key
					#debug:
					#var: keyout

				- name: save key
				  copy:
					content: "{{keyout.key.private_key}}"
					dest: ./sample.pem
				  when: keyout.changed

				- name: start an instance
				  amazon.aws.ec2_instance:
					name: "public-compute-instance"
					key_name: "sample"
					  #vpc_subnet_id: subnet-5ca1ab1e
					instance_type: t3.micro
					security_group: default
					  #network_interfaces:
					  #- assign_public_ip: true
					image_id: ami-0caa91d6b7bee0ed0
					exact_count: 1
					region: us-west-2
					tags:
					  Environment: Testing


--- AWS Part -2:

	- VPC -> Your VPCs -> Create VPC
	- VPC -> Subnets -> Create subnet
	- VPC -> Subnets -> instance -> Route table
	- VPC -> Internet gateways -> Create internet gateway
	- VPC -> Internet gateways -> instance -> Actions -> Attach to VPC
	- VPC -> Route tables -> instance -> Routes
	- VPC -> Route tables -> instance -> Subnet associations -> Edit subnet associations
	- VPC -> Route tables -> instance -> Create route table
	- VPC -> Elastic IPs -> Allocate Elastic IP address
	- VPC -> NAT gateways -> Create NAT gateway
	- VPC -> Peering connections -> Create peering connection
	- VPC -> Peering connections -> Actions -> Accept request
	- IAM -> Roles -> Create role
	- EC2 -> Instances -> instance -> Actions -> Security -> Modify IAM role
	- CloudWatch -> Logs -> Log groups -> Log streams
	- CloudWatch -> Logs -> Metric filters -> Create metric filter
	
	
	
	- Google search:
	
		Terraform AWS VPC
		
		Terraform registry
		
		access logs loadbalancer
	
	- Commands:
	
		- cmd: scp -i vpro-bastion-key.pem web-key.pem ubuntu@54.177.221.62:/home/ubuntu/
		- cmd: chmod 400 web-key.pem
		- cmd: yum install httpd wget unzip -y
		- cmd: wget https://www.tooplate.com/zip-templates/2136_kool_form_pack.zip
		- cmd: tail -f access_log
		- cmd: tar czvf wave-web01-httpdlogs19122020.tar.gz *
		- cmd: cat /dev/null > access_log error_log
		- cmd: ls -ltr /tmp/logs-wave/
		- cmd: yum install awscli -y
		- cmd: aws s3 ls
		- cmd: aws s3 sync /tmp/logs-wave/ s3://wave-web-logs-111222/
		- cmd: yum install awslogs -y
		- cmd: sudo yum install amazon-cloudwatch-agent -y
		- cmd: rm -rf .aws/credentials
		- cmd: systemctl restart awslogsd
		- cmd: systemctl enable awslogsd
		- cmd: sudo systemctl status awslogsd
		- cmd: sudo cat /etc/awslogs/awslogs.conf
		- cmd: service awslogsd restart
		- cmd: cat /var/log/awslogs.log
		- cmd: vim /etc/awslogs/awslogs.conf
		
		
		
		
		
		
		
		
	
	AWS Part -2 - Mô hình mô phỏng một “Corporate Datacenter” (trung tâm dữ liệu doanh nghiệp
	truyền thống), dùng để so sánh với Amazon VPC (Virtual Private Cloud) trong AWS:

		Tổng quan

			Hình này mô tả một trung tâm dữ liệu vật lý (on-premises), nơi doanh nghiệp quản lý hạ tầng mạng, máy chủ, router, switch,...

				Toàn bộ hệ thống được chia nhỏ thành nhiều subnet — tương tự như cách AWS chia VPC thành subnet.

				Các subnet này kết nối với nhau qua router trung tâm.

				Network ACL (Access Control List) được đặt giữa router và subnet để kiểm soát lưu lượng.

		Các thành phần chính trong sơ đồ
		
			Router (bộ định tuyến)

				Nằm ở trên cùng.

				Có nhiệm vụ kết nối các subnet khác nhau trong cùng mạng nội bộ.

				Cũng có thể kết nối trung tâm dữ liệu này với Internet hoặc các mạng khác (WAN, VPN,...).

			Network ACL

				Là bộ lọc bảo mật ở mức subnet.

				Quyết định gói tin nào được phép đi vào hoặc đi ra subnet (theo IP, port, protocol,...).

				Tương tự Network ACL trong AWS VPC, nằm giữa router và subnet.

			Subnet (mạng con)

				Mỗi màu trong hình là một subnet:

					🟧 172.20.1.0/24

					🟦 192.168.0.0/24

					🟩 10.0.0.0/16

				Mỗi subnet đại diện cho một phân đoạn mạng riêng biệt trong doanh nghiệp.
				
					Ví dụ:

						Một subnet cho bộ phận kế toán.

						Một subnet cho bộ phận kỹ thuật.

						Một subnet cho server nội bộ.

		Bên trong mỗi subnet

			Mỗi subnet có:

				Switch – để kết nối các máy tính nội bộ trong cùng subnet.

				Network of computers – nhóm máy trạm hoặc máy chủ.

				Mainframe – các máy chủ lớn, chịu trách nhiệm xử lý dữ liệu trung tâm.

			Tất cả máy trong cùng subnet có thể giao tiếp trực tiếp với nhau qua switch mà không cần đi qua router.
			
	AWS Part -2 - Kiến trúc mạng cơ bản của một hệ thống chạy trên AWS VPC (Virtual Private Cloud):
	
		Tổng quan: Amazon VPC (Virtual Private Cloud)

			VPC là một mạng riêng ảo bên trong AWS, nơi bạn có thể triển khai các tài nguyên như EC2, RDS, Load Balancer...
			VPC cho phép bạn kiểm soát hoàn toàn mạng lưới, gồm:

				Dải IP (CIDR)

				Subnet

				Routing table

				Security Group và Network ACL
				
		Các thành phần trong hình
		
			Internet Gateway (IGW)

					Là cổng kết nối giữa VPC và Internet.

					Cho phép các instance trong public subnet có thể giao tiếp ra/vào Internet.

					IGW gắn trực tiếp vào VPC.

				Vai trò:
				
					Các web server muốn được truy cập công khai phải đi qua IGW.
					
			Public Subnet

				Đây là subnet có route ra Internet Gateway.

				Chứa các tài nguyên cần công khai ra ngoài, ví dụ:
				
					Web Server, NAT Gateway, Load Balancer, v.v.

				Trong hình:

					Có hai WEB SERVER trong public subnet.

					Có VPC NAT Gateway nằm cùng subnet.
					
			Private Subnet

				Subnet không có route trực tiếp ra Internet.

				Chỉ có thể truy cập Internet gián tiếp thông qua NAT Gateway hoặc từ public subnet.

				Dùng cho các tài nguyên nội bộ cần bảo mật cao:

					App Server

					Database Server

				Trong hình:

					Có 1 APP SERVER và 2 DB SERVER trong private subnet.
					
			VPC NAT Gateway

					“NAT” = Network Address Translation.

					Nằm trong public subnet và có Elastic IP gắn vào.

					Cho phép các instance trong private subnet truy cập Internet để cập nhật package, tải dữ liệu, v.v.,
					nhưng Internet không thể truy cập ngược lại vào private subnet.

				Ví dụ:

					APP SERVER trong private subnet muốn tải cập nhật từ Internet → đi qua NAT Gateway → Internet.

					Nhưng Internet không thể vào trực tiếp APP SERVER.
					
			Availability Zone (AZ)

				AWS chia mỗi region (ví dụ: us-east-1) thành nhiều AZ — các trung tâm dữ liệu vật lý riêng biệt.

				Trong hình:

					Availability Zone A: chứa public subnet (WEB SERVER, NAT Gateway)

					Availability Zone B: chứa private subnet (APP SERVER, DB SERVER)

				🧭 Mục tiêu: tăng độ tin cậy và tính sẵn sàng (high availability).
				
		Luồng hoạt động thực tế

			Người dùng truy cập web qua Internet → đi vào Internet Gateway → đến WEB SERVER trong public subnet.

			WEB SERVER có thể:

				Gửi request đến APP SERVER trong private subnet.

				APP SERVER xử lý, truy vấn DB SERVER (cũng trong private subnet).

			Nếu APP SERVER cần ra Internet (ví dụ tải dependency) → nó đi qua NAT Gateway trong public subnet → Internet.
			
		Bảo mật

			Security Group (SG): kiểm soát traffic theo instance (firewall cấp EC2).
			
				Ví dụ:

					Web server SG cho phép inbound HTTP (80) và HTTPS (443).

					App/DB server SG chỉ cho phép inbound từ web server.

			Network ACL (NACL): kiểm soát traffic theo subnet (firewall cấp mạng).
			
	AWS Part -2 - Kiến trúc mạng VPC trong AWS:
	
		Mục tiêu của sơ đồ

			Hình này cho ta thấy cách AWS chia mạng trong VPC thành hai phần:

				Public Subnet: cho phép EC2 giao tiếp trực tiếp với Internet

				Private Subnet: hoàn toàn cách ly, chỉ giao tiếp nội bộ hoặc qua VPN/NAT
				
		Các thành phần chính trong hình
		
			Amazon VPC

				Là “mạng ảo riêng” của bạn trong AWS (Virtual Private Cloud).

				Bạn có thể kiểm soát toàn bộ mạng, IP, routing, subnet, bảo mật…

				Mỗi VPC nằm trong một region (ví dụ: us-east-1).
				
			Public Subnet

				Subnet này có route ra Internet Gateway → cho phép các instance bên trong giao tiếp trực tiếp với Internet.

				Trong hình:

					Có 1 EC2 instance

					Gắn Security Group (bảo mật cấp instance)

					Có Network ACL (bảo mật cấp subnet)

					Có Route Table (định tuyến 172.16.0.0, 172.16.1.0, 172.16.2.0)

				Tác dụng:
				
					Dành cho các tài nguyên cần công khai, ví dụ:

						Web server

						Load balancer

						NAT Gateway
						
			Private Subnet

				Subnet này không có route trực tiếp ra Internet Gateway, chỉ có thể kết nối nội bộ.

				Trong hình:

					Cũng có EC2 instance

					Có Security Group, Network ACL, Route Table riêng

				Tác dụng:
				
					Dành cho tài nguyên chỉ dùng nội bộ, như:

						App Server

						Database Server

						Internal API
						
			Route Table (Bảng định tuyến)

				Điều khiển “hướng đi” của traffic trong VPC.

				Mỗi subnet được gán 1 Route Table.

					Public Subnet: có route 0.0.0.0/0 → Internet Gateway

					Private Subnet: không có route đó → chặn Internet trực tiếp

				Trong hình: Route Table liệt kê các dải IP nội bộ (172.16.x.x) dùng để định tuyến
				giữa các subnet trong VPC.
				
			Network ACL (Access Control List)

				Là tường lửa cấp subnet, kiểm soát inbound/outbound traffic.

				Có thể cho phép hoặc chặn dựa trên rule theo IP, port, protocol.
				
			Security Group

				Tường lửa cấp instance.

				Kiểm soát traffic inbound/outbound cho từng EC2.

				Mặc định: chặn mọi inbound, cho phép outbound.
				
			Router

				Là router logic bên trong VPC (AWS quản lý tự động).

				Kết nối các subnet với nhau và với gateway (Internet hoặc VPN).

				Bạn không cấu hình trực tiếp router, mà điều khiển qua Route Table.
				
			Internet Gateway (IGW)

				Cổng kết nối VPC ↔ Internet.

				Cần thiết nếu EC2 trong public subnet muốn có IP công cộng và truy cập ra ngoài.

				Chỉ hoạt động với subnet có route tới IGW.
				
			VPN Gateway

				Cho phép kết nối VPC với datacenter on-premises (qua đường VPN bảo mật).

				Dành cho mô hình hybrid cloud (vừa dùng AWS vừa dùng hạ tầng nội bộ).
				
		Luồng hoạt động minh họa

			EC2 trong Public Subnet có thể giao tiếp Internet trực tiếp qua Internet Gateway.

			EC2 trong Private Subnet:

				Không thể đi ra Internet trực tiếp.

				Có thể đi qua VPN Gateway để kết nối về hệ thống nội bộ (corporate network).

			Traffic nội bộ giữa các subnet (172.16.0.x ↔ 172.16.2.x) đi qua Router của VPC, theo Route Table.
			
	AWS Part -2 - Thiết kế kiến trúc mạng (VPC Blueprint) trong AWS ở region us-west-1, với đầy đủ subnet,
	route table, NAT, Internet Gateway, và VPC Peering:
	
		Thông tin tổng quan

			Region: us-west-1 (California)

			VPC Range: 172.20.0.0/16
			
				→ dải IP tổng cho toàn bộ VPC (65,536 địa chỉ IP khả dụng).
				
				→ các subnet con sẽ chia nhỏ dải này (theo /24).

		Cấu trúc Subnet

			VPC này có 4 subnet chia làm 2 public và 2 private, đặt ở 2 Availability Zones (AZs) khác nhau:

				Subnet 			CIDR				Loại		AZ	Ghi chú
				172.20.1.0/24	Public Subnet 1		us-west-1a	pub-sub-1
				172.20.2.0/24	Public Subnet 2		us-west-1b	pub-sub-2
				172.20.3.0/24	Private Subnet 1	us-west-1a	priv-sub-1
				172.20.4.0/24	Private Subnet 2	us-west-1b	priv-sub-2

			Mục đích:

				Public subnet: chứa các tài nguyên cần truy cập từ Internet (ví dụ: Bastion host, Load Balancer, NAT Gateway).

				Private subnet: chứa các ứng dụng nội bộ (App Server, Database, Cache…).

		Internet Gateway (IGW)

			Có 1 Internet Gateway gắn vào VPC này.

			Cho phép các instance trong public subnet giao tiếp trực tiếp với Internet.
			
		NAT Gateway (2 cái)

			Đặt trong public subnet, mỗi AZ một cái.

			Dùng để cho phép các instance trong private subnet truy cập Internet ra ngoài, nhưng Internet không
			truy cập ngược lại được.

			Ví dụ: private EC2 có thể tải gói update, patch, v.v… mà vẫn giữ an toàn.
			
		Elastic IP (EIP)

			Mỗi NAT Gateway cần một địa chỉ IP tĩnh (EIP) để ra Internet.

			Do đó, ở đây có 1 EIP (hoặc 2 nếu mỗi NAT Gateway 1 IP).
			
		Route Tables (Bảng định tuyến)

			Có 2 Route Tables:

				Tên Route Table		Dành cho		Định tuyến chính
				Public Subnet RT	Public Subnet	0.0.0.0/0 → Internet Gateway
				Private Subnet RT	Private Subnet	0.0.0.0/0 → NAT Gateway

			Giải thích:

				Public subnet có route trực tiếp ra IGW.

				Private subnet không có route ra IGW, chỉ route qua NAT Gateway.
				
		Bastion Host

			1 Bastion host nằm trong public subnet.

			Là EC2 trung gian, dùng để SSH vào các instance trong private subnet.

			Vì private subnet không có IP public, nên bạn SSH vào Bastion trước → rồi từ đó SSH
			tiếp vào các private EC2.
			
		Network ACL (NACL)

			Có nhắc đến “NACL” ở cuối — nghĩa là sẽ có Network ACL để kiểm soát traffic ở cấp subnet.

			Dùng để tăng thêm lớp bảo mật, thường cấu hình song song với Security Group.
			
		VPC Peering

			Dòng cuối “1 more VPC → VPC Peering” nghĩa là:

				Có một VPC khác trong cùng hoặc khác region.

				Sẽ tạo VPC Peering để hai VPC giao tiếp với nhau qua mạng riêng (không qua Internet).

			Ứng dụng:

			Dùng để kết nối VPC của team khác, môi trường dev/staging, hoặc hệ thống on-premises (khi không dùng VPN).
			
	- Google search:
	
		online subnet calculator
		
	- Code trong file /etc/awslogs/awslogs.conf:
	
		Case 1:
		
		
			[/var/log/messages]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/messages
			buffer_duration = 5000
			log_stream_name = web01-sys-logs
			initial_position = start_of_file
			log_group_name = wave-web

			[/var/log/httpd/access_log]
			datetime_format = %b %d %H:%M:%S
			file = /var/log/httpd/access_log
			buffer_duration = 5000
			log_stream_name = web01-httpd-access
			initial_position = start_of_file
			log_group_name = wave-web

	- Code trong s3-policy.json:
	
		Case 1:
		
			{
			  "Version":"2012-10-17",		 	 	 
			  "Statement": [
				{
				  "Effect": "Allow",
				  "Principal": {
					"AWS": "arn:aws:iam::127311923021:root"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*"
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:PutObject",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222/elb-wave/AWSLogs/914938480601/*",
				  "Condition": {
					"StringEquals": {
						"s3:x-amz-acl": "bucket-owner-full-control"
					}
				  }
				},

				{
				  "Effect": "Allow",
				  "Principal": {
					"Service": "delivery.logs.amazonaws.com"
				  },
				  "Action": "s3:GetBucketAcl",
				  "Resource": "arn:aws:s3:::wave-web-logs-111222"
				}
			  ]
			}


--- AWS CI CD Project:

	- Elastic Beanstalk -> Create application
	- Codebuild -> Getting started -> Create project
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Start build
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Phase details
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Edit
	- Codebuild -> Build -> Build projects -> Build project -> instance -> Build logs
	- Codebuild -> Pipeline -> Pipelines -> Create pipeline
	- Bitbucket -> Settings -> Personal Bitbucket settings -> SSH keys -> Add key
	- Bitbucket -> Settings -> Personal Bitbucket settings -> App passwords
	
	- Google search:
	
		bitbucket
		
		codebuild buildspec file documentation

	- Commands:
	
		- cmd: chmod 400 vprobeankey.pem
		- cmd: dnf search mysql
		- cmd: dnf install mariadb1011 -y
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts
		- cmd: wget https://raw.githubusercontent.com/hkhcoder/vprofile-project/refs/heads/aws-ci/src/main/resources/db_backup.sql
		- cmd: mysql -h vprords.cvrzecdaz2zy.us-east-1.rds.amazonaws.com -u admin -pYhEQ2ZAXD3d97PsHZXhb accounts < db_backup.sql
		- cmd: ls .ssh/
		- cmd: cat vprobit_rsa.pub
		- cmd: ssh -T git@bitbucket.org
		- cmd: ls -ltr
		- cmd: cat .git/config
		- cmd: git fetch --tags
		- cmd: git remote rm origin
		- cmd: git remote add origin git@bitbucket.org:devopscicd1122/vproapp.git
		- cmd: git push origin --all
		
		
		
		
	- Code trong file config:
	
		Case 1:
		
			# bitbucket.org
			Host bitbucket.org
			  PreferredAuthentications publickey
			  IdentityFile ~/.ssh/vprobit_rsa
			  
--- Docker:

	- Commands:
	
		- cmd: docker images
		- cmd: sudo vim /etc/group
		- cmd: sudo usermod -aG docker ubuntu
		- cmd: id ubuntu
		- cmd: docker run hello-world
		- cmd: docker ps
		- cmd: docker ps -a
		- cmd: docker pull nginx:stable-alpine3.21-perl
		- cmd: docker pull nginx
		- cmd: docker run --name myweb -p 7090:80 -d nginx
		- cmd: docker stop 2c0ebbbbc6cc
		- cmd: docker start myweb
		- cmd: ps -ef
		- cmd: cd /var/lib/docker/
		- cmd: root@ip-172-31-35-202:/var/lib/docker# cd containers/
		- cmd: du -sh 6e18faf905a703aad2a20e13e47edcd43a8e3fa8463dc22a76ef653133f04fe5
		- cmd: root@ip-172-31-35-202:/var/lib/docker# ls image/overlay2/
		- cmd: docker exec myweb ls /
		- cmd: docker exec myweb /bin/bash
		- cmd: docker exec -it myweb /bin/bash
		- cmd: apt install procps -y
		- cmd: docker rmi nginx:stable-alpine3.21-perl
		- cmd: docker rm 2c0ebbbbc6cc
		- cmd: docker pull ubuntu
		- cmd: docker run ubuntu
		- cmd: docker run -it ubuntu /bin/bash
		- cmd: docker rm sleepy_clarke amazing_ganguly tender_kilby
		- cmd: docker rmi ubuntu hello-world
		- cmd: docker inspect nginx
		- cmd: docker run -d -P nginx
		- cmd: docker logs peaceful_einstein
		- cmd: docker run -P nginx
		- cmd: docker run -d -P mysql:5.7
		- cmd: docker run -d -P -e MYSQL_ROOT_PASSWORD=mypass mysql:5.7
		- cmd: docker pull mysql:5.7
		- cmd: docker inspect mysql:5.7
		- cmd: docker exec -it vprodb /bin/bash
		- cmd: bash-4.2# cd /var/lib/mysql
		- cmd: docker stop vprodb
		- cmd: docker rm vprodb
		- cmd: docker volume
		- cmd: docker volume create mydbdata
		- cmd: docker volume ls
		- cmd: docker run --name vprodb -d -e MYSQL_ROOT_PASSWORD=secretpass -p 3030:3306 -v mydbdata:/var/lib/mysql mysql:5.7
		- cmd: ls /var/lib/docker/volumes/
		- cmd: ls /var/lib/docker/volumes/mydbdata/_data/
		- cmd: 	sudo apt update
				sudo apt install -y mysql-client
		- cmd: sudo apt install unzip -y
		- cmd: tar czvf nano.tar.gz *
		- cmd: mv nano.tar.gz ../
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg
		- cmd: docker build -t nanoimg:V2 .
		- cmd: docker run -d --name nanowebsite -p 9080:80 nanoimg:V2
		- cmd: docker build --no-cache -t nanoimg:V2 .
		
			Chạy để tránh nó chạy với cache cũ
			
		- cmd: docker login
		- cmd: docker build -t ngoctuan99/nanoimg:V2 .
		- cmd: docker push ngoctuan99/nanoimg:V2
		- cmd: ubuntu@ip-172-31-35-202:~/EntryCMD$ docker build -t printer:v1 -f cmd/Dockerfile .
		- cmd: docker build -t printer:v2 entry/
		- cmd: docker-compose up
		- cmd: curl http://localhost:8000
		- cmd: docker compose up -d
		- cmd: docker compose ps
		- cmd: docker compose top
		- cmd: git clone -b docker https://github.com/devopshydclub/vprofile-project.git
		
		
		
		
			
		
		
	

		
		
		
		
	
	- Google search:
	
		docker ubuntu install
		
		dockerhub official images
		
		hub docker
		
		docker compose install
		
		docker compose get started
		
	- Code trong Dockerfile:
	
		Case 1:
		
			FROM ubuntu:latest
			LABEL "Author"="Imran"
			LABEL "Project"="nano"
			ENV DEBIAN_FRONTEND=noninteractive
			RUN apt update && apt install git -y
			RUN apt install apache2 -y
			CMD ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
			EXPOSE 80
			WORKDIR /var/www/html
			VOLUME /var/log/apache2
			ADD nano.tar.gz /var/www/html
			#COPY nano.tar.gz /var/www/html
			
		Case 2:
		
			# syntax=docker/dockerfile:1
			FROM python:3.10-alpine
			WORKDIR /code
			ENV FLASK_APP=app.py
			ENV FLASK_RUN_HOST=0.0.0.0
			RUN apk add --no-cache gcc musl-dev linux-headers
			COPY requirements.txt requirements.txt
			RUN pip install -r requirements.txt
			EXPOSE 5000
			COPY . .
			CMD ["flask", "run"]
			
		Case 3:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
				volumes:
				  - .:/code
				environment:
				  FLASK_ENV: development
			  redis:
				image: "redis:alpine"

		Case 4:
		
			FROM ubuntu:latest
			CMD ["echo", "hello"]
			
		Case 5:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			
		Case 6:
		
			FROM ubuntu:latest
			ENTRYPOINT ["echo"]
			cmd ["hello"]
			
		Case 7:
		
			FROM tomcat:8-jre11
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			WORKDIR /usr/local/tomcat/
			VOLUME /usr/local/tomcat/webappsubuntu@ip-172-31-35-202:~/composetest/vprofile-project/Docker-files/app$
			
		Case 8:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11

			RUN rm -rf /usr/local/tomcat/webapps/*

			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]



	-  Code trong app.py:
	
		Case 1:
		
			import time

			import redis
			from flask import Flask

			app = Flask(__name__)
			cache = redis.Redis(host='redis', port=6379)

			def get_hit_count():
				retries = 5
				while True:
					try:
						return cache.incr('hits')
					except redis.exceptions.ConnectionError as exc:
						if retries == 0:
							raise exc
						retries -= 1
						time.sleep(0.5)

			@app.route('/')
			def hello():
				count = get_hit_count()
				return 'Hello World! I have been seen {} times.\n'.format(count)
				
	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
			  web:
				build: .
				ports:
				  - "8000:5000"
			  redis:
				image: "redis:alpine"

	- Code trong file requirements.txt:
	
		Case 1:
		
			flask
			redis
			
--- Containerization:

	Git bash không thể chạy nhiều lệnh 1 lúc đượcA

	- Google search:
	
		Docker ubuntu install
		
		Dockerfile reference
		
	Containerization -  Sự khác nhau giữa / và /root:
	
		Trong Ubuntu (và Linux nói chung), hai thư mục / và /root hoàn toàn khác nhau về vai trò và quyền hạn.
		
		/ (root directory)

			/ được gọi là thư mục gốc của toàn bộ hệ thống tập tin (root filesystem).

			Tất cả mọi thứ trong Linux — file hệ thống, người dùng, thiết bị, ứng dụng, v.v. — đều nằm dưới thư mục này.

			Nó giống như ổ C: trong Windows, nhưng rộng hơn, vì nó chứa luôn các “ổ đĩa” và mount khác.

			Cấu trúc thường thấy trong /:
	
				/
				├── bin        → Chứa lệnh cơ bản (ls, cp, mv, cat, v.v.)
				├── boot       → Chứa file khởi động hệ thống (kernel, grub)
				├── dev        → Thiết bị (ổ cứng, USB, tty,...)
				├── etc        → File cấu hình hệ thống
				├── home       → Thư mục người dùng bình thường (vd: /home/nhuan)
				├── lib        → Thư viện hệ thống
				├── root       → Thư mục cá nhân của tài khoản root
				├── tmp        → File tạm thời
				├── usr        → Chương trình, thư viện dùng chung
				├── var        → Log, mail, cache,...
				
			/root

				/root là thư mục cá nhân (home directory) của người dùng root (superuser).

				Khi bạn đăng nhập bằng tài khoản root, đây là nơi mặc định bạn sẽ “đứng” (cd).

				Tương tự như /home/username của người dùng thường, nhưng được đặt ở đây vì:

				Root cần truy cập được ngay cả khi /home chưa mount (ví dụ trong chế độ khôi phục).

				Bảo mật và tách biệt với user thông thường.

	- Commands:
	
		- cmd: usermod -aG docker vagrant
		- cmd: id vagrant
		- cmd: docker compose up -d
		- cmd: docker compose down
		- cmd: docker volume ls
		- cmd: docker volume rm vagrant_vproappdata vagrant_vprodbdata
		- cmd: docker volume prune
		- cmd: docker system prune -a
		- cmd: code .
		- cmd: id
		- cmd: docker-compose --version
		- cmd: docker-compose build
		
		
		
		
		
	
	- Code trong file Dockerfile:
	
		Case 1:
		
			FROM maven:3.9.9-eclipse-temurin-21-jammy AS BUILD_IMAGE
			RUN git clone https://github.com/hkhcoder/vprofile-project.git
			RUN cd vprofile-project && git checkout containers && mvn install

			FROM tomcat:10-jdk21
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 2:
		
			FROM mysql:8.0.33
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			ENV MYSQL_ROOT_PASSWORD="vprodbpass"
			ENV MYSQL_DATABASE="accounts"

			ADD db_backup.sql docker-entrypoint-initdb.d/db_backup.sql
			
		Case 3:
		
			FROM nginx
			LABEL "Project"="Vprofile"
			LABEL "Author"="Imran"

			RUN rm -rf /etc/nginx/conf.d/default.conf
			COPY nginvproapp.conf /etc/nginx/conf.d/vproapp.conf
			
		Case 4:
		
			FROM openjdk:11 AS BUILD_IMAGE
			RUN apt update && apt install maven -y
			RUN git clone https://github.com/devopshydclub/vprofile-project.git
			#ADD ../../vprofile-project
			RUN cd vprofile-project && git checkout docker && mvn install

			FROM tomcat:9-jre11
			LABEL "Project"="vprofile"
			LABEL "Author"="Imran"
			RUN rm -rf /usr/local/tomcat/webapps/*
			COPY --from=BUILD_IMAGE vprofile-project/target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

			EXPOSE 8080
			CMD ["catalina.sh", "run"]
			
		Case 5:
		
			FROM node:14 AS web-build
			WORKDIR /usr/src/app
			COPY ./ ./client
			RUN cd client && npm install && npm run build --prod

			# Use official nginx image as the base image
			FROM nginx:latest

			# Copy the build output to replace the default nginx contents.
			COPY --from=web-build /usr/src/app/client/dist/client/ /usr/share/nginx/html
			COPY nginx.conf /etc/nginx/conf.d/default.conf

			# Expose port 4200
			EXPOSE 4200
			
		Case 6:
		
			FROM node:14 AS nodeapi-build
			WORKDIR /usr/src/app
			COPY ./ ./nodeapi/
			RUN cd nodeapi && npm install

			FROM node:14
			WORKDIR /usr/src/app/
			COPY --from=nodeapi-build /usr/src/app/nodeapi/ ./
			RUN ls
			EXPOSE 5000
			CMD ["/bin/sh", "-c", "cd /usr/src/app/ && npm start"]
			# Test3

		Case 7:
		
			FROM openjdk:8 AS BUILD_IMAGE
			WORKDIR /usr/src/app/
			RUN apt update && apt install maven -y
			COPY ./ /usr/src/app/
			RUN mvn install -DskipTests

			FROM openjdk:8

			WORKDIR /usr/src/app/
			COPY --from=BUILD_IMAGE /usr/src/app/target/book-work-0.0.1-SNAPSHOT.jar ./book-work-0.0.1.jar

			EXPOSE 9000
			ENTRYPOINT ["java","-jar","book-work-0.0.1.jar"]
			# Test


			
	- Code trong file nginvproapp.conf:
	
		Case 1:
		
			upstream vproapp {
			 server vproapp:8080;
			}
			server {
			  listen 80;
			location / {
			  proxy_pass http://vproapp;
			}
			}

	- Code trong file docker-compose.yml:
	
		Case 1:
		
			services:
				vprodb:
				  build:
					context: ./Docker-files/db
				  image: ngoctuan99/vprofiledb
				  container_name: vprodb
				  ports:
					- "3306:3306"
				  volumes:
					- vprodbdata:/var/lib/mysql
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vprocache01:
				  image: memcached
				  container_name: vprocache01
				  ports:
					- "11211:11211"

				vpromq01:
				  image: rabbitmq
				  container_name: vpromq01
				  ports:
					- "5672:5672"
				  environment:
					- RABBITMQ_DEFAULT_USER=guest
					- RABBITMQ_DEFAULT_PASS=guest

				vproapp:
				  build:
					context: ./Docker-files/app
				  image: ngoctuan99/vprofileapp
				  container_name: vproapp
				  ports:
					- "8080:8080"
				  volumes:
					- vproappdata:/usr/local/tomcat/webapps
				  environment:
					- MYSQL_ROOT_PASSWORD=vprodbpass

				vproweb:
				  build:
					context: ./Docker-files/web
				  image: ngoctuan99/vprofileweb
				  container_name: vproweb
				  ports:
					- "80:80"

			volumes:
			  vprodbdata: {}
			  vproappdata: {}
			  
		Case 2:
		
			version: "3.8"

			services:
			  client:
				build:
				  context: ./client
				ports:
				  - "4200:4200"
				container_name: client
				depends_on:
				  - api
				  - webapi

			  api:
				build:
				  context: ./nodeapi
				ports:
				  - "5000:5000"
				restart: always
				container_name: api
				depends_on:
				  - nginx
				  - emongo

			  webapi:
				build:
				  context: ./javaapi
				ports:
				  - "9000:9000"
				restart: always
				container_name: webapi
				depends_on:
				  - emartdb

			  nginx:
				restart: always
				image: nginx:latest
				container_name: nginx
				volumes:
				  - "./nginx/default.conf:/etc/nginx/conf.d/default.conf"
				ports:
				  - "80:80"

			  emongo:
				image: mongo:4
				container_name: emongo
				environment:
				  - MONGO_INITDB_DATABASE=epoc
				ports:
				  - "27017:27017"

			  emartdb:
				image: mysql:8.0.33
				container_name: emartdb
				ports:
				  - "3306:3306"
				environment:
				  - MYSQL_ROOT_PASSWORD=emartdbpass
				  - MYSQL_DATABASE=books

			  
	- Code trong file nginx.conf:
	
		Case 1:
		
			server {
				listen       4200;
				listen  [::]:4200;
				server_name  localhost;

				#access_log  /var/log/nginx/host.access.log  main;

				location / {
					root   /usr/share/nginx/html;
					index  index.html index.htm;
				}

				#error_page  404              /404.html;

				# redirect server error pages to the static page /50x.html
				#
				error_page   500 502 503 504  /50x.html;
				location = /50x.html {
					root   /usr/share/nginx/html;
				}

				# proxy the PHP scripts to Apache listening on 127.0.0.1:80
				#
				#location ~ \.php$ {
				#    proxy_pass   http://127.0.0.1;
				#}

				# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
				#
				#location ~ \.php$ {
				#    root           html;
				#    fastcgi_pass   127.0.0.1:9000;
				#    fastcgi_index  index.php;
				#    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
				#    include        fastcgi_params;
				#}

				# deny access to .htaccess files, if Apache's document root
				# concurs with nginx's one
				#
				#location ~ /\.ht {
				#    deny  all;
				#}
			}

	- Code trong file default.conf:
	
		Case 1:
		
			#upstream api {
			#    server api:5000; 
			#}
			#upstream webapi {
			#    server webapi:9000;
			#}
			upstream client {
				server client:4200;
			}
			server {
				listen 80;
				location / {
					proxy_set_header Host $host;
					proxy_set_header X-Real-IP $remote_addr;
					proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
					proxy_set_header X-Forwarded-Proto $scheme; 

					proxy_http_version 1.1;
					proxy_set_header Upgrade $http_upgrade;
					proxy_set_header Connection "upgrade";
					proxy_pass http://client/;
				}
				location /api {
			#        rewrite ^/api/(.*) /$1 break; # works for both /api and /api/
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			#        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme; 
			#        proxy_http_version 1.1;

					proxy_pass http://api:5000;
				}
				location /webapi {
			#        rewrite ^/webapi/(.*) /$1 break;
			#        proxy_set_header Host $host;
			#        proxy_set_header X-Real-IP $remote_addr;
			##        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			#        proxy_set_header X-Forwarded-Proto $scheme;
					proxy_pass http://webapi:9000;
				}
			}

	- Code trong Advanced detials của EC2:
	
		Case 1:
		
			#!/bin/bash

			# Install docker on Ubuntu
			sudo apt-get update
				sudo apt-get install \
				 ca-certificates \
				 curl \
				 gnupg \
				 lsb-release -y
				curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
				echo \
				"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
				$(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

			# Install docker-compose
				sudo apt-get update
				sudo apt-get install docker-ce docker-ce-cli containerd.io -y
				sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
				sudo chmod +x /usr/local/bin/docker-compose

			# Add ubuntu user into docker group
				sudo usermod -a -G docker ubuntu

--- Kubernetes:

	- Route 53 -> Dashboard -> Create hosted zone
	- Amazon Elastic Kubernetes Service (Amazon EKS)

	- Google search:
	
		kubernetes docs
		
		minikube get started
		
		install kops
		
		install kubectl on linux
		
		kubernetes pod
		
		kubernetes deployment
		
		install kubectl on window
		
		kubernetes replicaset
		
		kubernetes deployment
		
		dockerfile reference
		
		define a command and arguments for a container
		
		kubernetes volume
		
		kubernetes configmaps
		
		kubernetes secrets
		
		kubernetes ingress
		
		kubectl cheatsheet
		
		kubernetes taint
		
		kubernetes limit
		
		kubernetes jobs
		
		kubernetes cronjob
		
		kubernetes daemonset
		
		k8slens.dev
		
		terraform modules
		
		
		
		
		
	Cần check code trong source từ git clone (emartapp)
		
		
		
		

	- Commands:
	
		- cmd: kops export kubecfg kubevpro.vnstudio.info --state=s3://kopsstate112211 --admin
		- cmd: choco install minikube kubernetes-cli -y
		- cmd: minikube.exe --help
		- cmd: minikube start
		- cmd: minikube start --no-vtx-check
		- cmd: minikube delete
		- cmd: cat .kube/config
		- cmd: kubectl.exe get nodes
		- cmd: kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.0
		- cmd: kubectl.exe get pod
		- cmd: kubectl.exe get deploy
		- cmd: kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0
		- cmd: minikube service hello-minikube --url
		- cmd: minikube service hello-minikube
		- cmd: kubectl.exe delete deploy hello-minikube
		- cmd: minikube.exe stop
		- cmd: minikube.exe delete
		- cmd: chmod +x kops
		- cmd: sudo mv kops /usr/local/bin/kops
		- cmd: kubectl version --client
		- cmd: cat .kube/config
		- cmd:
		
				kops create cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate956
				--zones=us-east-1a,us-east-1b --node-count=2 --node-size=t3.small --control-plane-size=t3.medium
				--dns-zone=kubevpro.vnstudio.info --node-volume-size=12 --control-plane-volume-size=12
				--ssh-public-key ~/.ssh/id_ed25519.pub
				
		- cmd: kops update cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes --admin
		- cmd: kubectl get nodes
		- cmd: kubectl config view
		- cmd: kubectl get ns
		- cmd: kubectl get all
		- cmd: kubectl get all --all-namespaces
		- cmd: kubectl get svc -n kube-system
		- cmd: kubectl create ns kubekart
		- cmd: kubectl run nginx1 --image=nginx -n kubekart
		- cmd: kubectl apply -f pod1.yaml
		- cmd: kubectl get pod -n kubekart
		- cmd: kubectl delete ns kubekart
		- cmd: kubectl create -f pod-setup.yml
		- cmd: kubectl describe pod webapp-pod
		- cmd: kubectl get pod webapp-pod -o yaml
		- cmd: kubectl get pod webapp-pod -o yaml > webpod-definition.yml
		- cmd: kubectl edit pod webapp-pod
		- cmd: kubectl delete pod nginx12
		- cmd: kubectl logs web2
		- cmd: history | grep test47
		- cmd: kubectl run nginx12 --image=nginx
		- cmd: kubectl get svc
		- cmd: kubectl describe svc helloworld-service
		- cmd: kubectl describe pod | grep IP
		- cmd: kubectl delete svc helloworld-services
		- cmd: kubectl get rs
		- cmd: kubectl delete pod frontend-z2qch frontend-zr67k
		- cmd: kubectl scale --replicas=1 rs/frontend
		- cmd: kubectl edit rs frontend
		- cmd: kubectl get deploy
		- cmd: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		- cmd: kubectl rollout undo deployment/nginx-deployment
		- cmd: kubectl describe pod nginx-deployment-647677fc66-62mt4 | grep Image
		- cmd: kubectl rollout history deployment/nginx-deployment
		- cmd: kubectl delete deploy nginx-deployment
		- cmd: kubectl apply -f samplecm.yaml
		- cmd: kubectl get cm game-demo -o yaml
		
		- cmd:
		
			ubuntu@ip-172-31-36-50:~$ kubectl exec --stdin --tty configmap-demo-pod -- /bin/sh
			/ # ls /config/
			game.properties            user-interface.properties
			/ # cd /config/
			/config # cat game.properties
			enemy.types=aliens,monsters
			player.maximum-lives=5
			/config # cat user-interface.properties
			color.good=purple
			color.bad=yellow
			allow.textmode=true
			/config # echo $PLAYER_INITIAL_LIVES
			3
			/config # echo $UI_PROPERTIES_FILE_NAME
			user-interface.properties
			/config # exit
			
		- cmd: echo -n "admin" | base64
		- cmd: echo -n "mysecretpass" | base64
		- cmd: echo 'c2VjcmV0cGFzcw==' | base64 --decode
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl get all -n ingress-nginx
		- cmd:
		
			ubuntu@ip-172-31-36-50:~/vprofile$ # Controller
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReated DNS Cname Record for LB
			ubuntu@ip-172-31-36-50:~/vprofile$ # CReate Ingress
			
		- cmd: kubectl delete ingress vpro-ingress
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: kubectl config view
		- cmd: kubectl run nginxpod --image=nginx --dry-run=client -o yaml > ngpod.yaml
		- cmd: cat ngpod.yaml
		- cmd: kubectl create deployment ngdep --image=nginx --dry-run=client -o yaml > ngdep.yaml
		- cmd: cat ngdep.yaml
		- cmd: kubectl get ds -A
		- cmd: kubectl get pod -n kube-system
		- cmd: choco uninstall terraform
		- cmd: choco install terraform -y
		- cmd: terraform init -upgrade
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vpro-eks
		- cmd: cat ~/.kube/config
		- cmd: choco install kubernetes-cli -y
		- cmd: kubectl get pods
		
		
		
		
		
		
		
		
		
		
		
		

		
		

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
	
		

	Kubernetes - Sơ đồ kiến trúc của Kubernetes (K8s):
	
		Tổng quan về Kubernetes Architecture

			Kubernetes chia hệ thống thành hai phần chính:

				Master Node (Control Plane) – quản lý toàn bộ cluster

				Worker Nodes – nơi thật sự chạy các container (ứng dụng của bạn)
				
		Master Node (Control Plane)

			Đây là bộ não của Kubernetes, chịu trách nhiệm điều phối và quản lý toàn bộ cluster.
			Trong hình, bạn thấy các thành phần chính sau:

			API Server

				Là trung tâm giao tiếp của Kubernetes.

				Mọi lệnh (kubectl apply, kubectl get pods, …) đều đi qua API Server.

				Nó nhận file YAML (bên trái hình), chứa định nghĩa Pod, Service, Deployment, v.v.

				Sau đó ghi thông tin đó vào etcd (database).

				Hiểu đơn giản: API Server là “cửa chính” của cluster.

			etcd

				Là cơ sở dữ liệu phân tán (key-value store) của Kubernetes.

				Lưu toàn bộ trạng thái của cluster: pods, nodes, cấu hình mạng, secrets, …

				Nếu mất etcd, bạn mất toàn bộ “trí nhớ” của hệ thống.

			⚙Controller Manager

				Theo dõi trạng thái cluster qua etcd.

				Nếu phát hiện sự khác biệt giữa trạng thái mong muốn và trạng thái thực tế, nó sẽ hành động để khôi phục lại.

				Ví dụ:

					YAML khai báo cần 3 pods, nhưng thực tế chỉ có 2 pods chạy → Controller Manager sẽ tạo thêm 1 pod mới.

			Scheduler

				Quyết định pod sẽ được chạy ở node nào.

				Dựa trên tài nguyên còn trống (CPU, RAM), các ràng buộc (nodeSelector, affinity, v.v.)

				Sau khi chọn node, Scheduler báo cho kubelet ở node đó để triển khai pod.
				
		Worker Nodes

			Mỗi Worker Node là một máy (hoặc VM) thật nơi container thực sự chạy.
			
			Trong hình bạn thấy ba Worker nodes, mỗi node gồm:

				Kubelet

					Agent chạy trên mỗi node, nhận lệnh từ API Server.

					Khi Scheduler quyết định gán Pod vào node này, Kubelet sẽ:

						Gọi Docker (hoặc container runtime khác như containerd)

						Kéo image về

						Tạo và khởi động container

						Báo lại trạng thái pod về API Server

				Kube Proxy

					Quản lý mạng và load balancing giữa các Pods.

					Giúp các Pods ở các node khác nhau có thể liên lạc với nhau.

					Duy trì service IP và routing rules trong hệ thống.

				Docker (hoặc container runtime)

					Là công cụ thật sự chạy container.

					Kubernetes chỉ “ra lệnh”, Docker thực thi việc tạo và quản lý container.

				Pods

					Đơn vị triển khai nhỏ nhất trong Kubernetes.

					Một Pod có thể chứa một hoặc nhiều container, chạy cùng trên một node.

					Nếu Pod chết, Kubernetes sẽ tự tạo lại Pod mới theo YAML định nghĩa (bạn thấy dấu “X đỏ” ở node
					cuối hình → một Pod chết, K8s sẽ khởi tạo lại).
					
		Luồng hoạt động tổng quát

			Dev gửi file YAML đến API Server (kubectl apply -f deployment.yaml)

			API Server lưu cấu hình vào etcd

			Controller Manager kiểm tra và ra lệnh tạo Pod

			Scheduler chọn node phù hợp

			Kubelet ở node đó nhận lệnh → chạy Pod bằng Docker

			Kube Proxy đảm bảo Pod có thể giao tiếp với các Pod khác
			
	- Code trong file pod1.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: nginx12
			  namespace: kubekart
			spec:
			  containers:
			  - name: nginx
				image: nginx:1.14.2
				ports:
				- containerPort: 80

	- Code trong file vproapppod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: vproapp
			  labels:
				app: vproapp
			spec:
			  containers:
				- name: appcontainer
				  image: imranvisualpath/freshtomapp:V7
				  ports:
					- name: vproapp-port
					  containerPort: 8080
	- Code trong file vproapp-nodeport.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 8090
				nodePort: 30001
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: NodePort

	- Code trong file vproapp-loadbalancer.yml
	
		Case 1:
		
			apiVersion: v1
			kind: Service
			metadata:
			  name: helloworld-service
			spec:
			  ports:
			  - port: 80
				targetPort: vproapp-port
				protocol: TCP
			  selector:
				app: vproapp
			  type: LoadBalancer

	- Code trong file replset.yaml
	
		Case 1:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 3
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
		Case 2:
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: frontend
			  labels:
				app: guestbook
				tier: frontend
			spec:
			  # modify replicas according to your case
			  replicas: 5
			  selector:
				matchLabels:
				  tier: frontend
			  template:
				metadata:
				  labels:
					tier: frontend
				spec:
				  containers:
				  - name: php-redis
					image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
					
	- Code trong file deployment.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: nginx-deployment
			  labels:
				app: nginx
			spec:
			  replicas: 3
			  selector:
				matchLabels:
				  app: nginx
			  template:
				metadata:
				  labels:
					app: nginx
				spec:
				  containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80

	- Code trong file com.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: command-demo
			  labels:
				purpose: demonstrate-command
			spec:
			  containers:
			  - name: command-demo-container
				image: debian
				command: ["printenv"]
				args: ["HOSTNAME", "KUBERNETES_PORT"]
			  restartPolicy: OnFailure
			  
	- Code trong file mysqlpod.yaml:
	
		Case 1: chạy gặp lỗi và được fix như ở Case 2
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: dbpod
			spec:
			  containers:
			  - image: mysql:5.7
				name: mysql
				volumeMounts:
				- mountPath: /var/lib/mysql
				  name: dbvol
			  volumes:
			  - name: dbvol
				hostPath:
				  # directory location on host
				  path: /data
				  # this field is optional
				  type: DirectoryOrCreate
				  
		Case 2:
		
			apiVersion: v1

			kind: Pod

			metadata:

			  name: dbpod

			spec:

			  containers:

			  - image: mysql:5.7

				name: mysql

				env:

				- name: MYSQL_ROOT_PASSWORD

				  value: examplepassword

				- name: MYSQL_DATABASE

				  value: exampledb

				- name: MYSQL_USER

				  value: exampleuser

				- name: MYSQL_PASSWORD

				  value: examplepassword

				volumeMounts:

				- mountPath: /var/lib/mysql

				  name: dbvol

			  volumes:

			  - name: dbvol

				hostPath:

				  # directory location on host

				  path: /data

				  # this field is optional

				  type: DirectoryOrCreate
				  
	- Code trong file samplecm.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: ConfigMap
			metadata:
			  name: game-demo
			data:
			  # property-like keys; each key maps to a simple value
			  player_initial_lives: "3"
			  ui_properties_file_name: "user-interface.properties"

			  # file-like keys
			  game.properties: |
				enemy.types=aliens,monsters
				player.maximum-lives=5    
			  user-interface.properties: |
				color.good=purple
				color.bad=yellow
				allow.textmode=true    

	- Code trong file readcmpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  name: configmap-demo-pod
			spec:
			  containers:
				- name: demo
				  image: alpine
				  command: ["sleep", "3600"]
				  env:
					# Define the environment variable
					- name: PLAYER_INITIAL_LIVES # Notice that the case is different here
												 # from the key name in the ConfigMap.
					  valueFrom:
						configMapKeyRef:
						  name: game-demo           # The ConfigMap this value comes from.
						  key: player_initial_lives # The key to fetch.
					- name: UI_PROPERTIES_FILE_NAME
					  valueFrom:
						configMapKeyRef:
						  name: game-demo
						  key: ui_properties_file_name
				  volumeMounts:
				  - name: config
					mountPath: "/config"
					readOnly: true
			  volumes:
			  # You set volumes at the Pod level, then mount them into containers inside that Pod
			  - name: config
				configMap:
				  # Provide the name of the ConfigMap you want to mount.
				  name: game-demo
				  # An array of keys from the ConfigMap to create as files
				  items:
				  - key: "game.properties"
					path: "game.properties"
				  - key: "user-interface.properties"
					path: "user-interface.properties"
					
	- Code trong file mysecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			
	- Code trong file readsecret.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Secret
			metadata:
			  name: mysecret
			data:
			  username: YWRtaW4=
			  password: bXlzZWNyZXRwYXNz
			type: Opaque
			ubuntu@ip-172-31-36-50:~$ cat readsecret.yaml
			apiVersion: v1
			kind: Pod
			metadata:
			  name: secret-env-pod
			spec:
			  containers:
				- name: mycontainer
				  image: redis
				  env:
					- name: SECRET_USERNAME
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: username
						  optional: false  # same as default; "mysecret" must exist and include a key named "username"

					- name: SECRET_PASSWORD
					  valueFrom:
						secretKeyRef:
						  name: mysecret
						  key: password
						  optional: false  # same as default; "mysecret" must exist and include a key named "password"

			  restartPolicy: Never
			  
	- Code trong file vprodep.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080

	- Code trong file vprosvc.yaml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: my-app
			spec:
			  selector:
				matchLabels:
				  run: my-app
			  replicas: 1
			  template:
				metadata:
				  labels:
					run: my-app
				spec:
				  containers:
				  - name: my-app
					image: imranvisualpath/vproappfix
					ports:
					- containerPort: 8080
			ubuntu@ip-172-31-36-50:~/vprofile$ cat vprosvc.yaml
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-app
			spec:
			  ports:
			  - port: 8080
				protocol: TCP
				targetPort: 8080
			  selector:
				run: my-app
			  type: ClusterIP

	- Code trong file vprosvc.yaml:

		apiVersion: v1
		kind: Service
		metadata:
		  name: my-app
		spec:
		  ports:
		  - port: 8080
			protocol: TCP
			targetPort: 8080
		  selector:
			run: my-app
		  type: ClusterIP
	- Code trong file vproingress.yaml:
	
		Case 1:
		
			apiVersion: networking.k8s.io/v1
			kind: Ingress
			metadata:
			  name: vpro-ingress
			  annotations:
				nginx.ingress.kubernetes.io/use-regex: "true"
			spec:
			  ingressClassName: nginx
			  rules:
			  - host: vprofile.vnstudio.info
				http:
				  paths:
				  - path: /
					pathType: Prefix
					backend:
					  service:
						name: my-app
						port:
						  number: 8080
						  
	- Code trong file ngpod.yaml:
	
		Case 1:
		
			apiVersion: v1
			kind: Pod
			metadata:
			  labels:
				run: nginxpod
			  name: nginxpod
			spec:
			  containers:
			  - image: nginx
				name: nginxpod

	- Code trong file sampleds.yaml:
	
		Case 1:
		
			apiVersion: apps/v1
			kind: DaemonSet
			metadata:
			  name: fluentd-elasticsearch
			  namespace: kube-system
			  labels:
				k8s-app: fluentd-logging
			spec:
			  selector:
				matchLabels:
				  name: fluentd-elasticsearch
			  template:
				metadata:
				  labels:
					name: fluentd-elasticsearch
				spec:
				  tolerations:
				  # these tolerations are to have the daemonset runnable on control plane nodes
				  # remove them if your control plane nodes should not run pods
				  - key: node-role.kubernetes.io/control-plane
					operator: Exists
					effect: NoSchedule
				  - key: node-role.kubernetes.io/master
					operator: Exists
					effect: NoSchedule
				  containers:
				  - name: fluentd-elasticsearch
					image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
					resources:
					  limits:
						memory: 200Mi
					  requests:
						cpu: 100m
						memory: 200Mi
					volumeMounts:
					- name: varlog
					  mountPath: /var/log
				  # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
				  # preempts running Pods
				  # priorityClassName: important
				  terminationGracePeriodSeconds: 30
				  volumes:
				  - name: varlog
					hostPath:
					  path: /var/log

	- Code trong file eks-cluster.tf:
	
		Case 1:
		
			module "eks" {
			  source = "terraform-aws-modules/eks/aws"
			  version = "19.0.4"

			  cluster_name = local.cluster_name
			  cluster_version = "1.30"

			  vpc_id = module.vpc.vpc_id
			  subnet_ids = module.vpc.private_subnets
			  cluster_endpoint_public_access = true

			  eks_managed_node_group_defaults = {
				ami_type = "AL2_x86_64"

			  }

			  eks_managed_node_groups = {
				one = {
				  name = "node-group-1"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 3
				  desired_size = 2
				}

				two = {
				  name = "node-group-2"

				  instance_types = ["t3.small"]

				  min_size = 1
				  max_size = 2
				  desired_size = 1
				}
			  }
			}

	- Code trong file main.tf:
	
		Case 1:
		
			provider "kubernetes" {
			  host = module.eks.cluster_endpoint
			  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
			}

			provider "aws" {
			  region = var.region
			}

			data "aws_availability_zones" "available" {}

			locals {
			  cluster_name = var.clusterName
			}

	- Code trong file outputs.tf:
	
		Case 1:
		
			output "cluster_name" {
			  description = "Amazon Web Service EKS Cluster Name"
			  value = module.eks.cluster_name
			}

			output "cluster_endpoint" {
			  description = "Endpoint for Amazon Web Service EKS "
			  value = module.eks.cluster_endpoint
			}

			output "region" {
			  description = "Amazon Web Service EKS Cluster region"
			  value = var.region
			}


			output "cluster_security_group_id" {
			  description = "Security group ID for the Amazon Web Service EKS Cluster "
			  value = module.eks.cluster_security_group_id
			}

	- Code trong file terraform.tf:
	
		Case 1:
		
			terraform {
			  required_providers {
				aws = {
				  source = "hashicorp/aws"
				  version = "~> 4.46.0"
				}

				random = {
				  source = "hashicorp/random"
				  version = "~> 3.4.3"
				}

				tls = {
				  source = "hashicorp/tls"
				  version = "~> 4.0.4"
				}

				cloudinit = {
				  source = "hashicorp/cloudinit"
				  version = "~> 2.2.0"
				}

				kubernetes = {
				  source = "hashicorp/kubernetes"
				  version = "~> 2.16.1"
				}
			  }

			  backend "s3" {
				bucket         	   = "terraform-eks112211"
				key              	   = "state/terraform.tfstate"
				region         	   = "us-east-1"
			  }

			  required_version = "~> 1.3"
			}
			
	- Code trong file variables.tf:
	
		Case 1:
		
			variable "region" {
			  description = "AWS region"
			 type = string
			 default = "us-east-1"
			}

			variable "clusterName" {
			  description = "Name of the EKS cluster"
			 type = string
			 default = "vpro-eks"
			}
			
	- Code trong file vpc.tf:
	
		Case 1:
		
			module "vpc" {
			  source = "terraform-aws-modules/vpc/aws"
			  version = "3.14.2"

			  name = "vprofile-eks"

			  cidr = "172.20.0.0/16"
			  azs = slice(data.aws_availability_zones.available.names, 0, 3)

			  private_subnets = ["172.20.1.0/24", "172.20.2.0/24", "172.20.3.0/24"]
			  public_subnets = ["172.20.4.0/24", "172.20.5.0/24", "172.20.6.0/24"]

			  enable_nat_gateway = true
			  single_nat_gateway = true
			  enable_dns_hostnames = true

			  public_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/elb" = 1
			  }

			  private_subnet_tags = {
				"kubernetes.io/cluster/${local.cluster_name}" = "shared"
				"kubernetes.io/role/internal-elb" = 1
			  }
			}


--- App Deployment on Kubernetes Cluster:

	- Google search:
	
		kubernetes persistent volume claim
		
		kubernetes deploy yaml
		
		https://github.com/marketplace?type=
		
		helm docs install helm
		
		

	- Commands:
	
		- cmd: echo -n "vprodbpass" | base64
		- cmd: echo -n "guest" | base64
		- cmd: kubectl create -f .
		- cmd: kubectl delete -f .
		- cmd: kubectl get sc
		- cmd: kubectl get pods -n kube-system | grep ingress
		- cmd: kubectl describe ingress vpro-ingress
		- cmd: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
		- cmd: kops delete cluster --name=kubevpro.vnstudio.info --state=s3://kopsstate112211 --yes
		- cmd: choco install kubernetes-helm
		- cmd: helm create vprofilecharts
		- cmd: aws eks list-clusters --region us-east-1
		
		
		
--- GitOps Project:

	- Amazon Container Registry
	- Gibhub -> Project instance -> Setting -> Branches -> Add branch protection rules -> Require a pull request before merging
	- Gibhub -> Project instance -> Pull requests -> New pull request
	- SonarCloud -> New... -> Create new organization -> create one manually
	- SonarCloud -> My Account -> Security -> Generate Token
	- SonarCloud -> My organization -> instance -> Quality Gates -> Create -> Add Condition
	- SonarCloud -> My Projects -> instance -> Administration -> Quality Gate -> Use a specific quality gate
	
	
	- Google search:
	
		terraform aws modules vpc
		
		


	- Command:
	
		- cmd: cd ~/.ssh
		- cmd: export GIT_SSH_COMMAND="ssh -i ~/.ssh/actions"
		- cmd: git config core.sshCommand "ssh -i ~/.ssh/actions -F /dev/null"
		- cmd: git config --global user.name ngoctuan99
		- cmd: git config --global user.email ngoctuanqng1@gmail.com
		- cmd: git fetch upstream
		- cmd: git merge stage
		- cmd: git fetch origin stage
		- cmd: git checkout main
		- cmd: git merge origin/stage
		- cmd: aws eks update-kubeconfig --region us-east-1 --name vprofile-eks
		- cmd: kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		- cmd: helm list
		- cmd: helm uninstall vprofile-stack
		- cmd: terraform init -backend-config="bucket=vprofileactions112211"
		
		
		
	GitOps Project - Sơ đồ tổng thể của GitOps CI/CD pipeline trong môi trường DevOps, kết hợp GitHub,
	Terraform, EKS, Docker, Maven, Helm, SonarCloud:
	
		Visual Studio Code (VS Code):
		
			Là môi trường local IDE của developer.

			Dev sẽ code, commit và push lên GitHub.

			Repo này có thể gồm:

				main branch (sản xuất)

				stage branch (môi trường staging/test)

			💡 Dev làm việc chủ yếu ở stage, sau khi test xong mới merge về main.
			
		GitHub (Git + Actions):
		
			Đây là trung tâm điều phối CI/CD.

			Gồm 2 workflow chính:
			
				Terraform Workflow (bên trái):

					Dành cho hạ tầng (Infrastructure as Code).

					Khi có thay đổi trong repo “IaC” (infrastructure code), GitHub Actions sẽ:

						Fetch stage branch → Lấy code hạ tầng từ nhánh stage.

						Terraform Plan/Test → Kiểm tra xem việc apply có an toàn không, hạ tầng có thay đổi gì.

						Khi PR được merge vào main, thì GitHub Actions chạy:

							terraform apply


								để triển khai (apply) thay đổi lên AWS.
		
				Build, Test & Deploy Workflow (bên phải):

					Dành cho ứng dụng (Application Code).

					Khi code app thay đổi, GitHub Actions sẽ:

						Fetch code từ repo (branch tương ứng).

						Maven → build và chạy unit test (Java project).

						SonarCloud → quét chất lượng code (code quality & security scan).

						Docker → build image và push lên Amazon ECR (Elastic Container Registry).

						Helm Charts → triển khai image mới lên Amazon EKS (Kubernetes cluster).		
		
		AWS Cloud (phần trung tâm)

			Đây là nơi toàn bộ hạ tầng và app chạy.

			Các thành phần chính:

				Amazon ECR: nơi lưu trữ Docker image (container registry của AWS).

				Amazon EKS: Kubernetes cluster — nơi ứng dụng thực tế được deploy.

				VPC Subnet: mạng nội bộ AWS, chứa EKS cluster, ECR, security groups, subnet, etc.

			Mối quan hệ:

				Terraform quản lý hạ tầng AWS (EKS, VPC, ECR,...).

				GitHub Actions quản lý pipeline build và deploy app vào đó.
				
		Quy trình tổng thể:
		
			Giai đoạn	Hành động												Công cụ
			Dev Code	Code và push lên GitHub (stage)							VS Code, Git
			IaC Test	GitHub Actions chạy terraform plan						Terraform
			Merge		Khi PR từ stage → main, GitHub chạy terraform apply		Terraform
			Build		Build app (Maven), test code (SonarCloud)				GitHub Actions
			Image		Docker build → push lên ECR								Docker, ECR
			Deploy		Helm deploy image lên EKS								Helm, Kubernetes
			Run	App 	chạy thực tế trên AWS									AWS EKS
		
		
		
	- Code trong file terraform.yml:

		Case 1:
		
			name: "Vprofile IAC"
			on:
			  push:
				  branches:
					  - main
					  - stage
				  paths:
					  - terraform/**
			  pull_request:
				  branches:
					  - main
				  paths:
					  - terraform/**

			env:
			  # Credentials for deployment to AWS
			  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
			  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
			  # S3 bucket for the Terraform state
			  BUCKET_TF_STATE: ${{ secrets.BUCKET_TF_STATE }}
			  AWS_REGION: us-east-1
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  terraform:
				name: "Apply terraform code changes"
				runs-on: ubuntu-latest
				defaults:
				  run:
					shell: bash
					working-directory: ./terraform
				steps:
				  - name: Checkout source code
					uses: actions/checkout@v2
				  - name: Setup Terraform with specified version on the runner
					uses: hashicorp/setup-terraform@v2
					with:
					  terraform_version: 1.6.3
				  - name: Terraform init
					id: init
					run: terraform init -backend-config="bucket=$BUCKET_TF_STATE"
				  - name: Terraform format
					id: fmt
					run: terraform fmt -check
				  - name: Terraform validate
					id: validate
					run: terraform validate
				  - name: Terraform plan
					id: plan
					run: terraform plan -no-color -input=false -out planfile
				  - name: Terraform plan status
					if: steps.plan.outcome == 'failure'
					run: exit 1

				  - name: Terraform Apply
					id: apple
					if: github.ref == 'refs/heads/main' && github.event_name == 'push'
					run: terraform apply -auto-approve -input=false -parallelism=1 planfile

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					id: getconfig
					if: steps.apple.outcome == 'success'
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Install Ingress controller
					if: steps.apple.outcome == 'success' && steps.getconfig.outcome == 'success'
					run: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml
		
	- Code trong file vproappdep.yml:

		Case 1:
		
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: vproapp
			  labels: 
				app: vproapp
			spec:
			  replicas: 1
			  selector:
				matchLabels:
				  app: vproapp
			  template:
				metadata:
				  labels:
					app: vproapp
				spec:
				  containers:
				  - name: vproapp
					image: "{{ .Values.appimage }}:{{ .Values.apptag }}"
					ports:
					- name: vproapp-port
					  containerPort: 8080
				  initContainers:
				  - name: init-mydb
					image: busybox
					command: ['sh', '-c', 'until nslookup vprodb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']
				  - name: init-memcache
					image: busybox
					command: ['sh', '-c', 'until nslookup vprocache01.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done;']

	- Code trong file main.yml:

		Case 1:
		
			name: vprofile actions
			on: workflow_dispatch
			env:
			  AWS_REGION: us-east-1
			  ECR_REPOSITORY: vprofileapp
			  EKS_CLUSTER: vprofile-eks

			jobs:
			  Testing:
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Maven test
					run: mvn test

				  - name: Checkstyle
					run: mvn checkstyle:checkstyle

				  # Setup java 11 to be default (sonar-scanner requirement as of 5.x)
				  - name: Set Java 11
					uses: actions/setup-java@v3
					with:
					  distribution: 'temurin' # See 'supported distributions' for availabel
					  java-version: '11'

				  # Setup sonar-scanner
				  - name: Setup SonarQube
					uses: warchant/setup-sonar-scanner@v7

				  # Run sonar-scanner
				  - name: SonarQube Scan
					run: sonar-scanner
						-Dsonar.host.url=${{ secrets.SONAR_URL }}
						-Dsonar.login=${{ secrets.SONAR_TOKEN }}
						-Dsonar.organization=${{ secrets.SONAR_ORGANIZATION }}
						-Dsonar.projectKey=${{ secrets.SONAR_PROJECT_KEY }}
						-Dsonar.sources=src/
						-Dsonar.junit.reportsPath=target/surefire-reports/
						-Dsonar.jacoco.reportsPath=target/jacoco.exec
						-Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml
						-Dsonar.java.binaries=target/test-classes/com/visualpathit/account

				  # Check the Quality Gate status.
				  - name: SonarQube Quality Gate check
					id: sonarqube-quality-gate-check
					uses: sonarsource/sonarqube-quality-gate-action@master
					# Force to fail step after specific time.
					timeout-minutes: 5
					env:
					  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
					  SONAR_HOST_URL: ${{ secrets.SONAR_URL }} # OPTIONAL

			  BUILD_AND_PUBLISH:
				needs: Testing
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Build & Upload image to ECR
					uses: appleboy/docker-ecr-action@master
					with:
						access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
						secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
						registry: ${{ secrets.REGISTRY }}
						repo: ${{ env.ECR_REPOSITORY }}
						region: ${{ env.AWS_REGION }}
						tags: latest,${{ github.run_number }}
						daemon_off: false
						dockerfile: ./Dockerfile
						context: ./

			  DeployToEKS:
				needs: BUILD_AND_PUBLISH
				runs-on: ubuntu-latest
				steps:
				  - name: Code checkout
					uses: actions/checkout@v4

				  - name: Configure AWS credentials
					uses: aws-actions/configure-aws-credentials@v1
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ env.AWS_REGION }}

				  - name: Get Kube config file
					run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

				  - name: Print config file
					run: cat ~/.kube/config

				  - name: Login to ECR
					run: |
					  kubectl delete secret regcred --ignore-not-found
					  kubectl create secret docker-registry regcred --docker-server=${{ secrets.REGISTRY }} --docker-username=AWS --docker-password=$(aws ecr get-login-password)

				  - name: Deploy Helm
					uses: bitovi/github-actions-deploy-eks-helm@v1.2.8
					with:
					  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
					  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
					  aws-region: ${{ secrets.AWS_REGION }}
					  cluster-name: ${{ env.EKS_CLUSTER }}
					  #configfiles: .github/values/dev.yaml
					  chart-path: helm/vprofilecharts
					  namespace: default
					  value: appimage=${{ secrets.REGISTRY }}/${{ env.ECR_REPOSITORY }},apptag=${{ github.run_number }}
					  name: vprofile-stack
		
		
		
		
		