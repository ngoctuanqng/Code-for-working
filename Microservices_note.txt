- @MappedSuperclass
- @Validated
- @Valid
- @Pattern
- @AllArgsConstructor
- @Positive
- @Tag
- Github Webhook


--- Config server:

		Để auto refresh khi thay đổi thì cần dùng rabbitmq và webhook
		
		Lệnh start  rabbitmq container:
		
			docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.12-management

		Khi không dùng chung 1 configserver:
		
			spring:
			  config:
				import:
				  - "application_qa.yml"
				  - "application_prod.yml"
			  profiles:
				active:
				  - "qa"
				  
		Khi dùng chung 1 configserver:
			
			configserver:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-config-server</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-config-monitor</artifactId>
				</dependency>	
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-bus-amqp</artifactId>
				</dependency>			
					
				spring:
				  cloud:
					config:
					  server:
						# native:
						  # search-locations: "classpath:/config"
						  # search-locations: "file:///Users//eazybytes//Documents//config"
						git:
						  uri: "https://github.com/eazybytes/eazybytes-config.git"
						  default-label: main
						  timeout: 5
						  clone-on-start: true
						  force-pull: true
				  rabbitmq:
					host: "localhost"
					port: 5672
					username: "guest"
					password: "guest"
				management:
				  endpoints:
					web:
					  exposure:
						include: "*"
						
			accounts:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-config</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-bus-amqp</artifactId>
				</dependency>		
				
				spring:
				  config:
					import: "optional:configserver:http://localhost:8071/"
				  rabbitmq:
					host: "localhost"
					port: 5672
					username: "guest"
					password: "guest"
				management:
				  endpoints:
					web:
					  exposure:
						include: "*"
						
		Ở đây chú ý phải start configserver trước, sau đó mới start accounts

		application.yml của accounts bắt buộc phải define spring.name=accounts, vì như vậy thì bên configserver mới
		bắt đúng được cho các file yml của accounts

		Sẽ luôn có 2 cái, 1 là configserver trong đó sẽ cài server config, và 1 cái là client config sẽ tham chiếu đến server
		config thông qua config.import=... để lấy config, nếu nó k tham chiếu thì sẽ mặc định lấy các file trong service của nó
		
		cards (pom.xml) Không có dependency spring-cloud-starter-config. Do đó khi chạy, Spring Boot không hiểu
		prefix configserver: trong SPRING_CONFIG_IMPORT → báo lỗi
		
		Ta dung spring-cloud-config-monitor, để monitor nó cần 1 amqp, nếu ta tắt đi rabbitmq thì sẽ sinh ra lỗi
		
		Cơ chế đọc spring.config.import:
		
			Với các property đơn lẻ (ví dụ server.port), thì env var > application.yml → sẽ ghi đè.

			Nhưng với property dạng list accumulator (như spring.config.import), Spring Boot sẽ nối chuỗi lại thay vì ghi đè.

			Cho nên localhost không bị bỏ đi, mà vẫn tồn tại song song.
		
			application.yml (bên trong JAR/service):

				spring:
				  config:
					import: "optional:configserver:http://localhost:8071"


			common-config.yml (Docker Compose env):

				environment:
				  SPRING_CONFIG_IMPORT=configserver:http://configserver:8071/


			Spring Boot sẽ merge hai nguồn lại, tạo thành danh sách:

				spring.config.import = [
				  "optional:configserver:http://localhost:8071",
				  "configserver:http://configserver:8071/"
				]
						
--- Using MySQL DBs inside microservices:

	application.yml:
	
		spring:
		  datasource:
			url: jdbc:mysql://localhost:3306/accountsdb
			username: root
			password: root
		  jpa:
			show-sql: true
		  sql:
			init:
			
	schema.sql:
	
		CREATE TABLE IF NOT EXISTS `customer` (
		  `customer_id` int AUTO_INCREMENT  PRIMARY KEY,
		  `name` varchar(100) NOT NULL,
		  `email` varchar(100) NOT NULL,
		  `mobile_number` varchar(20) NOT NULL,
		  `created_at` date NOT NULL,
		  `created_by` varchar(20) NOT NULL,
		  `updated_at` date DEFAULT NULL,
			`updated_by` varchar(20) DEFAULT NULL
		);

		CREATE TABLE IF NOT EXISTS `accounts` (
		  `customer_id` int NOT NULL,
		   `account_number` int AUTO_INCREMENT  PRIMARY KEY,
		  `account_type` varchar(100) NOT NULL,
		  `branch_address` varchar(200) NOT NULL,
		  `created_at` date NOT NULL,
		   `created_by` varchar(20) NOT NULL,
		   `updated_at` date DEFAULT NULL,
			`updated_by` varchar(20) DEFAULT NULL
		);		
			
	pom.xml:
	
		<dependency>
			<groupId>com.mysql</groupId>
			<artifactId>mysql-connector-j</artifactId>
			<scope>runtime</scope>
		</dependency>

	common-config.yml:
	
		services:
		  network-deploy-service:
			networks:
			  - eazybank

		  microservice-db-config:
			extends:
			  service: network-deploy-service
			image: mysql
			healthcheck:
			  test: [ "CMD", "mysqladmin" ,"ping", "-h", "localhost" ]
			  timeout: 10s
			  retries: 10
			  interval: 10s
			  start_period: 10s
			environment:
			  MYSQL_ROOT_PASSWORD: root
			  
	docker-compose.yml:
	
		services:
		  accountsdb:
			container_name: accountsdb
			ports:
			  - 3306:3306
			environment:
			  MYSQL_DATABASE: accountsdb
			extends:
			  file: common-config.yml
			  service: microservice-db-config
			  
		  accounts:
			image: "eazybytes/accounts:s7"
			container_name: accounts-ms
			ports:
			  - "8080:8080"
			environment:
			  SPRING_APPLICATION_NAME: "accounts"
			  SPRING_DATASOURCE_URL: "jdbc:mysql://accountsdb:3306/accountsdb"
			depends_on:
			  accountsdb:
				condition: service_healthy
			  configserver:
				condition: service_healthy
			extends:
			  file: common-config.yml
			  service: microservice-configserver-config
			  
	Cổng a:b với mysql thì b sẽ thường là 3306, đây là cổng mặc định, còn cổng a thì có thể tùy biến
	
	spring.jpa.sql.init.mode=always là một thuộc tính cấu hình trong Spring Boot dùng để điều khiển việc khởi tạo dữ
	liệu SQL (chạy các file schema.sql, data.sql, hoặc các file tùy chỉnh) khi ứng dụng khởi động.
	
	Trong kiến trúc microservices, database thường được coi là một service riêng biệt và có thể chạy trong một
	container riêng (thường là Docker container).
			  
--- Service Discovery & Service Registration in microservices:

		eurekaserver:
		
			pom.xml:
		
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
				</dependency>
				
			eurekaserver.yml:
			
				eureka:
				  instance:
					hostname: localhost
				  client:
					fetchRegistry: false
					registerWithEureka: false
					serviceUrl:
					  defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/
				
			EurekaserverApplication.java:
			
				@SpringBootApplication
				@EnableEurekaServer
				public class EurekaserverApplication {

					public static void main(String[] args) {
						SpringApplication.run(EurekaserverApplication.class, args);
					}

				}
				
		accounts:
		
			pom.xml:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-openfeign</artifactId>
				</dependency>
				
			application.yml:
			
				eureka:
				  instance:
					preferIpAddress: true
				  client:
					fetchRegistry: true
					registerWithEureka: true
					serviceUrl:
					  defaultZone: http://localhost:8070/eureka/
					  
			AccountsApplication.java:
			
				@EnableFeignClients
					  
			CardsFeignClient.java:
			
				@FeignClient("cards")
				public interface CardsFeignClient {

					@GetMapping(value = "/api/fetch",consumes = "application/json")
					public ResponseEntity<CardsDto> fetchCardDetails(@RequestParam String mobileNumber);

				}
				
			CustomersServiceImpl.java:
			
				private CardsFeignClient cardsFeignClient;
			
				@Override
				public CustomerDetailsDto fetchCustomerDetails(String mobileNumber) {
					...

					ResponseEntity<CardsDto> cardsDtoResponseEntity = cardsFeignClient.fetchCardDetails(mobileNumber);
					
					...

				}
				
--- Gateway, Routing & Cross cutting concerns in Microservices:

	gatewayserver:
	
		pom.xml:
	
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-starter-gateway</artifactId>
			</dependency>
			
		application.yml:
		
			spring:
			  cloud:
				gateway:
				  discovery:
					locator:
					  enabled: false
					  lowerCaseServiceId: true
					  
			management:
			  endpoints:
				web:
				  exposure:
					include: "*"
			  endpoint:
				gateway:
				  access: unrestricted
			  info:
				env:
				  enabled: true
					  
				Phần này chú ý có update liên quan đến webflux với version mới
				
		GatewayserverApplication.java:
		
			@SpringBootApplication
			public class GatewayserverApplication {

				public static void main(String[] args) {
					SpringApplication.run(GatewayserverApplication.class, args);
				}

				@Bean
				public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
					return routeLocatorBuilder.routes()
									.route(p -> p
											.path("/eazybank/accounts/**")
											.filters( f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment}")
													.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
											.uri("lb://ACCOUNTS"))
								.route(p -> p
										.path("/eazybank/loans/**")
										.filters( f -> f.rewritePath("/eazybank/loans/(?<segment>.*)","/${segment}")
												.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
										.uri("lb://LOANS"))
								.route(p -> p
										.path("/eazybank/cards/**")
										.filters( f -> f.rewritePath("/eazybank/cards/(?<segment>.*)","/${segment}")
												.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
										.uri("lb://CARDS")).build();
				}
			}
			
		FilterUtility.java:
		
			@Component
			public class FilterUtility {

				public static final String CORRELATION_ID = "eazybank-correlation-id";

				public String getCorrelationId(HttpHeaders requestHeaders) {
					if (requestHeaders.get(CORRELATION_ID) != null) {
						List<String> requestHeaderList = requestHeaders.get(CORRELATION_ID);
						return requestHeaderList.stream().findFirst().get();
					} else {
						return null;
					}
				}

				public ServerWebExchange setRequestHeader(ServerWebExchange exchange, String name, String value) {
					return exchange.mutate().request(exchange.getRequest().mutate().header(name, value).build()).build();
				}

				public ServerWebExchange setCorrelationId(ServerWebExchange exchange, String correlationId) {
					return this.setRequestHeader(exchange, CORRELATION_ID, correlationId);
				}

			}
			
		RequestTraceFilter.java:
		
			@Order(1)
			@Component
			public class RequestTraceFilter implements GlobalFilter {

				private static final Logger logger = LoggerFactory.getLogger(RequestTraceFilter.class);

				@Autowired
				FilterUtility filterUtility;

				@Override
				public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
					HttpHeaders requestHeaders = exchange.getRequest().getHeaders();
					if (isCorrelationIdPresent(requestHeaders)) {
						logger.debug("eazyBank-correlation-id found in RequestTraceFilter : {}",
								filterUtility.getCorrelationId(requestHeaders));
					} else {
						String correlationID = generateCorrelationId();
						exchange = filterUtility.setCorrelationId(exchange, correlationID);
						logger.debug("eazyBank-correlation-id generated in RequestTraceFilter : {}", correlationID);
					}
					return chain.filter(exchange);
				}

				private boolean isCorrelationIdPresent(HttpHeaders requestHeaders) {
					if (filterUtility.getCorrelationId(requestHeaders) != null) {
						return true;
					} else {
						return false;
					}
				}

				private String generateCorrelationId() {
					return java.util.UUID.randomUUID().toString();
				}

			}
			
		ResponseTraceFilter.java:
		
			@Configuration
			public class ResponseTraceFilter {

				private static final Logger logger = LoggerFactory.getLogger(ResponseTraceFilter.class);

				@Autowired
				FilterUtility filterUtility;

				@Bean
				public GlobalFilter postGlobalFilter() {
					return (exchange, chain) -> {
						return chain.filter(exchange).then(Mono.fromRunnable(() -> {
							HttpHeaders requestHeaders = exchange.getRequest().getHeaders();
							String correlationId = filterUtility.getCorrelationId(requestHeaders);
							logger.debug("Updated the correlation id to the outbound headers: {}", correlationId);
							exchange.getResponse().getHeaders().add(filterUtility.CORRELATION_ID, correlationId);
						}));
					};
				}
			}
			
	Với các version maven mới thì dùng như sau:
	
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-gateway-server-webflux</artifactId>
		</dependency>
		
		spring:
		  cloud:
			gateway:
			  server:
				webflux:
				  discovery:
					locator:
					  enabled: true
					  lowerCaseServiceId: true	

	Code config đường dẫn trong gateway:

		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment"))
						.uri("lb://ACCOUNTS"))
		
			routeLocatorBuilder.routes()
			
				Đây là cách định nghĩa các route trong Spring Cloud Gateway.

				Mỗi route sẽ ánh xạ từ một request client gửi vào Gateway → đến một service đích (thông qua uri).
				
			.route(p -> p.path("/eazybank/accounts/**") ... )
			
				Nghĩa là bất kỳ request nào có path bắt đầu bằng /eazybank/accounts/ sẽ match route này.
				
			.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}"))
			
				Đây là filter rewrite path, nó dùng Regex để đổi lại đường dẫn trước khi forward về service đích.
				
				(?<segment>.*) nghĩa là lấy tất cả phần còn lại sau /eazybank/accounts/ và gán vào biến segment.
				
				/${segment} Tức là khi forward, chỉ giữ lại phần segment.
				
					Ví dụ:
					
						Request đến Gateway: /eazybank/accounts/api/create
						
						Sau khi rewrite: /api/create
						
			.uri("lb://ACCOUNTS")
			
				lb:// = Load Balancer (tức là sẽ dùng Service Discovery như Eureka/Consul).
				
				"ACCOUNTS" = tên service đã đăng ký trên Service Registry.

				Gateway sẽ forward request đến một instance của service ACCOUNTS.
				
		Trong container, localhost chỉ trỏ đến chính container gatewayserver, không phải Keycloak.

		Bạn đang chạy Keycloak ở container riêng (keycloak), cổng nội bộ của nó là 8080, còn khi ra host thì bạn map 7080:8080.

		→ Vì gatewayserver cũng chạy trong Docker network, nên phải dùng DNS name của service Keycloak.
		
			SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI: "http://keycloak:8080/realms/master/protocol/openid-connect/certs"

		Nếu chạy local (gatewayserver ở IntelliJ, Keycloak container) thì URI phải là:
		
			http://localhost:7080/realms/master/protocol/openid-connect/certs
		
--- Making Microservices Resilient:

		gatewayserver:
		
			pom.xml:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-circuitbreaker-reactor-resilience4j</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.boot</groupId>
					<artifactId>spring-boot-starter-data-redis-reactive</artifactId>
				</dependency>
				
			application.yml:
			
				spring:
				  cloud:
					gateway:
					  discovery:
						locator:
						  enabled: false
						  lowerCaseServiceId: true
					  httpclient:
						connect-timeout: 1000
						response-timeout: 2s
				  data:
					redis:
					  connect-timeout: 2s
					  host: localhost
					  port: 6379
					  timeout: 1s
			
				resilience4j.circuitbreaker:
				  configs:
					default:
					  slidingWindowSize: 10
					  permittedNumberOfCallsInHalfOpenState: 2
					  failureRateThreshold: 50
					  waitDurationInOpenState: 10000
					  
			GatewayserverApplication.java:
			
				@SpringBootApplication
				public class GatewayserverApplication {

					public static void main(String[] args) {
						SpringApplication.run(GatewayserverApplication.class, args);
					}

					@Bean
					public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
						return routeLocatorBuilder.routes()
										.route(p -> p
												.path("/eazybank/accounts/**")
												.filters( f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment}")
														.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
														.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
																.setFallbackUri("forward:/contactSupport")))
												.uri("lb://ACCOUNTS"))
									.route(p -> p
											.path("/eazybank/loans/**")
											.filters( f -> f.rewritePath("/eazybank/loans/(?<segment>.*)","/${segment}")
													.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
													.retry(retryConfig -> retryConfig.setRetries(3)
															.setMethods(HttpMethod.GET)
															.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true)))
											.uri("lb://LOANS"))
									.route(p -> p
											.path("/eazybank/cards/**")
											.filters( f -> f.rewritePath("/eazybank/cards/(?<segment>.*)","/${segment}")
													.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
													.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
															.setKeyResolver(userKeyResolver())))
											.uri("lb://CARDS")).build();
					}

					@Bean
					public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
						return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
								.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
								.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
					}

					@Bean
					public RedisRateLimiter redisRateLimiter() {
						return new RedisRateLimiter(1, 1, 1);
					}

					@Bean
					KeyResolver userKeyResolver() {
						return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
								.defaultIfEmpty("anonymous");
					}

				}
				
		FallbackController.java:
		
			@RestController
			public class FallbackController {

				@RequestMapping("/contactSupport")
				public Mono<String> contactSupport() {
					return Mono.just("An error occurred. Please try after some time or contact support team!!!");
				}

			}
				
	accounts:
	
		pom.xml:
	
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-starter-circuitbreaker-resilience4j</artifactId>
			</dependency>
			
		application.yml:
		
			spring:
			  cloud:
				openfeign:
				  circuitbreaker:
					enabled: true
		
			resilience4j.circuitbreaker:
			  configs:
				default:
				  slidingWindowSize: 10
				  permittedNumberOfCallsInHalfOpenState: 2
				  failureRateThreshold: 50
				  waitDurationInOpenState: 10000

			resilience4j.retry:
			  configs:
				default:
				  maxAttempts: 3
				  waitDuration: 500
				  enableExponentialBackoff: true
				  exponentialBackoffMultiplier: 2
				  ignoreExceptions:
					- java.lang.NullPointerException
				  retryExceptions:
					- java.util.concurrent.TimeoutException

			resilience4j.ratelimiter:
			  configs:
				default:
				  timeoutDuration: 1000
				  limitRefreshPeriod: 5000
				  limitForPeriod: 1
				  
	Circuit Breaker:
	
		Chu trình trạng thái Circuit Breaker:
		
			CLOSED:
			
				Mọi request đều được phép qua.

				Nếu failureRateThreshold bị vượt qua → chuyển sang OPEN.
				
			OPEN:
			
				Mọi request bị chặn.

				Sau waitDurationInOpenState → chuyển sang HALF_OPEN.
				
			HALF_OPEN:
			
				Chỉ permittedNumberOfCallInHalfOpenState request được cho phép.

				Nếu chúng thành công → chuyển về CLOSED.

				Nếu có lỗi → quay lại OPEN.

		resilience4j.circuitbreaker:
		  configs:
			default:
			  slidingWindowSize: 10
			  permittedNumberOfCallInHalfOpenState: 2
			  failureRateThreshold: 50
			  waitDurationInOpenState: 10000
			  
			 slidingWindowSize
			 
				Kích thước cửa sổ trượt (số lượng cuộc gọi gần nhất để theo dõi)
				
				Circuit breaker sẽ quan sát 10 request gần nhất để tính toán tỷ lệ lỗi.
				
			permittedNumberOfCallInHalfOpenState
			
				Số lượng request được phép đi qua trong trạng thái HALF_OPEN
				
				Khi circuit breaker chuyển sang trạng thái HALF_OPEN, nó sẽ chỉ cho phép 2 request
				thử nghiệm đi qua. Nếu cả 2 đều thành công, circuit breaker sẽ trở lại CLOSED.
				
			failureRateThreshold
			
				Ngưỡng lỗi để mở circuit
				
				Nếu có hơn 50% request bị lỗi trong slidingWindowSize (tức là 5 lỗi/10 request),
				circuit breaker sẽ chuyển sang OPEN.
				
			waitDurationInOpenState
			
				Thời gian chờ trước khi chuyển từ OPEN sang HALF_OPEN (tính bằng milliseconds)
				
				Circuit breaker sẽ giữ trạng thái OPEN trong 10 giây, rồi mới chuyển sang HALF_OPEN để thử lại.
				
		@Bean
		public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
			return routeLocatorBuilder.routes()
					.route(p -> p.path("/eazybank/accounts/**")
							.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment}")
									.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
									.retry(retryconfig -> retryconfig.setRetries(3)
											.setMethods(HttpMethod.GET)
											.setBackoff(Duration.ofMillis(100), Duration.ofMillis(1000), 2, true))
									.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
											.setFallbackUri("forward:/contactSupport"))
									)
							.uri("lb://ACCOUNTS"))
		}
				
		Ví dụ thực tế:
		
			Bạn có một service gọi đến một hệ thống bên ngoài (ví dụ: accountsService)

			Cấu hình trên giúp bảo vệ hệ thống khỏi việc liên tục gọi vào một service đang "chết"

			Nếu 5/10 request gần nhất lỗi → Circuit breaker sẽ ngăn không cho gọi tiếp trong 10 giây

			Sau đó thử lại 2 request → nếu ổn → trở lại bình thường
			
		Mục tiêu chính của Circuit Breaker

			Giảm thiểu downtime của hệ thống

			Ngăn ngừa gọi lặp vào service đang lỗi, tránh quá tải hoặc treo toàn hệ thống

			Tăng khả năng phục hồi (resilience) của hệ thống
			
		@Bean
		public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
			return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
					.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
							.timeLimiterConfig((TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build())).build());
		}
		
			Đây là một đoạn cấu hình trong Spring Boot (Reactive Web) dùng để custom mặc định cấu hình cho Circuit
			Breaker khi sử dụng Resilience4j với ReactiveResilience4JCircuitBreakerFactory.
			
			Customizer<ReactiveResilience4JCircuitBreakerFactory>

				Là một hàm tùy biến (customizer) cho factory tạo ra các circuit breaker.

				Cho phép bạn cấu hình mặc định cho tất cả circuit breaker sử dụng factory này.
				
			factory.configureDefault(...)

				Dùng để thiết lập cấu hình mặc định cho mọi circuit breaker được tạo bởi factory.

				id là tên (identifier) của circuit breaker.
				
			new Resilience4JConfigBuilder(id)

				Tạo builder cho cấu hình resilience4j, sử dụng id để đặt tên.
				
			.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())

				Áp dụng cấu hình Circuit Breaker mặc định:

				failureRateThreshold: 50%

				slidingWindowSize: 100

				minimumNumberOfCalls: 100

				waitDurationInOpenState: 60s

				permittedNumberOfCallsInHalfOpenState: 10

				v.v...

				Bạn có thể override các giá trị này nếu muốn tinh chỉnh.
				
			.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build())

				Áp dụng cấu hình Time Limiter:

				Nếu request mất hơn 4 giây → tự động timeout.

				Bảo vệ service khỏi bị treo khi gọi service chậm.
				
			.build()

				Kết thúc builder → tạo ra đối tượng cấu hình hoàn chỉnh.
			
	Retry Pattern:
	
		Là một kỹ thuật trong thiết kế hệ thống phân tán để xử lý các lỗi tạm thời (transient failures) như:

			Network timeout

			Service tạm thời không phản hồi

			Kết nối DB/API bị reset

		Thay vì thất bại ngay, hệ thống sẽ tự động thử lại sau một khoảng thời gian nhất định.

		Tác dụng của Retry Pattern
		
			Tăng độ tin cậy	Hệ thống không dễ thất bại khi gặp lỗi tạm thời
			
			Giảm lỗi giả	Thử lại giúp vượt qua các lỗi do trễ mạng, quá tải
			
			Kết hợp với Circuit Breaker	Tránh spam service lỗi → chặn lại khi retry vẫn không hiệu quả
			
		resilience4j.retry:
		  configs:
			default:
			  maxRetryAttemps: 3
			  waitDuration: 100
			  enableExponentialBackoff: true
			  exponentialBackoffMutiplier: 2
			  ignoreExceptions:
				- java.lang.NullPointerException
			  retryException:
				- java.util.concurrent.TimeoutException
				
			maxRetryAttemps: 3
			
				Tối đa thử lại 3 lần (tính cả lần đầu tiên là lần gọi đầu).
				
				Nếu request thất bại, sẽ retry thêm 2 lần nữa.

			waitDuration: 100
			
				Thời gian chờ giữa mỗi lần retry là 100ms (milliseconds).
				
				Nhưng nếu có exponential backoff, giá trị này sẽ được nhân dần.

			enableExponentialBackoff: true
			
				Kích hoạt Exponential Backoff → mỗi lần retry sẽ chờ lâu hơn theo cấp số nhân.

			exponentialBackoffMutiplier: 2
			
				Mỗi lần retry, thời gian chờ sẽ nhân đôi:

					Retry 1: 100ms

					Retry 2: 200ms

					Retry 3: 400ms
				
				Điều này giúp giảm áp lực lên hệ thống khi lỗi đang xảy ra liên tục.
				
			ignoreExceptions
			
				Nếu một trong các exception liệt kê ở đây xảy ra, sẽ không retry, mà fail luôn.
				
				Trong ví dụ: NullPointerException sẽ bị bỏ qua retry (vì là lỗi lập trình, không phải lỗi tạm thời).

			retryException
			
				Chỉ retry khi xảy ra TimeoutException.

	Rate limiter:
	
		Giới hạn số lượng request được gửi đến hệ thống trong một khoảng thời gian, để ngăn người dùng
		hoặc bot lạm dụng API.
	
		@Bean
		public RedisRateLimiter redisRateLimiter() {
			return new RedisRateLimiter(1, 1, 1);
		}
		
			Đây là cấu hình rate limiter dựa trên Redis.
			
			Tham số
				
				replenishRate		Số token thêm vào mỗi giây (tốc độ nạp lại token)
				
				burstCapacity		Tổng số token tối đa được lưu trữ (để xử lý burst traffic)
				
				requestedTokens		Số token cần mỗi lần request
				
			Mỗi giây người dùng chỉ được 1 request (replenishRate = 1)

			Không được phép gửi nhiều hơn 1 request cùng lúc (burstCapacity = 1)

			Mỗi request dùng 1 token (requestedTokens = 1)
		

		@Bean
		KeyResolver userKeyResolver() {
			return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
					.defaultIfEmpty("anonymous");
		}
		
			Dùng để xác định key cho từng người dùng mà RedisRateLimiter sẽ áp dụng hạn chế.
				
			Hoạt động:

				Đọc header user trong request HTTP

				Nếu có: lấy giá trị làm key

				Nếu không có: dùng "anonymous"			
				  
--- Observability and monitoring of microservices:

		gatewayserver:
		
			pom.xml:
			
				<dependency>
					<groupId>io.opentelemetry.javaagent</groupId>
					<artifactId>opentelemetry-javaagent</artifactId>
					<version>${otelVersion}</version>
					<scope>runtime</scope>
				</dependency>
				<dependency>
					<groupId>io.micrometer</groupId>
					<artifactId>micrometer-registry-prometheus</artifactId>
				</dependency>
				
		application.yml:
		
			management:
			  endpoints:
				web:
				  exposure:
					include: "*"
			  endpoint:
				gateway:
				  access: unrestricted
			  info:
				env:
				  enabled: true
			  metrics:
				tags:
				  application: ${spring.application.name}

			logging:
			  level:
				com:
				  eazybytes:
					gatewayserver: DEBUG
			  pattern:
				level: "%5p [${spring.application.name},%X{trace_id},%X{span_id}]"
				
	accounts:
	
		pom.xml:
		
			<dependency>
				<groupId>io.opentelemetry.javaagent</groupId>
				<artifactId>opentelemetry-javaagent</artifactId>
				<version>${otelVersion}</version>
				<scope>runtime</scope>
			</dependency>
			<dependency>
				<groupId>io.micrometer</groupId>
				<artifactId>micrometer-registry-prometheus</artifactId>
			</dependency>
			
		application.yml:
		
			management:
			  endpoints:
				web:
				  exposure:
					include: "*"
			  endpoint:
				shutdown:
				  access: unrestricted
				health:
				  probes:
					enabled: true
			  info:
				env:
				  enabled: true
			  metrics:
				tags:
				  application: ${spring.application.name}
				  
			logging:
			  level:
				com:
				  eazybytes:
					accounts: DEBUG
			  pattern:
				level: "%5p [${spring.application.name},%X{trace_id},%X{span_id}]"
				
	Alloy

		Tác dụng:
		
			Alloy là một công cụ được sử dụng để kết nối và thu thập logs và metrics từ các dịch vụ khác trong
			hệ thống. Nó cung cấp một giao diện để giám sát các dịch vụ và giúp dễ dàng truy xuất và phân tích logs.

		Mối liên hệ:
		
			Alloy có thể gửi các logs thu thập được từ các microservices (hoặc các dịch vụ khác như Minio) đến
			hệ thống giám sát như Prometheus hoặc Grafana. Thông qua việc cấu hình đúng các công cụ, Alloy sẽ giúp cung cấp
			các logs chi tiết về hoạt động của các microservices.

	Minio

		Tác dụng:
		
			Minio là một dịch vụ lưu trữ đối tượng (object storage) tương tự như AWS S3, thường được sử dụng để lưu
			trữ logs, metrics, hoặc các file dữ liệu lớn trong các hệ thống microservice. Minio có thể lưu trữ các logs từ các
			dịch vụ khác như Promtail, Alloy, hoặc các microservices khác.

		Mối liên hệ:
		
			Minio thường kết hợp với Promtail để lưu trữ logs hoặc các dữ liệu không cấu trúc khác từ các dịch vụ
			trong hệ thống. Ngoài ra, Minio có thể là nơi chứa các bản sao của dữ liệu metrics từ Prometheus.

	Prometheus

		Tác dụng:
		
			Prometheus là một hệ thống giám sát và thu thập metrics mạnh mẽ, có khả năng lưu trữ thời gian thực và thực
			hiện truy vấn dữ liệu. Nó thu thập metrics từ các microservices và hệ thống phụ trợ thông qua các endpoints HTTP cung
			cấp các số liệu thống kê như số lượng request, thời gian phản hồi, tỷ lệ lỗi, v.v.

		Mối liên hệ:
		
			Prometheus có thể thu thập dữ liệu từ các microservices hoặc từ Alloy nếu Alloy được cấu hình để xuất
			metrics. Prometheus sẽ lưu trữ các metrics này và cung cấp chúng cho Grafana để trực quan hóa và theo dõi tình trạng hệ thống.

	Grafana

		Tác dụng:
		
			Grafana là công cụ trực quan hóa dữ liệu phổ biến. Grafana sử dụng dữ liệu từ các nguồn như Prometheus để tạo
			ra các dashboard tương tác, cho phép theo dõi và phân tích các chỉ số của hệ thống và các dịch vụ microservice. Các dashboards
			này có thể bao gồm các biểu đồ về hiệu suất hệ thống, số liệu logs, và nhiều dữ liệu khác.

		Mối liên hệ:
		
			Grafana lấy dữ liệu từ Prometheus và hiển thị chúng trên các dashboard. Người dùng có thể theo dõi các chỉ số
			như độ trễ, số lượng request, tài nguyên sử dụng, lỗi, và nhiều chỉ số khác trong thời gian thực. Grafana cũng có thể tích
			hợp với các công cụ khác như Minio để theo dõi trạng thái lưu trữ đối tượng.

	Promtail

		Tác dụng:
		
			Promtail là một công cụ thu thập logs từ các dịch vụ và gửi chúng đến Loki (một hệ thống lưu trữ logs tương tự như
			Prometheus nhưng dành riêng cho logs). Promtail giúp thu thập logs từ các container Docker hoặc các dịch vụ khác trong hệ
			thống microservice và gửi chúng đến Loki để phân tích.

		Mối liên hệ:
		
			Promtail tích hợp với Loki để thu thập logs và gửi chúng đến hệ thống phân tích. Grafana có thể sử dụng Loki
			làm nguồn dữ liệu và hiển thị các logs trong các dashboard, kết hợp với các metrics từ Prometheus.
			
	Loki:
	
		Tác dụng:
	
			Loki giống như Prometheus, nhưng thay vì lưu trữ metrics (chỉ số hiệu suất), Loki lưu trữ logs. Điều này giúp bạn dễ
			dàng truy vấn và phân tích logs của các dịch vụ trong hệ thống.

			Loki có thể lưu trữ logs từ các dịch vụ khác nhau, bao gồm logs từ các microservices, Docker containers, và các dịch
			vụ backend khác.
			
		Mối liên hệ:
		
			Grafana sử dụng Loki làm một nguồn dữ liệu logs. Khi bạn kết hợp Loki với Grafana, bạn có thể tạo ra các dashboards
			hiển thị logs theo thời gian thực, giúp dễ dàng theo dõi và phân tích các sự kiện xảy ra trong hệ thống.
		
	Mối Quan Hệ Giữa Các Công Cụ:

		Prometheus và Grafana:

			Prometheus thu thập và lưu trữ metrics từ các dịch vụ trong hệ thống (microservices).

			Grafana sử dụng dữ liệu từ Prometheus để trực quan hóa các chỉ số này, giúp giám sát và phân tích hiệu suất của hệ thống.

		Loki và Promtail:

			Promtail thu thập logs từ các dịch vụ trong hệ thống và gửi chúng đến Loki.

			Grafana sử dụng Loki để hiển thị logs trên các dashboard, giúp theo dõi và phân tích logs của các dịch vụ.

		Minio:

			Minio là nơi lưu trữ logs và metrics (có thể từ Promtail và Prometheus).

			Minio có thể lưu trữ các file dữ liệu lớn hoặc backup cho các logs và metrics, giúp dễ dàng phục hồi hoặc phân tích sau này.

		Alloy:

			Alloy thu thập logs và metrics từ các dịch vụ khác và gửi chúng cho các công cụ giám sát như Prometheus và Grafana.

			Alloy giúp kết nối các dịch vụ trong hệ thống và cung cấp các thông tin chi tiết về hiệu suất và logs.
			
		Loki và Promtail:

			Promtail thu thập logs từ các dịch vụ và gửi chúng đến Loki.

			Loki sẽ phân loại các logs theo các labels (giống như cách Prometheus phân loại metrics) để dễ dàng
			truy vấn và phân tích.

		Loki và Grafana:

			Grafana sử dụng Loki làm nguồn dữ liệu để trực quan hóa logs, giúp người dùng dễ dàng theo dõi các sự
			kiện và lỗi trong hệ thống microservices.

		Loki và Prometheus:

			Prometheus thu thập metrics (chỉ số hiệu suất) và Loki thu thập logs. Trong Grafana, bạn có thể kết hợp
			dữ liệu từ cả Prometheus và Loki vào cùng một dashboard, giúp bạn có cái nhìn toàn diện về hiệu suất và
			logs của hệ thống.
			
			
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-registry-prometheus</artifactId>
    </dependency>
	
		Để có thể mở được actuator/prometheus thì cần dependency sau
				
--- Microservices Security:

		gatewayserver:
		
			pom.xml:

				<dependency>
					<groupId>org.springframework.boot</groupId>
					<artifactId>spring-boot-starter-security</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.security</groupId>
					<artifactId>spring-security-oauth2-resource-server</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.security</groupId>
					<artifactId>spring-security-oauth2-jose</artifactId>
				</dependency>
				
		application.yml:
		
			spring:
			  security:
				oauth2:
				  resourceserver:
					jwt:
					  jwk-set-uri: "http://localhost:7080/realms/master/protocol/openid-connect/certs"
					  
		KeycloakRoleConverter.java:
		
			public class KeycloakRoleConverter  implements Converter<Jwt, Collection<GrantedAuthority>> {

				@Override
				public Collection<GrantedAuthority> convert(Jwt source) {
					Map<String, Object> realmAccess = (Map<String, Object>) source.getClaims().get("realm_access");
					if (realmAccess == null || realmAccess.isEmpty()) {
						return new ArrayList<>();
					}
					Collection<GrantedAuthority> returnValue = ((List<String>) realmAccess.get("roles"))
							.stream().map(roleName -> "ROLE_" + roleName)
							.map(SimpleGrantedAuthority::new)
							.collect(Collectors.toList());
					return returnValue;
				}
			}
			
		SecurityConfig.java:
		
			@Configuration
			@EnableWebFluxSecurity
			public class SecurityConfig {

				@Bean
				public SecurityWebFilterChain springSecurityFilterChain(ServerHttpSecurity serverHttpSecurity) {
					serverHttpSecurity.authorizeExchange(exchanges -> exchanges.pathMatchers(HttpMethod.GET).permitAll()
							.pathMatchers("/eazybank/accounts/**").hasRole("ACCOUNTS")
							.pathMatchers("/eazybank/cards/**").hasRole("CARDS")
							.pathMatchers("/eazybank/loans/**").hasRole("LOANS"))
							.oauth2ResourceServer(oAuth2ResourceServerSpec -> oAuth2ResourceServerSpec
									.jwt(jwtSpec -> jwtSpec.jwtAuthenticationConverter(grantedAuthoritiesExtractor())));
					serverHttpSecurity.csrf(csrfSpec -> csrfSpec.disable());
					return serverHttpSecurity.build();
				}

				private Converter<Jwt, Mono<AbstractAuthenticationToken>> grantedAuthoritiesExtractor() {
					JwtAuthenticationConverter jwtAuthenticationConverter =
							new JwtAuthenticationConverter();
					jwtAuthenticationConverter.setJwtGrantedAuthoritiesConverter
							(new KeycloakRoleConverter());
					return new ReactiveJwtAuthenticationConverterAdapter(jwtAuthenticationConverter);
				}

			}
			
	SecurityWebFilterChain springSecurityFilterChain(ServerHttpSecurity serverHttpSecurity) {
		serverHttpSecurity
			.authorizeExchange(exchanges -> exchanges
				.pathMatchers(HttpMethod.GET).permitAll()
				.pathMatchers("/eazybank/accounts/**").hasRole("ACCOUNTS")
				.pathMatchers("/eazybank/cards/**").hasRole("CARDS"))
			.oauth2ResourceServer(oAuth2ResourceServerSpec ->
				oAuth2ResourceServerSpec.jwt(Customizer.withDefaults()));
		serverHttpSecurity.csrf(csrfSpec -> csrfSpec.disable());
		return serverHttpSecurity.build();
	}
	
		Trong Spring WebFlux, SecurityWebFilterChain thay cho SecurityFilterChain (dùng trong Spring MVC).Đây là
		bean trả về cấu hình bảo mật cho toàn bộ ứng dụng reactive.
		.oauth2ResourceServer(oAuth2ResourceServerSpec -> oAuth2ResourceServerSpec.jwt(Customizer.withDefaults()))
			Kích hoạt OAuth2 Resource Server với cơ chế JWT.
			Nghĩa là app này sẽ verify JWT tokens (ví dụ phát hành từ Keycloak, Auth0, v.v.) cho mỗi request.
			Customizer.withDefaults() → dùng cấu hình mặc định để parse và validate JWT.	
			
	private Converter<Jwt, Mono<AbstractAuthenticationToken>> grantedAuthoritiesExtractor() {
		JwtAuthenticationConverter jwtAuthenticationConverter =
				new JwtAuthenticationConverter();
		jwtAuthenticationConverter.setJwtGrantedAuthoritiesConverter(new KeycloakRoleConverter());
		return new ReactiveJwtAuthenticationConverterAdapter(jwtAuthenticationConverter);
	}
	
		Converter<Jwt, Mono<AbstractAuthenticationToken>>

			Đây là một converter function:

			Input: một Jwt (token JWT đã được decode).

			Output: một Mono<AbstractAuthenticationToken> (WebFlux dùng Mono vì reactive).

			AbstractAuthenticationToken là base class của các token trong Spring Security (ví dụ: JwtAuthenticationToken).	

		JwtAuthenticationConverter jwtAuthenticationConverter = new JwtAuthenticationConverter();

			Đây là converter mặc định của Spring Security để chuyển một JWT thành JwtAuthenticationToken.

			Nhưng mặc định nó chỉ lấy các claim chuẩn (scope, scp) để map thành authorities.

			Nó không hiểu cấu trúc đặc thù của Keycloak.

		jwtAuthenticationConverter.setJwtGrantedAuthoritiesConverter(new KeycloakRoleConverter());

			Ở đây bạn custom lại cách lấy GrantedAuthorities (roles) từ JWT.

			KeycloakRoleConverter là class tự viết, implement Converter<Jwt, Collection<GrantedAuthority>>.

			Nó đọc roles từ claim Keycloak. Rồi convert thành ROLE_ACCOUNTS, ROLE_CARDS, ROLE_USER, ROLE_ADMIN.
			Nhờ vậy khi bạn cấu hình hasRole("ACCOUNTS"), Spring Security mới match được.
			
				{
				  "realm_access": {
					"roles": ["ACCOUNTS", "CARDS"]
				  },
				  "resource_access": {
					"my-client": {
					  "roles": ["USER", "ADMIN"]
					}
				  }
				}

		return new ReactiveJwtAuthenticationConverterAdapter(jwtAuthenticationConverter);

			JwtAuthenticationConverter là imperative (blocking).

			Trong WebFlux, bạn cần reactive converter (Mono).

			ReactiveJwtAuthenticationConverterAdapter là adapter để biến converter cũ thành reactive.

			Kết quả: trả về một converter phù hợp cho springSecurityFilterChain.

    public Collection<GrantedAuthority> convert(Jwt source) {
        Map<String, Object> realmAccess = (Map<String, Object>) source.getClaims().get("realm_access");
        if (realmAccess == null || realmAccess.isEmpty()) {
            return new ArrayList<>();
        }
        Collection<GrantedAuthority> returnValue = ((List<String>) realmAccess.get("roles"))
                .stream().map(roleName -> "ROLE_" + roleName)
                .map(SimpleGrantedAuthority::new)
                .collect(Collectors.toList());

        return returnValue;

    }
	
		Map<String, Object> realmAccess = (Map<String, Object>) source.getClaims().get("realm_access");

		source là JWT (Keycloak phát hành).

		Trong JWT của Keycloak thường có claim kiểu:		

			{
			  "realm_access": {
				"roles": ["ACCOUNTS", "CARDS"]
			  }
			}
			
--- Event Driven microservices using RabbitMQ,Spring Cloud Functions & Stream:

		message:
		
			pom.xml:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream-binder-rabbit</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream-test-binder</artifactId>
					<scope>test</scope>
				</dependency>
				
		application.yml:
		
			spring:
			  application:
				name: "message"
			  cloud:
				function:
				  definition: email|sms
				stream:
				  bindings:
					emailsms-in-0:
					  destination: send-communication
					  group: ${spring.application.name}
					emailsms-out-0:
					  destination: communication-sent
			  rabbitmq:
				host: localhost
				port: 5672
				username: guest
				password: guest
				connection-timeout: 10s
				
		MessageFunctions.java:
		
			@Configuration
			public class MessageFunctions {

				private static final Logger log = LoggerFactory.getLogger(MessageFunctions.class);

				@Bean
				public Function<AccountsMsgDto,AccountsMsgDto> email() {
					return accountsMsgDto -> {
						log.info("Sending email with the details : " +  accountsMsgDto.toString());
						return accountsMsgDto;
					};
				}

				@Bean
				public Function<AccountsMsgDto,Long> sms() {
					return accountsMsgDto -> {
						log.info("Sending sms with the details : " +  accountsMsgDto.toString());
						return accountsMsgDto.accountNumber();
					};
				}

			}
			
	accounts:
	
		pom.xml:
		
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-stream</artifactId>
			</dependency>
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-stream-binder-rabbit</artifactId>
			</dependency>
			
		application.yml:
		
			spring:
			  application:
				name: "accounts"
			  cloud:
				function:
				  definition: updateCommunication
				stream:
				  bindings:
					updateCommunication-in-0:
					  destination: communication-sent
					  group: ${spring.application.name}
					sendCommunication-out-0:
					  destination: send-communication
			  rabbitmq:
				host: localhost
				port: 5672
				username: guest
				password: guest
				connection-timeout: 10s
				
		AccountsFunctions.java:
		
			@Configuration
			public class AccountsFunctions {

				private static final Logger log = LoggerFactory.getLogger(AccountsFunctions.class);

				@Bean
				public Consumer<Long> updateCommunication(IAccountsService accountsService) {
					return accountNumber -> {
						log.info("Updating Communication status for the account number : " + accountNumber.toString());
						accountsService.updateCommunicationStatus(accountNumber);
					};
				}

			}
			
		AccountsServiceImpl.java:
		
			private final StreamBridge streamBridge;
			
			@Override
			public void createAccount(CustomerDto customerDto) {
				Customer customer = CustomerMapper.mapToCustomer(customerDto, new Customer());
				Optional<Customer> optionalCustomer = customerRepository.findByMobileNumber(customerDto.getMobileNumber());
				if(optionalCustomer.isPresent()) {
					throw new CustomerAlreadyExistsException("Customer already registered with given mobileNumber "
							+customerDto.getMobileNumber());
				}
				Customer savedCustomer = customerRepository.save(customer);
				Accounts savedAccount = accountsRepository.save(createNewAccount(savedCustomer));
				sendCommunication(savedAccount, savedCustomer);
			}

			private void sendCommunication(Accounts account, Customer customer) {
				var accountsMsgDto = new AccountsMsgDto(account.getAccountNumber(), customer.getName(),
						customer.getEmail(), customer.getMobileNumber());
				log.info("Sending Communication request for the details: {}", accountsMsgDto);
				var result = streamBridge.send("sendCommunication-out-0", accountsMsgDto);
				log.info("Is the Communication request successfully triggered ? : {}", result);
			}
			
			@Override
			public boolean updateCommunicationStatus(Long accountNumber) {
				boolean isUpdated = false;
				if(accountNumber !=null ){
					Accounts accounts = accountsRepository.findById(accountNumber).orElseThrow(
							() -> new ResourceNotFoundException("Account", "AccountNumber", accountNumber.toString())
					);
					accounts.setCommunicationSw(true);
					accountsRepository.save(accounts);
					isUpdated = true;
				}
				return  isUpdated;
			}
			
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-stream</artifactId>
    </dependency>
	
		Chú ý thêm cấu hình này để có thể gửi nhận message

	streamBridge.send:
	
		StreamBridge là một bean do Spring Cloud Stream cung cấp.

		Nó cho phép bạn gửi message thủ công (imperative way), mà không cần phụ thuộc hoàn toàn vào Function/Consumer/Supplier.

		Bạn có thể gửi dữ liệu đến output binding (ví dụ Kafka topic, RabbitMQ exchange) theo tên kênh.
		
		spring:
		  cloud:
			function:
			  definition: updateCommunication
			stream:
			  bindings:
				sendCommunication-out-0:   # Đây là channel output
				  destination: communication-sent   # Kafka topic / Rabbit queue
				  
			Có nghĩa là khi bạn gọi streamBridge.send("sendCommunication-out-0", data), message data sẽ được
			gửi tới topic communication-sent.
			
		Khi dùng spring.cloud.function.definition:
		
			Bạn viết hàm Function / Consumer / Supplier.

			Spring Cloud Stream sẽ tự động binding input/output theo tên.

			Message đến → tự động gọi function → kết quả gửi ra output binding.

			Cách này: reactive, event-driven, tự động.
			
		Khi dùng streamBridge.send:
		
			Dùng trong code như một API thủ công để publish message.

			Bạn không cần khai báo function.

			Bạn chỉ cần có một binding output, rồi gọi lệnh gửi.


	spring.cloud.function.definition = xác định function nào sẽ chạy.

		Output binding (-out-*) = nơi kết quả của function đó sẽ được publish.

		Nếu function có return → chắc chắn sẽ có ít nhất một -out-0.

		Nếu function là Consumer → không có output binding.

		Nếu function là Supplier → chỉ có -out-0, không có -in-0.
		
		Hàm (function/consumer/supplier) mà bạn khai báo trong spring.cloud.function.definition sẽ không tự động chạy
		ngay khi ứng dụng start.

		Nó sẽ chỉ chạy khi có sự kiện kích hoạt, ví dụ:

		Consumer (hàm nhận in-0): chạy khi có message đến từ queue/topic đã được bind.

		Supplier (hàm cung cấp out-0): chạy theo lịch (nếu có) hoặc khi khởi động ứng dụng (nếu bạn cấu hình spring.cloud.stream.poller).

		Function (hàm có in-0 và out-0): chạy khi có message vào → xử lý → gửi kết quả ra.
		
		spring:
		  cloud:
			function:
			  definition: processMessage
			stream:
			  bindings:
				processMessage-in-0:
				  destination: input-topic
				processMessage-out-0:
				  destination: output-topic
				  
		@Bean
		public Function<String, String> processMessage() {
			return msg -> "Processed: " + msg;
		}
		Khi có message publish vào Kafka topic input-topic, function processMessage sẽ được gọi.

		Sau khi xử lý, kết quả sẽ gửi ra output-topic.

		Nếu bạn không gửi message vào input-topic, thì processMessage sẽ không bao giờ được gọi.

	Input (-in-*) mới là trigger

		Các binding kiểu xxx-in-0, xxx-in-1, … là consumer channel.

		Khi có message đi vào từ RabbitMQ/Kafka (qua exchange/topic đã config trong destination), Spring sẽ gọi function tương ứng.

		👉 Đây mới là “cò súng” (trigger) để code trong function chạy.

	2. Output (-out-*) không trigger hàm

		Các binding kiểu xxx-out-0, xxx-out-1, … là producer channel.

		Chúng không bao giờ tự động làm chạy hàm.

		Chúng chỉ được dùng để publish kết quả của hàm sau khi hàm chạy nhờ input.
		
		spring.cloud.function.definition = xác định function nào sẽ chạy.

		Output binding (-out-*) = nơi kết quả của function đó sẽ được publish.

		Nếu function có return → chắc chắn sẽ có ít nhất một -out-0.

		Nếu function là Consumer → không có output binding.

		Nếu function là Supplier → chỉ có -out-0, không có -in-0.

	Khai niem ve spring.cloud.function.definition
	
		Spring Cloud Function cơ bản
		
			Trong code Spring Boot, bạn có thể định nghĩa function dưới dạng bean, chú ý phải đánh dấu
			với @Bean thì code mới chạy được, tại khi đánh dấu là @Bean thì spring mới hiểu và behind
			the scene được:
			
				@Bean
				public Function<String, String> upperCase() {
					return message -> message.toUpperCase();
				}

				@Bean
				public Consumer<String> logger() {
					return message -> System.out.println("Received: " + message);
				}

				@Bean
				public Supplier<String> greeter() {
					return () -> "Hello from Supplier!";
				}
				
					Function<T,R>: có input + output.

					Consumer<T>: chỉ có input, không có output.

					Supplier<T>: không có input, chỉ có output.
					
		updateCommunication-in-0
		
			Function updateCommunication chỉ có 1 input → nên Spring đánh số input đầu tiên là 0.

			Nếu sau này bạn có thêm input thứ 2 thì nó sẽ là updateCommunication-in-1.
			
			@Bean
			public BiFunction<String, Integer, String> process() {
				return (text, number) -> text + number;
			}
			
				Function process có 2 input (String, Integer) và 1 output (String).

				Spring sẽ tạo ra binding:

				process-in-0 → cho String

				process-in-1 → cho Integer

				process-out-0 → cho output String
					
		Vai trò của spring.cloud.function.definition
		
			Đây là cách bạn nói với Spring Cloud Stream:
			"Service này sẽ chạy function nào để kết nối với message broker (RabbitMQ/Kafka)".
			
			spring:
			  cloud:
				function:
				  definition: updateCommunication
				
				Nghĩa là function bean updateCommunication sẽ được gắn vào nếu có input binding (updateCommunication-in-0)
				và nếu có output thì gắn vào output binding (updateCommunication-out-0).
				
		Khi có nhiều function:
			
			Bạn có thể có nhiều function trong cùng service.
			
				@Bean
				public Function<String, String> email() { ... }

				@Bean
				public Function<String, String> sms() { ... }
				
			Nếu bạn không khai báo function.definition, Spring sẽ không biết phải chọn function nào để mapping.
			
			spring:
			  cloud:
				function:
				  definition: email|sms

			Ở đây email|sms nghĩa là compose/chaining:
				Message → email() → output của email sẽ đi vào sms() → output cuối publish ra ngoài.
				
			function:
			  definition: f1|f2|f3
			  
				Input message → f1 → f2 → f3 → Output message

		spring.cloud.function.definition chỉ định function nào (hoặc chuỗi function nào) trong code sẽ được
		Spring Cloud Stream dùng để kết nối với broker.

		Nếu có nhiều function trong code → bắt buộc phải dùng definition để Spring biết chọn function nào.

		Nếu chỉ có 1 function trong toàn service → Spring có thể tự động bind, không cần khai báo.

		Khi khai báo nhiều function với dấu | → bạn đang định nghĩa pipeline xử lý message.

	spring:
	  cloud:
		function:
		  definition: updateCommunication
		stream:
		  bindings:
			updateCommunication-in-0:
			  destination: communication-sent
			  group: ${spring.application.name}
			sendCommunication-out-0:
			  destination: send-communication
			  
		updateCommunication — đây là tên của một Spring Cloud Function (bean Java kiểu Function, Consumer,
		hoặc Supplier) trong code của bạn.

		Các binding (updateCommunication-in-0, sendCommunication-out-0) chỉ là kênh kết nối (input/output channels)
		của function đó, chứ không phải là các hàm riêng.
		
		updateCommunication-in-0 (consumer binding)
		
			Đây là input channel cho function updateCommunication.

			destination: communication-sent: RabbitMQ sẽ tạo một exchange communication-sent.

			group: ${spring.application.name}:

			RabbitMQ sẽ tạo queue cố định, ví dụ communication-sent.update-service.

			Các instance cùng group sẽ share queue → load balancing (1 message chỉ xử lý bởi 1 instance).

			Nếu bỏ group, mỗi instance sẽ có queue ephemeral riêng → message broadcast đến tất cả.

			👉 Tóm lại: updateCommunication-in-0 = lắng nghe message từ exchange communication-sent thông qua queue cố định.
			
		sendCommunication-out-0 (producer binding)
		
			Đây là output channel cho function updateCommunication.

			Sau khi xử lý xong input, function sẽ publish message vào exchange send-communication.

			Ở đây không có group, vì producer không cần queue → chỉ cần gửi vào exchange, còn queue nào nhận là do consumer quyết định.
			  
		


	spring:
	  cloud:
		stream:
		  bindings:
			sendCommunication-out-0:
			  destination: send-communication
			  
    private void sendCommunication(Accounts account, Customer customer) {
        var accountsMsgDto = new AccountsMsgDto(account.getAccountNumber(), customer.getName(),
                customer.getEmail(), customer.getMobileNumber());
        log.info("Sending communication rquest for the detail {}", accountsMsgDto);
        var result = streamBridge.send("sendCommunication-out-0", accountsMsgDto);
        log.info("Is the communication request successfully processed ?: {}", result);

    }
	
	spring:
	  cloud:
		function:
		  definition: email|sms
		stream:
		  bindings:
			emailsms-in-0:
			  destination: send-communication
			  group: ${spring.application.name}	
    @Bean
    public Function<AccountsMsgDto, AccountsMsgDto> email() {
        return accountsMsgDto -> {
            log.info("Sending email with the details: " + accountsMsgDto.toString());
            return accountsMsgDto;
        };
    }

    @Bean
    public Function<AccountsMsgDto, Long> sms() {
        return accountsMsgDto -> {
            log.info("Sending sms with the details: " + accountsMsgDto.toString());
            return accountsMsgDto.accountNumber();
        };
    }
			
--- Event Driven microservices using Kafka,Spring Cloud Functions & Stream:

	Tương tự như RabbitMQ
	
	Một số điểm khác:

		message:
		
			pom.xml:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream-binder-kafka</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream-test-binder</artifactId>
					<scope>test</scope>
				</dependency>
			
			application.yml:

				spring:
				  cloud:
					function:
					  definition: email|sms
					stream:
					  bindings:
						emailsms-in-0:
						  destination: send-communication
						  group: ${spring.application.name}
						emailsms-out-0:
						  destination: communication-sent
					  kafka:
						binder:
						  brokers:
							- localhost:9092	
						
		accounts:
		
			pom.xml:
			
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-stream-binder-kafka</artifactId>
				</dependency>
				
			application.yml:
			
				spring:
				  application:
					name: "accounts"
				  cloud:
					function:
					  definition: updateCommunication
					stream:
					  bindings:
						updateCommunication-in-0:
						  destination: communication-sent
						  group: ${spring.application.name}
						sendCommunication-out-0:
						  destination: send-communication
					  kafka:
						binder:
						  brokers:
							- localhost:9092
							
							
	<dependency>
		<groupId>org.springframework.cloud</groupId>
		<artifactId>spring-cloud-starter-bus-amqp</artifactId>
	</dependency>
	
		Bạn vẫn dùng spring-cloud-starter-bus-amqp, mà bên trong nó phụ thuộc RabbitMQ binder
							
--- Swagger UI:

		accounts:
		
			<dependency>
				<groupId>org.springdoc</groupId>
				<artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>
				<version>2.8.1</version>
			</dependency>
			
		AccountsApplication.java:
		
			@SpringBootApplication
			/*@ComponentScans({ @ComponentScan("com.eazybytes.accounts.controller") })
			@EnableJpaRepositories("com.eazybytes.accounts.repository")
			@EntityScan("com.eazybytes.accounts.model")*/
			@EnableJpaAuditing(auditorAwareRef = "auditAwareImpl")
			@OpenAPIDefinition(
					info = @Info(
							title = "Accounts microservice REST API Documentation",
							description = "EazyBank Accounts microservice REST API Documentation",
							version = "v1",
							contact = @Contact(
									name = "Madan Reddy",
									email = "tutor@eazybytes.com",
									url = "https://www.eazybytes.com"
							),
							license = @License(
									name = "Apache 2.0",
									url = "https://www.eazybytes.com"
							)
					),
					externalDocs = @ExternalDocumentation(
							description =  "EazyBank Accounts microservice REST API Documentation",
							url = "https://www.eazybytes.com/swagger-ui.html"
					)
			)
			public class AccountsApplication {

				public static void main(String[] args) {
					SpringApplication.run(AccountsApplication.class, args);
				}

			}
			
		AccountsController.java:
		
			@Tag(
					name = "CRUD REST APIs for Accounts in EazyBank",
					description = "CRUD REST APIs in EazyBank to CREATE, UPDATE, FETCH AND DELETE account details"
			)
			@RestController
			@RequestMapping(path="/api", produces = {MediaType.APPLICATION_JSON_VALUE})
			@AllArgsConstructor
			@Validated
			public class AccountsController {

				private IAccountsService iAccountsService;

				@Operation(
						summary = "Create Account REST API",
						description = "REST API to create new Customer &  Account inside EazyBank"
				)
				@ApiResponses({
						@ApiResponse(
								responseCode = "201",
								description = "HTTP Status CREATED"
						),
						@ApiResponse(
								responseCode = "500",
								description = "HTTP Status Internal Server Error",
								content = @Content(
										schema = @Schema(implementation = ErrorResponseDto.class)
								)
						)
				}
				)
			}
			
		AccountsDto.java:
		
			@Data
			@Schema(
					name = "Accounts",
					description = "Schema to hold Account information"
			)
			public class AccountsDto {

				@NotEmpty(message = "AccountNumber can not be a null or empty")
				@Pattern(regexp="(^$|[0-9]{10})",message = "AccountNumber must be 10 digits")
				@Schema(
						description = "Account Number of Eazy Bank account", example = "3454433243"
				)
				private Long accountNumber;

				@NotEmpty(message = "AccountType can not be a null or empty")
				@Schema(
						description = "Account type of Eazy Bank account", example = "Savings"
				)
				private String accountType;

				@NotEmpty(message = "BranchAddress can not be a null or empty")
				@Schema(
						description = "Eazy Bank branch address", example = "123 NewYork"
				)
				private String branchAddress;
			}
			
	

--- Intellij Config:

	Để có thể debug thì cần thiết lập đầy đủ về maven và jdk, version của jdk cần check trong file pom
	(thiết lập phải đầy đủ trong Setting và Project Structure)
	
	Chú ý build bằng Intellij và chạy cmd sẽ lấy jdk và maven khác nhau, cmd lấy từ biến môi trường trong
	khi Intellij lấy từ đường dẫn mình setting trong nó
	
--- Pom.xml:

	<dependency>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-web</artifactId>
	</dependency>
	
		Nếu không có dependency này thì web sẽ chạy rồi tự động dừng
		
		Khi bạn thêm spring-boot-starter-web vào pom.xml, Spring Boot sẽ tự động cấu hình một web
		server (mặc định là Tomcat) và ứng dụng của bạn sẽ không dừng ngay sau khi khởi động, vì web
		server này sẽ tiếp tục chạy và lắng nghe các yêu cầu HTTP.

		Nếu không có dependency này, Spring Boot không tự động khởi tạo một server HTTP và ứng dụng sẽ
		không có gì để làm sau khi hoàn tất khởi động.
		
		Web server (Tomcat, Jetty, Undertow, etc.) trong Spring Boot giữ cho ứng dụng sống bằng cách
		tiếp tục xử lý các kết nối và yêu cầu, do đó ứng dụng của bạn sẽ không dừng.

		Nếu không có web server, Spring Boot không có bất kỳ dịch vụ nào đang chạy liên tục, và sẽ
		ngừng ngay sau khi hoàn tất việc khởi động.
		
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-function-web</artifactId>
			<scope>test</scope>
		</dependency>
		
			Chú ý scope là test ở đây khiến code chạy không đúng.

--- FeignClient:
	
	Bản chất là nó đi thẳng từ Controller đến repository của 1 service khác.

--- Trong Docker, khi bạn sử dụng cú pháp -p a:b, các tham số a và b có nghĩa như sau:
		
		Cổng trên máy chủ (a):
		
			Cổng mà bạn sử dụng để kết nối đến dịch vụ từ bên ngoài container
		
			Đây là cổng trên máy tính của bạn (máy chủ) mà người dùng sẽ kết nối đến để truy cập dịch vụ đang chạy trong container.
			
			Nếu bạn ánh xạ cổng 3308, thì bạn sẽ truy cập dịch vụ qua localhost:3308 hoặc 127.0.0.1:3308.
			
		Cổng trên container (b):
		
			Cổng mà ứng dụng trong container sử dụng để tiếp nhận kết nối
			
			Đây là cổng mà ứng dụng bên trong container đang lắng nghe. Mỗi ứng dụng thường có một cổng mặc định mà nó sử dụng
			để tiếp nhận kết nối.
			
			Đối với MySQL, cổng mặc định là 3306. Khi bạn chạy MySQL trong container, nó sẽ lắng nghe trên cổng 3306.

--- Docker:

	- Command:

		- cmd: docker images
		- cmd: docker run -d -p 8080:8080 eazybytes/accounts:s4
		- cmd: docker ps
		- cmd: docker image push docker.io/eazybytes/accounts:s4
		- cmd: docker pull eazybytes/cards:s4

	Thêm dependency để build bằng Google Jib:
	
		<build>
			<plugins>
				<plugin>
					<groupId>com.google.cloud.tools</groupId>
					<artifactId>jib-maven-plugin</artifactId>
					<version>3.4.2</version>
					<configuration>
						<to>
							<image>eazybytes/${project.artifactId}:s6</image>
						</to>
					</configuration>
				</plugin>
			</plugins>
		</build>
		
		Chạy lệnh sau để build image:
		
			mvn compile jib:dockerBuild
		
	Thêm dependency để build bằng Buildpacks:
	
		<build>
			<plugins>
				<plugin>
					<groupId>org.springframework.boot</groupId>
					<artifactId>spring-boot-maven-plugin</artifactId>
					<configuration>
						<image>
							<name>eazybytes/${project.artifactId}:s4</name>
						</image>
					</configuration>
				</plugin>
			</plugins>
		</build>
		
		Chạy lệnh sau để build image:
		
			mvn spring-boot:build-image

	Sử dụng dockerfile để build image:
	
		Dockerfile:
		
			#Start with a base image containing Java runtime
			FROM openjdk:21-jdk-slim

			# MAINTAINER instruction is deprecated in favor of using label
			# MAINTAINER eazybytes.com
			#Information around who maintains the image
			LABEL "org.opencontainers.image.authors"="eazybytes.com"

			# Add the application's jar to the image
			COPY target/accounts-0.0.1-SNAPSHOT.jar accounts-0.0.1-SNAPSHOT.jar

			# execute the application
			ENTRYPOINT ["java", "-jar", "accounts-0.0.1-SNAPSHOT.jar"]
			
				Giải thích:

					# Start with a base image containing Java runtime
					FROM openjdk:21-jdk-slim
					
						FROM: Chỉ định hình ảnh cơ sở mà Docker sẽ sử dụng để xây dựng hình ảnh mới. Ở đây, hình ảnh cơ sở
						là openjdk:21-jdk-slim, cung cấp môi trường Java với phiên bản JDK 21, đã được tối ưu hóa để nhẹ.

					# MAINTAINER instruction is deprecated in favor of using label
					# MAINTAINER eazybytes.com
					# Information around who maintains the image
					LABEL "org.opencontainers.image.authors"="eazybytes.com"
					
						LABEL: Cung cấp thông tin về hình ảnh, chẳng hạn như tác giả. Ở đây, org.opencontainers.image.authors
						được sử dụng để ghi nhận thông tin về ai duy trì hình ảnh, thay thế cho chỉ thị MAINTAINER đã lỗi thời.

					# Add the application's jar to the image
					COPY target/accounts-0.0.1-SNAPSHOT.jar accounts-0.0.1-SNAPSHOT.jar
					
						COPY: Sao chép tệp JAR của ứng dụng từ thư mục target trên máy chủ vào hình ảnh Docker. Tệp JAR này sẽ
						được chạy khi container khởi động.

					# execute the application
					ENTRYPOINT ["java", "-jar", "accounts-0.0.1-SNAPSHOT.jar"]
					
						ENTRYPOINT: Chỉ định lệnh sẽ được thực thi khi container khởi động. Ở đây, lệnh java -jar accounts-0.0.1-SNAPSHOT.jar sẽ
						được thực thi, giúp chạy ứng dụng Java.
						
					Tóm lại:
					
						Tệp Dockerfile này xây dựng một hình ảnh Docker cho ứng dụng Java bằng cách sử dụng hình ảnh cơ sở chứa JDK, thêm
						tệp JAR của ứng dụng vào hình ảnh, và thiết lập lệnh khởi động cho container.
						
		Chạy lệnh sau để build image:
		
			docker build . -t eazybytes/accounts:s4
		
--- Docker compose:

	- Command:
	
		- cmd: docker compose version
		- cmd: docker compose up
		- cmd: docker compose up -d
		- cmd: docker compose down
		- cmd: docker compose down -d
		- cmd: docker compose stop
		- cmd: docker compose start
		
	- Code của docker-compose.yml:
	
		Case 1:
		
			common-config.yml:
			
				services:
				  network-deploy-service:
					networks:
					  - eazybank

				  microservice-base-config:
					extends:
					  service: network-deploy-service
					deploy:
					  resources:
						limits:
						  memory: 700m

				  microservice-configserver-config:
					extends:
					  service: microservice-base-config
					environment:
					  SPRING_PROFILES_ACTIVE: default
					  SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/

				  microservice-eureka-config:
					extends:
					  service: microservice-configserver-config
					environment:
					  EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://eurekaserver:8070/eureka/
					  
			docker-compose.yml:
			
				services:
				  redis:
					image: redis
					ports:
					  - "6379:6379"
					healthcheck:
					  test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
					  timeout: 10s
					  retries: 10
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  configserver:
					image: "eazybytes/configserver:s10"
					container_name: configserver-ms
					ports:
					  - "8071:8071"
					healthcheck:
					  test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					extends:
					  file: common-config.yml
					  service: microservice-base-config

				  eurekaserver:
					image: "eazybytes/eurekaserver:s10"
					container_name: eurekaserver-ms
					ports:
					  - "8070:8070"
					depends_on:
					  configserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:8070/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					extends:
					  file: common-config.yml
					  service: microservice-configserver-config
					environment:
					  SPRING_APPLICATION_NAME: "eurekaserver"

				  accounts:
					image: "eazybytes/accounts:s10"
					container_name: accounts-ms
					ports:
					  - "8080:8080"
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:8080/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					environment:
					  SPRING_APPLICATION_NAME: "accounts"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  loans:
					image: "eazybytes/loans:s10"
					container_name: loans-ms
					ports:
					  - "8090:8090"
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:8090/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					environment:
					  SPRING_APPLICATION_NAME: "loans"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  cards:
					image: "eazybytes/cards:s10"
					container_name: cards-ms
					ports:
					  - "9000:9000"
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:9000/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					environment:
					  SPRING_APPLICATION_NAME: "cards"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  gatewayserver:
					image: "eazybytes/gatewayserver:s10"
					container_name: gatewayserver-ms
					ports:
					  - "8072:8072"
					depends_on:
					  accounts:
						condition: service_healthy
					  loans:
						condition: service_healthy
					  cards:
						condition: service_healthy
					  redis:
						condition: service_healthy
					environment:
					  SPRING_APPLICATION_NAME: "gatewayserver"
					  SPRING_DATA_REDIS_CONNECT-TIMEOUT: 2s
					  SPRING_DATA_REDIS_HOST: redis
					  SPRING_DATA_REDIS_PORT: 6379
					  SPRING_DATA_REDIS_TIMEOUT: 1s
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				networks:
				  eazybank:
					driver: "bridge"
					
		Case 2:
		
			common-config.yml:
			
				services:
				  network-deploy-service:
					networks:
					  - eazybank

				  microservice-db-config:
					extends:
					  service: network-deploy-service
					image: mysql
					healthcheck:
					  test: [ "CMD", "mysqladmin" ,"ping", "-h", "localhost" ]
					  timeout: 10s
					  retries: 10
					  interval: 10s
					  start_period: 10s
					environment:
					  MYSQL_ROOT_PASSWORD: root

				  microservice-base-config:
					extends:
					  service: network-deploy-service
					deploy:
					  resources:
						limits:
						  memory: 700m

				  microservice-configserver-config:
					extends:
					  service: microservice-base-config
					environment:
					  SPRING_PROFILES_ACTIVE: default
					  SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/
					  SPRING_DATASOURCE_USERNAME: root
					  SPRING_DATASOURCE_PASSWORD: root
					  
			docker-compose.yml:
			
				services:
				  accountsdb:
					container_name: accountsdb
					ports:
					  - 3306:3306
					environment:
					  MYSQL_DATABASE: accountsdb
					extends:
					  file: common-config.yml
					  service: microservice-db-config

				  loansdb:
					container_name: loansdb
					ports:
					  - 3307:3306
					environment:
					  MYSQL_DATABASE: loansdb
					extends:
					  file: common-config.yml
					  service: microservice-db-config

				  cardsdb:
					container_name: cardsdb
					ports:
					  - 3308:3306
					environment:
					  MYSQL_DATABASE: cardsdb
					extends:
					  file: common-config.yml
					  service: microservice-db-config

				  configserver:
					image: "eazybytes/configserver:s7"
					container_name: configserver-ms
					ports:
					  - "8071:8071"
					healthcheck:
					  test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 10s
					extends:
					  file: common-config.yml
					  service: microservice-base-config

				  accounts:
					image: "eazybytes/accounts:s7"
					container_name: accounts-ms
					ports:
					  - "8080:8080"
					environment:
					  SPRING_APPLICATION_NAME: "accounts"
					  SPRING_DATASOURCE_URL: "jdbc:mysql://accountsdb:3306/accountsdb"
					depends_on:
					  accountsdb:
						condition: service_healthy
					  configserver:
						condition: service_healthy
					extends:
					  file: common-config.yml
					  service: microservice-configserver-config

				  loans:
					image: "eazybytes/loans:s7"
					container_name: loans-ms
					ports:
					  - "8090:8090"
					environment:
					  SPRING_APPLICATION_NAME: "loans"
					  SPRING_DATASOURCE_URL: "jdbc:mysql://loansdb:3306/loansdb"
					depends_on:
					  loansdb:
						condition: service_healthy
					  configserver:
						condition: service_healthy
					extends:
					  file: common-config.yml
					  service: microservice-configserver-config

				  cards:
					image: "eazybytes/cards:s7"
					container_name: cards-ms
					ports:
					  - "9000:9000"
					environment:
					  SPRING_APPLICATION_NAME: "cards"
					  SPRING_DATASOURCE_URL: "jdbc:mysql://cardsdb:3306/cardsdb"
					depends_on:
					  cardsdb:
						condition: service_healthy
					  configserver:
						condition: service_healthy
					extends:
					  file: common-config.yml
					  service: microservice-configserver-config

				networks:
				  eazybank:
					driver: "bridge"
	
		Case 3:
		
			common-config.yml:
		
				services:
				  network-deploy-service:
					networks:
					  - eazybank

				  microservice-base-config:
					extends:
					  service: network-deploy-service
					deploy:
					  resources:
						limits:
						  memory: 700m
					environment:
					  JAVA_TOOL_OPTIONS: "-javaagent:/app/libs/opentelemetry-javaagent-2.11.0.jar"
					  OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4318
					  OTEL_METRICS_EXPORTER: none
					  OTEL_LOGS_EXPORTER: none

				  microservice-configserver-config:
					extends:
					  service: microservice-base-config
					environment:
					  SPRING_PROFILES_ACTIVE: default
					  SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/

				  microservice-eureka-config:
					extends:
					  service: microservice-configserver-config
					environment:
					  EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://eurekaserver:8070/eureka/	

			docker-compose.yml:
			
				services:

				  kafka:
					image: apache/kafka:4.1.0
					hostname: kafka
					container_name: kafka
					ports:
					  - "9092:9092"
					environment:
					  KAFKA_BROKER_ID: 1
					  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
					  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
					  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
					  KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
					  KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
					  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
					  KAFKA_PROCESS_ROLES: broker,controller
					  KAFKA_NODE_ID: 1
					  KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
					  KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://kafka:9092
					  KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
					  KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
					  KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
					  CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
					healthcheck:
					  test: [ "CMD-SHELL", "nc -z kafka 9092 || exit 1" ]
					  interval: 10s
					  timeout: 5s
					  retries: 10
					  start_period: 5s
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  keycloak:
					image: quay.io/keycloak/keycloak:26.4.0
					container_name: keycloak
					ports:
					  - "127.0.0.1:7080:8080"
					environment:
					  KC_BOOTSTRAP_ADMIN_USERNAME: "admin"
					  KC_BOOTSTRAP_ADMIN_PASSWORD: "admin"
					command: "start-dev"
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  read:
					image: grafana/loki:3.1.2
					command: "-config.file=/etc/loki/config.yaml -target=read"
					ports:
					  - 3101:3100
					  - 7946
					  - 9095
					volumes:
					  - ../observability/loki/loki-config.yaml:/etc/loki/config.yaml
					depends_on:
					  - minio
					healthcheck:
					  test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
					  interval: 10s
					  timeout: 5s
					  retries: 5
					networks: &loki-dns
					  eazybank:
						aliases:
						  - loki

				  write:
					image: grafana/loki:3.1.2
					command: "-config.file=/etc/loki/config.yaml -target=write"
					ports:
					  - 3102:3100
					  - 7946
					  - 9095
					volumes:
					  - ../observability/loki/loki-config.yaml:/etc/loki/config.yaml
					healthcheck:
					  test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
					  interval: 10s
					  timeout: 5s
					  retries: 5
					depends_on:
					  - minio
					networks:
					  <<: *loki-dns

				  alloy:
					image: grafana/alloy:v1.5.1
					volumes:
					  - ../observability/alloy/alloy-local-config.yaml:/etc/alloy/config.alloy:ro
					  - /var/run/docker.sock:/var/run/docker.sock
					command: run --server.http.listen-addr=0.0.0.0:12345 --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy
					ports:
					  - 12345:12345
					depends_on:
					  - gateway
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  minio:
					image: minio/minio:RELEASE.2024-12-18T13-15-44Z
					entrypoint:
					  - sh
					  - -euc
					  - |
						mkdir -p /data/loki-data && \
						mkdir -p /data/loki-ruler && \
						minio server /data
					environment:
					  - MINIO_ROOT_USER=loki
					  - MINIO_ROOT_PASSWORD=supersecret
					  - MINIO_PROMETHEUS_AUTH_TYPE=public
					  - MINIO_UPDATE=off
					ports:
					  - 9000
					volumes:
					  - ./.data/minio:/data
					healthcheck:
					  test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
					  interval: 15s
					  timeout: 20s
					  retries: 5
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  prometheus:
					image: prom/prometheus:v3.1.0
					container_name: prometheus
					ports:
					  - "9090:9090"
					volumes:
					  - ../observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  tempo:
					image: grafana/tempo:2.6.1
					container_name: tempo
					command: -config.file /etc/tempo-config.yml
					ports:
					  - "3110:3100"
					  - "4318:4318"
					volumes:
					  - ../observability/tempo/tempo.yml:/etc/tempo-config.yml
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  grafana:
					image: grafana/grafana:11.4.0
					environment:
					  - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
					  - GF_AUTH_ANONYMOUS_ENABLED=true
					  - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
					depends_on:
					  - gateway
					entrypoint:
					  - sh
					  - -euc
					  - |
						/run.sh
					ports:
					  - "3000:3000"
					volumes:
					  - ../observability/grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
					healthcheck:
					  test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1" ]
					  interval: 10s
					  timeout: 5s
					  retries: 5
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  backend:
					image: grafana/loki:3.1.2
					volumes:
					  - ../observability/loki/loki-config.yaml:/etc/loki/config.yaml
					ports:
					  - "3100"
					  - "7946"
					command: "-config.file=/etc/loki/config.yaml -target=backend -legacy-read-mode=false"
					depends_on:
					  - gateway
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  gateway:
					image: nginx:1.27.3
					depends_on:
					  - read
					  - write
					entrypoint:
					  - sh
					  - -euc
					  - |
						cat <<EOF > /etc/nginx/nginx.conf
						user  nginx;
						worker_processes  5;  ## Default: 1

						events {
						  worker_connections   1000;
						}

						http {
						  resolver 127.0.0.11;

						  server {
							listen             3100;

							location = / {
							  return 200 'OK';
							  auth_basic off;
							}

							location = /api/prom/push {
							  proxy_pass       http://write:3100\$$request_uri;
							}

							location = /api/prom/tail {
							  proxy_pass       http://read:3100\$$request_uri;
							  proxy_set_header Upgrade \$$http_upgrade;
							  proxy_set_header Connection "upgrade";
							}

							location ~ /api/prom/.* {
							  proxy_pass       http://read:3100\$$request_uri;
							}

							location = /loki/api/v1/push {
							  proxy_pass       http://write:3100\$$request_uri;
							}

							location = /loki/api/v1/tail {
							  proxy_pass       http://read:3100\$$request_uri;
							  proxy_set_header Upgrade \$$http_upgrade;
							  proxy_set_header Connection "upgrade";
							}

							location ~ /loki/api/.* {
							  proxy_pass       http://read:3100\$$request_uri;
							}
						  }
						}
						EOF
						/docker-entrypoint.sh nginx -g "daemon off;"
					ports:
					  - "3100:3100"
					healthcheck:
					  test: [ "CMD", "service", "nginx", "status" ]
					  interval: 10s
					  timeout: 5s
					  retries: 5
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  configserver:
					image: "eazybytes/configserver:s14"
					container_name: configserver-ms
					ports:
					  - "8071:8071"
					healthcheck:
					  test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
					  interval: 20s
					  timeout: 5s
					  retries: 20
					  start_period: 10s
					extends:
					  file: common-config.yml
					  service: microservice-base-config
					environment:
					  OTEL_SERVICE_NAME: "configserver"

				  eurekaserver:
					image: "eazybytes/eurekaserver:s14"
					container_name: eurekaserver-ms
					ports:
					  - "8070:8070"
					depends_on:
					  configserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:8070/actuator/health/readiness | grep UP || exit 1"
					  interval: 20s
					  timeout: 5s
					  retries: 20
					  start_period: 10s
					extends:
					  file: common-config.yml
					  service: microservice-configserver-config
					environment:
					  SPRING_APPLICATION_NAME: "eurekaserver"
					  OTEL_SERVICE_NAME: "eurekaserver"

				  accounts:
					image: "eazybytes/accounts:s14"
					container_name: accounts-ms
					healthcheck:
					  test: "curl --fail --silent localhost:8080/actuator/health/readiness | grep UP || exit 1"
					  interval: 20s
					  timeout: 5s
					  retries: 20
					  start_period: 10s
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					  kafka:
						condition: service_healthy
					environment:
					  SPRING_APPLICATION_NAME: "accounts"
					  OTEL_SERVICE_NAME: "accounts"
					  SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: "kafka:9092"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  loans:
					image: "eazybytes/loans:s14"
					container_name: loans-ms
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:8090/actuator/health/readiness | grep UP || exit 1"
					  interval: 20s
					  timeout: 5s
					  retries: 20
					  start_period: 10s
					environment:
					  SPRING_APPLICATION_NAME: "loans"
					  OTEL_SERVICE_NAME: "loans"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  cards:
					image: "eazybytes/cards:s14"
					container_name: cards-ms
					depends_on:
					  configserver:
						condition: service_healthy
					  eurekaserver:
						condition: service_healthy
					healthcheck:
					  test: "curl --fail --silent localhost:9000/actuator/health/readiness | grep UP || exit 1"
					  interval: 20s
					  timeout: 5s
					  retries: 20
					  start_period: 10s
					environment:
					  SPRING_APPLICATION_NAME: "cards"
					  OTEL_SERVICE_NAME: "cards"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				  message:
					image: "eazybytes/message:s14"
					container_name: message-ms
					depends_on:
					  kafka:
						condition: service_healthy
					environment:
					  SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: "kafka:9092"
					extends:
					  file: common-config.yml
					  service: network-deploy-service

				  gatewayserver:
					image: "eazybytes/gatewayserver:s14"
					container_name: gatewayserver-ms
					ports:
					  - "8072:8072"
					depends_on:
					  accounts:
						condition: service_healthy
					  loans:
						condition: service_healthy
					  cards:
						condition: service_healthy
					environment:
					  SPRING_APPLICATION_NAME: "gatewayserver"
					  OTEL_SERVICE_NAME: "gatewayserver"
					  SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI: "http://keycloak:8080/realms/master/protocol/openid-connect/certs"
					extends:
					  file: common-config.yml
					  service: microservice-eureka-config

				networks:
				  eazybank:
					driver: "bridge"
				

	Trong naỳ khi gọi đến service khác phải dùng name thay vì localhost, bởi vì docker compose trong này sẽ
	dùng để giao tiếp giữa các container, localhost chỉ dùng riêng cho chính container đó
	
    healthcheck:
        test: "curl --fail --silent localhost:8070/actuator/health/readiness | grep UP || exit 1"
        interval: 60s
        timeout: 30s
        retries: 10
        start_period: 60s
	  
		curl --fail --silent localhost:8070/actuator/health/readiness | grep UP || exit 1
		
			curl --fail → gọi API http://localhost:8070/actuator/health/readiness, nếu HTTP status khác 2xx thì coi như fail.

			--silent → không in ra log chi tiết của curl.

			| grep UP → tìm chữ "UP" trong kết quả trả về (nghĩa là service đã sẵn sàng).

			|| exit 1 → nếu không tìm thấy "UP", thì trả về code 1 (thất bại).

			Nghĩa là: healthcheck chỉ pass nếu endpoint readiness trả về UP.
			
		interval: 60s

			Cứ 60 giây Docker sẽ chạy lại lệnh test một lần để kiểm tra service.
			
		timeout: 30s

			Mỗi lần chạy test, nếu sau 30 giây mà không có kết quả thì coi như fail.
			
		retries: 10

			Nếu test fail liên tiếp 10 lần, thì container sẽ bị đánh dấu là unhealthy.
			
		start_period: 60s

			Trong 60 giây đầu sau khi container start, Docker không tính fail healthcheck (cho service có thời gian khởi động).

			Sau 60s, nếu test fail thì bắt đầu đếm retries.
			
		Chú ý:
			
			Phải có config sau mới bật được /actuator/health/readiness, lúc đó mới heck được điều kiện trên
			
				management:
				  endpoints:
					web:
					  exposure:
						include: "*"
				  health:
					readiness-state:
					  enabled: true
					liveness-state:
					  enabled: true
				  endpoint:
					health:
					  probes:
						enabled: true			
			
	Khi ta run container thì giữa việc dùng docker-compose.yml và việc ta thao tác tay trên docker desktop là khác nhau,
	với docker-compose.yml sẽ có các lệnh, thiết lập biến...còn với docker desktop thì đơn giản chỉ là run container từ file image
	
	dependency failed to start: container account-ms has no healthcheck configured
	
		Trong docker-compose.yml, service account-ms đang được một service khác depends_on.

		Nhưng account-ms không có healthcheck → Docker không biết cách xác định khi nào nó "healthy".

		Vì vậy container phụ thuộc sẽ không start được, báo lỗi như trên.
	
--- Port:

		Trong accounts bạn đang set:

		SPRING_DATASOURCE_URL: "jdbc:mysql://accountsdb:3310/accountsdb"


		accountsdb → là tên service/container (Docker DNS sẽ resolve được).

		3310 → là host port được map ra ngoài, nhưng các container trong cùng 1 network Docker không cần dùng
		host port, mà dùng container port nội bộ.

		Nội bộ container MySQL luôn expose port 3306, không phải 3310.

		👉 Vì thế khi accounts connect tới accountsdb:3310 thì sẽ không được, phải dùng accountsdb:3306.

--- Thứ tự start các container:

		Trong trường hợp bạn có các microservices như Eureka, Accounts, Cards, và GatewayServer, thứ tự khởi động
		của các service rất quan trọng để đảm bảo chúng có thể giao tiếp với nhau đúng cách. Dưới đây là thứ tự
		khởi động đề xuất:

		1. Eureka Server (Discovery Service):

			Eureka Server là service quản lý discovery của các microservices trong hệ thống. Nó cần phải chạy đầu
			tiên, vì các microservices khác (Accounts, Cards, GatewayServer) cần phải đăng ký vào Eureka để chúng
			có thể giao tiếp với nhau.

			Thứ tự: Khởi động Eureka đầu tiên.

			Lý do: Các microservices như Accounts, Cards, và GatewayServer sẽ cần đăng ký với Eureka để có thể được
			phát hiện (discovered) bởi các service khác. Nếu Eureka không chạy trước, các microservices này sẽ không
			thể đăng ký với Eureka.

		2. ConfigServer (Nếu có):

			Nếu bạn đang sử dụng Spring Cloud Config Server để quản lý cấu hình cho các microservices, thì ConfigServer
			cũng nên được khởi động ngay sau Eureka. Điều này sẽ đảm bảo rằng các microservices có thể tải cấu hình từ
			Config Server khi chúng khởi động.

			Thứ tự: Khởi động ConfigServer sau Eureka.

			Lý do: Các microservices như Accounts, Cards, và GatewayServer sẽ cần tải cấu hình từ ConfigServer, do đó
			ConfigServer cần phải có mặt trước khi chúng khởi động.

		3. Các Microservices (Accounts, Cards):

			Các microservices như Accounts và Cards có thể được khởi động sau khi Eureka và ConfigServer đã sẵn sàng. Các
			service này sẽ đăng ký vào Eureka để có thể được phát hiện và giao tiếp với các service khác trong hệ thống.

			Thứ tự: Khởi động Accounts và Cards sau Eureka và ConfigServer.

			Lý do: Accounts và Cards cần đăng ký với Eureka và có thể lấy cấu hình từ ConfigServer.

		4. GatewayServer:

			GatewayServer là lớp API Gateway cho hệ thống microservices của bạn. Nó thường sẽ đứng sau các service như Accounts
			và Cards để có thể định tuyến các yêu cầu từ người dùng tới các microservices tương ứng (thường thông qua các dịch
			vụ khác như Eureka).

			Thứ tự: Khởi động GatewayServer cuối cùng.

			Lý do: GatewayServer cần biết các dịch vụ khác như Accounts và Cards thông qua Eureka. Nếu Eureka chưa khởi động,
			GatewayServer sẽ không thể tìm thấy các dịch vụ này.

-- So sánh image, container và volume
		Docker Image – như file .iso hoặc .exe
			Là template để tạo container.
			Không có trạng thái, không chạy được trực tiếp.
			Được build từ Dockerfile.
			Template, không thay đổi
			
		Docker Container – như một máy ảo đang chạy
			Là instance của image khi được chạy.
			Bạn có thể chạy nhiều container từ một image.
			Dữ liệu bên trong sẽ mất nếu container bị xóa (trừ khi dùng volume).
			Instance đang chạy, có thể thay đổi
			
		Docker Volume – ổ đĩa riêng cho dữ liệu
			Lưu trữ dữ liệu tách biệt với container.
			Không bị mất nếu container bị xóa.
			Dùng để lưu database, file upload, v.v.
			Dữ liệu lưu bền vững, dùng lại được
		
		Docker Image     →      Container       +        Volume
		(Recipe)         	(Chạy thật - app)        (Data bền vững)
	
--- properties file:

	networks:
	  eazybank:
		driver: "bridge"
		
		networks: Đây là phần khai báo mạng trong Docker Compose. Nó cho phép bạn xác định các mạng mà
		các dịch vụ có thể kết nối với nhau.
		
		eazybank: Đây là tên của mạng mà bạn đang định nghĩa. Bạn có thể đặt tên bất kỳ cho mạng này.
		
		driver: Thuộc tính này chỉ định kiểu driver mà mạng sẽ sử dụng. Trong trường hợp này, nó được đặt thành "bridge".
		
		bridge: Đây là loại driver mặc định trong Docker. Nó tạo một mạng lưới riêng cho các container, cho phép
		chúng giao tiếp với nhau mà không cần phải tiếp xúc trực tiếp với mạng bên ngoài.
		
		Tóm lại:
			Đoạn mã này tạo ra một mạng tên là eazybank sử dụng driver bridge, giúp cho các container trong cùng
			một mạng có thể giao tiếp dễ dàng với nhau.

--- Kubernetes:

	Cài đặt kubernete dashboard

	- Command:
	
		- cmd: kubectl config get-contexts
		
			Lệnh này hiển thị danh sách tất cả các “context” có trong file cấu hình của kubectl (thường là file ~/.kube/config).
			
			Mỗi context đại diện cho một cấu hình kết nối đến một cluster Kubernetes cụ thể, bao gồm:

				Cụm (cluster) nào bạn đang kết nối tới

				Người dùng (user) nào đang được dùng để xác thực

				Namespace mặc định khi chạy lệnh kubectl
				
			Cấu trúc lệnh:
			
				kubectl config get-contexts [options]
				
			Một số tuỳ chọn thường dùng:

				--output=name → chỉ in ra tên các context (ngắn gọn hơn)

				--no-headers → ẩn dòng tiêu đề

				--kubeconfig <file> → chỉ định file kubeconfig khác thay vì mặc định ~/.kube/config
				
		
		- cmd: kubectl config get-clusters	

			Lệnh này dùng để liệt kê tất cả các cluster được khai báo trong file cấu hình của kubectl — thường
			là file: ~/.kube/config
			
			Mỗi cluster ở đây là một cụm Kubernetes mà bạn có thể kết nối đến, với thông tin như:

				Server API endpoint (địa chỉ kết nối đến Kubernetes API)

				Chứng chỉ xác thực (certificate-authority)

				Các thông số bảo mật khác
				
			Cấu trúc lệnh:

				kubectl config get-clusters [options]
				
			Một số tùy chọn thường gặp:

				--kubeconfig <file>	→ Chỉ định file kubeconfig khác thay vì mặc định ~/.kube/config
				
				--output=name	→ Chỉ in ra tên các cluster, không có cột tiêu đề
		
		- cmd: kubectl config use-context docker-desktop

			Lệnh này dùng để chuyển “context” hiện tại của kubectl sang context có tên là docker-desktop.

			Mỗi context trong Kubernetes đại diện cho một cấu hình kết nối cụ thể, bao gồm:

				Cluster (cụm Kubernetes nào bạn muốn điều khiển)

				User (người dùng hoặc service account nào sẽ được dùng để xác thực)

				Namespace mặc định khi chạy lệnh

			Khi bạn đổi context, bạn đang nói cho kubectl biết:
			
				“Từ giờ, hãy gửi tất cả lệnh đến cluster, user và namespace được định nghĩa trong context docker-desktop.”
				
			Cấu trúc tổng quát:

				kubectl config use-context <context-name>

					Trong đó <context-name> là tên của context có trong file cấu hình ~/.kube/config.
		
		- cmd: kubectl get nodes

			Lệnh này được dùng để liệt kê tất cả các node (nút) trong cluster Kubernetes mà bạn đang kết nối tới.
			
			Mỗi node là một máy chủ (vật lý hoặc ảo) — nơi Kubernetes chạy các Pod (container).

			Nói đơn giản:
			
				kubectl get nodes = “Hiển thị danh sách các máy trong cụm Kubernetes.”
				
			Cấu trúc lệnh:
			
				kubectl get nodes [options]

					Một số tùy chọn hay dùng:

						Tuỳ chọn				Mô tả
						
						-o wide					Hiển thị thêm thông tin chi tiết (IP, OS, kernel, container runtime, v.v.)
						-o json / -o yaml		Xuất kết quả ở định dạng JSON hoặc YAML
						--selector				Lọc node theo nhãn (label)
						--no-headers			Ẩn dòng tiêu đề (nếu cần dùng trong script)
						
		- cmd: kubectl apply -f dashboard-adminuser.yaml

			Lệnh này dùng để tạo hoặc cập nhật tài nguyên (resource) trong Kubernetes dựa trên nội dung của file cấu
			hình YAML — ở đây là file dashboard-adminuser.yaml.

			Cụ thể:

				dashboard-adminuser.yaml thường chứa định nghĩa về một ServiceAccount, Role, và RoleBinding hoặc ClusterRoleBinding
				để tạo người dùng quản trị (admin user) cho Kubernetes Dashboard.

				Khi bạn chạy lệnh này, Kubernetes sẽ đọc file YAML, sau đó tạo các đối tượng được mô tả trong đó trong cluster.
		
		- cmd: kubectl -n kubernetes-dashboard create token admin-user
		
			Lệnh này dùng để tạo một “access token” (mã truy cập) cho ServiceAccount có tên là admin-user trong
			namespace kubernetes-dashboard.

			Token này thường được dùng để đăng nhập vào giao diện web của Kubernetes Dashboard với quyền quản trị (admin).		
		
		- cmd: kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d
		
			Lệnh này được dùng để lấy token (mã đăng nhập) của ServiceAccount admin-user trong namespace kubernetes-dashboard, rồi
			giải mã (decode) token đó từ định dạng Base64 sang dạng chuỗi thật mà bạn có thể dùng để đăng nhập vào Kubernetes Dashboard.

			Nói ngắn gọn:
			
				Lấy token đăng nhập Dashboard cho user admin-user (cách cũ — dùng cho Kubernetes ≤ 1.23).

			-n kubernetes-dashboard

				Chỉ định namespace của Secret đó (ở đây là kubernetes-dashboard).

				Nếu không thêm -n, kubectl sẽ tìm trong namespace mặc định (default), và có thể không thấy gì.

			-o jsonpath={".data.token"}

				Tùy chọn -o (output): định dạng đầu ra.

				jsonpath={".data.token"} có nghĩa là:

					Lấy trường .data.token từ phần dữ liệu JSON của Secret.
					
			| base64 -d

				Dấu | là pipe: chuyển kết quả của lệnh trước sang lệnh sau.

				base64 -d nghĩa là decode (giải mã) chuỗi Base64 → chuỗi gốc (chuỗi token thật).

			Dữ liệu trong trường .data của Secret luôn ở dạng Base64 (vì lý do bảo mật).
		
		- cmd: kubectl port-forward svc/keycloak 8080:80	

			Lệnh này dùng để mở đường truyền (port forwarding) từ máy cục bộ (local) của bạn tới dịch vụ (Service) keycloak trong cluster Kubernetes.

			Cụ thể:

				Cổng 8080 trên máy bạn (local) được chuyển tiếp (forward) đến

				Cổng 80 trên dịch vụ keycloak trong cluster.

			Kết quả: bạn có thể truy cập ứng dụng Keycloak đang chạy trong Kubernetes bằng địa chỉ: http://localhost:8080
		
		- cmd: kubectl logs keycloak-0 -c keycloak	

			Lệnh này dùng để xem log (nhật ký hoạt động) của container keycloak nằm trong Pod keycloak-0.

			Nói cách khác:

				“Hiển thị log của container có tên keycloak đang chạy trong Pod keycloak-0.”

			Cấu trúc tổng quát:
			
				kubectl logs <pod-name> [-c <container-name>] [options]

				Thành phần			Ý nghĩa
				
				kubectl				Công cụ dòng lệnh quản lý Kubernetes
				logs				Xem log của container hoặc pod
				keycloak-0			Tên của Pod cần xem log
				-c keycloak			(Tùy chọn) Tên container trong Pod cần xem log
			
			Vì sao cần -c keycloak?

				Trong Kubernetes, một Pod có thể chứa nhiều container.
				
				Ví dụ, Pod keycloak-0 có thể gồm:

					Container chính: keycloak (chạy ứng dụng Keycloak)

					Container phụ (sidecar): ví dụ metrics, backup, hoặc init

				Khi bạn chỉ chạy:

					kubectl logs keycloak-0

				mà Pod có nhiều container, Kubernetes sẽ báo lỗi:

					error: a container name must be specified for pod keycloak-0, choose one of: [keycloak metrics]


				Vì vậy, bạn cần chỉ rõ container nào bằng -c <container-name>:

					kubectl logs keycloak-0 -c keycloak
		
		- cmd: kubectl get pods -n kong		
		
			Lệnh này dùng để liệt kê tất cả các Pod đang chạy trong namespace kong của cụm Kubernetes.

			Nói đơn giản:

				“Hiển thị danh sách các Pod (ứng dụng, container) thuộc namespace kong.”
	
		- cmd: kubectl logs kubernetes-dashboard-kong-79867c9c48-dmbnv -n kubernetes-dashboard	

			Lệnh này dùng để xem log (nhật ký hoạt động) của Pod kubernetes-dashboard-kong-79867c9c48-dmbnv trong
			namespace kubernetes-dashboard.

			➡️ Nói đơn giản:

				“Hiển thị nội dung log (thông báo, lỗi, quá trình khởi động, v.v.) của Pod kubernetes-dashboard-kong-... thuộc
				namespace kubernetes-dashboard.”

			Cấu trúc tổng quát:
			
				kubectl logs <pod-name> [-n <namespace>] [options]

					Thành phần										Ý nghĩa
					
					kubectl											Công cụ dòng lệnh quản lý Kubernetes
					logs											Lệnh để xem log (nhật ký) của container hoặc Pod
					kubernetes-dashboard-kong-79867c9c48-dmbnv		Tên Pod cần xem log
					-n kubernetes-dashboard							Namespace chứa Pod đó
		
		- cmd: kubectl proxy
		
			Lệnh kubectl proxy được dùng để tạo một proxy (cổng trung gian) giữa máy cục bộ (local) của bạn
			và Kubernetes API server.

			Nói cách khác:

				“Nó mở một đường hầm bảo mật, cho phép bạn truy cập API của Kubernetes hoặc giao diện web (như Dashboard)
				thông qua địa chỉ http://localhost.”

			Cách hoạt động

				Khi bạn chạy:

					kubectl proxy


					kubectl sẽ:

						Đọc thông tin kết nối từ kubeconfig (file ~/.kube/config)

						Mở một cổng trên máy local, mặc định là 8001

						Tạo một proxy HTTP giữa máy bạn và Kubernetes API Server

				Sau đó bạn có thể truy cập:

					http://localhost:8001

				Khi đó:

					Request của bạn gửi tới localhost:8001
					
					→ kubectl proxy nhận request
					
					→ chuyển tiếp (forward) nó tới API server của cluster.

			Ví dụ thường gặp:
			
				Truy cập Kubernetes Dashboard (thường dùng nhất)

					Sau khi cài Dashboard xong, bạn chạy:

						kubectl proxy


					Sau đó mở trình duyệt và truy cập:

						http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/


					Đây là cách an toàn nhất để mở Dashboard vì:

						Không cần mở port ra ngoài

						Chỉ bạn (máy local) có thể truy cập

						kubectl tự xử lý chứng thực bằng token trong kubeconfig

				Chạy trên cổng khác
				
					kubectl proxy --port=9090


					Proxy sẽ chạy trên http://localhost:9090

					Cho phép truy cập từ thiết bị khác (trong mạng LAN)
					
						kubectl proxy --address=0.0.0.0 --accept-hosts='.*'

					→ Cho phép máy khác trong mạng cùng truy cập qua IP của bạn
					
					⚠️ Không nên dùng trong môi trường production (vì sẽ mở API server cho mọi thiết bị).

			Tùy chọn phổ biến
			
				Tùy chọn						Mô tả
				
				--port=<số>						Chọn cổng local (mặc định: 8001)
				--address=<IP>					Chọn địa chỉ IP để lắng nghe (mặc định: localhost)
				--accept-hosts=<regex>			Giới hạn host nào được phép kết nối
				--accept-paths=<regex>			Giới hạn đường dẫn API nào được phép
				--disable-filter				Bỏ lọc đường dẫn API (cho phép tất cả request)
			
			Khi chạy, bạn sẽ thấy:
			
				Starting to serve on 127.0.0.1:8001

				Nghĩa là proxy đã chạy.
				
				Giờ bạn có thể truy cập:

					http://127.0.0.1:8001

				Khi nhấn Ctrl + C, proxy sẽ dừng.

			Khi nào dùng kubectl proxy
			
				Trường hợp															Mục đích
				
				Truy cập Kubernetes Dashboard										An toàn và nhanh, không cần mở port công khai
				Gọi API trực tiếp (REST API)										Test hoặc tích hợp hệ thống
				Muốn truy cập cluster mà không dùng LoadBalancer hoặc Ingress		Dùng proxy cục bộ để vào API server			
		
		- cmd: kubectl get svc -n kubernetes-dashboard		
		
			Lệnh kubectl get svc -n kubernetes-dashboard liệt kê tất cả Services trong
			namespace kubernetes-dashboard.

				kubectl get svc: hiển thị danh sách các Service đang chạy trong cluster.

				-n kubernetes-dashboard: giới hạn phạm vi truy vấn trong namespace kubernetes-dashboard.

			Kết quả cho biết tên Service, kiểu (ClusterIP, NodePort, LoadBalancer), địa chỉ IP nội bộ,
			cổng (PORT), và đích (TARGET PORT).		
		
		- cmd: kubectl get pods -n kubernetes-dashboard -o wide		
		
				kubectl get pods: liệt kê tất cả Pods đang chạy trong cluster.

				-n kubernetes-dashboard: chỉ lấy Pods thuộc namespace kubernetes-dashboard.

				-o wide: hiển thị thêm thông tin chi tiết cho mỗi Pod.

			Cụ thể, ngoài các cột mặc định như NAME, READY, STATUS, RESTARTS, AGE, khi thêm -o wide bạn sẽ thấy thêm:

				IP: địa chỉ IP nội bộ của Pod.

				NODE: tên node (máy chủ) mà Pod đang chạy trên đó.

				NOMINATED NODE và READINESS GATES (nếu có).

			Lệnh này thường dùng để kiểm tra Pod đang chạy ở node nào và IP nội bộ của nó.		
		
		- cmd: kubectl create serviceaccount dashboard-admin-sa -n kubernetes-dashboard		
		
				kubectl create serviceaccount: tạo một ServiceAccount mới trong Kubernetes.

				dashboard-admin-sa: tên của ServiceAccount được tạo.

				-n kubernetes-dashboard: tạo ServiceAccount này trong namespace kubernetes-dashboard.

			Giải thích thêm:

				ServiceAccount là một loại tài khoản dành cho ứng dụng hoặc Pod (không phải người dùng). Nó dùng để xác thực
				khi Pod cần tương tác với API Server của Kubernetes.

				Mỗi Pod có thể gắn với một ServiceAccount, và ServiceAccount đó sẽ có token giúp Pod thực hiện các thao tác được phép.

			Tình huống sử dụng:
			
				Lệnh này thường được dùng khi bạn muốn tạo một ServiceAccount có quyền quản trị (admin) để đăng nhập vào Kubernetes
				Dashboard, giúp truy cập và quản lý tài nguyên trong cluster.		
		
		- cmd: kubectl create clusterrolebinding dashboard-admin-sa  --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin-sa 
		
		
		
		- cmd: kubectl -n kubernetes-dashboard create token dashboard-admin-sa	

			kubectl create clusterrolebinding:
			
				Tạo một ClusterRoleBinding, tức là liên kết một ClusterRole (vai trò có quyền trên toàn cluster) với một subject (như user, group, hay service account).

			dashboard-admin-sa:
			
				Tên của ClusterRoleBinding bạn đang tạo.

			--clusterrole=cluster-admin:
			
				Chỉ định ClusterRole mà bạn muốn gán.
			
				Ở đây là cluster-admin, tức là vai trò cao nhất trong Kubernetes, có toàn quyền trên mọi tài nguyên trong toàn cluster.

			--serviceaccount=kubernetes-dashboard:dashboard-admin-sa:
			
				Chỉ định ServiceAccount được gán quyền này:

					kubernetes-dashboard là namespace của ServiceAccount.

					dashboard-admin-sa là tên của ServiceAccount đã tạo trước đó.

			Tóm lại:
			
				Lệnh này gán quyền quản trị toàn cluster (cluster-admin) cho ServiceAccount dashboard-admin-sa
				trong namespace kubernetes-dashboard.

			Ứng dụng thực tế:
			
				Lệnh này thường được dùng khi bạn muốn đăng nhập vào Kubernetes Dashboard với quyền admin đầy đủ, cho phép xem và
				thao tác với mọi tài nguyên trong cluster (không bị giới hạn quyền).
		
		- cmd: kubectl get services		
		
			Liệt kê tất cả các Service trong namespace hiện tại (mặc định là default, trừ khi bạn chỉ định -n).

			Giải thích từng phần:

				kubectl: công cụ dòng lệnh dùng để tương tác với Kubernetes cluster.

				get: lệnh dùng để xem danh sách hoặc thông tin tài nguyên.

				services (hoặc svc): loại tài nguyên muốn xem — ở đây là Service, dùng để cung cấp điểm truy cập (endpoint) cho các Pod.

			Kết quả hiển thị thường có các cột:
			
				Cột							Ý nghĩa
				
				NAME						Tên của Service
				TYPE						Kiểu Service (ClusterIP, NodePort, LoadBalancer, ExternalName)
				CLUSTER-IP					IP nội bộ trong cluster mà các Pod khác có thể dùng để truy cập
				EXTERNAL-IP					IP bên ngoài (nếu có, ví dụ khi dùng LoadBalancer)
				PORT(S)						Cổng mà Service lắng nghe và ánh xạ đến Pod
				AGE							Thời gian tồn tại của Service
			
			Ví dụ:
			
				kubectl get services


			Kết quả có thể như:

				NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
				
				kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        10d
				my-app-svc   NodePort    10.102.36.117   <none>        80:30080/TCP   2d


			Giải thích:

				kubernetes: service mặc định cho API Server.

				my-app-svc: service cho ứng dụng my-app, có thể truy cập qua node port 30080.		
		
		- cmd: kubectl get replicaset	

			Liệt kê tất cả các ReplicaSet trong namespace hiện tại (mặc định là default, trừ khi bạn chỉ định -n <namespace>).

			Giải thích từng phần:

				kubectl: công cụ dòng lệnh để quản lý Kubernetes.

				get: hiển thị danh sách tài nguyên.

				replicaset (hoặc rs): loại tài nguyên bạn muốn xem — ReplicaSet.

			ReplicaSet là gì?

				ReplicaSet là một đối tượng trong Kubernetes có nhiệm vụ đảm bảo số lượng Pod đang chạy luôn đúng với
				số lượng mong muốn (số replicas).
				
				Nếu một Pod bị xóa hoặc lỗi, ReplicaSet sẽ tự động tạo mới Pod khác để thay thế.

				ReplicaSet thường được tạo tự động bởi Deployment, chứ ít khi được tạo thủ công.

			Kết quả của lệnh thường có dạng:
			
				Cột					Ý nghĩa
				
				NAME				Tên của ReplicaSet
				DESIRED				Số Pod mong muốn (replicas)
				CURRENT				Số Pod hiện đang tồn tại
				READY				Số Pod đang sẵn sàng phục vụ
				AGE					Thời gian tồn tại của ReplicaSet
			
			Ví dụ:
			
				kubectl get replicaset


			Kết quả:

				NAME                        DESIRED   CURRENT   READY   AGE
				nginx-deployment-5c689d4b4  3         3         3       2d


			Giải thích:

				ReplicaSet tên nginx-deployment-5c689d4b4 có:

					DESIRED = 3: mong muốn có 3 Pod chạy.

					CURRENT = 3: hiện tại đang có 3 Pod tồn tại.

					READY = 3: cả 3 Pod đều đang hoạt động tốt.
		
		- cmd: kubectl get deployments		
		
			Liệt kê tất cả các Deployment trong namespace hiện tại (mặc định là default, trừ khi bạn chỉ định -n <namespace>).

			Giải thích từng phần:

				kubectl: công cụ dòng lệnh tương tác với Kubernetes cluster.

				get: hiển thị danh sách hoặc thông tin tài nguyên.

				deployments: loại tài nguyên muốn xem — Deployment.

			Deployment là gì?

				Deployment là một đối tượng quản lý cấp cao trong Kubernetes, dùng để:

					Triển khai và cập nhật ứng dụng (thông qua các Pod).

					Tự động tạo và quản lý ReplicaSet.

					Đảm bảo số lượng Pod mong muốn luôn hoạt động ổn định.

					Hỗ trợ rolling update và rollback (cập nhật phiên bản mới an toàn, có thể quay lại nếu lỗi).

			Khi chạy lệnh:
			
				kubectl get deployments

			Kết quả thường như sau:

				NAME              READY   UP-TO-DATE   AVAILABLE   AGE
				nginx-deployment  3/3     3            3           5d

			Giải thích các cột:
			
				Cột					Ý nghĩa
				
				NAME				Tên của Deployment
				READY				Số Pod sẵn sàng / tổng số Pod mong muốn
				UP-TO-DATE			Số Pod đã được cập nhật với cấu hình mới nhất
				AVAILABLE			Số Pod hiện đang hoạt động và phục vụ yêu cầu
				AGE					Thời gian tồn tại của Deployment
			
			Ví dụ:
			
				kubectl get deployments -n kubernetes-dashboard


			Hiển thị tất cả Deployment trong namespace kubernetes-dashboard, ví dụ:

				NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
				kubernetes-dashboard   1/1     1            1           3d


				Nghĩa là có 1 Pod của kubernetes-dashboard đang chạy và hoạt động bình thường.		
		
		- cmd: kubectl delete pod accounts-deployment-7f7f75d899-vtvbq

			Giải thích từng phần:

				kubectl: công cụ dòng lệnh để quản lý Kubernetes cluster.

				delete: dùng để xóa một tài nguyên trong cluster.

				pod: loại tài nguyên muốn xóa.

				accounts-deployment-7f7f75d899-vtvbq: tên cụ thể của Pod cần xóa.

			Khi chạy lệnh này, chuyện gì xảy ra?

				Kubernetes sẽ xóa Pod có tên accounts-deployment-7f7f75d899-vtvbq.

				Nếu Pod này được tạo bởi một Deployment hoặc ReplicaSet, Kubernetes sẽ tự động tạo lại một Pod mới để đảm bảo số
				lượng Pod đúng với cấu hình mong muốn (số replicas).

				Vì thế, việc xóa một Pod "thuộc" Deployment không làm ứng dụng ngừng chạy — nó chỉ tạm thời khởi động lại Pod mới.

			Ví dụ:

				Giả sử bạn có Deployment tên accounts-deployment với 3 replicas (3 Pod).
				
				Danh sách Pod:

					accounts-deployment-7f7f75d899-vtvbq
					accounts-deployment-7f7f75d899-kwm7c
					accounts-deployment-7f7f75d899-bj8h9


			Nếu bạn chạy:

				kubectl delete pod accounts-deployment-7f7f75d899-vtvbq


				Pod đó sẽ bị xóa.
				Ngay sau đó, ReplicaSet của Deployment sẽ tạo một Pod mới (ví dụ tên accounts-deployment-7f7f75d899-x9d4p) để thay thế.

			Ứng dụng thực tế:

				Dùng để "restart" thủ công một Pod bị lỗi.

				Dùng để kiểm tra cơ chế tự phục hồi (self-healing) của Deployment/ReplicaSet.

				Dùng để giải phóng Pod đang treo hoặc bị stuck.
		
		- cmd: kubectl describe pod gatewayserver-deployment-54cd64599-2j4jz	

			Mục đích:

				Lệnh này dùng để xem chi tiết toàn bộ thông tin về một Pod cụ thể trong Kubernetes.
				
				Nó giúp bạn chẩn đoán lỗi, kiểm tra cấu hình, hoặc xem sự kiện (events) liên quan đến Pod đó.

			Giải thích từng phần:

				kubectl → công cụ dòng lệnh để làm việc với Kubernetes.

				describe → hiển thị mô tả chi tiết (thông tin đầy đủ hơn kubectl get).

				pod → loại tài nguyên muốn mô tả.

				gatewayserver-deployment-54cd64599-2j4jz → tên cụ thể của Pod.

			Khi chạy lệnh này, bạn sẽ thấy thông tin gồm nhiều phần như:
			
				1. Basic Info (Thông tin cơ bản):
				
					Name:         gatewayserver-deployment-54cd64599-2j4jz
					Namespace:    default
					Node:         worker-node1/192.168.1.10
					Start Time:   Tue, 28 Oct 2025 14:10:23 +0700
					Labels:       app=gatewayserver
					Status:       Running
					IP:           10.244.1.5
					Controlled By: ReplicaSet/gatewayserver-deployment-54cd64599


					Cho biết Pod đang chạy ở node nào, IP nội bộ, thuộc ReplicaSet nào, và trạng thái hiện tại.

			Containers (Các container bên trong Pod):
				
				Containers:
				  gatewayserver:
					Image:        myregistry/gatewayserver:v1.0
					Ports:        8080/TCP
					State:        Running
					Ready:        True
					Restart Count: 0


				Mỗi Pod có thể chứa nhiều container. Phần này cho biết:

					Tên container (gatewayserver)

					Image (phiên bản container)

					Cổng đang mở

					Trạng thái: Running / Waiting / Terminated

					Số lần khởi động lại (Restart Count)

			Environment & Mounts (Biến môi trường, volume, mount):

				Cho biết container có gắn volume, secret, configmap nào hay không.

			Conditions (Điều kiện hoạt động):
			
				Conditions:
				  Type              Status
				  Initialized       True
				  Ready             True
				  ContainersReady   True
				  PodScheduled      True


				Tất cả True nghĩa là Pod hoạt động bình thường.

			Events (Sự kiện quan trọng):
			
				Events:
				  Type    Reason     Age   From               Message
				  ----    ------     ----  ----               -------
				  Normal  Scheduled  2m    default-scheduler  Successfully assigned default/gatewayserver-deployment-54cd64599-2j4jz to worker-node1
				  Normal  Pulled     2m    kubelet            Container image "myregistry/gatewayserver:v1.0" already present on machine
				  Normal  Started    2m    kubelet            Started container gatewayserver


				Các sự kiện giúp bạn hiểu quá trình khởi tạo hoặc lỗi (ví dụ: kéo image thất bại, không đủ tài nguyên, crash loop...).

			Ứng dụng thực tế:

				Kiểm tra tình trạng chi tiết của Pod (đang chạy, pending, crash...).

				Xem nguyên nhân Pod lỗi (như image lỗi, lệnh khởi chạy sai, thiếu volume).

				Xem Pod đang chạy trên node nào và đang dùng image nào.
		
		- cmd: kubectl set image deployment gatewayserver-deployment gatewayserver=eazybytes/gatewayserver:s6 --record

			Mục đích:

				Lệnh này được dùng để cập nhật (thay đổi) image của container trong một Deployment — thường dùng khi bạn muốn triển khai phiên bản mới của ứng dụng.

			Giải thích từng phần:
			
				Thành phần										Ý nghĩa
				kubectl											Công cụ dòng lệnh để quản lý Kubernetes cluster
				set image										Dùng để thay đổi (update) image container trong một tài nguyên (Deployment, DaemonSet, StatefulSet, v.v.)
				deployment gatewayserver-deployment				Chỉ định loại tài nguyên (deployment) và tên của nó (gatewayserver-deployment)
				gatewayserver=eazybytes/gatewayserver:s6		Gán container có tên gatewayserver trong Deployment này sử dụng image mới eazybytes/gatewayserver:s6
				--record										(Tùy chọn cũ, một số phiên bản mới đã bỏ) — ghi lại thay đổi này vào lịch sử rollout để có thể rollback sau này
			
			Hiểu rõ hơn phần thay đổi image:

				Một Deployment có thể quản lý nhiều container.
			
			Cú pháp:

				<container_name>=<new_image>


			Trong ví dụ này:

				Tên container: gatewayserver

				Image mới: eazybytes/gatewayserver:s6
				→ tức là bạn đang cập nhật container đó sang phiên bản mới s6.

			Khi bạn chạy lệnh này:

				Kubernetes tạo một ReplicaSet mới với image eazybytes/gatewayserver:s6.

				Bắt đầu rolling update (cập nhật dần dần):

					Dừng từng Pod cũ (dùng image cũ).

					Tạo Pod mới với image mới.

				Sau khi cập nhật xong, ReplicaSet cũ vẫn được giữ lại (để rollback nếu cần).

			Kiểm tra tiến trình cập nhật:

				Bạn có thể dùng:

					kubectl rollout status deployment gatewayserver-deployment


				hoặc xem lịch sử:

					kubectl rollout history deployment gatewayserver-deployment


				Nếu có lỗi, bạn có thể quay lại bản cũ:

					kubectl rollout undo deployment gatewayserver-deployment
		
		- cmd: kubectl get events --sort-by=.metadata.creationTimestamp	

			Mục đích:

				Lệnh này được dùng để xem danh sách tất cả các sự kiện (Events) trong cluster Kubernetes, theo thứ tự thời gian — từ cũ đến mới.
				
				Rất hữu ích để kiểm tra nguyên nhân lỗi khi Pod, Deployment hoặc Service gặp sự cố.

			Giải thích từng phần:
			
				Thành phần								Ý nghĩa
				
				kubectl									Công cụ dòng lệnh để làm việc với Kubernetes
				get events								Lấy danh sách các sự kiện (Events) do Kubernetes ghi lại
				--sort-by=.metadata.creationTimestamp	Sắp xếp kết quả theo thời gian tạo của mỗi event (từ cũ → mới)
			
			Events trong Kubernetes là gì?

				Event là các bản ghi tạm thời mà Kubernetes tạo ra để mô tả những gì đang xảy ra trong cluster, ví dụ:

					Một Pod được tạo / xóa.

					Container bắt đầu chạy hoặc bị crash.

					Scheduler gán Pod vào một node.

					Image được kéo (pulled) thành công hoặc thất bại.

					Lỗi khi mount volume hoặc thiếu tài nguyên CPU/RAM.

			Kết quả ví dụ:
			
				LAST SEEN   TYPE      REASON              OBJECT                              	 MESSAGE
				12m         Normal    Scheduled           pod/nginx-deployment-7f7f75d899-5s9p2  Successfully assigned default/nginx-deployment-7f7f75d899-5s9p2 to worker-node1
				12m         Normal    Pulling             pod/nginx-deployment-7f7f75d899-5s9p2  Pulling image "nginx:latest"
				11m         Normal    Pulled              pod/nginx-deployment-7f7f75d899-5s9p2  Successfully pulled image "nginx:latest"
				11m         Normal    Started             pod/nginx-deployment-7f7f75d899-5s9p2  Started container nginx
				5m          Warning   BackOff             pod/app-7d89f65c9d-xj8df               Back-off restarting failed container

			Giải thích các cột:
			
				Cột					Ý nghĩa
				
				LAST SEEN			Thời điểm event được ghi nhận lần cuối
				TYPE				Loại sự kiện: Normal hoặc Warning
				REASON				Nguyên nhân / hành động (ví dụ: Scheduled, Failed, Started, BackOff, …)
				OBJECT				Tài nguyên (Pod, Deployment, Node, …) liên quan
				MESSAGE				Mô tả chi tiết về sự kiện
				
			Ứng dụng thực tế:

				Debug lỗi Pod: Kiểm tra xem Pod có bị crash, không kéo được image, hay thiếu tài nguyên không.

				Theo dõi hoạt động cluster: Biết khi nào một Pod, Node, hay Deployment được tạo hoặc thay đổi.

				Kết hợp lọc: Có thể thêm -n <namespace> để xem events trong một namespace cụ thể.

			Ví dụ:

				kubectl get events -n kubernetes-dashboard --sort-by=.metadata.creationTimestamp

		
		- cmd: kubectl rollout history deployment gatewayserver-deployment	
		
			Mục đích:

				Lệnh này được dùng để xem lịch sử các lần triển khai (rollout) của một Deployment trong Kubernetes.
				
				Nó giúp bạn biết Deployment đã được cập nhật bao nhiêu lần, khi nào, và sử dụng image nào trong từng phiên bản.

			Giải thích từng phần:
			
				Thành phần								Ý nghĩa
					
				kubectl									Công cụ dòng lệnh làm việc với Kubernetes
				rollout									Nhóm lệnh quản lý quá trình triển khai (rollout) của Deployment / DaemonSet / StatefulSet
				history									Hiển thị lịch sử các lần cập nhật
				deployment gatewayserver-deployment		Chỉ định tên Deployment mà bạn muốn xem lịch sử
				
			Khi chạy lệnh, bạn sẽ thấy kết quả như:
			
				REVISION  CHANGE-CAUSE
				1         <none>
				2         kubectl set image deployment gatewayserver-deployment gatewayserver=eazybytes/gatewayserver:s6 --record=true
				3         kubectl set image deployment gatewayserver-deployment gatewayserver=eazybytes/gatewayserver:s7 --record=true

			Giải thích các cột:
			
				Cột				Ý nghĩa
				
				REVISION		Số thứ tự của lần triển khai (Deployment revision)
				CHANGE-CAUSE	Ghi chú nguyên nhân thay đổi — thường là câu lệnh bạn dùng để thay đổi image (nếu có --record)
				
			Cách hoạt động của Rollout History:

				Mỗi khi bạn cập nhật Deployment (ví dụ: đổi image bằng kubectl set image), Kubernetes sẽ:

					Tạo một ReplicaSet mới (ứng với revision mới).

					Giữ lại các ReplicaSet cũ để có thể rollback khi cần.

				Nếu bạn đã dùng --record trong các lệnh trước (như khi set image), Kubernetes sẽ lưu lại lệnh đó trong
				phần CHANGE-CAUSE, giúp bạn biết rõ mỗi lần thay đổi đã làm gì.

			Để xem chi tiết 1 phiên bản cụ thể:

				Bạn có thể chạy:

					kubectl rollout history deployment gatewayserver-deployment --revision=2


				Kết quả sẽ cho biết:

					Image được dùng,

					Số lượng replicas,

					Labels,

					Container name,
					v.v.

			Ứng dụng thực tế:

				Theo dõi lịch sử cập nhật ứng dụng (ai thay đổi, thay đổi gì).

				Rollback (quay lại phiên bản cũ) nếu phiên bản mới bị lỗi.
				
				→ Dùng lệnh:

					kubectl rollout undo deployment gatewayserver-deployment --to-revision=2
		
		
		
		- cmd: kubectl rollout undo deployment gatewayserve-deployment --to-revision=1

			Dùng để quay lại (rollback) Deployment gatewayserve-deployment về phiên bản (revision) số 1 trong lịch sử rollout.

			Giải thích từng phần:
			
				Thành phần							Ý nghĩa
							
				kubectl								Công cụ dòng lệnh quản lý Kubernetes
				rollout undo						Thực hiện hoàn tác (rollback) bản cập nhật gần đây nhất của Deployment
				deployment gatewayserve-deployment	Tên của Deployment muốn rollback
				--to-revision=1						(Tùy chọn) Chỉ định số phiên bản cụ thể muốn quay lại (ở đây là revision 1). Nếu không ghi, mặc định rollback về phiên bản trước đó
			
			Khi chạy lệnh:

				Kubernetes sẽ:

					Xác định ReplicaSet tương ứng với revision số 1.

					Khôi phục cấu hình Deployment về trạng thái ở revision đó (ví dụ image, số replicas, config…).

					Thực hiện rolling update ngược — dừng Pod của bản mới, tạo lại Pod theo cấu hình cũ.

			Kiểm tra lại sau rollback:
			
				kubectl rollout history deployment gatewayserve-deployment
				
				kubectl get pods
				
				kubectl describe deployment gatewayserve-deployment
		
		- cmd: kubectl get pvc		
		
			Lệnh này dùng để liệt kê tất cả các PersistentVolumeClaim (PVC) trong namespace hiện tại (mặc định là default, trừ khi bạn chỉ định -n <namespace>).

			Nói đơn giản: nó giúp bạn xem các yêu cầu lưu trữ dữ liệu (storage) mà các ứng dụng trong Kubernetes đang sử dụng.

			Giải thích từng phần:
			
			Thành phần			Ý nghĩa
			
			kubectl				Công cụ dòng lệnh để quản lý Kubernetes cluster
			get					Hiển thị danh sách tài nguyên
			pvc					Viết tắt của PersistentVolumeClaim – tức là “yêu cầu lưu trữ cố định”
			
			PVC là gì?

				PVC (PersistentVolumeClaim) là yêu cầu của người dùng để sử dụng một ổ lưu trữ (Persistent Volume).

				Nó hoạt động giống như “tôi cần một ổ đĩa dung lượng 10Gi để lưu dữ liệu ứng dụng này”.

				Kubernetes sẽ tìm hoặc tạo một PersistentVolume (PV) phù hợp để gán cho PVC đó.

			Khi bạn chạy lệnh:
			
				kubectl get pvc

				Kết quả sẽ trông như sau:

				NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
				mysql-pv-claim     Bound    pvc-3b63f77b-bb5a-4d69-9dc5-0a6d62132c2f   10Gi       RWO            standard       5d
				redis-pv-claim     Pending                                             1Gi        RWO            fast           10m

			Giải thích các cột:
			
				Cột					Ý nghĩa
				NAME				Tên của PVC
				STATUS				Trạng thái của PVC:
									🔸 Pending: chưa được gắn với PersistentVolume nào.
									🔸 Bound: đã được gắn (được PV đáp ứng).
									🔸 Lost: PV bị mất hoặc xóa.
				VOLUME				Tên của PersistentVolume (PV) được gắn với PVC
				CAPACITY			Dung lượng mà PVC yêu cầu
				ACCESS MODES		Chế độ truy cập (ví dụ: RWO, RWX, ROX):
									🔸 RWO – ReadWriteOnce: chỉ một node có thể ghi.
									🔸 RWX – ReadWriteMany: nhiều node có thể đọc/ghi cùng lúc.
									🔸 ROX – ReadOnlyMany: nhiều node chỉ đọc.
				STORAGECLASS		Tên của StorageClass (quy định cách tạo volume)
				AGE					Thời gian PVC đã tồn tại
			
			Ví dụ thực tế:
			
				kubectl get pvc -n mysql

					Hiển thị các PVC trong namespace mysql.

					Nếu thấy STATUS = Bound, nghĩa là volume đã sẵn sàng để Pod sử dụng.

			Ứng dụng thực tế:

				Kiểm tra ứng dụng có được gắn volume thành công hay chưa.

				Theo dõi dung lượng và trạng thái lưu trữ của database, app, v.v.

				Debug lỗi Pod không mount được volume (thường do PVC chưa Bound).		
		
		- cmd: kubectl delete deployment kubernetes-dashboard-kong -n kubernetes-dashboard	

			Mục đích:

				Lệnh này dùng để xóa một Deployment có tên là kubernetes-dashboard-kong trong namespace kubernetes-dashboard.

				Nói đơn giản:
				
					Bạn đang xóa ứng dụng (Deployment) có tên kubernetes-dashboard-kong ra khỏi namespace kubernetes-dashboard.

			Giải thích từng phần:
			
				Thành phần					Ý nghĩa
				
				kubectl						Công cụ dòng lệnh để tương tác với Kubernetes
				delete						Thực hiện thao tác xóa tài nguyên
				deployment					Loại tài nguyên muốn xóa — ở đây là một Deployment
				kubernetes-dashboard-kong	Tên của Deployment cần xóa
				-n kubernetes-dashboard		Chỉ định namespace mà Deployment này đang tồn tại
				
			Khi chạy lệnh:

				Kubernetes sẽ:

					Xóa đối tượng Deployment tên kubernetes-dashboard-kong.

					Tự động xóa tất cả các Pod do Deployment đó quản lý (vì Pod được tạo bởi ReplicaSet của Deployment).

					ReplicaSet liên quan đến Deployment này cũng sẽ bị xóa.

			🔹 Ví dụ trước và sau khi xóa:

				Trước:

					kubectl get deployments -n kubernetes-dashboard
					NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
					kubernetes-dashboard        1/1     1            1           3d
					kubernetes-dashboard-kong   1/1     1            1           3d


			Sau khi chạy:

				kubectl delete deployment kubernetes-dashboard-kong -n kubernetes-dashboard

			Kết quả:

				deployment.apps "kubernetes-dashboard-kong" deleted

			Kiểm tra lại:

				kubectl get deployments -n kubernetes-dashboard


					→ Chỉ còn kubernetes-dashboard, kubernetes-dashboard-kong đã biến mất.

			Lưu ý:

				Nếu chỉ muốn tạm dừng (scale về 0) thay vì xóa hoàn toàn, có thể dùng:

					kubectl scale deployment kubernetes-dashboard-kong -n kubernetes-dashboard --replicas=0

				Nếu muốn xóa toàn bộ tài nguyên có liên quan (Deployment, Service, ConfigMap, Secret...), bạn có thể dùng file YAML gốc:

					kubectl delete -f dashboard_kong.yml

		
		- cmd: kubectl delete svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard	

			Mục đích:

				Lệnh này dùng để xóa một Service có tên là kubernetes-dashboard-kong-proxy trong namespace kubernetes-dashboard.

				Nói ngắn gọn: bạn đang gỡ bỏ “cổng truy cập mạng” (Service) của thành phần kubernetes-dashboard-kong.

			Giải thích từng phần:
			
				Thành phần							Ý nghĩa
				
				kubectl								Công cụ dòng lệnh để làm việc với Kubernetes
				delete								Thao tác xóa tài nguyên
				svc									Viết tắt của Service — là tài nguyên định nghĩa cách truy cập đến các Pod
				kubernetes-dashboard-kong-proxy		Tên của Service cần xóa
				-n kubernetes-dashboard				Xác định namespace chứa Service đó
			
			Khi chạy lệnh này:

				Kubernetes sẽ:

					Xóa Service kubernetes-dashboard-kong-proxy.

					Service này sẽ không còn phân phối lưu lượng (traffic) đến các Pod của kubernetes-dashboard-kong nữa.

					Không ảnh hưởng trực tiếp đến các Pod hoặc Deployment — chúng vẫn còn tồn tại, chỉ là không thể truy cập được qua Service này.

			Trước khi xóa:

				Khi bạn chạy:

					kubectl get svc -n kubernetes-dashboard

			Có thể bạn thấy kết quả như:

			NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   		AGE
			kubernetes-dashboard             ClusterIP   10.96.0.12      <none>        443/TCP   		3d
			kubernetes-dashboard-kong-proxy  NodePort    10.96.0.45      <none>        80:30080/TCP   	3d

			Service kubernetes-dashboard-kong-proxy ở đây là “proxy” cho Dashboard qua Kong — thường dùng để truy cập Dashboard từ bên ngoài
			cluster (qua NodePort hoặc LoadBalancer).

			Sau khi xóa:

				Chạy:

					kubectl delete svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard

				Kết quả:

					service "kubernetes-dashboard-kong-proxy" deleted

				Và khi kiểm tra lại:

					kubectl get svc -n kubernetes-dashboard

					→ Service này không còn xuất hiện, nghĩa là:

					Dashboard không thể truy cập được thông qua Kong nữa.

					Nếu vẫn cần truy cập Dashboard, bạn phải dùng các cách khác như:

						kubectl proxy

					hoặc

						kubectl port-forward svc/kubernetes-dashboard 8443:443 -n kubernetes-dashboard

		
		- cmd: kubectl delete pod -l app.kubernetes.io/name=kong -n kubernetes-dashboard

			Mục đích:

				Lệnh này dùng để xóa tất cả các Pod trong namespace kubernetes-dashboard có label app.kubernetes.io/name=kong.

				Nói đơn giản: bạn đang xóa tất cả các Pod của ứng dụng Kong trong namespace kubernetes-dashboard.

			Giải thích từng phần:
			
				Thành phần							Ý nghĩa
				
				kubectl								Công cụ dòng lệnh quản lý Kubernetes
				delete pod							Xóa một hoặc nhiều Pod
				-l app.kubernetes.io/name=kong		Tùy chọn lọc (label selector) — chỉ chọn các Pod có nhãn (label) app.kubernetes.io/name=kong
				-n kubernetes-dashboard				Giới hạn phạm vi tìm Pod trong namespace kubernetes-dashboard
				
			Về “label selector” (-l):

				Label (nhãn) là các cặp key=value gắn lên tài nguyên Kubernetes để phân loại, tìm kiếm, hoặc quản lý.

				Ví dụ, một Pod của Kong có thể có nhãn như sau:

					metadata:
					  labels:
						app.kubernetes.io/name: kong
						app.kubernetes.io/part-of: kubernetes-dashboard


				Vậy khi bạn dùng -l app.kubernetes.io/name=kong, Kubernetes sẽ chọn tất cả Pod nào có nhãn đó, không cần chỉ rõ tên cụ thể.

			Khi bạn chạy lệnh:

				Kubernetes sẽ tìm tất cả các Pod có nhãn app.kubernetes.io/name=kong trong namespace kubernetes-dashboard
				và xóa toàn bộ chúng.

				Ví dụ:

					kubectl get pods -n kubernetes-dashboard -l app.kubernetes.io/name=kong

				Trước khi xóa, có thể thấy:

					NAME                                        READY   STATUS    RESTARTS   AGE
					kubernetes-dashboard-kong-64d9b4c9d8-abc1   1/1     Running   0          2d
					kubernetes-dashboard-kong-64d9b4c9d8-xyz2   1/1     Running   0          2d

				Sau khi chạy:

					kubectl delete pod -l app.kubernetes.io/name=kong -n kubernetes-dashboard


				Kết quả:

					pod "kubernetes-dashboard-kong-64d9b4c9d8-abc1" deleted
					pod "kubernetes-dashboard-kong-64d9b4c9d8-xyz2" deleted

			Điều gì xảy ra sau khi xóa?

				Nếu các Pod này được quản lý bởi một Deployment hoặc ReplicaSet, thì Kubernetes sẽ tự động tạo lại
				các Pod mới để đảm bảo số lượng replicas đúng như cấu hình.

				Nếu đây là Pod độc lập (không thuộc Deployment), thì khi bị xóa → biến mất hoàn toàn, không được tạo lại.

			Mục đích thường gặp:

				Dùng để restart nhanh các Pod của ứng dụng Kong (xóa để chúng tự tạo lại).

				Dùng khi Kong bị lỗi hoặc cấu hình mới chưa áp dụng.

				Dọn dẹp môi trường khi bạn không cần Kong trong namespace kubernetes-dashboard nữa.
		
		- cmd: kubectl describe pod kong-kong-5c8c45bd6d-kj5sl -n kubernetes-dashboard	

			Mục đích:

				Lệnh này dùng để xem thông tin chi tiết về Pod có tên kong-kong-5c8c45bd6d-kj5sl trong namespace kubernetes-dashboard.

				Nói cách khác: đây là cách bạn kiểm tra mọi thứ bên trong và xung quanh một Pod, như container,
				image, sự kiện (events), IP, node, lý do Pod lỗi, v.v.

			Giải thích từng phần:
			
				Thành phần					Ý nghĩa
				
				kubectl						Công cụ dòng lệnh để quản lý Kubernetes
				describe					Hiển thị thông tin chi tiết của một tài nguyên (chi tiết hơn get)
				pod							Loại tài nguyên cần xem (ở đây là Pod)
				kong-kong-5c8c45bd6d-kj5sl	Tên cụ thể của Pod cần xem
				-n kubernetes-dashboard		Namespace chứa Pod đó
			
			Khi chạy lệnh, bạn sẽ thấy gì?

				Kết quả sẽ là một báo cáo chi tiết gồm nhiều phần, ví dụ:

					Name:         kong-kong-5c8c45bd6d-kj5sl
					Namespace:    kubernetes-dashboard
					Priority:     0
					Node:         worker-node-1/10.0.1.23
					Start Time:   Tue, 28 Oct 2025 16:30:12 +0700
					Labels:       app.kubernetes.io/component=app
								  app.kubernetes.io/instance=kong
								  app.kubernetes.io/name=kong
					Status:       Running
					IP:           10.244.2.14
					Controlled By: ReplicaSet/kong-kong-5c8c45bd6d
					Containers:
					  kong:
						Container ID:   docker://a7d5...
						Image:          kong/kong-gateway:3.5
						Image ID:       docker-pullable://kong/kong-gateway@sha256:...
						Port:           8000/TCP, 8443/TCP, 8001/TCP, 8444/TCP
						State:          Running
						  Started:      Tue, 28 Oct 2025 16:30:15 +0700
						Ready:          True
						Restart Count:  0
						Environment:
						  KONG_DATABASE:              off
						  KONG_DECLARATIVE_CONFIG:    /kong_dbless/kong.yml
						Mounts:
						  /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7f8gm (ro)
					Events:
					  Type    Reason     Age   From               Message
					  ----    ------     ----  ----               -------
					  Normal  Scheduled  12m   default-scheduler  Successfully assigned kubernetes-dashboard/kong-kong-5c8c45bd6d-kj5sl to worker-node-1
					  Normal  Pulled     12m   kubelet            Container image "kong/kong-gateway:3.5" already present on machine
					  Normal  Created    12m   kubelet            Created container kong
					  Normal  Started    12m   kubelet            Started container kong

			Giải thích các phần chính:
			
				Phần					Ý nghĩa
				
				Name / Namespace		Tên và không gian tên của Pod
				Node					Tên node vật lý (hoặc VM) mà Pod đang chạy trên đó
				Start Time				Thời điểm Pod được khởi tạo
				Labels / Annotations	Thông tin định danh, dùng cho chọn lọc (selector)
				Status					Trạng thái tổng thể của Pod (Pending, Running, Succeeded, Failed, CrashLoopBackOff, …)
				IP						Địa chỉ IP nội bộ của Pod
				Controlled By			Pod này do đối tượng nào tạo ra (thường là ReplicaSet hoặc Deployment)
				Containers				Mô tả chi tiết các container bên trong Pod:
				 ↳ Image				Docker image mà container sử dụng (ở đây là kong/kong-gateway)
				 ↳ Ports				Các cổng container mở ra
				 ↳ State				Trạng thái hiện tại: Running, Waiting, hoặc Terminated
				 ↳ Restart Count		Số lần container đã bị khởi động lại
				 ↳ Environment			Các biến môi trường (env) cấu hình trong Pod
				Events					Nhật ký sự kiện quan trọng (Pod được tạo, image được kéo, container khởi động hoặc lỗi, v.v.)
			
			Mục đích sử dụng thực tế:

				Debug lỗi Pod (ví dụ: image không kéo được, container crash, thiếu volume).

				Xem chi tiết cấu hình và môi trường của container.

				Kiểm tra log sự kiện (Events) liên quan đến Pod để biết nguyên nhân khi Pod không khởi chạy được.

				Xác định mối quan hệ giữa Pod ↔ ReplicaSet ↔ Deployment.
		
		- cmd: kubectl describe pod kong-kong-5c8c45bd6d-kj5sl -n kubernetes-dashboard | findstr "Failed"	

			Mục đích:

				Lệnh này dùng để tìm nhanh các dòng có chứa chữ “Failed” trong phần mô tả chi tiết của Pod
				kong-kong-5c8c45bd6d-kj5sl trong namespace kubernetes-dashboard.

				Nói đơn giản: nó giúp bạn lọc ra lỗi (Failed events) từ kết quả kubectl describe pod, thay vì phải đọc thủ công toàn bộ thông tin dài dòng.

			Giải thích từng phần:
			
				Thành phần																Ý nghĩa
				
				kubectl describe pod kong-kong-5c8c45bd6d-kj5sl -n kubernetes-dashboard	Hiển thị thông tin chi tiết của Pod (như bạn đã biết ở lệnh trước).
				**`																		` (pipe)**
				findstr "Failed"														Lệnh của Windows, có tác dụng tìm và hiển thị các dòng chứa chuỗi ký tự “Failed” trong kết quả đầu vào.
			
			Cách hoạt động:

				kubectl describe pod ... in ra toàn bộ thông tin chi tiết của Pod (nhiều dòng).

				Kết quả đó được chuyển qua lệnh findstr.

				findstr chỉ hiển thị những dòng có chứa từ “Failed”, ví dụ:

					  Warning  FailedScheduling  3m    default-scheduler  pod has unbound immediate PersistentVolumeClaims
					  Warning  FailedMount       2m    kubelet            Unable to attach or mount volumes: timed out waiting for the condition
					  Warning  Failed            1m    kubelet            Error: ImagePullBackOff


				Nhờ vậy bạn phát hiện lỗi nhanh hơn, không phải cuộn xem từng phần trong mô tả.

			Khi nào dùng lệnh này:

				Khi Pod không khởi động được (STATUS=CrashLoopBackOff, Pending, ImagePullBackOff, …).

				Khi bạn nghi ngờ Pod bị lỗi volume, image, hoặc kết nối mạng.

				Khi bạn chỉ cần xem dòng có chữ “Failed” để khoanh vùng sự cố.

			Mẹo thêm:

				Nếu bạn đang dùng Linux hoặc macOS, lệnh tương đương là:

				kubectl describe pod kong-kong-5c8c45bd6d-kj5sl -n kubernetes-dashboard | grep "Failed"

		
		- cmd: kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard-kong-proxy		
		
			Mục đích:

				Lệnh này dùng để mở và chỉnh sửa trực tiếp cấu hình của Service có tên
				kubernetes-dashboard-kong-proxy trong namespace kubernetes-dashboard.

				Nói cách khác, bạn đang mở file YAML của Service đó ra để sửa ngay trong trình soạn thảo (thường là vi hoặc nano)
				và sau khi lưu lại, Kubernetes sẽ áp dụng thay đổi mới ngay lập tức.

			Giải thích từng phần:
			
				Thành phần							Ý nghĩa
				
				kubectl								Công cụ dòng lệnh làm việc với Kubernetes
				-n kubernetes-dashboard				Chỉ định namespace cần thao tác
				edit								Mở trình chỉnh sửa trực tiếp cấu hình tài nguyên trên cluster
				svc									Loại tài nguyên muốn chỉnh sửa — ở đây là Service
				kubernetes-dashboard-kong-proxy		Tên Service cụ thể cần chỉnh sửa
			
			Khi chạy lệnh:

				Kubernetes sẽ tải cấu hình YAML hiện tại của Service kubernetes-dashboard-kong-proxy.

				Mở nội dung đó trong trình soạn thảo (mặc định là vi, trừ khi bạn cấu hình khác).

				Bạn có thể thay đổi các giá trị như:

					type (ClusterIP → NodePort / LoadBalancer)

					ports

					selector

					annotations

					labels

				Khi bạn lưu và thoát, Kubernetes sẽ:

					So sánh thay đổi.

					Áp dụng cấu hình mới lên cluster ngay.

			Ví dụ: nội dung Service trước khi chỉnh sửa
			
				apiVersion: v1
				kind: Service
				metadata:
				  name: kubernetes-dashboard-kong-proxy
				  namespace: kubernetes-dashboard
				spec:
				  type: ClusterIP
				  selector:
					app.kubernetes.io/name: kong
				  ports:
					- port: 80
					  targetPort: 8000

			Ví dụ thay đổi:

				Nếu bạn muốn truy cập từ bên ngoài cluster, có thể đổi type từ ClusterIP thành NodePort:

				spec:
				  type: NodePort
				  selector:
					app.kubernetes.io/name: kong
				  ports:
					- port: 80
					  targetPort: 8000
					  nodePort: 32080


				Sau khi lưu và thoát (:wq trong vi), bạn sẽ thấy thông báo:

					service/kubernetes-dashboard-kong-proxy edited


				Kubernetes ngay lập tức cập nhật Service đó.

			Kiểm tra lại thay đổi:
			
				kubectl get svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard


				hoặc xem chi tiết YAML:

					kubectl get svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard -o yaml

			Mục đích thực tế của lệnh:

				Dùng để chỉnh nhanh cấu hình Service mà không cần tìm và chỉnh file YAML gốc.

				Thường dùng để:

					Mở truy cập bên ngoài (ClusterIP → NodePort / LoadBalancer)

					Thay đổi port hoặc selector

					Thêm annotation phục vụ Ingress Controller, DNS, hay Load Balancer

					Gỡ lỗi (debug) Service routing		
		
		- cmd: kubectl get pvc -n default | findstr keycloak
		
			Giải thích tổng thể:

				Lệnh này dùng để tìm các PersistentVolumeClaim (PVC) trong namespace default có chứa chuỗi “keycloak” trong tên hoặc thông tin hiển thị.

			Phân tích từng phần:
			
				Thành phần				Ý nghĩa
				
				kubectl get pvc			Liệt kê tất cả các PersistentVolumeClaim (yêu cầu lưu trữ dữ liệu) trong namespace được chỉ định.
				-n default				Chỉ định namespace là default, tức namespace mặc định trong cluster Kubernetes.
				**`						`**
				findstr keycloak		Lệnh của Windows, dùng để lọc và chỉ hiển thị các dòng có chứa chữ “keycloak” trong kết quả đầu ra.
			
			Ví dụ minh họa:

				Giả sử bạn có kết quả của:

					kubectl get pvc -n default


				là:

					NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   AGE
					mysql-pvc           Bound    pvc-12345678-aaaa-bbbb-cccc-111122223333   1Gi        RWO            2d
					keycloak-data-pvc   Bound    pvc-87654321-dddd-eeee-ffff-444455556666   5Gi        RWO            1d
					redis-pvc           Bound    pvc-99998888-aaaa-bbbb-cccc-777766665555   1Gi        RWO            5h


				Khi chạy:

					kubectl get pvc -n default | findstr keycloak

				Kết quả sẽ là:

					keycloak-data-pvc   Bound    pvc-87654321-dddd-eeee-ffff-444455556666   5Gi        RWO            1d

				Nghĩa là chỉ hiện PVC có liên quan đến Keycloak.		
		
		- cmd: kubectl get secret -n default | findstr keycloak
		
			Mục đích:

				Lệnh này được dùng để tìm các Secret có liên quan đến Keycloak trong namespace default.
				
				Tức là nó lọc danh sách các Secret trong namespace default và chỉ hiển thị những secret có tên chứa chữ “keycloak”.

			Giải thích chi tiết từng phần:
			
				Thành phần				Ý nghĩa
				
				kubectl get secret		Hiển thị danh sách tất cả các Secret trong namespace chỉ định.
										Secret là nơi Kubernetes lưu thông tin nhạy cảm như mật khẩu, token, key, chứng chỉ TLS, v.v.
				-n default				Giới hạn phạm vi tìm kiếm trong namespace default.
				**`						` (pipe)**
				findstr keycloak		Lệnh trong Windows để lọc ra các dòng chứa chuỗi “keycloak”.
										(Nếu bạn đang dùng Linux/macOS thì lệnh tương đương là grep keycloak.)
			
			Ví dụ minh họa:

				Giả sử khi bạn chạy:

					kubectl get secret -n default

				bạn nhận được:

					NAME                              TYPE                                  AGE
					default-token-7d2ht               kubernetes.io/service-account-token   3d
					keycloak-db-secret                Opaque                                2d
					keycloak-tls-secret               kubernetes.io/tls                     2d
					myapp-secret                      Opaque                                1d


				Nếu bạn lọc bằng:

					kubectl get secret -n default | findstr keycloak

				Kết quả sẽ là:

					keycloak-db-secret                Opaque                                2d
					keycloak-tls-secret               kubernetes.io/tls                     2d


				Nghĩa là bạn chỉ thấy các Secret liên quan đến Keycloak.

			🔹 Khi nào dùng lệnh này:

				Khi bạn muốn kiểm tra Keycloak có tạo ra secret nào không, ví dụ như:

					Mật khẩu admin của Keycloak (keycloak-admin-secret)

					Chứng chỉ HTTPS (keycloak-tls-secret)

					Thông tin kết nối database (keycloak-db-secret)

				Khi bạn cần xem hoặc xóa các secret cũ của Keycloak.

				Khi debug lỗi khởi động của Keycloak mà nghi ngờ thiếu secret cần thiết.

			Nếu muốn xem chi tiết một secret:

			Sau khi biết tên secret (ví dụ keycloak-db-secret), bạn có thể xem chi tiết bằng:

				kubectl describe secret keycloak-db-secret -n default

			hoặc (để xem nội dung thật, ở dạng giải mã base64):

				kubectl get secret keycloak-db-secret -n default -o yaml
		
		
		- cmd: kubectl get configmap -n default | findstr keycloak
		
			Mục đích:

				Lệnh này được dùng để tìm các ConfigMap có liên quan đến Keycloak trong namespace default.
				
				Nói cách khác, nó liệt kê các ConfigMap trong namespace default, rồi lọc những dòng có chứa chữ “keycloak”.

			Giải thích chi tiết từng phần:
			
				Thành phần				Ý nghĩa
				
				kubectl get configmap	Lấy danh sách tất cả ConfigMap trong namespace được chỉ định.
										ConfigMap là đối tượng trong Kubernetes dùng để lưu trữ cấu hình không nhạy cảm (ví dụ: file cấu hình, biến môi trường, URL, cổng dịch vụ, ...).
				-n default				Chỉ định namespace cần tìm — ở đây là default.
				**`						` (pipe)**
				findstr keycloak		Lệnh trong Windows để lọc và chỉ hiển thị các dòng có chứa chữ “keycloak”.
										(Tương đương với grep keycloak trong Linux/macOS.)
			
			Ví dụ minh họa:

				Khi bạn chạy:

					kubectl get configmap -n default

				Kết quả có thể là:

					NAME                          DATA   AGE
					keycloak-config               3      2d
					keycloak-env                  5      2d
					app-settings                  2      1d
					database-config               4      3d


				Khi bạn chạy:

					kubectl get configmap -n default | findstr keycloak


				Kết quả lọc ra:

					keycloak-config               3      2d
					keycloak-env                  5      2d


				Chỉ hiển thị các ConfigMap có tên chứa “keycloak”.

			Khi nào bạn dùng lệnh này:

				Khi bạn muốn kiểm tra Keycloak có tạo ra ConfigMap nào không.
				(VD: Keycloak có thể tạo ConfigMap để chứa biến môi trường hoặc cấu hình mạng.)

				Khi bạn muốn sửa hoặc xóa ConfigMap cũ của Keycloak.

				Khi debug lỗi Keycloak mà nghi ngờ liên quan đến file cấu hình (env, database URL, realm, …).

				🔹 Để xem chi tiết nội dung của một ConfigMap:

				Sau khi biết tên, ví dụ keycloak-config, bạn có thể xem bằng:

				kubectl describe configmap keycloak-config -n default


				hoặc nếu muốn xem nội dung YAML đầy đủ:

					kubectl get configmap keycloak-config -n default -o yaml
		
		
		- cmd: kubectl get sa -n default | findstr keycloak	
		
			Mục đích:

				Lệnh này dùng để tìm tất cả các ServiceAccount có liên quan đến Keycloak trong namespace default.
				
				Nói cách khác, nó liệt kê toàn bộ ServiceAccount (SA) trong namespace default rồi lọc ra những cái có tên chứa chữ “keycloak”.

			Giải thích từng phần:
			
				Thành phần					Ý nghĩa
				
				kubectl get sa				Lấy danh sách tất cả các ServiceAccount trong namespace được chỉ định.
											ServiceAccount là tài khoản kỹ thuật mà Pod hoặc ứng dụng trong Kubernetes dùng để xác thực với API Server (để có quyền truy cập vào tài nguyên).
				-n default					Chỉ định namespace là default, tức namespace mặc định của cluster.
				**`							` (pipe)**
				findstr keycloak			Lọc ra các dòng có chứa chữ “keycloak” (chỉ có trong Windows CMD/PowerShell).
											(Tương đương grep keycloak trên Linux/macOS)
			
			Ví dụ minh họa:

				Giả sử bạn chạy:

					kubectl get sa -n default

				và nhận được:

					NAME                   SECRETS   AGE
					default                1         5d
					keycloak               1         2d
					keycloak-admin-sa      1         2d
					app-serviceaccount     1         1d


				Nếu bạn chạy:

					kubectl get sa -n default | findstr keycloak

				Kết quả sẽ là:

					keycloak               1         2d
					keycloak-admin-sa      1         2d


				Chỉ các ServiceAccount có tên chứa “keycloak” sẽ được hiển thị.

			Khi nào bạn dùng lệnh này:

				Khi muốn kiểm tra Keycloak có ServiceAccount nào đang tồn tại không.
				(Ví dụ: keycloak hoặc keycloak-admin-sa).

				Khi bạn cần liên kết quyền (ClusterRoleBinding / RoleBinding) cho ServiceAccount của Keycloak.

				Khi bạn cần xóa hoặc sửa quyền truy cập của Keycloak trong cluster.

				Khi debug lỗi "permission denied" của Pod Keycloak.

			Để xem chi tiết về một ServiceAccount:

				Sau khi biết tên SA, bạn có thể xem thông tin chi tiết bằng:

					kubectl describe sa keycloak-admin-sa -n default


				Lệnh này sẽ hiển thị:

					Các Secret token liên kết với SA đó.

					Các annotations hoặc imagePullSecrets.

					Namespace mà nó thuộc về.		
		
		- cmd: kubectl delete namespace kubernetes-dashboard	

			Mục đích:

				Lệnh này dùng để xóa toàn bộ namespace kubernetes-dashboard cùng với tất cả tài nguyên (resources) nằm trong đó.

				Nói cách khác: bạn đang xóa luôn toàn bộ Kubernetes Dashboard và mọi thứ đi kèm (pod, service, deployment,
				secret, configmap, v.v.).

			
			Giải thích từng phần:
			
				Thành phần				Ý nghĩa
				
				kubectl					Công cụ dòng lệnh để làm việc với Kubernetes.
				delete					Thao tác xóa một hoặc nhiều đối tượng (resources).
				namespace				Loại đối tượng bạn muốn xóa — ở đây là namespace.
				kubernetes-dashboard	Tên namespace cần xóa.
			
			Khi thực hiện, Kubernetes sẽ:

				Đánh dấu namespace là “Terminating” (đang xóa).

				Xóa toàn bộ tài nguyên bên trong, bao gồm:

					Pods

					Deployments

					Services

					Secrets

					ConfigMaps

					ServiceAccounts

					RoleBindings / ClusterRoleBindings

					PersistentVolumeClaims (PVCs)

					v.v.

				Sau khi tất cả tài nguyên trong namespace bị xóa xong → namespace cũng sẽ biến mất hoàn toàn khỏi cluster.

			🔹 Ví dụ:

				Trước khi xóa:

					kubectl get ns

				Kết quả:

					NAME                   STATUS   AGE
					default                Active   10d
					kube-system            Active   10d
					kubernetes-dashboard   Active   2d

				Sau khi chạy:

					kubectl delete namespace kubernetes-dashboard

				Kết quả:

					namespace "kubernetes-dashboard" deleted

				Và nếu kiểm tra lại:

					kubectl get ns

				Thì bạn sẽ thấy:

					NAME                   STATUS   AGE
					default                Active   10d
					kube-system            Active   10d

				(không còn kubernetes-dashboard nữa)

			Lưu ý quan trọng:

				Xóa namespace sẽ xóa toàn bộ dữ liệu và cấu hình bên trong — thao tác không thể khôi phục.

				Một số namespace (như kube-system, default) không nên xóa, vì chúng chứa thành phần lõi của Kubernetes.

				Nếu namespace bị kẹt ở trạng thái Terminating, bạn có thể cần xóa thủ công các finalizer trong YAML của
				namespace (dành cho trường hợp đặc biệt khi có tài nguyên không tự xóa được).

			Nếu bạn chỉ muốn xóa Dashboard mà không xóa cả namespace, bạn có thể chạy:
			
				kubectl delete all --all -n kubernetes-dashboard

			Lệnh này chỉ xóa toàn bộ tài nguyên bên trong namespace, nhưng vẫn giữ lại namespace.
		
		- cmd: kubectl delete crd probes.monitoring.coreos.com	

			Mục đích:

				Lệnh này dùng để xóa CustomResourceDefinition (CRD) có tên probes.monitoring.coreos.com khỏi cluster Kubernetes.

				Nói cách khác: bạn đang gỡ bỏ hoàn toàn kiểu tài nguyên “Probe” do hệ thống giám sát (Monitoring) — thường
				là Prometheus Operator — tạo ra.

			Giải thích từng phần:
			
				Thành phần						Ý nghĩa
				kubectl							Công cụ dòng lệnh để tương tác với Kubernetes API.
				delete							Hành động xóa tài nguyên.
				crd								Viết tắt của CustomResourceDefinition — là loại tài nguyên tùy chỉnh mà người dùng hoặc
												operator định nghĩa thêm vào Kubernetes (ngoài các loại mặc định như Pod, Deployment, Service...).
				probes.monitoring.coreos.com	Tên đầy đủ (fully qualified name) của CRD cần xóa.
												→ Đây là CRD thuộc nhóm monitoring.coreos.com — thường được cài đặt khi bạn dùng Prometheus
												Operator hoặc kube-prometheus-stack.
			
			CRD là gì?

				CRD (CustomResourceDefinition) cho phép bạn mở rộng API của Kubernetes.

				Khi một CRD được tạo, Kubernetes hiểu rằng có một loại tài nguyên mới tồn tại — ví dụ:

					prometheuses.monitoring.coreos.com

					alertmanagers.monitoring.coreos.com

					servicemonitors.monitoring.coreos.com

					probes.monitoring.coreos.com ← chính là cái bạn đang xóa.

				CRD này định nghĩa cách hoạt động của đối tượng “Probe” — dùng để cấu hình các kiểm tra (probes) do Prometheus thực hiện.

			Khi bạn chạy lệnh:

				Kubernetes sẽ:

					Gỡ bỏ định nghĩa của CRD probes.monitoring.coreos.com.

					Xóa tất cả các đối tượng (instances) thuộc loại này — ví dụ:

						kubectl get probes.monitoring.coreos.com

						→ Tất cả các “Probe” này sẽ biến mất.

					Sau khi xóa, Kubernetes không còn hiểu loại tài nguyên Probe nữa.

			Ví dụ thực tế:

				Trước khi xóa:

					kubectl get crd | grep probe

				Kết quả:

					probes.monitoring.coreos.com                   2024-09-01T10:12:30Z

				Sau khi chạy:

					kubectl delete crd probes.monitoring.coreos.com

				Kết quả:

					customresourcedefinition.apiextensions.k8s.io "probes.monitoring.coreos.com" deleted

				Nếu bạn kiểm tra lại:

					kubectl get crd | grep probe

					→ Không còn thấy probes.monitoring.coreos.com.

			Lưu ý quan trọng:

				Việc xóa CRD này sẽ xóa toàn bộ các tài nguyên “Probe” trong cluster.

				Nếu hệ thống giám sát (như Prometheus Operator) vẫn đang chạy, nó có thể gặp lỗi vì không tìm thấy CRD cần thiết.

				Thường, bạn chỉ nên xóa CRD khi:

					Bạn gỡ bỏ hoàn toàn Prometheus Operator hoặc monitoring stack, hoặc

					CRD bị lỗi / xung đột và cần cài đặt lại.

			Nếu bạn muốn chỉ xem CRD thay vì xóa:
			
				kubectl get crd probes.monitoring.coreos.com -o yaml

				Nếu bạn muốn xóa tất cả CRD của Prometheus Operator, có thể dùng:

					kubectl get crd | findstr monitoring.coreos.com

				(hoặc grep trên Linux), rồi xóa từng cái nếu cần.
		
		- cmd: kubectl describe pod prometheus-prometheus-kube-prometheus-prometheus-0 -n default		
		
			Mục đích:

				Lệnh này dùng để xem chi tiết thông tin hoạt động của Pod Prometheus có tên
				prometheus-prometheus-kube-prometheus-prometheus-0 trong namespace default.

				Nói đơn giản: nó giúp bạn xem mọi thứ đang diễn ra bên trong Pod Prometheus, bao gồm cấu hình,
				container, volume, trạng thái, và cả lỗi nếu có.

			Phân tích từng phần:
			
				Thành phần												Ý nghĩa
				
				kubectl													Công cụ dòng lệnh để tương tác với Kubernetes cluster.
				describe												Hiển thị thông tin chi tiết (mô tả đầy đủ) của một tài nguyên Kubernetes.
				pod														Loại tài nguyên mà bạn muốn xem chi tiết — ở đây là Pod.
				prometheus-prometheus-kube-prometheus-prometheus-0		Tên cụ thể của Pod Prometheus.
																		Đây thường là Pod StatefulSet của Prometheus trong stack kube-prometheus.
				-n default												Chỉ định namespace là default, tức là Pod này nằm trong namespace mặc định.
			
			Khi chạy lệnh, bạn sẽ thấy những gì:

				Kubernetes sẽ hiển thị một bản mô tả chi tiết của Pod, gồm nhiều phần như sau:

					Basic Info (thông tin cơ bản)

						Ví dụ:

						Name:         prometheus-prometheus-kube-prometheus-prometheus-0
						Namespace:    default
						Node:         worker-node1/10.0.0.5
						Start Time:   Mon, 28 Oct 2025 16:30:00 +0700
						Status:       Running
						IP:           10.244.1.12
						Controlled By: StatefulSet/prometheus-prometheus-kube-prometheus-prometheus


						Cho biết:

							Tên Pod, namespace, node nơi Pod đang chạy.

							Trạng thái hiện tại (Running, Pending, CrashLoopBackOff, …).

							Ai quản lý Pod này (ở đây là StatefulSet).

					Containers

						Phần này mô tả từng container trong Pod:

							Containers:
							  prometheus:
								Image:      quay.io/prometheus/prometheus:v2.54.1
								Port:       9090/TCP
								State:      Running
								Ready:      True
								Restart Count:  0

						Thông tin bao gồm:

							Tên container (prometheus)

							Image đang sử dụng

							Port mà container mở

							Trạng thái (Running, Waiting, Terminated)

							Số lần khởi động lại (Restart Count)

							Các volume mount (nơi lưu trữ dữ liệu Prometheus, thường ở /prometheus)

					Conditions (Điều kiện hoạt động)
					
						Conditions:
						  Type              Status
						  Initialized       True
						  Ready             True
						  ContainersReady   True
						  PodScheduled      True


						Nếu tất cả điều kiện là True, nghĩa là Pod đang chạy ổn định.

					Volumes (Ổ lưu trữ gắn vào Pod)
					
						Volumes:
						  prometheus-prometheus-kube-prometheus-prometheus-db:
							Type: PersistentVolumeClaim (PVC)
							ClaimName: prometheus-prometheus-kube-prometheus-prometheus-db-prometheus-prometheus-kube-prometheus-prometheus-0


						Phần này cho thấy Pod Prometheus đang dùng PVC (PersistentVolumeClaim) để lưu trữ dữ liệu metric bền vững.

					Events (Sự kiện gần đây)

					Ví dụ:

						Events:
						  Type    Reason     Age   From               Message
						  ----    ------     ----  ----               -------
						  Normal  Scheduled  5m    default-scheduler  Successfully assigned default/prometheus-prometheus-kube-prometheus-prometheus-0 to worker-node1
						  Normal  Pulled     5m    kubelet            Container image "quay.io/prometheus/prometheus:v2.54.1" already present on machine
						  Normal  Started    5m    kubelet            Started container prometheus


					Phần này cực kỳ hữu ích để debug lỗi, vì nó cho biết:

						Khi nào Pod được gán vào node.

						Có tải image thành công không.

						Có lỗi nào xảy ra khi khởi động không (như CrashLoopBackOff, ImagePullBackOff, v.v.)

			Pod này là gì?

				Pod có tên dài như prometheus-prometheus-kube-prometheus-prometheus-0 thường là:

				Pod chính của Prometheus trong kube-prometheus-stack (của Prometheus Operator).

				Được quản lý bởi một StatefulSet, nên có số thứ tự ở cuối (-0, -1, ...).

				Dùng để thu thập metric từ cluster Kubernetes.		
		
		- cmd: kubectl delete configmap eazybankprod-configmap -n default	

			Mục đích:
			
				Lệnh này dùng để xóa một ConfigMap có tên là eazybankprod-configmap trong namespace default.

				Nói cách khác: bạn đang xóa cấu hình môi trường (dạng key–value) đã được Kubernetes lưu cho ứng
				dụng “EazyBank” ở môi trường production (prod).


			Phân tích từng phần:
			
				Thành phần					Ý nghĩa
				
				kubectl						Công cụ dòng lệnh để làm việc với Kubernetes cluster.
				delete						Lệnh dùng để xóa tài nguyên (Pod, Service, ConfigMap, Secret, v.v.).
				configmap					Loại tài nguyên cần xóa — ở đây là ConfigMap, dùng để lưu trữ cấu hình (key-value) mà Pod có thể đọc.
				eazybankprod-configmap		Tên của ConfigMap cụ thể cần xóa.
				-n default					Chỉ định namespace chứa ConfigMap này là default.
			
			ConfigMap là gì?
			
				ConfigMap là đối tượng Kubernetes dùng để lưu trữ cấu hình dưới dạng key–value pairs.
				
				Các Pod hoặc container trong cluster có thể:

				Đọc dữ liệu từ ConfigMap qua biến môi trường, hoặc

				Mount ConfigMap vào container dưới dạng file cấu hình.


				Ví dụ một ConfigMap (được tạo từ YAML):
				
					apiVersion: v1
					kind: ConfigMap
					metadata:
					  name: eazybankprod-configmap
					  namespace: default
					data:
					  DATABASE_URL: jdbc:mysql://mysql:3306/eazybank
					  LOG_LEVEL: INFO
					  FEATURE_TOGGLE: "true"


			Khi bạn chạy lệnh delete này:
			
				Kubernetes sẽ:

					Xóa ConfigMap eazybankprod-configmap khỏi namespace default.

				Mọi Pod đang sử dụng ConfigMap này sẽ không tự động khởi động lại, nhưng nếu Pod bị restart hoặc redeploy, nó
				sẽ không còn đọc được cấu hình đó nữa (vì ConfigMap đã bị xóa).

			Kết quả sau khi chạy lệnh:
			
				Bạn sẽ thấy thông báo như:
				
				configmap "eazybankprod-configmap" deleted

			Lưu ý quan trọng:


				Sau khi xóa ConfigMap, nếu có Deployment hoặc Pod nào phụ thuộc vào nó, bạn có thể gặp lỗi như:
				
					MountVolume.SetUp failed for volume "config-volume" : configmap "eazybankprod-configmap" not found

				Để tránh lỗi, nên kiểm tra trước bằng:
				
					kubectl get pod -n default -o yaml | findstr eazybankprod-configmap

				để xem có Pod nào đang sử dụng ConfigMap này không.

			Mẹo quản lý an toàn:
			
				Nếu bạn chỉ muốn cập nhật cấu hình, không cần xóa hẳn, có thể dùng:
				
					kubectl apply -f eazybankprod-configmap.yml

				hoặc:
					
					kubectl create configmap eazybankprod-configmap --from-literal=KEY=VALUE --dry-run=client -o yaml | kubectl apply -f -

				→ Cách này sẽ cập nhật ConfigMap mà không phải xóa đi tạo lại, giúp hệ thống ổn định hơn.
		
		- cmd: kubectl delete svc cards -n default	
	
			Mục đích:

				Lệnh này dùng để xóa một Service có tên là cards trong namespace default.

				Nói cách khác: bạn đang gỡ bỏ cơ chế truy cập mạng nội bộ (hoặc bên ngoài) cho ứng dụng cards trong Kubernetes.

			Phân tích từng phần:
			
				Thành phần					Ý nghĩa
				
				kubectl						Công cụ dòng lệnh để tương tác với Kubernetes cluster.
				delete						Dùng để xóa tài nguyên (Pod, Service, ConfigMap, Deployment, v.v.).
				svc (viết tắt của service)	Loại tài nguyên cần xóa — ở đây là Service, chịu trách nhiệm định tuyến network tới các Pod.
				cards						Tên Service cần xóa (ứng dụng “cards” của bạn).
				-n default					Chỉ định namespace là default (vì Kubernetes có thể có nhiều namespace).
				
			Service là gì trong Kubernetes?

				Service là một tài nguyên giúp ổn định địa chỉ mạng cho các Pod trong cùng một ứng dụng.

				Vì Pod có thể bị xóa và tạo lại (thay đổi IP), Service cung cấp một điểm truy cập cố định để:

					Các Pod khác trong cluster có thể truy cập ổn định qua tên Service (DNS).

					(Nếu là NodePort hoặc LoadBalancer) các client bên ngoài cluster cũng có thể truy cập vào ứng dụng.

			🔹 Ví dụ:

				Giả sử Service cards được tạo bằng YAML như sau:

					apiVersion: v1
					kind: Service
					metadata:
					  name: cards
					  namespace: default
					spec:
					  selector:
						app: cards
					  ports:
						- port: 8080
						  targetPort: 8080
					  type: ClusterIP


				Service này cho phép các Pod khác trong cluster gọi http://cards:8080 để truy cập ứng dụng cards.

			Khi bạn chạy lệnh xóa:
			
				kubectl delete svc cards -n default


			Kubernetes sẽ:

				Xóa Service cards khỏi namespace default.

				Ngắt toàn bộ kết nối đến các Pod cards thông qua DNS hoặc cổng NodePort/LoadBalancer.

				Các Pod thật (ứng dụng cards) vẫn còn, nhưng không thể được truy cập qua Service nữa.

			Kết quả hiển thị:

				Nếu xóa thành công, bạn sẽ thấy:

					service "cards" deleted

			Lưu ý:

				Việc xóa Service không xóa các Pod hay Deployment liên quan.

				Tuy nhiên, bất kỳ ứng dụng nào đang gọi đến cards (qua DNS cards.default.svc.cluster.local) sẽ bị lỗi kết nối (Connection refused, Name not found, v.v.).

				Nếu bạn dùng Ingress hoặc LoadBalancer phụ thuộc vào Service này, nó cũng sẽ mất đường dẫn truy cập.

			Kiểm tra trước khi xóa (gợi ý):

				Trước khi xóa, bạn có thể kiểm tra xem Service đang hoạt động thế nào:

					kubectl get svc cards -n default -o wide

				Hoặc xem có Ingress nào đang dùng Service cards:

					kubectl get ingress -n default -o yaml | findstr cards
		
		- cmd: docker pull kong:3.9		
		
			Lệnh docker pull kong:3.9 dùng để tải (pull) một image của Kong Gateway phiên bản 3.9 từ Docker Hub về máy của bạn.
			

			Cú pháp:
			
				docker pull <tên_image>:<tag>


				Trong trường hợp này:

					docker pull kong:3.9

			Giải nghĩa từng phần:
			
				Thành phần				Ý nghĩa
				
				docker					Lệnh chính của Docker CLI (command-line interface)
				pull					Hành động tải image từ registry (thường là Docker Hub)
				kong					Tên image — đây là image chính thức của Kong Gateway (API Gateway mã nguồn mở)
				3.9						Tag chỉ phiên bản của image (ở đây là Kong phiên bản 3.9)
			
			Khi chạy lệnh này:

				Docker sẽ kiểm tra xem image kong:3.9 có sẵn trong máy bạn chưa.

					Nếu đã có, Docker sẽ bỏ qua (trừ khi bạn ép cập nhật).

					Nếu chưa có, Docker sẽ tải từ Docker Hub (mặc định registry).

				Sau khi tải xong, bạn có thể xác nhận bằng:

					docker images


					→ Sẽ thấy dòng tương tự:

					REPOSITORY   TAG     IMAGE ID       CREATED       SIZE
					kong         3.9     <some_id>      <date>        <size>

			Ví dụ sử dụng tiếp theo:

				Sau khi pull xong, bạn có thể chạy Kong bằng:

					docker run -d --name kong \
					  -p 8000:8000 -p 8001:8001 \
					  kong:3.9


				Lúc này Kong Gateway sẽ khởi động và lắng nghe cổng 8000 (proxy) và 8001 (admin API).

			Ghi chú:

				Nếu bạn không chỉ định tag (ví dụ docker pull kong), Docker sẽ mặc định tải kong:latest — tức là phiên bản mới nhất.

				Việc chỉ định tag (như 3.9) giúp đảm bảo môi trường ổn định, tránh bị thay đổi khi Kong phát hành phiên bản mới.		
		
		- cmd: helm repo add kong https://charts.konghq.com
		
			Lệnh helm repo add kong https://charts.konghq.com dùng để thêm (add) một Helm chart repository mới — cụ thể là repository
			chính thức của Kong — vào cấu hình của Helm trên máy của bạn.

			Cú pháp chung:
			
				helm repo add <tên_repo> <đường_dẫn_repo>

			Giải thích từng phần trong lệnh:
			
				helm repo add kong https://charts.konghq.com

				Thành phần					Ý nghĩa
				
				helm						Công cụ quản lý package (chart) cho Kubernetes
				repo						Từ khóa chỉ hành động làm việc với repositories (nơi chứa các chart)
				add							Thao tác để thêm một repository mới
				kong						Tên định danh (alias) bạn muốn đặt cho repository này — có thể dùng tên khác
											nếu muốn (vd: kong-gateway)
				https://charts.konghq.com	Đường dẫn (URL) đến repository chính thức của Kong trên Internet, nơi lưu trữ các Helm
											charts cho Kong Gateway, Kong Ingress Controller, v.v.
			
			Khi bạn chạy lệnh này:

				Helm sẽ kết nối tới URL https://charts.konghq.com.

				Nó sẽ tải về metadata (index.yaml) mô tả các chart có sẵn trong repo đó.

				Helm sẽ lưu lại repo này trong danh sách repo cục bộ của bạn.

			Kiểm tra lại:

				Sau khi thêm xong, bạn có thể kiểm tra:

					helm repo list


				Sẽ thấy dòng tương tự:

					NAME    URL
					kong    https://charts.konghq.com

			Cập nhật danh sách chart:

				Để đảm bảo Helm có danh sách chart mới nhất:

					helm repo update

			Sử dụng sau đó:

				Sau khi đã add repo, bạn có thể cài đặt Kong Gateway vào Kubernetes bằng Helm:

					helm install kong kong/kong

				Ở đây:

					kong/kong nghĩa là: cài chart kong từ repo kong.		
		
		- cmd: helm repo update	
		
			Lệnh helm repo update được dùng để cập nhật (update) lại danh sách các Helm charts từ tất cả các
			repository mà bạn đã thêm vào Helm trước đó.

			Cú pháp:
			
				helm repo update

			Giải thích chi tiết:
			
				Thành phần		Ý nghĩa
				
				helm			Lệnh chính của công cụ Helm
				repo			Làm việc với các repository (kho chứa chart)
				update			Hành động cập nhật thông tin (metadata) của các chart trong repo
			
			Khi bạn chạy lệnh này:

				Helm sẽ duyệt qua toàn bộ danh sách các repo mà bạn đã thêm bằng helm repo add (ví dụ: kong, bitnami, prometheus-community, ...).

				Nó sẽ tải lại file index.yaml từ mỗi repo đó.

				File này chứa danh sách các chart, phiên bản, và metadata tương ứng.

				Helm sẽ lưu dữ liệu mới nhất vào bộ nhớ cục bộ (~/.cache/helm hoặc ~/.config/helm tùy phiên bản).

			Ví dụ:

				Giả sử bạn đã thêm 2 repo:

					helm repo add kong https://charts.konghq.com
					
					helm repo add bitnami https://charts.bitnami.com/bitnami

				Khi bạn chạy:

					helm repo update

				Kết quả sẽ tương tự:

					Hang tight while we grab the latest from your chart repositories...
					...Successfully got an update from the "kong" chart repository
					...Successfully got an update from the "bitnami" chart repository
					Update Complete. ⎈Happy Helming!⎈

			Tại sao cần chạy helm repo update?

				Khi bạn vừa thêm repo mới (helm repo add ...), Helm chỉ biết URL, chưa tải danh sách chart.

				Hoặc khi repo có chart mới / version mới, bạn cần update để thấy và cài được bản đó.

				Nếu không chạy lệnh này, bạn có thể gặp lỗi như:

					Error: chart "kong" not found in repo "kong"

				→ Do Helm chưa có dữ liệu mới nhất từ repo.		
		
		- cmd: helm create eazybank-common	

			Lệnh helm create eazybank-common được dùng để tạo một Helm chart mới có tên là eazybank-common — tức là một
			bộ khung (template) sẵn cho bạn bắt đầu triển khai ứng dụng trên Kubernetes bằng Helm.

			Cú pháp:
			
				helm create <tên_chart>

			Giải thích từng phần:
			
				Thành phần			Ý nghĩa
				
				helm				Lệnh chính của công cụ Helm
				create				Hành động để tạo mới một chart mẫu
				eazybank-common		Tên chart mới mà bạn muốn tạo (thư mục sẽ có tên này)
			
			Khi bạn chạy:
			
				helm create eazybank-common


				Helm sẽ:

					Tạo một thư mục mới có tên eazybank-common

					Bên trong thư mục đó là cấu trúc mặc định của một Helm chart

			Cấu trúc thư mục được tạo:
			
				eazybank-common/
				│
				├── Chart.yaml           # Thông tin metadata của chart (tên, version, mô tả...)
				├── values.yaml          # File cấu hình mặc định (giá trị có thể override)
				│
				├── charts/              # Nơi chứa các chart phụ (dependencies)
				│
				├── templates/           # Các file template YAML cho Kubernetes
				│   ├── deployment.yaml
				│   ├── service.yaml
				│   ├── ingress.yaml
				│   ├── _helpers.tpl     # Các hàm hoặc biến dùng chung
				│   └── NOTES.txt        # Hướng dẫn hiển thị sau khi cài chart
				│
				└── .helmignore          # Giống .gitignore – bỏ qua file khi đóng gói chart

			Ý nghĩa của từng phần chính:
			
				File / Thư mục				Chức năng
				
				Chart.yaml					Mô tả chart: tên (name), phiên bản (version), mô tả (description),...
				values.yaml					Nơi bạn định nghĩa các giá trị có thể tùy chỉnh (VD: image, port, replicas, env variables...)
				templates/					Chứa các file YAML được Helm render ra Kubernetes manifests thực tế
				_helpers.tpl				Chứa các hàm dùng lại (ví dụ: đặt tên tài nguyên)
				NOTES.txt					Hiển thị hướng dẫn sau khi người dùng cài chart (ví dụ URL truy cập app)
			
			Ví dụ:

				Sau khi tạo xong, bạn có thể chỉnh sửa values.yaml:

					replicaCount: 2
					image:
					  repository: eazybank/backend
					  tag: "1.0.0"
					service:
					  type: ClusterIP
					  port: 8080

				Và triển khai chart đó lên Kubernetes:

					helm install eazybank eazybank-common/

			Khi nào dùng helm create

				Khi bạn muốn tự tạo một chart mới cho ứng dụng của mình.

				Khi bạn muốn chuẩn hóa cấu trúc triển khai giữa nhiều service (vd: eazybank-common dùng chung cho nhiều module: accounts, loans, cards,…).
		
		- cmd: helm dependencies build		
		
			Lệnh helm dependencies build (hoặc viết đầy đủ là helm dependency build) được dùng để tải và cài đặt các chart
			phụ thuộc (dependencies) được khai báo trong file Chart.yaml của Helm chart hiện tại.

			Cú pháp:
			
				helm dependency build [đường_dẫn_đến_chart]


				Nếu không chỉ định đường dẫn, Helm mặc định dùng thư mục hiện tại (.).

			Ví dụ:

				helm dependency build

				hoặc

					helm dependency build ./eazybank-common

			Giải thích từng phần:
			
				Thành phần							Ý nghĩa
				
				helm								Công cụ quản lý Helm chart
				dependency (hoặc dependencies)		Nhóm lệnh làm việc với các chart phụ thuộc
				build								Hành động tải và xây dựng các chart phụ thuộc về local
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Đọc file Chart.yaml trong chart hiện tại.
				
				Trong đó có phần:

					dependencies:
					  - name: postgresql
						version: 12.5.8
						repository: https://charts.bitnami.com/bitnami
					  - name: redis
						version: 17.3.2
						repository: https://charts.bitnami.com/bitnami


				Helm sẽ:

					Kết nối đến các repo tương ứng (bitnami, kong, v.v.)

					Tải (download) các chart phụ thuộc được liệt kê ở trên.

					Lưu chúng vào thư mục:

						charts/


					dưới dạng .tgz (ví dụ: postgresql-12.5.8.tgz, redis-17.3.2.tgz).

					Nếu thư mục charts/ đã có file cũ, Helm sẽ xóa hoặc cập nhật lại cho đúng version.

			Kết quả sau khi chạy:

				Thư mục chart của bạn sẽ trông như sau:

					eazybank-common/
					├── Chart.yaml
					├── values.yaml
					├── templates/
					└── charts/
						├── postgresql-12.5.8.tgz
						└── redis-17.3.2.tgz

			So sánh với các lệnh khác:
			
				Lệnh						Mục đích
				
				helm dependency update		Tải hoặc cập nhật dependencies (giống build, nhưng không kiểm tra chart lock file)
				helm dependency build		Dựa vào file Chart.lock nếu có để đảm bảo đúng phiên bản dependency
				helm dependency list		Hiển thị danh sách các dependencies của chart
			
			Khi nào cần chạy helm dependency build:

				Khi bạn mới clone một repo chart từ GitHub — vì chart phụ thuộc không được commit sẵn.

				Khi bạn thêm hoặc thay đổi dependencies trong Chart.yaml.

				Khi bạn muốn đảm bảo các chart phụ thuộc được tải đúng version (theo Chart.lock).		
		
		- cmd: helm template .	
		
			Lệnh helm template . được dùng để render (biên dịch) các Helm templates trong chart hiện tại (thư mục .) thành các file
			YAML thuần của Kubernetes, nhưng không triển khai lên cluster.

			Cú pháp:
			
				helm template [RELEASE_NAME] [PATH_TO_CHART]

			Ví dụ:

				helm template .          # render chart hiện tại, không chỉ định tên release  
				helm template eazybank . # render chart hiện tại, với tên release là "eazybank"

			Giải thích từng phần:
			
				Thành phần				Ý nghĩa
				
				helm					Công cụ Helm
				template				Subcommand dùng để biên dịch (render) các file template thành YAML thật
				.						Nghĩa là chart nằm ở thư mục hiện tại (current directory)
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Đọc toàn bộ nội dung chart hiện tại, bao gồm:

					Chart.yaml

					values.yaml

					Các file trong templates/

					Bất kỳ chart phụ thuộc nào trong charts/

				Áp dụng các giá trị từ values.yaml (và các giá trị bạn truyền thêm bằng --set hoặc -f file.yaml nếu có).

				Render (thay thế biến, biểu thức) trong các file template YAML (templates/*.yaml) thành YAML hoàn chỉnh mà Kubernetes có thể hiểu.

				In ra kết quả ra terminal (stdout) mà không triển khai gì lên cluster.

			Ví dụ minh họa:

				Giả sử bạn có trong templates/deployment.yaml:

					apiVersion: apps/v1
					kind: Deployment
					metadata:
					  name: {{ .Release.Name }}-{{ .Chart.Name }}
					spec:
					  replicas: {{ .Values.replicaCount }}

				và trong values.yaml:

					replicaCount: 2

				Khi bạn chạy:

					helm template eazybank .

				Kết quả hiển thị sẽ là:

					---
					# Source: eazybank/templates/deployment.yaml
					apiVersion: apps/v1
					kind: Deployment
					metadata:
					  name: eazybank-eazybank
					spec:
					  replicas: 2

			Khi nào nên dùng helm template:
			
				Mục đích							Mô tả
				
				Kiểm tra trước khi triển khai		Xem Kubernetes YAML được sinh ra có đúng không (giúp tránh lỗi)
				Debug								Dễ dàng phát hiện lỗi cú pháp hoặc biến không tồn tại
				Xuất YAML để dùng ngoài Helm		Ví dụ: xuất file YAML ra để áp dụng thủ công bằng kubectl apply -f
				Dùng trong CI/CD					Render YAML ra trước khi deploy trong pipeline
			
			Ví dụ nâng cao:

				Render chart với file values riêng:

					helm template eazybank . -f prod-values.yaml

				Xuất ra file YAML:

					helm template eazybank . > output.yaml		
		
		- cmd: helm install keycloak keycloak	

			Lệnh helm install keycloak keycloak được dùng để cài đặt (deploy) ứng dụng Keycloak lên Kubernetes bằng
			Helm chart, với tên release là keycloak.

			Cú pháp chung:
			
				helm install <tên_release> <tên_chart_hoặc_đường_dẫn>

			Trong trường hợp này:

				helm install keycloak keycloak

			Giải thích từng phần:
			
				Thành phần				Ý nghĩa
				
				helm					Công cụ quản lý Helm chart
				install					Hành động cài đặt chart lên Kubernetes cluster
				keycloak (thứ nhất)		Tên release bạn muốn đặt (tức là tên “bản cài đặt”)
				keycloak (thứ hai)		Tên chart mà Helm sẽ dùng để cài đặt
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Tìm chart có tên keycloak:

					Nếu bạn đã thêm repo Keycloak (ví dụ: bitnami hoặc codecentric), Helm sẽ tìm trong đó.

					Hoặc nếu có thư mục ./keycloak trong máy, Helm sẽ cài từ thư mục đó.

				Đọc cấu hình từ:

					Chart.yaml, values.yaml của chart Keycloak

					Bất kỳ file giá trị nào bạn truyền thêm bằng -f hoặc --set

				Render (biên dịch) các template YAML
				
					→ thành các manifest Kubernetes thật (Deployment, Service, Ingress, Secret, ...)

				Gửi chúng đến API Server của Kubernetes
				
					→ để tạo ra toàn bộ tài nguyên cho Keycloak trong cluster.

			Sau khi cài, bạn có thể kiểm tra:
			
				helm list

				→ Sẽ thấy:

					NAME       NAMESPACE   REVISION   UPDATED        STATUS    CHART            APP VERSION
					keycloak   default     1          1m ago         deployed  keycloak-22.3.0  24.0.3

				Hoặc kiểm tra pod đang chạy:

					kubectl get pods

			Ví dụ cụ thể:
			
				Cài Keycloak từ repo Bitnami:
				
					helm repo add bitnami https://charts.bitnami.com/bitnami
					
					helm repo update
					
					helm install keycloak bitnami/keycloak

				Cài Keycloak từ thư mục chart local:
				
					helm install keycloak ./keycloak

			Sau khi cài:

				Helm sẽ tạo ra các tài nguyên như:

					Deployment: chạy container Keycloak

					Service: mở port cho các pod Keycloak

					Secret / ConfigMap: lưu username, password, cấu hình DB

					Ingress (nếu cấu hình): cho phép truy cập qua domain

			Tùy chọn phổ biến khi cài:
			
				Tùy chọn						Mô tả						Ví dụ
				-f <file>						Dùng file cấu hình riêng	-f values-prod.yaml
				--set key=value					Ghi đè cấu hình nhanh		--set replicaCount=2
				--namespace <ns>				Cài vào namespace khác		--namespace keycloak
				
				Ví dụ thực tế:
				
					helm install keycloak bitnami/keycloak \
					  --set auth.adminUser=admin \
					  --set auth.adminPassword=admin123 \
					  --set service.type=LoadBalancer

		
		- cmd: helm list -A		
		
			Lệnh helm list -A (hoặc viết đầy đủ là helm list --all-namespaces) được dùng để liệt kê tất cả các release Helm đang
			được cài đặt trên toàn bộ các namespace trong cluster Kubernetes.

			Cú pháp:
			
				helm list -A

				hoặc tương đương:

					helm list --all-namespaces

			Giải thích từng phần:
			
				Thành phần					Ý nghĩa
				
				helm						Công cụ quản lý Helm chart
				list						Lệnh dùng để liệt kê (xem danh sách) các release đã cài
				-A hoặc --all-namespaces	Hiển thị tất cả release ở mọi namespace (thay vì chỉ namespace hiện tại)
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Kết nối đến Kubernetes cluster hiện tại (theo context của kubectl).

				Tìm trong mọi namespace xem có release Helm nào đang được quản lý không.

				In ra bảng danh sách các release kèm thông tin chi tiết.

			Kết quả ví dụ:
			
				$ helm list -A
				
				NAME        	NAMESPACE    	REVISION	UPDATED                             	STATUS  	CHART              		APP VERSION
				keycloak    	default      	1       	2025-10-29 16:03:47.012345 +0700 +07	deployed	keycloak-22.3.0     	24.0.3
				kong        	kong         	2       	2025-10-29 15:57:10.987654 +0700 +07	deployed	kong-3.9.0          	3.9
				prometheus  	monitoring   	1       	2025-10-28 09:40:12.123456 +0700 +07	deployed	kube-prometheus-51.1.0	v0.67.0

			Ý nghĩa các cột:
			
				Cột						Giải thích
				
				NAME					Tên release (do bạn đặt khi helm install)
				NAMESPACE				Namespace mà release đó được cài vào
				REVISION				Số lần nâng cấp / thay đổi chart (mỗi helm upgrade tăng 1)
				UPDATED					Thời gian cài đặt hoặc cập nhật gần nhất
				STATUS					Trạng thái hiện tại (ví dụ: deployed, failed, pending-upgrade, uninstalled)
				CHART					Tên và phiên bản của Helm chart được dùng
				APP VERSION				Phiên bản thực tế của ứng dụng (ví dụ: Keycloak 24.0.3, Kong 3.9, v.v.)
			
			Nếu không dùng -A thì sao?

				Mặc định:

					helm list

					→ Chỉ hiển thị các release trong namespace hiện tại (namespace mà kubectl đang dùng).
					
				Bạn có thể kiểm tra namespace hiện tại bằng:

					kubectl config view --minify --output 'jsonpath={..namespace}'

			Một vài ví dụ hữu ích:
			
				Mục đích								Lệnh
				
				Liệt kê tất cả release trong cluster	helm list -A
				Liệt kê release trong namespace cụ thể	helm list -n kong
				Xem 									release cụ thể	helm status keycloak -n default
				Xóa 									release	helm uninstall keycloak -n default		
		
		- cmd: helm uninstall keycloak		
		
			Lệnh helm uninstall keycloak được dùng để gỡ bỏ (xóa) Helm release có tên là keycloak khỏi Kubernetes cluster.
			
			Nói cách khác: lệnh này xóa toàn bộ tài nguyên Kubernetes (Deployment, Service, Secret, v.v.) mà Helm đã
			cài khi bạn chạy helm install keycloak ....

			Cú pháp:
			
				helm uninstall <tên_release> [tùy_chọn]

			Ví dụ:

				helm uninstall keycloak

			Giải thích từng phần:
			
				Thành phần				Ý nghĩa
				
				helm					Công cụ quản lý Helm chart
				uninstall				Hành động xóa một release đã được cài bằng Helm
				keycloak				Tên release bạn muốn gỡ bỏ (tên bạn đã dùng khi cài đặt, ví dụ helm install keycloak bitnami/keycloak)
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Kết nối đến Kubernetes cluster hiện tại.

				Tìm release có tên keycloak (ví dụ trong namespace default).

				Xóa toàn bộ tài nguyên mà release đó đã tạo ra:

					Deployments

					Services

					Secrets / ConfigMaps

					Ingresses

					v.v.

				Gỡ bỏ release ra khỏi danh sách Helm (helm list sẽ không còn hiển thị nữa).

				Lưu thông tin gỡ bỏ trong Helm history (có thể khôi phục nếu muốn, trừ khi xóa hoàn toàn dữ liệu Helm).

			Ví dụ minh họa:

				Giả sử bạn đã cài Keycloak:

					helm install keycloak bitnami/keycloak

				Khi muốn gỡ bỏ:

					helm uninstall keycloak

				Kết quả:

					release "keycloak" uninstalled

			Sau khi gỡ bỏ:

				Các tài nguyên Kubernetes (pods, service, secret, configmap, ingress, ...) được xóa khỏi cluster.

				Tuy nhiên, nếu chart có tạo PersistentVolumeClaims (PVCs), thì dữ liệu trong volume thường không bị xóa (để tránh mất dữ liệu).

					Bạn có thể xóa thủ công bằng:

						kubectl delete pvc -l app.kubernetes.io/instance=keycloak

			Nếu bạn cài ở namespace khác:

				Bạn cần chỉ rõ namespace bằng -n:

					helm uninstall keycloak -n kong

			Các tùy chọn hữu ích:
			
				Tùy chọn				Mô tả
				
				-n, --namespace <ns>	Gỡ release trong namespace cụ thể
				--keep-history			Giữ lại lịch sử release (để sau có thể xem lại bằng helm history)
				--dry-run				Chạy thử, không thật sự xóa gì (chỉ mô phỏng)
			
			So sánh nhanh:
			
				Lệnh										Chức năng
				
				helm install keycloak bitnami/keycloak		Cài Keycloak vào cluster
				helm list									Xem danh sách release đã cài
				helm uninstall keycloak						Gỡ bỏ release Keycloak
				helm history keycloak						Xem lịch sử các bản cài/upgrade của release Keycloak		
		
		- cmd: helm ls		
		
			Lệnh helm ls (viết tắt của helm list) được dùng để liệt kê tất cả các Helm release đang được cài đặt
			trong namespace hiện tại của Kubernetes.

			Cú pháp:
			
				helm ls

			hoặc đầy đủ:

				helm list

			Giải thích từng phần:
			
				Thành phần			Ý nghĩa
				
				helm				Công cụ quản lý Helm chart
				ls					Viết tắt của list — dùng để liệt kê các release đã được cài đặt
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Kết nối tới Kubernetes cluster (dựa trên kubectl config current-context)

				Xác định namespace hiện tại (thường là default, trừ khi bạn chỉ định namespace khác)

				Lấy danh sách tất cả Helm release trong namespace đó

				Hiển thị thông tin chi tiết của từng release (tên, trạng thái, version chart, v.v.)

			Kết quả ví dụ:
			
				$ helm ls
				
				NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART              		APP VERSION
				keycloak  	default  	1        	2025-10-29 16:03:47.012345 +0700 +07 	deployed	keycloak-22.3.0     	24.0.3
				kong      	default  	2        	2025-10-29 15:57:10.987654 +0700 +07 	deployed	kong-3.9.0          	3.9

			Ý nghĩa các cột:
			
				Cột							Giải thích
				
				NAME						Tên release (do bạn đặt khi helm install)
				NAMESPACE					Namespace chứa release đó
				REVISION					Số lần release được nâng cấp (mỗi lần helm upgrade sẽ tăng 1)
				UPDATED						Thời gian cài đặt hoặc cập nhật gần nhất
				STATUS						Trạng thái của release (deployed, failed, pending-upgrade, uninstalled, …)
				CHART						Tên và version của chart được dùng
				APP VERSION					Phiên bản thực tế của ứng dụng bên trong chart
			
			Nếu bạn muốn xem tất cả các namespace:
			
				helm ls -A

			hoặc

				helm list --all-namespaces

			Một vài ví dụ hữu ích:
			
				Mục đích								Lệnh
				
				Xem release trong namespace hiện tại	helm ls
				Xem release trong namespace cụ thể		helm ls -n kong
				Xem tất cả release trong cluster		helm ls -A
				Xem chi tiết release					helm status keycloak
			
			Mẹo nhỏ:

				Namespace mặc định mà Helm dùng chính là namespace mà kubectl đang trỏ tới.
				Bạn có thể kiểm tra bằng:

					kubectl config view --minify --output 'jsonpath={..namespace}'		
		
		- cmd: helm seach hub wordpress	
		
			Lệnh helm search hub wordpress được dùng để tìm (search) các Helm chart có liên quan đến WordPress trong Helm
			Hub (Artifact Hub) — tức là kho lưu trữ trung tâm chứa hàng ngàn Helm chart công khai.

			Cú pháp:
			
				helm search hub <từ_khóa>


			Ví dụ:

				helm search hub wordpress

			Giải thích từng phần:
			
				Thành phần					Ý nghĩa
				
				helm						Công cụ quản lý Helm chart
				search						Nhóm lệnh dùng để tìm kiếm chart
				hub							Chỉ định tìm trên Helm Hub (Artifact Hub) — trang web tập hợp hàng ngàn chart từ nhiều nhà phát triển khác nhau
				wordpress					Từ khóa bạn muốn tìm — ở đây là các chart liên quan đến WordPress
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Kết nối tới Artifact Hub (https://artifacthub.io
				).

				Gửi truy vấn tìm kiếm với từ khóa wordpress.

				Hiển thị danh sách các chart phù hợp từ nhiều repo khác nhau (Bitnami, Groundhog, …).

			Ví dụ kết quả:
			
				$ helm search hub wordpress
				
				URL                                                			CHART VERSION	APP 			VERSION	DESCRIPTION
				
				https://artifacthub.io/packages/helm/bitnami/wordpress		16.3.4        	6.5.4       	Web publishing platform for building blogs and websites.
				https://artifacthub.io/packages/helm/groundhog/wordpress	0.3.2         	6.2.0       	WordPress for Kubernetes.
			
			Ý nghĩa các cột:
			
				Cột					Giải thích
				
				URL					Đường dẫn đến trang chi tiết chart trên Artifact Hub
				CHART VERSION		Phiên bản của Helm chart
				APP VERSION			Phiên bản ứng dụng WordPress thực tế
				DESCRIPTION			Mô tả ngắn gọn về chart
				
			Bước tiếp theo sau khi tìm thấy chart mong muốn:

					Giả sử bạn muốn cài chart WordPress của Bitnami, bạn sẽ:

					Thêm repo Bitnami:

						helm repo add bitnami https://charts.bitnami.com/bitnami
						
						helm repo update

					Cài đặt WordPress:

						helm install my-wordpress bitnami/wordpress

			So sánh nhanh:
			
				Lệnh						Nơi tìm kiếm					Mô tả
				
				helm search hub <keyword>	Artifact Hub (trực tuyến)		Tìm chart công khai trên Internet
				helm search repo <keyword>	Repo cục bộ (đã helm repo add)	Tìm chart có sẵn trong các repo bạn đã thêm vào máy		
		
		- cmd: helm repo add bitnami https://charts.bitnami.com/bitnami		
		
			Lệnh helm repo add bitnami https://charts.bitnami.com/bitnami dùng để thêm kho Helm chart chính thức của
			Bitnami vào danh sách các repository trên máy của bạn.

			Cú pháp tổng quát:
			
				helm repo add <tên_repo> <đường_dẫn_repo>

			Giải thích từng phần trong lệnh:
			
				helm repo add bitnami https://charts.bitnami.com/bitnami

				Thành phần							Ý nghĩa
							
				helm								Công cụ quản lý Helm chart (package manager cho Kubernetes)
				repo								Nhóm lệnh làm việc với Helm repositories (kho lưu trữ chart)
				add									Hành động thêm một repository mới
				bitnami								Tên (alias) bạn đặt cho repository — ở đây là bitnami
				https://charts.bitnami.com/bitnami	Đường dẫn URL đến kho chart chính thức của Bitnami
			
			Bitnami là gì?

				Bitnami là một nhà phát triển cung cấp Helm chart tiêu chuẩn hóa, đáng tin cậy cho hàng trăm ứng dụng phổ biến như:

					WordPress

					MySQL

					PostgreSQL

					MongoDB

					Redis

					Nginx

					Jenkins, GitLab, v.v.

				Các chart của Bitnami được dùng rất rộng rãi vì:

					Được cập nhật thường xuyên

					Có cấu hình an toàn & dễ dùng

					Tương thích tốt với nhiều nền tảng Kubernetes khác nhau (GKE, AKS, EKS, Minikube, v.v.)

			Khi bạn chạy lệnh này:

				Helm sẽ:

					Kết nối đến https://charts.bitnami.com/bitnami

					Tải file index.yaml (danh sách các chart và phiên bản)

					Lưu thông tin đó trong cấu hình Helm trên máy bạn (thường ở ~/.config/helm/repositories.yaml)

			Kiểm tra lại sau khi thêm:
			
				helm repo list

			Kết quả sẽ có dòng như:

				NAME    	URL
				bitnami 	https://charts.bitnami.com/bitnami

			Cập nhật danh sách chart mới nhất:
			
				helm repo update

			Giúp Helm tải về danh sách chart mới hoặc version mới từ Bitnami.

			Ví dụ sử dụng sau đó:

				Sau khi đã thêm repo Bitnami, bạn có thể dễ dàng cài ứng dụng từ đó:

					helm install my-wordpress bitnami/wordpress

				→ Helm sẽ lấy chart wordpress từ repo bitnami và triển khai vào Kubernetes cluster của bạn.		
		
		- cmd: helm install happy-panda bitnami/wordpress		
		
			Lệnh helm install happy-panda bitnami/wordpress được dùng để cài đặt ứng dụng WordPress lên Kubernetes cluster bằng Helm chart của Bitnami.

			Cú pháp chung:
			
				helm install <tên_release> <repo/chart> [tùy_chọn]

			Giải thích từng phần trong lệnh:
			
				helm install happy-panda bitnami/wordpress

				Thành phần				Ý nghĩa
				
				helm					Công cụ quản lý Helm chart
				install					Hành động cài đặt một chart (triển khai ứng dụng vào Kubernetes)
				happy-panda				Tên release mà bạn đặt cho lần cài đặt này (tên tùy chọn, có thể đặt tùy ý, ví dụ: wordpress-prod, myblog, v.v.)
				bitnami/wordpress		Tên chart cần cài: chart wordpress lấy từ repo bitnami (đã được thêm bằng helm repo add bitnami https://charts.bitnami.com/bitnami)
			
			Khi bạn chạy lệnh này, Helm sẽ:

				Tải chart wordpress từ repository Bitnami về.

				Tạo ra các manifest YAML (Deployment, Service, Secret, PVC, ConfigMap, v.v.) tương ứng với WordPress.

				Gửi các manifest này lên Kubernetes API để tạo các resource thật trong cluster.

				Theo dõi tiến trình triển khai (cho đến khi tất cả pod, service, PVC được tạo thành công).

			Kết quả sau khi chạy:

				Helm sẽ hiển thị thông tin tương tự như sau:

					NAME: happy-panda
					LAST DEPLOYED: Wed Oct 29 16:30:45 2025
					NAMESPACE: default
					STATUS: deployed
					REVISION: 1
					NOTES:
					CHART NAME: wordpress
					CHART VERSION: 16.3.4
					APP VERSION: 6.5.4

			Các tài nguyên được tạo trong Kubernetes:

				Chart bitnami/wordpress sẽ tự động triển khai:

					Pod WordPress (chạy container web WordPress)

					Pod MariaDB (cơ sở dữ liệu backend)

					Service (mở port truy cập cho WordPress)

					PersistentVolumeClaim (PVC) (lưu trữ dữ liệu website)

					Secrets (lưu mật khẩu admin, DB, v.v.)

			Kiểm tra lại:

				Bạn có thể kiểm tra release vừa cài:

					helm ls

				hoặc:

					kubectl get pods

				Xem chi tiết cấu hình:

					helm status happy-panda

			Tùy chọn mở rộng:
			
				Cài trong namespace riêng:
					
					helm install happy-panda bitnami/wordpress -n blog --create-namespace

				Tùy chỉnh cấu hình (ví dụ thay đổi mật khẩu, storage):
					
					helm install happy-panda bitnami/wordpress --set wordpressUsername=admin,wordpressPassword=secret123

				Cài với file cấu hình riêng:
				
					helm install happy-panda bitnami/wordpress -f values.yaml		
		
		- cmd: helm env		
		
			============================================================
			LỆNH: helm env
			============================================================

			1. MỤC ĐÍCH:
			   - Lệnh `helm env` dùng để hiển thị danh sách các biến môi trường (environment variables)
				 mà Helm đang sử dụng trong hệ thống.
			   - Giúp người dùng biết Helm đang lưu cache, cấu hình, plugin, và các file registry ở đâu.

			------------------------------------------------------------
			2. CÚ PHÁP:
			
			   helm env

			------------------------------------------------------------
			3. KHI CHẠY LỆNH, KẾT QUẢ SẼ HIỂN THỊ NHƯ SAU (ví dụ):

			   HELM_BIN="helm"
			   HELM_DEBUG="false"
			   HELM_KUBEAPISERVER=""
			   HELM_KUBECAFILE=""
			   HELM_KUBECONTEXT=""
			   HELM_KUBETOKEN=""
			   HELM_NAMESPACE="default"
			   HELM_PLUGINS="/home/user/.local/share/helm/plugins"
			   HELM_REGISTRY_CONFIG="/home/user/.config/helm/registry.json"
			   HELM_REPOSITORY_CACHE="/home/user/.cache/helm/repository"
			   HELM_REPOSITORY_CONFIG="/home/user/.config/helm/repositories.yaml"
			   HELM_HISTORY_MAX="10"

			------------------------------------------------------------
			4. GIẢI THÍCH TỪNG BIẾN:

			   HELM_BIN
				   → Đường dẫn hoặc tên file thực thi (binary) của Helm.

			   HELM_DEBUG
				   → Trạng thái bật/tắt chế độ debug (true hoặc false).

			   HELM_KUBEAPISERVER
				   → URL của Kubernetes API Server (nếu được chỉ định thủ công).

			   HELM_KUBECAFILE
				   → Đường dẫn đến file CA certificate dùng để xác thực cluster.

			   HELM_KUBECONTEXT
				   → Tên context trong kubeconfig mà Helm đang sử dụng.

			   HELM_KUBETOKEN
				   → Token xác thực để truy cập cluster Kubernetes (nếu có).

			   HELM_NAMESPACE
				   → Namespace mặc định mà Helm sẽ thao tác nếu không chỉ định bằng cờ `-n`.

			   HELM_PLUGINS
				   → Thư mục lưu các plugin đã cài đặt cho Helm.

			   HELM_REGISTRY_CONFIG
				   → File cấu hình registry (OCI registry / container registry) mà Helm dùng để lưu thông tin đăng nhập.

			   HELM_REPOSITORY_CACHE
				   → Thư mục cache lưu các file index.yaml được tải về từ các repository Helm.

			   HELM_REPOSITORY_CONFIG
				   → File cấu hình liệt kê danh sách các repository mà Helm đã thêm (`helm repo add`).

			   HELM_HISTORY_MAX
				   → Số lượng tối đa các bản ghi (revision) mà Helm sẽ lưu cho mỗi release.

			------------------------------------------------------------
			5. TÓM TẮT:
			   - `helm env` giúp bạn xem toàn bộ các đường dẫn và thiết lập môi trường mà Helm đang sử dụng.
			   - Lệnh này hữu ích khi cần kiểm tra hoặc xử lý lỗi liên quan đến cache, repo, hoặc cấu hình của Helm.
			============================================================
		
		
		- cmd: helm uninstall kubernetes-dashboard -n kubernetes-dashboard		
		
			============================================================
			LỆNH: helm uninstall kubernetes-dashboard -n kubernetes-dashboard
			============================================================

			1. MỤC ĐÍCH:
			   - Lệnh này dùng để gỡ cài đặt (xóa) một Helm release tên là "kubernetes-dashboard"
				 ra khỏi Kubernetes cluster.
			   - Release này được cài đặt trong namespace "kubernetes-dashboard".

			------------------------------------------------------------
			2. CÚ PHÁP CHUNG:
			   helm uninstall <tên_release> [các_tùy_chọn]

			------------------------------------------------------------
			3. GIẢI THÍCH CỤ THỂ:

			   helm
				   → Công cụ quản lý Helm chart, dùng để cài đặt, nâng cấp hoặc gỡ ứng dụng trên Kubernetes.

			   uninstall
				   → Hành động gỡ bỏ một Helm release (xóa toàn bộ các tài nguyên do release đó tạo ra).

			   kubernetes-dashboard
				   → Tên của release cần gỡ bỏ. Đây là tên bạn đã đặt khi cài bằng lệnh:
					 helm install kubernetes-dashboard <chart_name>

			   -n kubernetes-dashboard
				   → Tùy chọn `-n` (hoặc `--namespace`) chỉ định namespace chứa release.
					 Ở đây release nằm trong namespace có tên là "kubernetes-dashboard".

			------------------------------------------------------------
			4. KHI CHẠY LỆNH NÀY, HELM SẼ THỰC HIỆN:
			   - Kết nối đến Kubernetes cluster theo context hiện tại.
			   - Xác định release "kubernetes-dashboard" trong namespace "kubernetes-dashboard".
			   - Xóa tất cả các tài nguyên Kubernetes được tạo ra bởi release đó, bao gồm:
				   • Deployment
				   • Service
				   • Secret
				   • ConfigMap
				   • Role / RoleBinding
				   • ServiceAccount
				   • PersistentVolumeClaim (nếu có)
			   - Cập nhật lại lịch sử Helm (xóa bản ghi release khỏi danh sách `helm ls`).

			------------------------------------------------------------
			5. KẾT QUẢ SAU KHI THỰC THI (VÍ DỤ):
			   Release "kubernetes-dashboard" uninstalled

			   → Nghĩa là Helm đã gỡ bỏ release thành công.

			------------------------------------------------------------
			6. LƯU Ý:
			   - Nếu bạn không chỉ định namespace bằng `-n`, Helm sẽ mặc định tìm release trong namespace `default`.
			   - Lệnh này **chỉ xóa các tài nguyên do Helm quản lý**, không ảnh hưởng đến các tài nguyên khác trong namespace.
			   - Bạn có thể kiểm tra lại bằng:
				   helm ls -A
				 để đảm bảo release đã được gỡ bỏ hoàn toàn.

			------------------------------------------------------------
			7. TÓM TẮT:
			   → Lệnh `helm uninstall kubernetes-dashboard -n kubernetes-dashboard`
				 dùng để xóa release "kubernetes-dashboard" và toàn bộ các tài nguyên liên quan
				 trong namespace "kubernetes-dashboard".
			============================================================
		
		
		- cmd: helm install prometheus kube-prometheus --skip-crds
		
			============================================================
			LỆNH: helm install prometheus kube-prometheus --skip-crds
			============================================================

			1. MỤC ĐÍCH CHUNG:
			   - Dùng Helm để **cài đặt một Helm release** có tên là "prometheus" từ chart "kube-prometheus".
			   - Sử dụng tùy chọn `--skip-crds` để **bỏ qua bước cài đặt các CRD (Custom Resource Definitions)** mà chart có thể định nghĩa.

			------------------------------------------------------------
			2. CÚ PHÁP CHUNG:
			   helm install <tên_release> <chart> [các_tùy_chọn]

			   Ví dụ cụ thể:
			   helm install prometheus kube-prometheus --skip-crds

			   Giải thích các phần:
				 • helm               → công cụ quản lý Helm chart
				 • install            → hành động cài đặt (triển khai) chart lên Kubernetes
				 • prometheus         → tên release bạn đặt (tên tùy ý)
				 • kube-prometheus    → tên chart (có thể là chart trong repo đã add hoặc thư mục local)
				 • --skip-crds        → cờ cho Helm **không cài** các CRD được định nghĩa trong chart

			------------------------------------------------------------
			3. VỀ CHART "kube-prometheus":
			   - "kube-prometheus" thường là một chart thu thập các thành phần giám sát Prometheus/Kube (Prometheus server, Alertmanager, node-exporter, kube-state-metrics, ServiceMonitors, PrometheusRules, ...)
			   - Chart này **thường** đi kèm với nhiều CRD (ví dụ: ServiceMonitor, PrometheusRule, PodMonitor, ThanosRuler, ...), những CRD cho phép Prometheus Operator / kube-prometheus quản lý tài nguyên mở rộng.

			------------------------------------------------------------
			4. Ý NGHĨA CỤ THỂ CỦA --skip-crds:
			   - `--skip-crds` = **bảo Helm bỏ qua việc áp dụng các CRD** mà chart cung cấp.
			   - Khi dùng `--skip-crds`, Helm sẽ **vẫn tạo các resource khác** (Deployment, Service, ConfigMap, Secret, RBAC, v.v.), nhưng **không** tạo các CRD nằm trong thư mục `crds/` của chart.
			   - Lý do hay dùng `--skip-crds`:
				   • Bạn đã **cài CRD trước đó** (thường khi dùng nhiều lần hoặc nhiều release cùng dùng chung CRD).
				   • Muốn **quản lý CRD riêng biệt** (ví dụ: áp dụng CRD qua GitOps hoặc một kịch bản riêng, hoặc trước khi nâng cấp).
				   • Tránh việc Helm cố gắng **ghi đè / cập nhật CRD** trong khi bạn muốn kiểm soát việc đó thủ công.

			------------------------------------------------------------
			5. HẬU QUẢ VÀ LƯU Ý KỸ THUẬT:
			   - Nếu CRD **chưa tồn tại** trên cluster và bạn dùng `--skip-crds`, thì các tài nguyên mà chart tạo ra **có thể thất bại** khi chúng tham chiếu tới các CustomResource (ví dụ: Prometheus, ServiceMonitor) vì Kubernetes chưa biết định nghĩa resource đó.
			   - CRD thường chỉ cần được cài **một lần cho toàn cluster** (không cần cài lại cho mỗi release). Vì vậy việc cài CRD tách (ngoài helm) hay dùng `--skip-crds` tuỳ thuộc cách bạn quản lý hạ tầng.
			   - Khi bạn muốn **cài CRD thủ công**, thường sẽ làm:
				   • Tìm file CRD (thường ở `charts/<chart>/crds/` hoặc trên repo chart) và áp dụng:
					 kubectl apply -f path/to/crds.yaml
				   • Hoặc lấy trực tiếp từ repo/charts upstream:
					 kubectl apply -f https://raw.githubusercontent.com/<repo>/.../crds.yaml
			   - Nếu sau này cần nâng cấp CRD, phải làm **thủ công** (kubectl apply) hoặc dùng công cụ quản lý CRD chuyên biệt — vì Helm mặc định không cập nhật CRD qua `helm upgrade` (trừ khi chart viết logic khác).

			------------------------------------------------------------
			6. VÍ DỤ KỊCH BẢN THỰC TẾ:
			   A) Cài toàn bộ (kèm CRD) — nếu bạn muốn Helm tự cài CRD:
				  helm install prometheus kube-prometheus
			   B) Cài tách CRD trước, sau đó cài release (an toàn cho nhiều release):
				  # 1) Cài CRD một lần
				  kubectl apply -f ./kube-prometheus/crds/
				  # 2) Cài chart (bỏ qua CRD vì đã có)
				  helm install prometheus kube-prometheus --skip-crds
			   C) Cài trong namespace cụ thể và tạo namespace nếu chưa có:
				  helm install prometheus kube-prometheus -n monitoring --create-namespace --skip-crds

			------------------------------------------------------------
			7. LỆNH KIỂM TRA SAU KHI CÀI:
			   - Kiểm tra release Helm:
				 helm ls -n <namespace>
				 helm status prometheus -n <namespace>
			   - Kiểm tra pod / deployment:
				 kubectl get pods -n <namespace>
				 kubectl get deploy,svc,sts -n <namespace>
			   - Kiểm tra CRD (nếu bạn mong chúng tồn tại):
				 kubectl get crd | grep -E "servicemonitors|prometheusrules|podmonitors"
			   - Nếu resource tạo lỗi do CRD thiếu, bạn sẽ thấy các error kiểu "no matches for kind \"Prometheus\" in version \"monitoring.coreos.com/v1\"" — báo rằng CRD chưa được cài.

			------------------------------------------------------------
			8. LƯU Ý VỀ BẢO TRÌ & NÂNG CẤP:
			   - CRD là thành phần nhạy cảm: khi nâng cấp chart có thay đổi CRD, bạn **phải đọc release notes** và thực hiện các bước nâng cấp CRD cẩn trọng.
			   - Nếu bạn đang chạy nhiều release hoặc chart khác nhau dùng chung CRD (ví dụ Prometheus Operator), nên cài/upgrade CRD một lần, tách riêng khỏi lifecycle của từng release.

			------------------------------------------------------------
			9. TÓM TẮT NGẮN GỌN:
			   - `helm install prometheus kube-prometheus --skip-crds`:
				   • Cài chart `kube-prometheus` thành release `prometheus`.
				   • Bỏ qua việc cài các CRD; bạn cần **tự đảm bảo CRD đã có** (hoặc cài sau).
			   - Dùng `--skip-crds` khi bạn đã quản lý CRD riêng (an toàn cho môi trường production hoặc đa-release).
			   - Nếu chưa có CRD, **không dùng** `--skip-crds` hoặc hãy cài CRD trước thủ công.

			============================================================
		
		
		- cmd: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ # Add kubernetes-dashboard repository
		
			============================================================
			LỆNH: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
			============================================================

			1. MỤC ĐÍCH CHUNG
			   - Thêm một Helm chart repository mới vào cấu hình Helm cục bộ của bạn.
			   - Repository này là kho chứa Helm chart cho Kubernetes Dashboard (official chart).
			   - Sau khi thêm, bạn có thể tìm, cập nhật và cài chart từ repo này bằng các lệnh Helm khác.

			------------------------------------------------------------
			2. CÚ PHÁP CHUNG
			   helm repo add <tên_repo> <url_repo>

			   Trong ví dụ này:
			   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/

			   Giải thích các phần:
				 • helm               → công cụ quản lý Helm chart
				 • repo add           → lệnh thêm repository
				 • kubernetes-dashboard → tên alias (bạn đặt) cho repo này
				 • https://kubernetes.github.io/dashboard/ → URL tới kho chart (index.yaml sẽ được tải từ đây)

			------------------------------------------------------------
			3. ĐIỀU GÌ XẢY RA KHI CHẠY LỆNH NÀY
			   - Helm lưu lại entry cho repo trong file cấu hình repo cục bộ (thường ở ~/.config/helm/repositories.yaml).
			   - Helm không tải toàn bộ chart ngay lập tức; nó chỉ ghi nhớ URL. (index.yaml được tải khi chạy `helm repo update` hoặc một số lệnh tìm kiếm).
			   - Sau khi add, bạn thường chạy `helm repo update` để kéo index.yml mới nhất từ repo và cache nó.

			------------------------------------------------------------
			4. LỆNH THƯỜNG DÙNG SAU KHI ADD
			   # Cập nhật cache/metadata từ tất cả repo đã thêm
			   helm repo update

			   # Kiểm tra repo đã thêm
			   helm repo list

			   # Tìm chart trong repo vừa thêm (ví dụ)
			   helm search repo kubernetes-dashboard

			   # Cài chart (ví dụ cài kubernetes-dashboard vào namespace riêng)
			   helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
				   -n kubernetes-dashboard --create-namespace

			------------------------------------------------------------
			5. LÝ DO VÌ SAO PHẢI DÙNG "helm repo add"
			   - Tiện lợi: cho phép cài chart bằng cú pháp <repo>/<chart>.
			   - Quản lý: Helm sẽ biết repo để tìm và tải chart khi cài.
			   - Phù hợp cho CI/CD: scripts chỉ cần gọi helm install <repo>/<chart>.

			------------------------------------------------------------
			6. LƯU Ý VỀ URL
			   - URL phải trỏ tới một Helm repo hợp lệ có file index.yaml tại gốc hoặc subpath.
			   - Dấu `/` ở cuối URL (trailing slash) thường không gây lỗi; Helm xử lý cả hai dạng.
			   - Repo public như https://kubernetes.github.io/dashboard/ thường không cần authentication.

			------------------------------------------------------------
			7. KIỂM TRA SAU KHI THÊM
			   # 1) Xem danh sách repo
			   helm repo list
			   # bạn sẽ thấy một dòng tương tự:
			   # NAME                      URL
			   # kubernetes-dashboard      https://kubernetes.github.io/dashboard/

			   # 2) Cập nhật và tải index
			   helm repo update
			   # Kiểm tra xem Helm có lấy index thành công không (output báo Successfully got an update...)

			   # 3) Tìm chart trong cache
			   helm search repo kubernetes-dashboard

			------------------------------------------------------------
			8. VẤN ĐỀ THƯỜNG GẶP VÀ CÁCH KHẮC PHỤC
			   - Lỗi kết nối / timeout:
				 • Kiểm tra Internet, proxy, firewall.
				 • Nếu dùng proxy, đảm bảo biến môi trường HTTP_PROXY/HTTPS_PROXY được set cho shell.
			   - SSL / Certificate errors:
				 • Nếu gặp lỗi chứng chỉ (rare với GitHub Pages), kiểm tra cài đặt CA trên máy.
			   - "index.yaml not found" hoặc 404:
				 • Kiểm tra URL có đúng không; mở URL trong trình duyệt để kiểm tra index.yaml tồn tại.
			   - Repo đã tồn tại:
				 • Nếu đã add cùng tên, bạn có thể xóa trước bằng `helm repo remove <name>` rồi add lại.
			   - Phiên bản Helm quá cũ:
				 • Một số tính năng repo/oci khác cần Helm 3+. Dùng `helm version` để kiểm tra.

			------------------------------------------------------------
			9. BẢO MẬT & QUẢN TRỊ
			   - Repo public (không cần auth): an toàn để add nhưng tuỳ policy tổ chức.
			   - Với repo private (cần username/password), Helm hỗ trợ credentials khi add (cú pháp phức tạp hơn hoặc qua file cấu hình).
			   - Đối với chart quan trọng (như các component cluster), hãy audit chart hoặc pin version cụ thể trước khi cài vào production.

			------------------------------------------------------------
			10. EXAMPLES THỰC TIỄN (copy-paste)
			   # 1. Thêm repo
			   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/

			   # 2. Cập nhật metadata từ tất cả repo
			   helm repo update

			   # 3. Xem repo đã add
			   helm repo list

			   # 4. Tìm chart 'kubernetes-dashboard' trong repo đã add
			   helm search repo kubernetes-dashboard

			   # 5. Cài dashboard vào namespace 'kubernetes-dashboard'
			   helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
				   -n kubernetes-dashboard --create-namespace

			   # 6. Gỡ cài nếu cần
			   helm uninstall kubernetes-dashboard -n kubernetes-dashboard

			------------------------------------------------------------
			11. TÓM TẮT NGẮN GỌN
			   - Lệnh `helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/`:
				 + Thêm repo chứa Helm chart cho Kubernetes Dashboard với alias 'kubernetes-dashboard'.
				 + Sau khi add, chạy `helm repo update` để tải index và có thể cài chart bằng `helm install`.
			   - Đây là thao tác cần thiết để Helm biết nơi lấy chart khi bạn muốn cài hoặc tìm chart theo cú pháp <repo>/<chart>.

			============================================================


		- cmd: helm install eazybank prod-env

			============================================================
			LỆNH: helm install eazybank prod-env
			============================================================

			1. MỤC ĐÍCH CHUNG
			   - Dùng Helm để **cài đặt (deploy)** một Helm chart lên Kubernetes cluster.
			   - Trong ví dụ này:
				   • "eazybank"  → tên release (tên phiên bản triển khai do bạn đặt).
				   • "prod-env"  → chart sẽ được cài; có thể là:
						- một chart nằm trong một repo đã add (ví dụ: <repo>/prod-env), hoặc
						- một thư mục chart local (ví dụ ./prod-env nếu bạn chạy từ thư mục cha).

			------------------------------------------------------------
			2. CÚ PHÁP CƠ BẢN
			   helm install <tên_release> <chart> [các_tùy_chọn]

			   Ví dụ:
			   helm install eazybank prod-env

			   Các thành phần:
				 • helm        → CLI Helm.
				 • install     → hành động cài đặt.
				 • eazybank    → tên release (dùng để quản lý: upgrade, uninstall, status,...).
				 • prod-env    → chart (tên chart trong repo hoặc đường dẫn tới thư mục chart).

			------------------------------------------------------------
			3. TRƯỜNG HỢP NGUỒN CHART
			   - Nếu "prod-env" là chart trong repo:
				   • Phải đảm bảo repo đã được add: helm repo add <name> <url>
				   • Và đã cập nhật index: helm repo update
				   • Khi đó cú pháp cài có thể là: helm install eazybank <repo>/prod-env
			   - Nếu "prod-env" là thư mục local:
				   • Sử dụng đường dẫn rõ ràng hoặc chạy từ thư mục cha:
					 helm install eazybank ./prod-env

			------------------------------------------------------------
			4. QUÁ TRÌNH HELM THỰC HIỆN
			   - Đọc chart: Chart.yaml, values.yaml, templates/, charts/ (dependencies).
			   - Kết hợp giá trị: dùng values mặc định + các giá trị override (--set/-f).
			   - Render templates -> sinh ra Kubernetes manifests (Deployment, Service, ConfigMap,...).
			   - Gửi manifests tới Kubernetes API để tạo tài nguyên.
			   - Ghi metadata release vào Helm storage (tùy backend, thường là Secret trong namespace).

			------------------------------------------------------------
			5. TÙY CHỌN THƯỜNG DÙNG
			   - -n, --namespace <ns>
				   → Cài vào namespace cụ thể. Kết hợp --create-namespace nếu namespace chưa tồn tại.
				   Ví dụ: -n production --create-namespace

			   - -f, --values <file>
				   → Dùng file values để override cấu hình mặc định.
				   Ví dụ: -f values-prod.yaml

			   - --set key=value
				   → Ghi đè nhanh trên CLI.
				   Ví dụ: --set image.tag=1.2.3 --set replicaCount=3

			   - --wait
				   → Chờ cho đến khi các resource quan trọng sẵn sàng (pods ready, PVC bound,...).

			   - --timeout <duration>
				   → Thời gian chờ tối đa khi dùng --wait (ví dụ 5m, 10m).

			   - --atomic
				   → Nếu cài thất bại thì tự động rollback.

			   - --version <chart-version>
				   → Cài phiên bản chart cụ thể (khi cài từ repo).

			   - --skip-crds
				   → Bỏ qua cài CRD (nếu chart kèm CRD). Dùng khi CRD đã được cài riêng.

			------------------------------------------------------------
			6. KẾT QUẢ MẪU SAU KHI CÀI THÀNH CÔNG
			   - Thông tin hiển thị (ví dụ):
				   NAME: eazybank
				   LAST DEPLOYED: <date>
				   NAMESPACE: default (hoặc bạn chỉ định)
				   STATUS: deployed
				   REVISION: 1
				   NOTES: (hướng dẫn truy cập nếu chart cung cấp)

			   - Tài nguyên tương ứng sẽ xuất hiện trong cluster: Pod, Deployment, Service, PVC, Secret,...

			------------------------------------------------------------
			7. LỆNH KIỂM TRA & XỬ LÝ
			   - Xem release:
				   helm ls -n <namespace>
				   helm status eazybank -n <namespace>

			   - Kiểm tra Kubernetes resources:
				   kubectl get pods,svc,deploy,sts -n <namespace>
				   kubectl describe pod <pod-name> -n <namespace>
				   kubectl logs <pod-name> -n <namespace>

			   - Nếu dùng --wait: lệnh sẽ chỉ kết thúc khi resource trở nên ready hoặc timeout.

			------------------------------------------------------------
			8. LỖI THƯỜNG GẶP VÀ CÁCH KHẮC PHỤC
			   - Chart không tìm thấy:
				   • Chưa add repo hoặc chưa helm repo update.
				   • Sai đường dẫn (nếu local, thiếu ./).
			   - Thiếu quyền (RBAC):
				   • Kiểm tra kubeconfig, context, quyền user/service account.
			   - ImagePullBackOff:
				   • Kiểm tra image name/tag và credentials (imagePullSecrets).
			   - PVC không bind:
				   • Kiểm tra StorageClass, quota, PV.
			   - Thiếu CRD:
				   • Lỗi "no matches for kind ..." → CRD chưa cài. Cài CRD trước bằng kubectl apply -f <crd-files> hoặc bỏ --skip-crds.
			   - Resource crash (CrashLoopBackOff):
				   • Xem logs, kiểm tra env vars, secrets, kết nối DB.

			------------------------------------------------------------
			9. QUẢN LÝ SAU KHI CÀI
			   - Nâng cấp:
				   helm upgrade eazybank prod-env -f values-prod.yaml -n <namespace>

			   - Xem lịch sử:
				   helm history eazybank -n <namespace>

			   - Rollback:
				   helm rollback eazybank <revision> -n <namespace>

			   - Gỡ:
				   helm uninstall eazybank -n <namespace>

			------------------------------------------------------------
			10. BEST PRACTICES
			   - Lưu values-prod.yaml trong VCS (versioned).
			   - Pin version chart bằng --version để tránh thay đổi bất ngờ.
			   - Dùng --wait --timeout trong CI/CD để đảm bảo deploy hoàn tất.
			   - Cài CRD một lần cho cluster (nếu nhiều release dùng chung), quản lý CRD tách khỏi lifecycle release.
			   - Sử dụng namespace riêng cho môi trường (production, staging).
			   - Giữ release name có ý nghĩa (ví dụ eazybank-prod nếu có nhiều môi trường).

			------------------------------------------------------------
			11. EXAMPLES (COPY-PASTE)
			   # 1) Cài với file values vào namespace production:
			   helm install eazybank prod-env -f values-prod.yaml -n production --create-namespace

			   # 2) Cài, chờ hoàn tất, rollback nếu lỗi:
			   helm install eazybank prod-env -n production --create-namespace --wait --timeout 10m --atomic -f values-prod.yaml

			   # 3) Nếu prod-env là thư mục local:
			   helm install eazybank ./prod-env -n production --create-namespace

			   # 4) Upgrade:
			   helm upgrade eazybank prod-env -f values-prod.yaml -n production

			   # 5) Uninstall:
			   helm uninstall eazybank -n production

			------------------------------------------------------------
			12. TÓM TẮT NGẮN GỌN
			   - `helm install eazybank prod-env`:
				   • Tạo release tên "eazybank" từ chart "prod-env".
				   • Helm render templates và tạo resource trên cluster.
			   - Nên kết hợp thêm: --namespace, -f/--set, --wait/--timeout/--atomic, và quản lý CRD cẩn thận.

			============================================================


		- cmd: help upgrade eazybank prod-env			

			============================================================
			LỆNH (HIỂU): helm upgrade eazybank prod-env
			============================================================

			1. MỤC ĐÍCH CHUNG
			   - Dùng Helm để **nâng cấp (upgrade)** một release đã tồn tại tên "eazybank"
				 bằng chart "prod-env".
			   - Nếu chart hoặc giá trị cấu hình thay đổi, lệnh này sẽ render template mới
				 và áp thay đổi đó lên cluster (k8s resources).
			   - Bạn cũng có thể dùng một vài tùy chọn để tự động cài nếu release chưa tồn tại,
				 kiểm soát hành vi rollback, chờ ready, v.v.

			------------------------------------------------------------
			2. SỰ KHÁC NHAU GIỮA install VÀ upgrade
			   - `helm install` tạo một release mới.
			   - `helm upgrade` thay đổi release đã có (tạo revision mới trong history).
			   - `helm upgrade --install` (viết tắt `helm upgrade -i`) = upgrade nếu tồn tại, 
				 nếu không tồn tại thì sẽ cài mới (useful cho idempotent CI/CD).

			------------------------------------------------------------
			3. CÚ PHÁP CƠ BẢN
			   helm upgrade [RELEASE] [CHART] [flags]

			   Ví dụ:
			   helm upgrade eazybank prod-env

			   Giải thích:
				 • RELEASE (eazybank) → tên release Helm đã được cài trước đó.
				 • CHART (prod-env)    → chart dùng để upgrade (có thể là repo/chart hoặc đường dẫn local).

			------------------------------------------------------------
			4. NGUỒN CHART
			   - Nếu CHART là repo/chart: đảm bảo repo đã add & update:
				   helm repo add <name> <url>
				   helm repo update
				 Ví dụ cài từ repo: helm upgrade eazybank myrepo/prod-env
			   - Nếu CHART là folder local: dùng đường dẫn hoặc ./prod-env:
				   helm upgrade eazybank ./prod-env

			------------------------------------------------------------
			5. HÀNH VI KHI CHẠY upgrade
			   - Helm đọc chart mới và values (mặc định + override).
			   - Helm render templates và so sánh với manifests hiện có.
			   - Helm gửi API requests để patch/create/delete resources tương ứng.
			   - Helm ghi lại revision mới trong history của release.
			   - Tùy option (ví dụ --wait) Helm có thể chờ cho resources sẵn sàng.

			------------------------------------------------------------
			6. TÙY CHỌN THƯỜNG DÙNG (và ý nghĩa)
			   - -i, --install
				   → Nếu release không tồn tại thì cài mới (upgrade or install).

			   - -f, --values <file>
				   → Sử dụng file values để override. Có thể truyền nhiều -f.
				   Ví dụ: -f values-prod.yaml

			   - --set key=value[,key2=val2]
				   → Ghi đè nhanh trên CLI.

			   - --reuse-values
				   → Giữ lại các giá trị hiện tại (từ revision trước) và chỉ ghi đè
					 những giá trị bạn cung cấp trong command này.

			   - --reset-values
				   → Bỏ hết values cũ, chỉ dùng values từ chart + -f/--set mới.

			   - --atomic
				   → Nếu upgrade thất bại, Helm tự động rollback về revision trước.

			   - --cleanup-on-fail
				   → Nếu upgrade thất bại, xóa release (thay vì giữ trạng thái failed).

			   - --force
				   → Thực hiện “recreate” hoặc update các trường không thể patch,
					 ví dụ khi cần replace immutable field (kết hợp tạo lại resource).
					 Cẩn trọng: có thể gây downtime.

			   - --wait
				   → Chờ cho các resource quan trọng trở nên Ready (pods, PVC bound, deployments).
					 Thường phối hợp với --timeout.

			   - --timeout <duration>
				   → Thời gian tối đa chờ khi dùng --wait (ví dụ 5m, 10m, 600s).

			   - --dry-run
				   → Render templates và mô phỏng upgrade mà không gửi thay đổi tới cluster.

			   - --no-hooks / --replace
				   → --no-hooks: bỏ qua hook lifecycle; --replace: thay thế release nếu trước đó đã bị xóa (rare).

			   - --description "text"
				   → Thêm mô tả cho revision mới trong history.

			   - --version <chart-version>
				   → Dùng version chart cụ thể (khi cài từ repo).

			   - --reuse-values vs --reset-values
				   → Chọn một trong hai: reuse giữ giá trị cũ; reset loại bỏ giá trị cũ.

			------------------------------------------------------------
			7. KHI NÀO NÊN DÙNG NHỮNG TÙY CHỌN TRÊN (GỢI Ý)
			   - CI/CD: helm upgrade --install -f values-prod.yaml --atomic --wait --timeout 10m
			   - Thử nghiệm: helm upgrade --dry-run --debug eazybank prod-env -f values-test.yaml
			   - Giữ config cũ và chỉ đổi image: helm upgrade eazybank prod-env --reuse-values --set image.tag=1.2.3
			   - Thay đổi cấu trúc lớn (muốn bắt đầu từ values mới): helm upgrade eazybank prod-env --reset-values -f new-values.yaml

			------------------------------------------------------------
			8. LƯU Ý VỀ CRD VÀ CUSTOM RESOURCES
			   - Nếu chart định nghĩa CRD trong thư mục crds/, Helm không tự động update CRD khi upgrade.
			   - Nếu upgrade cần thay đổi CRD, bạn phải apply CRD thủ công (kubectl apply) trước hoặc quản lý CRD riêng.
			   - Nếu các tài nguyên mới phụ thuộc CRD mà chưa tồn tại, upgrade sẽ lỗi ("no matches for kind ...").

			------------------------------------------------------------
			9. HOOKS VÀ TÁC ĐỘNG
			   - Chart có thể định nghĩa hooks (pre-upgrade, post-upgrade, pre-install...).
			   - Hooks sẽ chạy tương ứng (trừ khi dùng --no-hooks).
			   - Hooks có thể thất bại và ảnh hưởng tới status; kiểm tra logs và sự kiện pod nếu cần.

			------------------------------------------------------------
			10. KIỂM TRA SAU KHI UPGRADE
			   - Xem lịch sử và status:
				   helm history eazybank -n <namespace>
				   helm status eazybank -n <namespace>

			   - Kiểm tra resources:
				   kubectl get pods,deploy,sts,svc -n <namespace>
				   kubectl describe pod <pod> -n <namespace>
				   kubectl logs <pod> -n <namespace>

			   - Nếu gặp vấn đề: rollback:
				   helm rollback eazybank <revision> -n <namespace>

			------------------------------------------------------------
			11. LỖI THƯỜNG GẶP VÀ GIẢI PHÁP
			   - Chart không tìm thấy:
				   • Chưa helm repo update hoặc sai đường dẫn (nếu là local, cần ./).
			   - Thiếu quyền (RBAC):
				   • Kiểm tra kubeconfig và quyền user.
			   - Upgrade tạo resource nhưng pod CrashLoopBackOff:
				   • Xem logs, kiểm tra env vars, secrets, kết nối DB.
			   - Immutable field error:
				   • Một số fields (ví dụ spec.template.spec.containers[].name?) không thể patch — dùng --force để recreate.
			   - Thiếu CRD:
				   • "no matches for kind ..." → cài CRD trước bằng kubectl apply.
			   - Hook thất bại:
				   • Kiểm tra pod/job tương ứng với hook; sửa input hoặc bỏ hook bằng --no-hooks (cẩn trọng).

			------------------------------------------------------------
			12. EXAMPLES (COPY-PASTE)
			   # 1) Nâng cấp đơn giản (chart trong repo hoặc local)
			   helm upgrade eazybank prod-env

			   # 2) Nâng cấp với file values, đợi đến khi hoàn tất, nếu lỗi rollback tự động
			   helm upgrade eazybank prod-env -f values-prod.yaml --wait --timeout 10m --atomic

			   # 3) Upgrade nếu tồn tại, nếu chưa có thì cài mới
			   helm upgrade --install eazybank prod-env -f values-prod.yaml -n production --create-namespace --wait --atomic

			   # 4) Chỉ đổi image tag, giữ các giá trị khác
			   helm upgrade eazybank prod-env --reuse-values --set image.tag=2.0.1

			   # 5) Thực hiện dry-run để kiểm tra thay đổi trước khi apply
			   helm upgrade eazybank prod-env -f values-prod.yaml --dry-run --debug

			   # 6) Force recreate nếu gặp immutable field errors (cẩn trọng)
			   helm upgrade eazybank prod-env -f values-prod.yaml --force

			   # 7) Reset toàn bộ values về mặc định chart, dùng file mới
			   helm upgrade eazybank prod-env --reset-values -f new-values.yaml

			------------------------------------------------------------
			13. QUẢN LÝ LỊCH SỬ & KHÔI PHỤC
			   - Xem history:
				   helm history eazybank -n <namespace>
			   - Rollback:
				   helm rollback eazybank <revision> -n <namespace>
				 → quay về revision cũ (có thể phối hợp với --wait).

			------------------------------------------------------------
			14. BEST PRACTICES
			   - Luôn test upgrade bằng --dry-run trong môi trường staging.
			   - Lưu tất cả file values (versioned) trong VCS.
			   - Dùng --atomic trong CI để tự rollback khi thất bại.
			   - Pin chart version bằng --version để tránh thay đổi đột ngột.
			   - Quản lý CRD tách khỏi release lifecycle nếu nhiều release dùng chung.
			   - Giữ tên release và namespace rõ ràng (eazybank-prod, namespace=production).

			------------------------------------------------------------
			15. TÓM TẮT NGẮN GỌN
			   - `helm upgrade eazybank prod-env`:
				   • Tạo một revision mới cho release "eazybank" bằng chart "prod-env".
				   • Áp các thay đổi manifest vào Kubernetes.
				   • Sử dụng các flag như --install, --wait, --atomic, --reuse-values để điều khiển hành vi.
			   - Trước khi upgrade: kiểm tra chart source, values, CRD, và test với --dry-run nếu cần.

			============================================================


		
		- cmd: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard # Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart	
		
			============================================================
			LỆNH: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
			============================================================

			1. MỤC ĐÍCH CHUNG
			   - Dùng Helm để **nâng cấp (upgrade)** release có tên "kubernetes-dashboard" nếu đã tồn tại,
				 hoặc **cài mới (install)** nếu release chưa tồn tại.
			   - Chart dùng để cài là `kubernetes-dashboard` lấy từ repo `kubernetes-dashboard` (cú pháp repo/chart).
			   - Tự động **tạo namespace `kubernetes-dashboard`** nếu namespace chưa tồn tại (--create-namespace).
			   - Tất cả thao tác sẽ được thực hiện trong namespace `kubernetes-dashboard` (--namespace).

			------------------------------------------------------------
			2. GIẢI THÍCH CÁC THÀNH PHẦN TRONG LỆNH

			   helm
				   → CLI của Helm, công cụ quản lý package (chart) cho Kubernetes.

			   upgrade
				   → Lệnh nâng cấp một release hiện có bằng chart mới / values mới.

			   --install  (hoặc -i)
				   → Nếu release không tồn tại thì **tự động cài mới** (kết hợp upgrade và install). 
					 Đây là pattern phổ biến trong CI/CD để đảm bảo tính idempotent.

			   kubernetes-dashboard
				   → **Tên release** (tên bạn đặt cho lần triển khai này). Dùng để quản lý (status, upgrade, uninstall).

			   kubernetes-dashboard/kubernetes-dashboard
				   → **Chart** được dùng để upgrade/install.
					 Cú pháp `<repo>/<chart>` nghĩa là chart `kubernetes-dashboard` trong repo đã add có tên `kubernetes-dashboard`.
					 (Bạn cần `helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/`
					  và `helm repo update` trước nếu chưa add.)

			   --create-namespace
				   → Nếu namespace đích không tồn tại thì Helm sẽ tự tạo namespace đó trước khi cài.

			   --namespace kubernetes-dashboard  (hoặc -n)
				   → Chỉ định namespace để cài/upgrade release. Trong trường hợp này là `kubernetes-dashboard`.

			------------------------------------------------------------
			3. YÊU CẦU TIỀN QUÁN (PRE-REQUISITES)
			   - Đã có Helm 3.x cài trên máy và kết nối tới Kubernetes cluster (kubeconfig).
			   - Repo `kubernetes-dashboard` đã được thêm hoặc chart có thể truy xuất:
				   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
				   helm repo update
			   - Quyền (RBAC) của user/service account đang dùng phải đủ để:
				   • Tạo Namespace (nếu dùng --create-namespace)
				   • Tạo Deployment, Service, RBAC, ServiceAccount, RoleBinding, ConfigMap, Secret, v.v.
			   - Nếu cluster policy yêu cầu, chuẩn bị imagePullSecrets hoặc network/policy tương ứng.

			------------------------------------------------------------
			4. HÀNH VI KHI CHẠY LỆNH
			   - Helm kiểm tra xem release `kubernetes-dashboard` đã tồn tại trong namespace `kubernetes-dashboard` chưa.
				   • Nếu không tồn tại → thực hiện cài (install).
				   • Nếu tồn tại → thực hiện upgrade (tạo revision mới).
			   - Nếu namespace chưa tồn tại và bạn dùng --create-namespace, Helm sẽ tạo namespace đó.
			   - Helm lấy chart từ cache repo (nếu cần sẽ tải index/chart), kết hợp values mặc định và các override (nếu bạn truyền).
			   - Helm render templates thành manifests Kubernetes và gửi lên Kubernetes API để tạo/cập nhật resources.
			   - Helm ghi metadata release vào store (thường là Secret trong namespace của release).

			------------------------------------------------------------
			5. CÁC TÙY CHỌN THƯỜNG KẾT HỢP VỚI LỆNH NÀY
			   - -f values.yaml
				   → Dùng file values để override cấu hình chart.
			   - --set key=value
				   → Ghi đè cấu hình nhanh trên CLI.
			   - --wait
				   → Chờ cho các resource thiết yếu ready (pods, PVCs,...).
			   - --timeout <duration>
				   → Thời gian chờ tối đa khi dùng --wait (ví dụ 5m, 10m).
			   - --atomic
				   → Nếu upgrade/install thất bại, Helm tự rollback.
			   - --dry-run --debug
				   → Mô phỏng (dry-run) và in debug output để kiểm tra trước khi apply.
			   - --version <chart-version>
				   → Dùng version chart cụ thể từ repo.

			------------------------------------------------------------
			6. CÁC TÁC VỤ/RESOURCE THƯỜNG ĐƯỢC TẠO CHO KUBERNETES DASHBOARD
			   - Namespace: kubernetes-dashboard (nếu tạo)
			   - ServiceAccount, ClusterRole / ClusterRoleBinding hoặc Role / RoleBinding tùy chart
			   - Deployment / StatefulSet cho dashboard
			   - Services (ClusterIP/NodePort/LoadBalancer) để truy cập dashboard
			   - ConfigMap / Secret cho cấu hình
			   - Ingress (nếu chart có cấu hình)
			   - ServiceMonitor/PrometheusRule (nếu chart hỗ trợ và CRD tồn tại)

			------------------------------------------------------------
			7. KIỂM TRA SAU KHI CHẠY
			   - Kiểm tra release Helm:
				   helm ls -n kubernetes-dashboard
				   helm status kubernetes-dashboard -n kubernetes-dashboard
			   - Kiểm tra namespace và pod:
				   kubectl get ns
				   kubectl get pods -n kubernetes-dashboard
			   - Kiểm tra logs nếu pod crash:
				   kubectl logs <pod-name> -n kubernetes-dashboard
			   - Kiểm tra service/ingress để biết cách truy cập:
				   kubectl get svc,ing -n kubernetes-dashboard

			------------------------------------------------------------
			8. LỖI THƯỜNG GẶP VÀ CÁCH XỬ LÝ
			   - Chart không tìm thấy:
				   • Chưa add repo hoặc chưa `helm repo update`.
				   • Nếu chart local thì dùng `./path/to/chart`.
			   - Quyền insufficient:
				   • Kiểm tra role/cluster-role của user trong kubeconfig.
			   - Namespace creation failed:
				   • User không có quyền tạo namespace → tạo namespace thủ công trước hoặc yêu cầu quyền.
			   - ServiceAccount/ClusterRoleBinding bị từ chối:
				   • Kiểm tra policy RBAC và PodSecurityPolicy / Admission controllers.
			   - ImagePullBackOff:
				   • Kiểm tra image name/tag, registry credentials (imagePullSecrets).
			   - CrashLoopBackOff:
				   • Xem logs và events, kiểm tra biến môi trường, secret, kết nối tới API server (nếu cần).
			   - Hook/Job thất bại:
				   • Chart có thể chạy các Job/hook (pre-install/pre-upgrade); kiểm tra job/pod logs và events.

			------------------------------------------------------------
			9. ROLLBACK & GHI CHÚ VỀ HISTORY
			   - Xem lịch sử release:
				   helm history kubernetes-dashboard -n kubernetes-dashboard
			   - Rollback về revision trước:
				   helm rollback kubernetes-dashboard <revision> -n kubernetes-dashboard
			   - Nếu dùng --atomic, trong trường hợp lỗi Helm sẽ tự rollback (nếu có hỗ trợ).

			------------------------------------------------------------
			10. BẢO MẬT & BEST PRACTICES
			   - Thêm repo và pin version chart:
				   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
				   helm repo update
				   helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --version X.Y.Z ...
			   - Cấu hình RBAC cẩn thận: Dashboard thường cần quyền để đọc/hiển thị thông tin cluster — hạn chế quyền theo nguyên tắc least privilege.
			   - Dùng values file (ví dụ values-prod.yaml) và lưu trong VCS.
			   - Kiểm tra trước với --dry-run --debug trên môi trường staging.
			   - Nếu deploy trong CI/CD, dùng --wait --timeout và --atomic để đảm bảo idempotency và rollback khi lỗi.
			   - Không expose dashboard public mà không có xác thực/ingress rule; cân nhắc dùng Proxy hoặc RBAC token.

			------------------------------------------------------------
			11. LỆNH THỰC TIỄN (COPY-PASTE) – CÁC KỊCH BẢN PHỔ BIẾN

			   # 1) Thêm repo (nếu chưa add) và cập nhật
			   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
			   helm repo update

			   # 2) Cài hoặc upgrade (tạo namespace nếu cần)
			   helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
				 --create-namespace --namespace kubernetes-dashboard

			   # 3) Cài/upgrade với file values và chờ hoàn tất, rollback nếu thất bại
			   helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
				 -f dashboard-values.yaml --create-namespace --namespace kubernetes-dashboard \
				 --wait --timeout 10m --atomic

			   # 4) Dry-run để kiểm tra trước khi apply thật
			   helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
				 -f dashboard-values.yaml --create-namespace --namespace kubernetes-dashboard \
				 --dry-run --debug

			   # 5) Gỡ cài
			   helm uninstall kubernetes-dashboard -n kubernetes-dashboard

			------------------------------------------------------------
			12. TÓM TẮT NGẮN GỌN
			   - Lệnh `helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard`
				 → đảm bảo release `kubernetes-dashboard` được cài hoặc nâng cấp trong namespace `kubernetes-dashboard`, tạo namespace nếu cần.
			   - Thích hợp dùng trong scripts/CI để triển khai idempotent.
			   - Trước khi chạy: kiểm tra repo, quyền, và chuẩn bị file values nếu cần.

			============================================================
		
		
		
	Deployment và Service:
	
		Deployment là gì?
		
			Deployment là một object trong Kubernetes dùng để quản lý Pod (ứng dụng chạy thực sự).

			Nó đảm bảo số lượng Pod bạn khai báo luôn chạy đủ, nếu Pod chết thì tự tạo lại.

			Nó cũng hỗ trợ Rolling Update (cập nhật version dần dần, không downtime).
			
		Service là gì?
		
			Service dùng để expose (mở ra) các Pod cho các Pod khác hoặc bên ngoài có thể truy cập.

			Vì Pod có IP thay đổi liên tục (mỗi lần restart Pod là IP đổi), nên bạn không thể truy cập trực
			tiếp. Service sẽ tạo một IP cố định (ClusterIP) hoặc một cổng (NodePort/LoadBalancer) để truy cập.

			Service còn có cơ chế load balancing giữa các Pod.
			
		Các loại Service phổ biến:
		
			ClusterIP (mặc định): chỉ cho phép truy cập trong cluster (nội bộ).

			NodePort: mở port trên node để có thể truy cập từ ngoài qua <NodeIP>:<NodePort>.

			LoadBalancer: (nếu cluster chạy trên cloud như AWS, GCP, Azure) sẽ tạo Load Balancer ngoài Internet.
			
		Mối quan hệ giữa Deployment và Service:
		
			Deployment chịu trách nhiệm: "Tạo và duy trì Pod".

			Service chịu trách nhiệm: "Expose và điều phối traffic đến Pod đó".
		
			Deployment và Pod:

				Deployment quản lý Pod.

				Bạn định nghĩa image, số replicas (số Pod muốn chạy), Kubernetes sẽ tạo ra Pod tương ứng.

				Nếu Pod chết → Deployment sẽ tự tạo lại.
				
			Service và Pod:
			
				Service không quản lý Pod, mà chỉ kết nối đến Pod dựa trên label selector.

				Service sẽ tìm Pod nào có label khớp với selector, rồi load balancing request đến chúng.
				
				Ví dụ:
				
					selector:
						app: my-app
					  
						Nó sẽ tự động tìm tất cả Pod do Deployment my-app tạo ra (có label app=my-app) để đưa request vào.
		
	Kubernetes dashboard:
	
		Cần 1 khoảng thời gian mới có thể truy cập vào được http://localhost:8001/
		
		Cài đặt bằng helm:
		
		Thêm Helm repo cho Kubernetes Dashboard:

			# Add kubernetes-dashboard repository
			helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
		
		Cài đặt Dashboard bằng Helm:

			Sau lệnh này, Helm sẽ tạo Deployment + Service cho Dashboard.

				# Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart
				helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
				
		Tạo ServiceAccount & RBAC (admin):

			Helm không tự tạo admin user. Bạn cần thêm dashboard-adminuser.yaml và dashboard-rolebinding.yaml

			Say đó áp dụng:

				kubectl apply -f dashboard-adminuser.yaml

				kubectl apply -f dashboard-rolebinding.yaml
				
		Lấy token đăng nhập:

			Lấy token ngắn hạn:

				Chạy lệnh để lấy token:

					kubectl -n kubernetes-dashboard create token admin-user
				
			Lấy token dài hạn:

				Cần file secret.yaml
				
				kubectl apply -f secret.yaml
				
				Chạy lệnh để lấy token:
				
					kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" | base64 -d
				
		Truy cập Dashboard:
		
			Chú ý phải có Kubernetes dashboard kong proxy
		
			Port-forward (nhanh nhất):
			
				kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443

				Mở:
					
					https://localhost:8443
			
			kubectl proxy:
			
				kubectl proxy

				Mở:
				
					http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard-kong-proxy:/proxy/
					
			Expose bằng NodePort (nếu muốn truy cập từ bên ngoài):
			
				Ví dụ sửa service thành NodePort, đổi type: ClusterIP → NodePort:
				
					kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard-kong-proxy

				Mở:
				
					https://<NodeIP>:<NodePort>

	Các loại dashboard của kubernetes:
	
	
		kubernetes-dashboard-web → giao diện web UI (frontend React app).

		kubernetes-dashboard-api → backend API phục vụ cho web UI.

		kubernetes-dashboard-auth → xử lý authentication & session cho người dùng.

		kubernetes-dashboard-metrics-scraper → component phụ để thu thập metrics từ các pods/nodes.

		kubernetes-dashboard-kong → Kong Gateway đóng vai trò reverse proxy / ingress nội bộ, gom API và route traffic giữa các thành phần.		
		
	
	Phân biệt ClusterIP / NodePort / LoadBalancer:
	
		ClusterIP:
		
			Mặc định khi bạn tạo Service.

			Chỉ expose service bên trong cluster (các Pod khác trong cluster mới gọi được).

			Không truy cập được từ ngoài cluster.
			
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-service
			spec:
			  type: ClusterIP
			  selector:
				app: my-app
			  ports:
				- protocol: TCP
				  port: 80        # service port
				  targetPort: 8080  # pod containerPort
				  
				Truy cập: http://my-service:80 (chỉ từ pod trong cluster).
				
		NodePort:
		
			Expose service ra ngoài qua mỗi Node IP ở 1 port tĩnh (trong range 30000-32767).

			Người ngoài có thể truy cập bằng NodeIP:NodePort.

			Không cần Ingress, nhưng port bị giới hạn.

			apiVersion: v1
			kind: Service
			metadata:
			  name: my-service
			spec:
			  type: NodePort
			  selector:
				app: my-app
			  ports:
				- protocol: TCP
				  port: 80
				  targetPort: 8080
				  nodePort: 30080   # expose qua NodeIP:30080
				  
				Truy cập: http://<NodeIP>:30080
				
		LoadBalancer:
		
			Chỉ dùng được khi cluster chạy trên cloud provider có Load Balancer (AWS ELB, GCP, Azure, v.v).

			Kubernetes sẽ yêu cầu cloud tạo LB → ánh xạ traffic vào NodePort/ClusterIP phía dưới.

			Dễ dùng cho production vì có IP public.
			
			apiVersion: v1
			kind: Service
			metadata:
			  name: my-service
			spec:
			  type: LoadBalancer
			  selector:
				app: my-app
			  ports:
				- protocol: TCP
				  port: 80
				  targetPort: 8080

				Truy cập: http://<External-IP>:80 (K8s tự cấp qua cloud LB).
		

	Kubernetes có rất nhiều kind (loại tài nguyên) khác nhau, mỗi loại đại diện cho một thành phần hoặc
	một tài nguyên trong hệ sinh thái Kubernetes. Dưới đây là danh sách các kind phổ biến trong Kubernetes:

	1. Pod

		Kind: Pod

		Đại diện cho một đơn vị triển khai cơ bản nhất trong Kubernetes. Một pod có thể chứa một hoặc
		nhiều container.

	2. Service

		Kind: Service

		Đại diện cho một endpoint mạng có thể truy cập được từ bên ngoài hoặc từ các pod khác trong cùng
		namespace. Có các loại service như ClusterIP, NodePort, LoadBalancer, và ExternalName.

	3. Deployment

		Kind: Deployment

		Quản lý việc triển khai các pod, đảm bảo số lượng pod đang chạy luôn đạt yêu cầu. Deployment tự
		động cập nhật phiên bản mới của ứng dụng.

	4. ReplicaSet

		Kind: ReplicaSet

		Đảm bảo rằng một số lượng cụ thể các bản sao của pod đang chạy tại mọi thời điểm. Deployment sử
		dụng ReplicaSet để duy trì các bản sao của pod.

	5. StatefulSet

		Kind: StatefulSet

		Dùng để triển khai các ứng dụng có trạng thái, đảm bảo các pod có tên và lưu trữ bền
		vững (persistent storage).

	6. DaemonSet

		Kind: DaemonSet

		Đảm bảo rằng một pod được chạy trên tất cả các node trong cluster, hoặc trên các node mà bạn chọn.

	7. Job

		Kind: Job

		Đảm bảo rằng một hoặc nhiều pod chạy và hoàn thành một nhiệm vụ trong một khoảng thời gian xác định.

	8. CronJob

		Kind: CronJob

		Quản lý các job theo lịch định kỳ, giống như cron job trên hệ điều hành Linux.

	9. Namespace

		Kind: Namespace

		Một không gian tên (namespace) dùng để phân tách tài nguyên trong Kubernetes, giúp dễ dàng quản
		lý các tài nguyên trong môi trường đa người dùng.

	10. ConfigMap

		Kind: ConfigMap

		Dùng để lưu trữ các thông tin cấu hình mà có thể được sử dụng bởi các pod trong cluster.

	11. Secret

		Kind: Secret

		Dùng để lưu trữ thông tin nhạy cảm như mật khẩu, token, hoặc chứng chỉ mà không muốn lưu trữ
		dưới dạng plain text trong cấu hình.

	12. Ingress

		Kind: Ingress

		Cung cấp các quy tắc để truy cập vào các dịch vụ trong Kubernetes từ ngoài cluster, thường
		được sử dụng để cấu hình HTTP/HTTPS routing.

	13. PersistentVolume (PV)

		Kind: PersistentVolume

		Đề cập đến một phần lưu trữ bền vững (bất kể là loại lưu trữ nào), có thể được sử dụng bởi các pod.

	14. PersistentVolumeClaim (PVC)

		Kind: PersistentVolumeClaim

		Đại diện cho yêu cầu sử dụng PersistentVolume. PVC có thể được gắn với một pod để sử dụng lưu trữ bền vững.

	15. HorizontalPodAutoscaler (HPA)

		Kind: HorizontalPodAutoscaler

		Tự động điều chỉnh số lượng pod trong một deployment hoặc replica set dựa trên chỉ số như CPU usage hoặc memory usage.

	16. PodDisruptionBudget (PDB)

		Kind: PodDisruptionBudget

		Đảm bảo rằng có một số lượng tối thiểu pod hoạt động trong khi thực hiện các hoạt động như upgrade hoặc maintenance.

	17. Role

		Kind: Role

		Định nghĩa quyền truy cập trong một namespace cụ thể. Thường được sử dụng kết hợp với RoleBinding.

	18. RoleBinding

		Kind: RoleBinding

		Gắn quyền truy cập từ Role với người dùng hoặc nhóm người dùng trong Kubernetes.

	19. ClusterRole

		Kind: ClusterRole

		Tương tự như Role, nhưng ClusterRole áp dụng cho toàn bộ cluster thay vì chỉ một namespace.

	20. ClusterRoleBinding

		Kind: ClusterRoleBinding

		Gắn quyền truy cập từ ClusterRole với người dùng hoặc nhóm người dùng trên toàn bộ cluster.

	21. NetworkPolicy

		Kind: NetworkPolicy

		Định nghĩa các quy tắc về bảo mật mạng, kiểm soát lưu lượng mạng giữa các pod.

	22. ServiceAccount

		Kind: ServiceAccount

		Cung cấp danh tính cho các pod, giúp xác thực và ủy quyền trong Kubernetes.

	23. APIService

		Kind: APIService

		Cung cấp khả năng mở rộng cho API server Kubernetes, giúp kết nối với các API bên ngoài.

	24. Endpoint

		Kind: Endpoints

		Định nghĩa một hoặc nhiều IP và port mà dịch vụ có thể sử dụng để giao tiếp với pod.

	25. Volume

		Kind: Volume

		Định nghĩa các loại lưu trữ có thể được sử dụng trong pod, ví dụ: emptyDir, hostPath, nfs, configMap, etc.

	26. Event

		Kind: Event

		Đại diện cho các sự kiện trong cluster Kubernetes, cung cấp thông tin về các hành động hoặc trạng thái của các tài nguyên.
		
	Pod:
		
		Pod là đơn vị cơ bản và nhỏ nhất trong Kubernetes dùng để triển khai ứng dụng. Một Pod có thể chứa một hoặc nhiều containers,
		thường là các container chạy các ứng dụng hoặc dịch vụ trong Kubernetes.

		Khi bạn triển khai ứng dụng trên Kubernetes, bạn sẽ triển khai nó dưới dạng một Pod. Mỗi Pod chạy trên một node trong cluster
		Kubernetes, và Kubernetes sẽ tự động quản lý vòng đời và sự tái tạo (rescheduling) của Pod khi có sự cố xảy ra.		
		
	Deployment và Service:
	
		Deployment đảm bảo rằng các Pod ứng dụng luôn được triển khai và duy trì ở trạng thái sẵn sàng.

		Service thì đảm bảo các Pod này có thể giao tiếp với các ứng dụng khác trong hoặc ngoài cluster Kubernetes. Service sẽ
		định tuyến lưu lượng tới các Pod do Deployment quản lý thông qua label selector.

		Việc để chúng trong cùng một file giúp bạn nhận ra rằng Service đang cung cấp truy cập đến các Pod được triển khai bởi
		Deployment. Đây là một cặp tài nguyên đi đôi với nhau và phụ thuộc vào nhau.
		
	Replicaset:
	
		ReplicaSet giám sát các pod và đảm bảo rằng số lượng pod đang chạy luôn bằng với số lượng yêu cầu (desired replicas).

		Nếu có một pod nào đó bị lỗi hoặc bị xóa, ReplicaSet sẽ tạo ra một pod mới để thay thế pod đó, đảm bảo số lượng pod không thay đổi.

		ReplicaSet sử dụng labels để xác định các pod mà nó quản lý.
		
		
	kubectl proxy là gì?

		Đây là một lệnh giúp mở một HTTP proxy server trên localhost, thường chạy ở port 8001.

		Khi bạn chạy proxy, nó sẽ chuyển tiếp (forward) tất cả request từ máy của bạn đến API Server của Kubernetes.

		Nhờ vậy bạn có thể truy cập API Kubernetes và cả Kubernetes Dashboard service thông qua localhost:8001.
		
		Khi bạn truy cập localhost:8001 sau khi chạy kubectl proxy, thì đó chính là kênh proxy vào API Server
		của Kubernetes, không liên quan gì đến Kong cả.

		Output bạn dán là toàn bộ các endpoints của Kubernetes API (/apis, /healthz, /readyz, /metrics, …), chứ
		không phải Admin API của Kong.
		
		kong-proxy (cho client bên ngoài)

		kong-admin (cho Admin API)
		
		Pod kubernetes-dashboard-kong → chạy Kong Gateway (reverse proxy).

		Service kubernetes-dashboard-kong-proxy → expose Kong ra bên ngoài namespace
		(kiểu ClusterIP/NodePort/LoadBalancer tùy manifest). Nó là “cửa ngõ” để bạn truy cập vào Dashboard.
		
		Người dùng không gọi trực tiếp vào kubernetes-dashboard-web hay kubernetes-dashboard-api.

		Thay vào đó, tất cả request sẽ đi qua Kong proxy (service kubernetes-dashboard-kong-proxy), sau
		đó Kong mới route tới các service khác (web, api, auth, metrics).
		
	kubectl get pods:
		
		Mặc định kubectl sẽ liệt kê Pod trong namespace default.

		Trong khi các Pod của Kubernetes Dashboard (API, Web, Auth, Kong, Metrics scraper) lại nằm trong namespace
		kubernetes-dashboard, nên bạn không thấy chúng trong output kia.
		
	Vì sao cần port-forward?
	
		Kubernetes Dashboard (và hầu hết các addon trong cluster) chạy trong mạng nội bộ của cluster (ClusterIP Service).

		Tức là các service như kubernetes-dashboard-kong-proxy chỉ lắng nghe trong cluster, không tự động
		expose ra ngoài laptop/PC của bạn.

		kubectl port-forward tạo một tunnel từ cổng máy bạn (localhost:8443) tới cổng service trong cluster (443),
		nhờ vậy bạn mới mở Dashboard trên trình duyệt.
	
	Code mẫu Kubernetes:
	
		Case 1:
	
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: keycloak
			  labels:
				app: keycloak
			spec:
			  replicas: 1
			  selector:
				matchLabels:
				  app: keycloak
			  template:
				metadata:
				  labels:
					app: keycloak
				spec:
				  containers:
					- name: keycloak
					  image: quay.io/keycloak/keycloak:26.0.7
					  args: ["start-dev"]
					  env:
					  - name: KEYCLOAK_ADMIN
						valueFrom:
						  configMapKeyRef:
							name: eazybank-configmap
							key: KEYCLOAK_ADMIN
					  - name: KEYCLOAK_ADMIN_PASSWORD
						valueFrom:
						  configMapKeyRef:
							name: eazybank-configmap
							key: KEYCLOAK_ADMIN_PASSWORD
					  ports:
						- name: http
						  containerPort: 8080
			---
			apiVersion: v1
			kind: Service
			metadata:
			  name: keycloak
			  labels:
				app: keycloak
			spec:
			  selector:
				app: keycloak
			  type: LoadBalancer
			  ports:
				- name: http
				  port: 7080
				  targetPort: 8080
				  
				Được phân chia ra thành 2 phần: Deployment và Service
				
					Deployment:
					
						apiVersion: apps/v1: Đây là phiên bản của API mà Kubernetes sử dụng để định nghĩa các
						tài nguyên loại Deployment (phiên bản v1 của API này).

						kind: Deployment: Đây là kiểu tài nguyên của Kubernetes, đại diện cho một deployment — việc
						triển khai và duy trì một ứng dụng với số lượng bản sao (replicas) cụ thể.

						metadata: Phần này cung cấp thông tin định danh cho tài nguyên.

						name: keycloak: Tên của deployment là keycloak.

						labels: Dùng để gắn nhãn cho deployment này, giúp phân loại tài nguyên trong Kubernetes. Ở đây,
						nhãn app: keycloak có thể dùng để lọc hoặc tìm kiếm tài nguyên liên quan đến Keycloak.

						spec: Phần này định nghĩa cấu hình chi tiết cho deployment.

						replicas: 1: Điều này yêu cầu Kubernetes chạy 1 pod cho deployment này.

						selector: Định nghĩa cách Kubernetes xác định các pod mà deployment này quản lý.

						matchLabels: app: keycloak: Chọn các pod có nhãn app: keycloak.

						template: Đây là mẫu (template) của pod mà Kubernetes sẽ tạo ra cho deployment này.

						metadata: Định nghĩa nhãn cho pod.

						spec: Cấu hình cho các container trong pod.

						containers: Danh sách các container trong pod. Mỗi container sẽ chạy một ứng dụng.

						name: keycloak: Tên container là keycloak.

						image: quay.io/keycloak/keycloak:26.0.7: Docker image sẽ được sử dụng để tạo container. Ở đây là
						image của Keycloak phiên bản 26.0.7.

						args: ["start-dev"]: Đây là các tham số sẽ được truyền cho container khi khởi động. start-dev là tham
						số để chạy Keycloak trong chế độ phát triển.

						env: Định nghĩa các biến môi trường cho container.

						KEYCLOAK_ADMIN và KEYCLOAK_ADMIN_PASSWORD: Các biến môi trường này lấy giá trị từ một ConfigMap có tên
						là eazybank-configmap. Chúng dùng để cấu hình tài khoản admin của Keycloak.

						valueFrom.configMapKeyRef: Lấy giá trị từ ConfigMap thay vì gán trực tiếp.

						ports: Cấu hình cổng mà container sẽ mở ra.

						containerPort: 8080: Keycloak sẽ chạy trên cổng 8080 bên trong container.

					Service:
					
						apiVersion: v1: Phiên bản API để định nghĩa tài nguyên Service.

						kind: Service: Đây là kiểu tài nguyên Kubernetes dùng để tạo một dịch vụ (service) mà các pod có
						thể truy cập từ bên ngoài hoặc các pod khác trong cluster.

						metadata: Cung cấp thông tin nhận diện cho service.

						name: keycloak: Tên của service là keycloak.

						labels: Nhãn tương tự như trong deployment, giúp phân loại.

						spec: Định nghĩa các cấu hình chi tiết của service.

						selector: Dùng để chỉ định các pod mà service này sẽ truy cập, trong trường hợp này là các pod có
						nhãn app: keycloak.

						type: LoadBalancer: Điều này nghĩa là Kubernetes sẽ cấu hình một LoadBalancer để cung cấp địa chỉ IP
						công cộng cho service. Service này có thể được truy cập từ ngoài cluster thông qua một IP công cộng hoặc DNS.

						ports: Định nghĩa các cổng mà service sẽ lắng nghe.

						port: 7080: Service sẽ lắng nghe trên cổng 7080 từ bên ngoài.

						targetPort: 8080: Khi có yêu cầu đến service, yêu cầu sẽ được chuyển tiếp đến cổng 8080 trên container.
						
		Case 2:
		
			1_keycloak.yml:
			
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: keycloak
				  labels:
					app: keycloak
				spec:
				  replicas: 1
				  selector:
					matchLabels:
					  app: keycloak
				  template:
					metadata:
					  labels:
						app: keycloak
					spec:
					  containers:
						- name: keycloak
						  image: quay.io/keycloak/keycloak:26.4.0
						  args: ["start-dev"]
						  env:
							- name: KC_BOOTSTRAP_ADMIN_USERNAME
							  valueFrom:
								configMapKeyRef:
								  name: eazybank-configmap
								  key: KC_BOOTSTRAP_ADMIN_USERNAME
							- name: KC_BOOTSTRAP_ADMIN_PASSWORD
							  valueFrom:
								configMapKeyRef:
								  name: eazybank-configmap
								  key: KC_BOOTSTRAP_ADMIN_PASSWORD
						  ports:
							- name: http
							  containerPort: 8080
				---
				apiVersion: v1
				kind: Service
				metadata:
				  name: keycloak
				  labels:
					app: keycloak
				spec:
				  selector:
					app: keycloak
				  type: LoadBalancer
				  ports:
					- name: http
					  port: 7080
					  targetPort: 8080
					  
			2_configmaps.yaml:
			
				apiVersion: v1
				kind: ConfigMap
				metadata:
				  name: eazybank-configmap
				data:
				  SPRING_PROFILES_ACTIVE: "prod"
				  SPRING_CONFIG_IMPORT: "configserver:http://configserver:8071/"
				  EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: "http://eurekaserver:8070/eureka/"
				  CONFIGSERVER_APPLICATION_NAME: "configserver"
				  EUREKA_APPLICATION_NAME: "eurekaserver"
				  ACCOUNTS_APPLICATION_NAME: "accounts"
				  LOANS_APPLICATION_NAME: "loans"
				  CARDS_APPLICATION_NAME: "cards"
				  GATEWAY_APPLICATION_NAME: "gatewayserver"
				  KC_BOOTSTRAP_ADMIN_USERNAME: "admin"
				  KC_BOOTSTRAP_ADMIN_PASSWORD: "admin"
				  SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI: "http://keycloak:7080/realms/master/protocol/openid-connect/certs"
				  
			3_configserver.yml:
			
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: configserver-deployment
				  labels:
					app: configserver
				spec:
				  replicas: 1
				  selector:
					matchLabels:
					  app: configserver
				  template:
					metadata:
					  labels:
						app: configserver
					spec:
					  containers:
					  - name: configserver
						image: eazybytes/configserver:s12
						ports:
						- containerPort: 8071
				---
				apiVersion: v1
				kind: Service
				metadata:
				  name: configserver
				spec:
				  selector:
					app: configserver
				  type: LoadBalancer
				  ports:
					- protocol: TCP
					  port: 8071
					  targetPort: 8071
					  
			4_eurekaserver.yml:
			
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: eurekaserver-deployment
				  labels:
					app: eurekaserver
				spec:
				  replicas: 1
				  selector:
					matchLabels:
					  app: eurekaserver
				  template:
					metadata:
					  labels:
						app: eurekaserver
					spec:
					  containers:
					  - name: eurekaserver
						image: eazybytes/eurekaserver:s12
						ports:
						- containerPort: 8070
						env:
						- name: SPRING_APPLICATION_NAME
						  valueFrom:
							configMapKeyRef:
							  name: eazybank-configmap
							  key: EUREKA_APPLICATION_NAME
						- name: SPRING_CONFIG_IMPORT
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_CONFIG_IMPORT
				---
				apiVersion: v1
				kind: Service
				metadata:
				  name: eurekaserver
				spec:
				  selector:
					app: eurekaserver
				  type: LoadBalancer
				  ports:
					- protocol: TCP
					  port: 8070
					  targetPort: 8070
					  
			5_accounts.yml:
			
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: accounts-deployment
				  labels:
					app: accounts
				spec:
				  replicas: 1
				  selector:
					matchLabels:
					  app: accounts
				  template:
					metadata:
					  labels:
						app: accounts
					spec:
					  containers:
					  - name: accounts
						image: eazybytes/accounts:s12
						ports:
						- containerPort: 8080
						env:
						- name: SPRING_APPLICATION_NAME
						  valueFrom:
							configMapKeyRef:
							  name: eazybank-configmap
							  key: ACCOUNTS_APPLICATION_NAME
						- name: SPRING_PROFILES_ACTIVE
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_PROFILES_ACTIVE
						- name: SPRING_CONFIG_IMPORT
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_CONFIG_IMPORT
						- name: EUREKA_CLIENT_SERVICEURL_DEFAULTZONE
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: EUREKA_CLIENT_SERVICEURL_DEFAULTZONE
				---
				apiVersion: v1
				kind: Service
				metadata:
				  name: accounts
				spec:
				  selector:
					app: accounts
				  type: LoadBalancer
				  ports:
					- protocol: TCP
					  port: 8080
					  targetPort: 8080
					  
			8_gateway.yml:
			
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: gatewayserver-deployment
				  labels:
					app: gatewayserver
				spec:
				  replicas: 1
				  selector:
					matchLabels:
					  app: gatewayserver
				  template:
					metadata:
					  labels:
						app: gatewayserver
					spec:
					  containers:
					  - name: gatewayserver
						image: eazybytes/gatewayserver:s12
						ports:
						- containerPort: 8072
						env:
						- name: SPRING_APPLICATION_NAME
						  valueFrom:
							configMapKeyRef:
							  name: eazybank-configmap
							  key: GATEWAY_APPLICATION_NAME
						- name: SPRING_PROFILES_ACTIVE
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_PROFILES_ACTIVE
						- name: SPRING_CONFIG_IMPORT
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_CONFIG_IMPORT
						- name: EUREKA_CLIENT_SERVICEURL_DEFAULTZONE
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: EUREKA_CLIENT_SERVICEURL_DEFAULTZONE
						- name: SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI
						  valueFrom: 
							configMapKeyRef:
							  name: eazybank-configmap
							  key: SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI
				---
				apiVersion: v1
				kind: Service
				metadata:
				  name: gatewayserver
				spec:
				  selector:
					app: gatewayserver
				  type: LoadBalancer
				  ports:
					- protocol: TCP
					  port: 8072
					  targetPort: 8072

	Các lệnh phổ biến của Kubernetes:
	
		kubectl set image <resource-type>/<resource-name> <container-name>=<new-image> [--record]
		
			Lệnh kubectl set image trong Kubernetes được sử dụng để thay đổi hoặc cập nhật image của container trong một resource
			như Deployment, Pod, DaemonSet, StatefulSet, hoặc ReplicaSet. Đây là một trong những lệnh quan trọng để bạn có thể triển
			khai các ứng dụng với các phiên bản image mới mà không cần phải tạo lại hoặc xóa đi các resource cũ.
			
			Khi bạn chạy kubectl set image để đổi image từ a → b, Kubernetes sẽ cập nhật Pod template trong Deployment/ReplicaSet.

			Kể từ thời điểm đó, Kubernetes sẽ tạo các Pod mới dựa trên template mới, nghĩa là dùng image b.

			Container cũ với image a vẫn chạy cho đến khi bị thay thế (rolling update) hoặc gặp sự cố.
			
		kubectl get events --sort-by=.metadata.creationTimestamp
	
	
		1. Quản lý Cluster và Contexts

			Kiểm tra thông tin cluster:

				kubectl cluster-info


			Kiểm tra các context và cluster hiện tại:

				kubectl config current-context


			Liệt kê các context:

				kubectl config get-contexts


			Chuyển đổi context (nếu bạn có nhiều clusters):

			kubectl config use-context <context-name>

		2. Quản lý Pods

			Liệt kê các pods trong một namespace (mặc định là default):

				kubectl get pods


			Liệt kê pods trong một namespace cụ thể:

				kubectl get pods -n <namespace>


			Kiểm tra chi tiết của một pod:

				kubectl describe pod <pod-name>


			Xem log của một pod:

				kubectl logs <pod-name>


			Xem log của container trong pod:

				kubectl logs <pod-name> -c <container-name>


			Tạo pod từ file YAML:

				kubectl apply -f <file-name>.yaml


			Xóa pod:

				kubectl delete pod <pod-name>

		3. Quản lý Deployments

			Liệt kê tất cả các deployments:

				kubectl get deployments


			Quản lý replicas của deployment (scale up/down):

				kubectl scale deployment <deployment-name> --replicas=<number-of-replicas>


			Kiểm tra chi tiết deployment:

				kubectl describe deployment <deployment-name>


			Xóa deployment:

				kubectl delete deployment <deployment-name>

		4. Quản lý Services

			Liệt kê tất cả các services:

				kubectl get svc


			Kiểm tra chi tiết một service:

				kubectl describe svc <service-name>


			Xóa service:

				kubectl delete svc <service-name>


			Kiểm tra IP của service (external/internal IP):

				kubectl get svc <service-name>

		5. Quản lý ReplicaSets

			Liệt kê các ReplicaSets:

				kubectl get replicasets


			Quản lý ReplicaSet:

				kubectl scale replicasets <replicaset-name> --replicas=<number-of-replicas>

		6. Quản lý ConfigMaps và Secrets

			Liệt kê các ConfigMap:

				kubectl get configmaps


			Xem chi tiết một ConfigMap:

				kubectl describe configmap <configmap-name>


			Tạo ConfigMap từ file:

				kubectl create configmap <configmap-name> --from-file=<file-path>


			Liệt kê các Secrets:

				kubectl get secrets


			Xem chi tiết một Secret:

				kubectl describe secret <secret-name>


			Tạo Secret từ file:

				kubectl create secret generic <secret-name> --from-file=<file-path>

		7. Quản lý Namespaces

			Liệt kê tất cả các namespaces:

				kubectl get namespaces


			Tạo namespace mới:

				kubectl create namespace <namespace-name>


			Xóa namespace:

				kubectl delete namespace <namespace-name>

		8. Quản lý Volumes

			Liệt kê các PersistentVolumes (PV):

				kubectl get pv


			Liệt kê các PersistentVolumeClaims (PVC):

				kubectl get pvc

		9. Quản lý Ingress

			Liệt kê các Ingress:

				kubectl get ingress


			Tạo Ingress từ file YAML:

				kubectl apply -f ingress.yaml

		10. Quản lý StatefulSets

			Liệt kê tất cả StatefulSets:

				kubectl get statefulsets


			Kiểm tra chi tiết StatefulSet:

				kubectl describe statefulset <statefulset-name>

		11. Quản lý Jobs và CronJobs

			Liệt kê các jobs:

				kubectl get jobs


			Liệt kê các cronjobs:

				kubectl get cronjobs


			Tạo cronjob từ file YAML:

				kubectl apply -f cronjob.yaml

		12. Quản lý Cluster Roles và RoleBindings

			Liệt kê ClusterRoles:

				kubectl get clusterroles


			Liệt kê RoleBindings:

				kubectl get rolebindings

		13. Khám phá Cluster

			Khám phá các node trong cluster:

				kubectl get nodes


			Thông tin chi tiết về một node:

				kubectl describe node <node-name>


			Kiểm tra tài nguyên của cluster (CPU, Memory):

				kubectl top nodes


			Kiểm tra tài nguyên của pods:

				kubectl top pods

		14. Khác

			Tạo một pod tạm thời (nginx ví dụ):

				kubectl run nginx --image=nginx --restart=Never


			Khởi động một pod trong chế độ interactive:

				kubectl run -i --tty --image=nginx nginx-shell --restart=Never -- bash


			Sao lưu cấu hình từ Kubernetes (cluster) ra file:

				kubectl get all --all-namespaces -o yaml > backup.yaml

			Lệnh Debug và Troubleshooting

				Khởi động một pod debug:

			kubectl run debug --rm -i --tty --image=busybox --restart=Never -- sh


			Tạo một pod mới để kiểm tra log:

				kubectl run <pod-name> --image=<image-name> --restart=Never --command -- <command>

services:
  accounts:
services: Phần này định nghĩa các dịch vụ mà Docker Compose sẽ quản lý. Mỗi dịch vụ sẽ chạy trong một container riêng biệt.
accounts: Tên của dịch vụ này. Bạn có thể gọi nó bất kỳ tên gì, nhưng trong trường hợp này, nó được gọi là accounts.

image: "eazybytes/accounts:s4"
image: Chỉ định hình ảnh Docker mà dịch vụ sẽ sử dụng. Ở đây, dịch vụ sẽ sử dụng hình ảnh eazybytes/accounts với thẻ s4.

container_name: accounts-ms
container_name: Đặt tên cụ thể cho container khi nó chạy. Trong trường hợp này, container sẽ được gọi là accounts-ms.

ports:
    - "8080:8080"
ports: Chỉ định các cổng mà container sẽ sử dụng. Ở đây, cổng 8080 trên máy chủ sẽ được ánh xạ tới cổng 8080 trong container, cho phép truy cập vào dịch vụ từ bên ngoài.

deploy:
  resources:
    limits:
      memory: 700m
deploy: Cung cấp cấu hình cho việc triển khai dịch vụ, thường dùng trong môi trường Docker Swarm.
resources: Định nghĩa các tài nguyên mà dịch vụ có thể sử dụng.
limits: Chỉ định giới hạn tài nguyên. Ở đây, dịch vụ accounts bị giới hạn sử dụng tối đa 700 MB bộ nhớ.

networks:
  - eazybank
networks: Chỉ định các mạng mà dịch vụ sẽ kết nối đến. Trong trường hợp này, dịch vụ sẽ được kết nối với mạng eazybank, cho phép nó giao tiếp với các dịch vụ khác trong mạng đó.
Tóm lại:
Đoạn mã này định nghĩa một dịch vụ tên là accounts, sử dụng hình ảnh Docker đã được chỉ định, thiết lập ánh xạ cổng, giới hạn tài nguyên bộ nhớ, và kết nối với mạng eazybank.

Mục đích của Dockerfile
	Xây dựng Hình ảnh Docker: Dockerfile chứa các chỉ thị cần thiết để tạo ra một hình ảnh Docker. Nó cho phép bạn định nghĩa môi trường mà ứng dụng của bạn sẽ chạy.
	Tự động hóa Quá trình Cài đặt: Các lệnh trong Dockerfile giúp tự động hóa việc cài đặt các phần mềm cần thiết, cấu hình môi trường, và sao chép mã nguồn vào hình ảnh.
	Tái sử dụng và Chia sẻ: Hình ảnh Docker được tạo ra từ Dockerfile có thể được lưu trữ và chia sẻ qua các registry như Docker Hub, giúp dễ dàng triển khai trên nhiều môi trường khác nhau.
	Quản lý Phiên bản: Bạn có thể theo dõi và quản lý các phiên bản của hình ảnh Docker bằng cách sử dụng các thẻ (tags) trong Dockerfile.
Mục đích của docker-compose.yml
	Quản lý Nhiều Dịch vụ: Docker Compose cho phép bạn định nghĩa và quản lý nhiều dịch vụ trong cùng một tệp cấu hình, giúp dễ dàng triển khai các ứng dụng phức tạp.
	Cấu hình Mạng và Volume: Bạn có thể thiết lập mạng và volume chia sẻ giữa các dịch vụ, giúp các container giao tiếp và lưu trữ dữ liệu một cách hiệu quả.
	Dễ dàng Khởi động và Dừng: Với một lệnh đơn giản (docker-compose up hoặc docker-compose down), bạn có thể khởi động hoặc dừng toàn bộ stack ứng dụng của mình.
	Tự động hóa Triển khai: Docker Compose giúp tự động hóa quá trình triển khai ứng dụng với các thông số đã được định nghĩa, giảm thiểu lỗi do thao tác thủ công.
Tóm lại
Dockerfile: Tạo ra hình ảnh Docker cho ứng dụng.
docker-compose.yml: Quản lý và triển khai các dịch vụ, mạng, và volume cho ứng dụng.

Có, giữa Dockerfile và docker-compose.yml có mối liên hệ chặt chẽ với nhau trong quá trình phát triển và triển khai ứng dụng. Dưới đây là các điểm liên quan:
1. Xây dựng Hình ảnh
Dockerfile: Chứa các chỉ thị để xây dựng hình ảnh Docker cho ứng dụng.
docker-compose.yml: Tham chiếu đến hình ảnh được tạo ra từ Dockerfile thông qua thuộc tính image. Nếu bạn không chỉ định hình ảnh đã có, Docker Compose sẽ tự động xây dựng hình ảnh từ Dockerfile khi chạy lệnh docker-compose up.
2. Triển khai Dịch vụ
Dockerfile: Định nghĩa môi trường cần thiết để ứng dụng chạy, như các thư viện và tệp cần thiết.
docker-compose.yml: Sử dụng hình ảnh được xây dựng từ Dockerfile để tạo và quản lý các dịch vụ. Nó cho phép bạn cấu hình các dịch vụ này với các thông số như cổng, mạng, và tài nguyên.
3. Quản lý Tính Tương Thích
Thông qua Dockerfile, bạn có thể đảm bảo rằng môi trường chạy ứng dụng tương thích với yêu cầu của nó. docker-compose.yml giúp bạn triển khai nhiều dịch vụ trong môi trường đó một cách dễ dàng.
4. Môi Trường Phát Triển và Sản Xuất
Cả hai tệp này có thể được sử dụng để tạo ra môi trường phát triển và sản xuất nhất quán. Dockerfile đảm bảo rằng hình ảnh của bạn luôn giống nhau, trong khi docker-compose.yml quản lý cách các dịch vụ tương tác.
Tóm lại
Dockerfile và docker-compose.yml bổ sung cho nhau: Dockerfile tạo ra hình ảnh ứng dụng, trong khi docker-compose.yml quản lý việc triển khai và cấu hình các dịch vụ dựa trên hình ảnh đó.



Khi sử dụng Docker, quy trình thực hiện thường diễn ra theo các bước sau:
1. Xây dựng Hình ảnh từ Dockerfile
Bước 1: Khi bạn muốn tạo một hình ảnh Docker, bạn sẽ sử dụng lệnh docker build để xây dựng hình ảnh từ tệp Dockerfile.
Bước 2: Docker sẽ thực hiện từng chỉ thị trong Dockerfile theo thứ tự từ trên xuống dưới, tạo ra một hình ảnh hoàn chỉnh cho ứng dụng của bạn.
2. Triển khai Dịch vụ với docker-compose.yml
Bước 3: Sau khi hình ảnh đã được xây dựng, bạn sẽ sử dụng lệnh docker-compose up để khởi động các dịch vụ được định nghĩa trong tệp docker-compose.yml.
Bước 4: Docker Compose sẽ kiểm tra xem hình ảnh đã được xây dựng hay chưa. Nếu chưa, nó sẽ tự động xây dựng hình ảnh từ Dockerfile (nếu có tham chiếu đến hình ảnh này trong docker-compose.yml).
Bước 5: Docker Compose sẽ tạo và khởi động các container dựa trên hình ảnh đã được xây dựng và các cấu hình được chỉ định trong docker-compose.yml.
Flow tổng quan
Viết Dockerfile: Định nghĩa cách xây dựng hình ảnh.
Chạy lệnh docker build: Tạo hình ảnh từ Dockerfile.
Viết docker-compose.yml: Định nghĩa cấu hình cho các dịch vụ.
Chạy lệnh docker-compose up: Khởi động toàn bộ stack ứng dụng.
Docker Compose: Kiểm tra hình ảnh, xây dựng nếu cần, và khởi động các container.
Tóm lại
Dockerfile: Chạy trước để xây dựng hình ảnh.
docker-compose.yml: Chạy sau để triển khai các dịch vụ từ hình ảnh đã xây dựng.



spring:
spring: Đây là phần cấu hình chính cho ứng dụng Spring Boot. Tất cả các cấu hình thuộc về Spring đều nằm trong khối này.

config:
    import:
      - "application_qa.yml"
      - "application_prod.yml"
config: Chỉ định các cài đặt liên quan đến cấu hình Spring.
import: Cho phép bạn nhập các tệp cấu hình khác vào tệp application.yml chính.
application_qa.yml: Tệp cấu hình cho môi trường Quality Assurance (QA).
application_prod.yml: Tệp cấu hình cho môi trường Production (Prod).
Việc nhập các tệp này cho phép bạn tổ chức và quản lý cấu hình cho các môi trường khác nhau một cách hiệu quả.

profiles:
    active:
      - "qa"
profiles: Chỉ định các profile (hồ sơ) mà ứng dụng sẽ sử dụng.
active: Xác định các profile đang hoạt động.
"qa": Chỉ định rằng profile QA sẽ được kích hoạt. Điều này có nghĩa là ứng dụng sẽ sử dụng các cấu hình từ tệp application_qa.yml và có thể áp dụng các cấu hình từ tệp application_prod.yml nếu cần.
Tóm lại
Tệp application.yml này cấu hình cho ứng dụng Spring Boot để nhập các tệp cấu hình cho các môi trường khác nhau (QA và Prod) và kích hoạt profile QA, giúp ứng dụng có thể dễ dàng cấu hình và quản lý
cho các môi trường khác nhau.

Nghia la o phia tren thi noi dung cua "application_qa.yml" hay "application_prod.yml" se duoc import vao application.yml
Đúng vậy! Nội dung của các tệp application_qa.yml và application_prod.yml sẽ được nhập vào tệp application.yml.
Giải thích cụ thể:
Khi bạn chỉ định import trong phần cấu hình spring.config, Spring Boot sẽ tìm và nạp các tệp được chỉ định (application_qa.yml và application_prod.yml) vào cấu hình chính.
Nội dung từ các tệp này sẽ được kết hợp với cấu hình trong application.yml, giúp bạn dễ dàng quản lý các cấu hình cho các môi trường khác nhau (QA và Prod).
Với profile được kích hoạt là qa, ứng dụng sẽ ưu tiên sử dụng các cấu hình từ application_qa.yml, nhưng vẫn có thể sử dụng các cấu hình từ application_prod.yml nếu chúng không bị ghi đè.
Tóm lại
Nội dung của application_qa.yml và application_prod.yml sẽ được nạp vào application.yml, cho phép cấu hình linh hoạt cho ứng dụng dựa trên môi trường đang hoạt động.



spring:
spring: Đây là phần cấu hình chính cho ứng dụng Spring Boot, nơi bạn định nghĩa các thông số cấu hình cho ứng dụng của mình.


config:
    activate:
      on-profile: "prod"
config: Chỉ định các cài đặt liên quan đến cấu hình Spring.
activate: Xác định điều kiện để kích hoạt các cấu hình trong tệp này.
on-profile: Chỉ định rằng các cấu hình trong tệp này sẽ chỉ được kích hoạt khi profile prod đang hoạt động.
"prod": Định nghĩa rằng đây là cấu hình dành riêng cho môi trường sản xuất (Production).
Tóm lại
Tệp application_prod.yml cấu hình để kích hoạt các cài đặt chỉ khi profile prod đang được sử dụng. Điều này giúp bạn quản lý và phân tách các cấu hình cho môi trường sản xuất một cách rõ
ràng và hiệu quả. Các cài đặt trong tệp này sẽ chỉ có hiệu lực khi ứng dụng chạy với profile prod.



spring:
spring: Đây là phần cấu hình chính cho ứng dụng Spring Boot, nơi bạn định nghĩa các tham số cấu hình cho ứng dụng của mình.

application:
    name: "accounts"
application: Phần này dùng để cấu hình các thông tin liên quan đến ứng dụng.
name: Đặt tên cho ứng dụng. Trong trường hợp này, ứng dụng được đặt tên là "accounts".

config:
    import: "optional:configserver:http://localhost:8071/"
config: Chỉ định các cài đặt liên quan đến cấu hình Spring.
import: Cho phép bạn nhập cấu hình từ một nguồn bên ngoài.
"optional:configserver:http://localhost:8071/": Đây là cú pháp để chỉ định rằng cấu hình sẽ được lấy từ một Spring Cloud Config Server chạy tại địa chỉ http://localhost:8071/.
optional: Từ khóa này cho biết rằng việc nhập cấu hình từ Config Server là không bắt buộc. Nếu Config Server không khả dụng, ứng dụng vẫn có thể khởi động mà không gặp lỗi.
Tóm lại
Tệp application.yml này cấu hình cho ứng dụng Spring Boot với tên là "accounts" và chỉ định rằng ứng dụng sẽ cố gắng nhập cấu hình từ một Spring Cloud Config Server
tại http://localhost:8071/. Việc nhập này là tùy chọn, cho phép ứng dụng hoạt động ngay cả khi Config Server không khả dụng.



management:
management: Đây là phần cấu hình liên quan đến quản lý trong ứng dụng Spring Boot, thường được sử dụng để cấu hình các endpoints quản lý.

endpoints:
    web:
      exposure:
        include: "*"
endpoints: Chỉ định các điểm cuối (endpoints) quản lý mà ứng dụng sẽ cung cấp.
web: Phần này cụ thể cho các endpoints quản lý có thể truy cập qua HTTP.
exposure: Xác định cách mà các endpoints này được hiển thị hoặc truy cập.
include: Chỉ định các endpoints nào sẽ được bao gồm trong quản lý.
"*": Dấu hoa thị này có nghĩa là tất cả các endpoints sẽ được bao gồm và có thể truy cập từ web. Điều này cho phép bất kỳ endpoint quản lý nào có sẵn trong ứng dụng đều có thể được truy cập thông qua HTTP.
Tóm lại
Cấu hình này cho phép tất cả các endpoints quản lý trong ứng dụng Spring Boot được hiển thị và truy cập thông qua giao thức web. Điều này rất hữu ích cho việc giám sát và quản lý ứng dụng, nhưng
cũng cần được sử dụng cẩn thận để đảm bảo an ninh, vì việc mở rộng tất cả các endpoints có thể dẫn đến rủi ro bảo mật.



spring:
spring: Đây là phần cấu hình chính cho ứng dụng Spring Boot.

application:
    name: "configserver"
application: Phần này dùng để cấu hình các thông tin liên quan đến ứng dụng.
name: Đặt tên cho ứng dụng là "configserver".


profiles:
    active: git
profiles: Chỉ định các profile mà ứng dụng sẽ sử dụng.
active: Đặt profile đang hoạt động là git. Profile này có thể được sử dụng để nạp các cấu hình từ nguồn Git.


cloud:
    config:
      server:
        git:
          uri: "https://github.com/eazybytes/eazybytes-config.git"
cloud: Cấu hình cho Spring Cloud.
config: Phần này liên quan đến cấu hình của Spring Cloud Config Server.
server: Chỉ định rằng ứng dụng này sẽ hoạt động như một Config Server.
git: Cấu hình cho việc lấy cấu hình từ một kho Git.
uri: Địa chỉ của kho Git chứa các tệp cấu hình, trong trường hợp này là https://github.com/eazybytes/eazybytes-config.git.


default-label: main
default-label: Chỉ định nhánh mặc định mà Config Server sẽ sử dụng khi truy cập kho Git, ở đây là nhánh main.

timeout: 5
timeout: Thời gian chờ (timeout) tối đa (tính bằng giây) khi cố gắng kết nối đến kho Git.


clone-on-start: true
clone-on-start: Nếu được đặt là true, Config Server sẽ sao chép kho Git vào bộ nhớ trong khi khởi động.

force-pull: true
force-pull: Nếu được đặt là true, server sẽ luôn kéo (pull) các thay đổi mới từ kho Git mỗi khi có yêu cầu, bất kể dữ liệu đã có trong bộ nhớ hay chưa.
Tóm lại
Cấu hình này thiết lập một Spring Cloud Config Server với tên là configserver, sử dụng kho Git https://github.com/eazybytes/eazybytes-config.git để lấy các tệp
cấu hình. Server sẽ sao chép kho Git khi khởi động, thiết lập nhánh mặc định là main, và có thời gian chờ là 5 giây cho các kết nối.





spring:
spring: Đây là phần cấu hình chính cho ứng dụng Spring Boot.

application:
    name: "cards"
application: Phần này dùng để cấu hình các thông tin liên quan đến ứng dụng.
name: Đặt tên cho ứng dụng là "cards".

profiles:
    active: "prod"
profiles: Chỉ định các profile mà ứng dụng sẽ sử dụng.
active: Đặt profile đang hoạt động là "prod". Điều này chỉ ra rằng ứng dụng sẽ chạy trong môi trường sản xuất.


config:
    import: "optional:configserver:http://localhost:8071/"
config: Phần này liên quan đến cấu hình Spring.
import: Cho phép bạn nhập cấu hình từ một nguồn bên ngoài.
"optional:configserver:http://localhost:8071/": Chỉ định rằng cấu hình sẽ được lấy từ một Spring Cloud Config Server chạy tại địa chỉ http://localhost:8071/.
optional: Từ khóa này có nghĩa là việc nhập cấu hình từ Config Server không bắt buộc. Nếu Config Server không khả dụng, ứng dụng vẫn có thể khởi động mà không gặp lỗi.
Tóm lại
Cấu hình này thiết lập một ứng dụng Spring Boot có tên là "cards" và chỉ định rằng ứng dụng sẽ chạy với profile prod. Nó cũng cấu hình để cố gắng nhập các tệp
cấu hình từ một Spring Cloud Config Server tại http://localhost:8071/, nhưng thao tác này là tùy chọn, cho phép ứng dụng hoạt động ngay cả khi Config Server không khả dụng.


services:
  rabbit:
    extends:
      file: common-config.yml
      service: network-deploy-service
services: Đây là phần cấu hình dành cho các dịch vụ trong ứng dụng. Trong ngữ cảnh này, có thể bạn đang sử dụng Docker Compose hoặc một công cụ tương tự để định nghĩa các dịch vụ.

rabbit: Đây là tên của dịch vụ mà bạn đang định nghĩa. Dịch vụ này có thể liên quan đến RabbitMQ (một hệ thống nhắn tin), nhưng tên cụ thể có thể thay đổi tùy thuộc vào ngữ cảnh của ứng dụng.

extends: Từ khóa này được sử dụng để chỉ định rằng dịch vụ rabbit sẽ mở rộng từ một dịch vụ hoặc cấu hình đã có sẵn.

file: Đây là tham số chỉ ra tệp cấu hình mà dịch vụ sẽ mở rộng từ đó. Trong trường hợp này, tệp là common-config.yml.

service: Đây là tên của dịch vụ trong tệp cấu hình được chỉ định mà bạn đang mở rộng. Ở đây, dịch vụ là network-deploy-service.

Tóm lại
Đoạn mã này định nghĩa một dịch vụ có tên là rabbit, mở rộng từ cấu hình của dịch vụ network-deploy-service trong tệp common-config.yml. Việc sử dụng extends giúp tái sử dụng cấu hình
và giảm thiểu sự lặp lại trong cấu hình dịch vụ.


services:
  configserver:
    image: "eazybytes/configserver:s6"
    container_name: configserver-ms
    ports:
      - "8071:8071"
    depends_on:
      rabbit:
        condition: service_healthy
services: Đây là phần cấu hình chính trong Docker Compose, nơi bạn định nghĩa các dịch vụ mà ứng dụng của bạn sẽ sử dụng.

configserver: Đây là tên của dịch vụ mà bạn đang định nghĩa. Trong trường hợp này, dịch vụ này có thể liên quan đến một server cấu hình (config server) trong kiến trúc microservices.

image:

eazybytes/configserver:s6: Đây là tên và tag của hình ảnh Docker mà dịch vụ này sẽ sử dụng. Nó cho biết rằng sẽ lấy hình ảnh từ kho lưu trữ Docker với tên eazybytes/configserver và tag là s6.
container_name:

configserver-ms: Đây là tên cụ thể mà container sẽ được gán khi chạy. Điều này hữu ích để dễ dàng quản lý và xác định container trong Docker.
ports:

- "8071:8071": Dòng này chỉ định ánh xạ cổng giữa máy chủ và container. Cổng 8071 của máy chủ sẽ được ánh xạ đến cổng 8071 của container. Điều này cho phép bạn truy cập dịch vụ từ máy chủ thông qua cổng 8071.
depends_on:

rabbit: Dịch vụ này phụ thuộc vào một dịch vụ khác có tên là rabbit.
condition: service_healthy: Điều này chỉ định rằng dịch vụ configserver sẽ chỉ được khởi động khi dịch vụ rabbit đã sẵn sàng và ở trạng thái khỏe mạnh (healthy). Điều kiện này thường được
sử dụng để đảm bảo rằng các dịch vụ phụ thuộc đã sẵn sàng trước khi dịch vụ chính bắt đầu.
Tóm lại
Đoạn mã này định nghĩa một dịch vụ configserver trong Docker Compose, với một hình ảnh cụ thể, ánh xạ cổng, tên container cụ thể và điều kiện phụ thuộc vào dịch vụ rabbit. Điều này giúp
đảm bảo rằng tất cả các dịch vụ cần thiết đều hoạt động đúng cách khi khởi động ứng dụng.



configserver:
  healthcheck:
    test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
    interval: 10s
    timeout: 5s
    retries: 10
    start_period: 10s
configserver: Đây là dịch vụ mà bạn đang định nghĩa healthcheck cho nó.

healthcheck: Phần này định nghĩa cách Docker sẽ kiểm tra tình trạng sức khỏe của dịch vụ configserver.

test: Đây là lệnh mà Docker sẽ thực thi để kiểm tra sức khỏe của dịch vụ.

curl --fail --silent localhost:8071/actuator/health/readiness: Lệnh này sử dụng curl để gửi yêu cầu đến endpoint /actuator/health/readiness của
dịch vụ, kiểm tra xem dịch vụ có sẵn sàng hay không.
| grep UP: Kết quả của lệnh curl sẽ được kiểm tra xem có chứa từ "UP" hay không, điều này cho thấy dịch vụ đang hoạt động và sẵn sàng.
|| exit 1: Nếu lệnh grep không tìm thấy "UP", lệnh sẽ trả về mã lỗi 1, cho biết rằng dịch vụ không khỏe mạnh.
interval:

10s: Đây là khoảng thời gian giữa các lần kiểm tra sức khỏe. Trong trường hợp này, Docker sẽ kiểm tra sức khỏe của dịch vụ sau mỗi 10 giây.
timeout:

5s: Đây là thời gian tối đa mà lệnh kiểm tra sức khỏe được phép chạy trước khi bị coi là không thành công. Nếu lệnh không hoàn thành trong 5
giây, nó sẽ được coi là lỗi.
retries:

10: Đây là số lần mà Docker sẽ thử lại kiểm tra sức khỏe trước khi coi dịch vụ là không khỏe mạnh. Trong trường hợp này, Docker sẽ thử lại 10 lần.
start_period:

10s: Đây là khoảng thời gian mà Docker sẽ chờ sau khi khởi động dịch vụ trước khi bắt đầu thực hiện các kiểm tra sức khỏe. Điều này cho phép dịch
vụ có thời gian để khởi động hoàn toàn trước khi bị kiểm tra.
Tóm lại
Đoạn mã này định nghĩa một kiểm tra sức khỏe cho dịch vụ configserver, sử dụng lệnh curl để kiểm tra trạng thái của
endpoint /actuator/health/readiness. Các tham số cấu hình cung cấp thông tin về tần suất kiểm tra, thời gian chờ, số
lần thử lại và thời gian chờ khởi động, giúp đảm bảo rằng dịch vụ chỉ được coi là khỏe mạnh khi nó thực sự sẵn sàng hoạt động.


environment:
  SPRING_RABBITMQ_HOST: "rabbit"
  SPRING_PROFILES_ACTIVE: default
  SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/
environment: Phần này định nghĩa các biến môi trường mà dịch vụ sẽ sử dụng khi chạy. Các biến này sẽ được truyền vào container của dịch vụ.

SPRING_RABBITMQ_HOST:

SPRING_RABBITMQ_HOST: "rabbit": Biến môi trường này chỉ định địa chỉ của RabbitMQ mà ứng dụng sẽ kết nối. Giá trị "rabbit" thường là
tên của dịch vụ RabbitMQ được định nghĩa trong cùng một Docker Compose file. Điều này cho phép ứng dụng dễ dàng tìm thấy và kết
nối đến RabbitMQ khi nó khởi động.
SPRING_PROFILES_ACTIVE:

SPRING_PROFILES_ACTIVE: default: Biến môi trường này chỉ định profile Spring mà ứng dụng sẽ sử dụng. Trong trường hợp này, profile
là default. Điều này có nghĩa là ứng dụng sẽ hoạt động với các cấu hình và tính năng được định nghĩa trong profile default, giúp quản lý các cấu hình khác nhau cho các môi trường khác nhau (như phát triển, kiểm thử, sản xuất).
SPRING_CONFIG_IMPORT:

SPRING_CONFIG_IMPORT: configserver:http://configserver:8071/: Biến môi trường này chỉ định rằng ứng dụng sẽ nhập cấu hình từ một
server cấu hình (config server) tại địa chỉ http://configserver:8071/. Điều này cho phép ứng dụng tải các cấu hình từ server bên
ngoài, giúp quản lý cấu hình một cách tập trung và linh hoạt hơn.
Tóm lại
Đoạn mã này thiết lập các biến môi trường cho dịch vụ, bao gồm địa chỉ của RabbitMQ, profile Spring đang sử dụng và nguồn cấu hình
từ một config server. Điều này giúp ứng dụng hoạt động hiệu quả và kết nối với các dịch vụ cần thiết khi chạy trong container.


Nếu bạn có cấu hình spring.application.name = "cards" trong file application.yml, thì bạn có thể sử dụng @FeignClient("cards") để gọi dịch vụ này.



eureka:
  instance:
    hostname: localhost
  client:
    fetchRegistry: false
    registerWithEureka: false
    serviceUrl:
      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/
eureka: Đây là phần cấu hình liên quan đến Eureka, một dịch vụ discovery server trong kiến trúc microservices.

instance:

hostname: localhost: Đây là tên máy chủ mà ứng dụng sẽ sử dụng. Trong trường hợp này, máy chủ được đặt là localhost, nghĩa
là ứng dụng sẽ chạy trên máy cục bộ.
client:

Phần này định nghĩa các cấu hình cho client Eureka.

fetchRegistry: false: Khi cấu hình này được đặt là false, ứng dụng sẽ không tải danh sách các dịch vụ đã đăng ký từ Eureka.
Điều này có nghĩa là client sẽ không nhận thông tin về các dịch vụ khác đã đăng ký.

registerWithEureka: false: Khi cấu hình này được đặt là false, ứng dụng sẽ không đăng ký với Eureka. Điều này có nghĩa là dịch
vụ này sẽ không được công nhận là một dịch vụ trong hệ thống discovery, và các dịch vụ khác sẽ không thể tìm thấy nó.

serviceUrl:

defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/: Đây là URL mà client sẽ sử dụng để kết nối với Eureka server.
${eureka.instance.hostname} sẽ được thay thế bằng giá trị của hostname (trong trường hợp này là localhost).
${server.port} sẽ được thay thế bằng cổng mà ứng dụng đang chạy. Nếu bạn không định nghĩa cổng, mặc định sẽ là 8080.
Kết quả cuối cùng sẽ là http://localhost:<server.port>/eureka/, nơi <server.port> là cổng mà ứng dụng đang sử dụng.
Tóm lại
Đoạn mã này cấu hình một ứng dụng Spring để kết nối với một server Eureka, nhưng ứng dụng này không đăng ký với Eureka và cũng
không tải danh sách các dịch vụ từ đó. Điều này thường được sử dụng trong các trường hợp thử nghiệm hoặc khi bạn không muốn ứng
dụng tham gia vào hệ thống discovery.



eureka:
  instance:
    preferIpAddress: true
  client:
    fetchRegistry: true
    registerWithEureka: true
    serviceUrl:
      defaultZone: http://localhost:8070/eureka/
eureka: Đây là phần cấu hình liên quan đến Eureka, một service discovery server trong kiến trúc microservices.

instance:

preferIpAddress: true: Khi cấu hình này được đặt là true, ứng dụng sẽ sử dụng địa chỉ IP của nó thay vì hostname khi đăng
ký với Eureka. Điều này có thể hữu ích trong một số trường hợp, chẳng hạn như khi hostname không thể được giải quyết từ các dịch vụ khác.
client:

Phần này định nghĩa các cấu hình cho client Eureka.

fetchRegistry: true: Khi cấu hình này được đặt là true, ứng dụng sẽ tải danh sách các dịch vụ đã đăng ký từ Eureka. Điều này
cho phép ứng dụng biết về các dịch vụ khác trong hệ thống.

registerWithEureka: true: Khi cấu hình này được đặt là true, ứng dụng sẽ đăng ký với Eureka. Điều này cho phép các dịch vụ khác
tìm thấy ứng dụng này thông qua Eureka.

serviceUrl:

defaultZone: http://localhost:8070/eureka/: Đây là URL mà client sẽ sử dụng để kết nối với Eureka server.
Trong trường hợp này, server Eureka đang chạy trên localhost và cổng 8070.
URL này sẽ được sử dụng để đăng ký dịch vụ và tải danh sách các dịch vụ khác.
Tóm lại
Đoạn mã này cấu hình một ứng dụng Spring để kết nối với một server Eureka, cho phép ứng dụng đăng ký với Eureka và tải danh
sách các dịch vụ đã đăng ký. Việc sử dụng địa chỉ IP thay vì hostname có thể giúp tránh những vấn đề liên quan đến việc phân
giải tên miền trong một số môi trường.



@Bean
public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
    return routeLocatorBuilder.routes()
        .route(p -> p
            .path("/eazybank/accounts/**")
            .filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
                .addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
            .uri("lb://ACCOUNTS"))
        .route(p -> p
            .path("/eazybank/loans/**")
            .filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
                .addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
            .uri("lb://LOANS"))
        .route(p -> p
            .path("/eazybank/cards/**")
            .filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
                .addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
            .uri("lb://CARDS"))
        .build();
}
@Bean:

Annotation này cho biết phương thức sẽ trả về một bean Spring, cho phép Spring quản lý vòng đời của nó.
RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder):

Phương thức này định nghĩa các tuyến đường (routes) cho một API Gateway (hoặc Spring Cloud Gateway).
routeLocatorBuilder.routes():

Bắt đầu quá trình định nghĩa các tuyến đường. RouteLocatorBuilder được sử dụng để tạo ra các route.
.route(p -> p.path("/eazybank/accounts/")...)**:

Định nghĩa một route cho các yêu cầu có đường dẫn bắt đầu bằng /eazybank/accounts/.
Các route tiếp theo cho các yêu cầu đến /eazybank/loans/** và /eazybank/cards/** được định nghĩa tương tự.
filters(...):

Định nghĩa các bộ lọc cho route. Các bộ lọc này sẽ được áp dụng cho các yêu cầu khớp với route.

rewritePath(...):

Thay đổi đường dẫn của yêu cầu.
Cụ thể, khi yêu cầu có đường dẫn /eazybank/accounts/something, nó sẽ được chuyển thành /something.
(?<segment>.*) là một nhóm regex để lấy phần còn lại của đường dẫn.
addResponseHeader(...):

Thêm một header vào phản hồi, trong trường hợp này là X-Response-Time, chứa thời gian hiện tại.
uri("lb://ACCOUNTS"):

Chỉ định địa chỉ của dịch vụ mà yêu cầu sẽ được chuyển tiếp đến.
lb:// cho biết đây là một dịch vụ được quản lý bởi load balancer. Tên dịch vụ là ACCOUNTS, LOANS, và CARDS tương ứng với các route.
.build():

Kết thúc việc định nghĩa các route và trả về một RouteLocator chứa tất cả các route đã được định nghĩa.

Tóm lại
Đoạn mã này định nghĩa các route cho một API Gateway, cho phép chuyển tiếp các yêu cầu đến các dịch vụ khác
nhau (ACCOUNTS, LOANS, CARDS) với việc thay đổi đường dẫn và thêm header vào phản hồi. Điều này giúp quản lý và
điều hướng lưu lượng truy cập đến các dịch vụ microservices một cách hiệu quả.





Câu lệnh .path("/eazybank/accounts/**") trong đoạn mã có ý nghĩa như sau:

Giải thích:
Path Matching:

Phần này định nghĩa một mẫu đường dẫn (path pattern) mà route sẽ kiểm tra.
Mẫu "/eazybank/accounts/**" có nghĩa là bất kỳ yêu cầu nào có đường dẫn bắt đầu bằng /eazybank/accounts/ sẽ khớp với route này.
Wildcard **:

Ký hiệu ** là một wildcard trong Spring Cloud Gateway.
Nó cho phép khớp với bất kỳ phần nào của đường dẫn phía sau /eazybank/accounts/, bao gồm cả các đường dẫn con.
Ví dụ: các đường dẫn như /eazybank/accounts/123, /eazybank/accounts/transactions, hoặc /eazybank/accounts/123/details đều sẽ khớp với route này.
Tóm lại
.path("/eazybank/accounts/**") chỉ định rằng route này sẽ xử lý tất cả các yêu cầu có đường dẫn bắt đầu
bằng /eazybank/accounts/, giúp định hướng lưu lượng đến dịch vụ tương ứng (trong trường hợp này là dịch vụ "ACCOUNTS").










Phần .filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}") trong đoạn mã có ý nghĩa như sau:

Giải thích:
rewritePath:

Đây là một bộ lọc (filter) được sử dụng để thay đổi đường dẫn của yêu cầu trước khi nó được chuyển tiếp đến dịch vụ đích.
Mẫu Đường Dẫn:

"/eazybank/accounts/(?<segment>.*)": Đây là mẫu đường dẫn sử dụng regex.
(?<segment>.*) là một nhóm (group) regex với tên là segment.
.* có nghĩa là nó sẽ khớp với bất kỳ ký tự nào (bao gồm cả các ký tự đặc biệt), và sẽ lấy tất cả các ký tự ở phía sau /eazybank/accounts/.
Thay Thế Đường Dẫn:

"/${segment}": Đây là đường dẫn mới mà yêu cầu sẽ được chuyển đến.
${segment} sẽ được thay thế bằng giá trị mà nhóm segment đã khớp.
Điều này có nghĩa là phần còn lại của đường dẫn sau /eazybank/accounts/ sẽ được giữ nguyên.
Ví dụ:
Nếu yêu cầu đến với đường dẫn /eazybank/accounts/123, thì sau khi áp dụng rewritePath, đường dẫn sẽ trở thành /123.
Nếu yêu cầu đến với đường dẫn /eazybank/accounts/transactions, thì đường dẫn sẽ trở thành /transactions.
Tóm lại:
Bộ lọc này cho phép bạn thay đổi đường dẫn của yêu cầu bằng cách loại bỏ phần /eazybank/accounts/ và giữ lại phần còn
lại, giúp điều hướng yêu cầu đến dịch vụ đúng với định dạng mà dịch vụ đó mong đợi.





Câu lệnh .uri("lb://LOANS") trong đoạn mã có ý nghĩa như sau:

Giải thích:
URI:

uri là phương thức được sử dụng để chỉ định địa chỉ mà yêu cầu sẽ được chuyển tiếp đến.
lb://:

Tiền tố lb:// chỉ định rằng đây là một dịch vụ được quản lý bởi một load balancer (cân bằng tải).
Điều này có nghĩa là khi client gửi yêu cầu đến route này, API Gateway sẽ tìm kiếm dịch vụ có tên là LOANS trong
danh sách các dịch vụ đã đăng ký với service discovery (như Eureka).
"LOANS":

Đây là tên dịch vụ mà yêu cầu sẽ được chuyển đến.
Khi có yêu cầu đến route này, API Gateway sẽ sử dụng load balancer để tìm và chuyển tiếp yêu cầu đến một trong các instance của dịch vụ LOANS.
Tóm lại:
.uri("lb://LOANS") chỉ định rằng yêu cầu sẽ được chuyển tiếp đến dịch vụ có tên LOANS, và việc chuyển tiếp này sẽ được
thực hiện thông qua một load balancer, cho phép phân phối tải giữa các instance của dịch vụ đó.







Dưới đây là giải thích cho đoạn mã .circuitBreaker(config -> config.setName("accountsCircuitBreaker").setFallbackUri("forward:/contactSupport")):

Giải thích:
circuitBreaker:

Đây là một phương thức được sử dụng để thêm một "circuit breaker" vào route. Circuit breaker là một mẫu thiết kế giúp
ngăn chặn việc gửi yêu cầu đến một dịch vụ không hoạt động hoặc bị quá tải, nhằm bảo vệ hệ thống và cải thiện độ tin cậy.
config -> ...:

Đây là một biểu thức lambda, cho phép bạn cấu hình các thuộc tính của circuit breaker.
setName("accountsCircuitBreaker"):

Phương thức này đặt tên cho circuit breaker.
Tên này có thể được sử dụng để theo dõi và giám sát circuit breaker trong hệ thống.
setFallbackUri("forward:/contactSupport"):

Phương thức này chỉ định một URI fallback (dự phòng) mà ứng dụng sẽ chuyển hướng đến khi circuit breaker đang mở (tức
là khi dịch vụ không phản hồi hoặc gặp lỗi).
Trong trường hợp này, nếu dịch vụ mà circuit breaker bảo vệ gặp sự cố, yêu cầu sẽ được chuyển hướng đến đường dẫn /contactSupport.
Việc này giúp cung cấp trải nghiệm người dùng tốt hơn bằng cách hướng dẫn họ đến trang hỗ trợ thay vì nhận được một lỗi không rõ ràng.
Tóm lại:
Đoạn mã này cấu hình một circuit breaker cho route, với tên là accountsCircuitBreaker, và chỉ định rằng nếu dịch vụ không
hoạt động, yêu cầu sẽ được chuyển hướng đến /contactSupport. Điều này giúp đảm bảo rằng người dùng không gặp lỗi trực tiếp
và có thể nhận được sự hỗ trợ khi cần thiết.






spring:
  application:
    name: "message"
  cloud:
    function:
      definition: email|sms
    stream:
      bindings:
        emailsms-in-0:
          destination: send-communication
          group: ${spring.application.name}
        emailsms-out-0:
          destination: communication-sent
spring.application.name: "message":

Đặt tên cho ứng dụng là "message". Tên này có thể được sử dụng trong các cấu hình khác hoặc trong hệ thống quản lý dịch vụ.
spring.cloud.function.definition: email|sms:

Định nghĩa một hàm (function) trong Spring Cloud, với hai lựa chọn là email và sms.
Dấu | giữa email và sms cho biết rằng hàm này có thể xử lý cả hai loại yêu cầu: gửi email và gửi tin nhắn SMS.
Điều này cho phép ứng dụng có thể lựa chọn cách gửi thông điệp dựa trên yêu cầu đầu vào.
spring.stream.bindings:

Đây là phần cấu hình cho Spring Cloud Stream, cho phép ứng dụng kết nối với các nguồn dữ liệu và điểm đến.
emailsms-in-0:

Đây là một binding đầu vào (input binding) cho phép ứng dụng nhận thông điệp từ một nguồn.
destination: send-communication: Xác định nguồn dữ liệu mà ứng dụng sẽ lắng nghe, trong trường hợp này là send-communication.
group: ${spring.application.name}: Đặt nhóm cho binding này, sử dụng tên ứng dụng "message". Điều này có thể giúp phân
phối thông điệp giữa các instance của ứng dụng.
emailsms-out-0:

Đây là một binding đầu ra (output binding) cho phép ứng dụng gửi thông điệp đến một đích.
destination: communication-sent: Xác định nơi mà thông điệp sẽ được gửi đi sau khi xử lý (ví dụ: khi một email hoặc SMS đã được gửi thành công).
Tóm lại
Cấu hình này cho phép ứng dụng "message" xử lý các yêu cầu gửi thông điệp qua hai phương thức: email và SMS. Nó nhận thông
điệp từ nguồn send-communication và gửi thông điệp đã xử lý đến communication-sent. Dấu | trong email|sms cho biết rằng hàm
có thể xử lý cả hai loại gửi thông điệp tùy thuộc vào yêu cầu đầu vào.












spring.cloud.function.definition được sử dụng để định nghĩa các hàm (functions) trong Spring Cloud Function. Dưới đây là
một số điểm chính về ý nghĩa và cách sử dụng của nó:

Mục đích:
Định nghĩa Hàm:

Cấu hình này cho phép bạn chỉ định một hoặc nhiều hàm mà ứng dụng sẽ sử dụng để xử lý dữ liệu đầu vào.
Hỗ trợ Đa dạng:

Bạn có thể định nghĩa nhiều hàm trong cùng một ứng dụng, và có thể sử dụng dấu phân cách (như |) để xác định nhiều hàm. Ví dụ: email|sms cho
phép ứng dụng xử lý cả hai loại yêu cầu gửi thông điệp.
Cách Hoạt Động:
Khi ứng dụng khởi động, Spring Cloud Function sẽ đọc cấu hình này và thiết lập các hàm được định nghĩa để có thể lắng nghe và
xử lý các sự kiện hoặc thông điệp.
Hàm có thể nhận đầu vào từ các nguồn khác nhau (như message queues, HTTP requests, v.v.) và trả về kết quả hoặc gửi thông điệp đến các đích khác.
Tóm lại:
spring.cloud.function.definition là một phần quan trọng trong Spring Cloud Function, cho phép bạn định nghĩa và cấu hình các
hàm để xử lý dữ liệu trong ứng dụng, hỗ trợ cho lập trình chức năng và linh hoạt trong việc xử lý các loại yêu cầu khác nhau.














spring:
  rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest
    connection-timeout: 10s
spring.rabbitmq:

Đây là phần cấu hình cho RabbitMQ trong ứng dụng Spring. Nó cho phép bạn thiết lập các thông số cần thiết để kết nối đến RabbitMQ.
host: localhost:

Chỉ định địa chỉ máy chủ RabbitMQ. localhost có nghĩa là RabbitMQ đang chạy trên cùng một máy với ứng dụng. Nếu RabbitMQ
chạy trên một máy chủ khác, bạn sẽ cần thay đổi địa chỉ này thành tên miền hoặc địa chỉ IP của máy chủ đó.
port: 5672:

Cổng mà RabbitMQ lắng nghe các kết nối. Cổng mặc định cho RabbitMQ là 5672. Nếu RabbitMQ được cấu hình để sử dụng cổng khác, bạn
cần thay đổi giá trị này.
username: guest:

Tên người dùng để xác thực với RabbitMQ. Tên người dùng mặc định là guest. Tuy nhiên, trong môi trường sản xuất, bạn nên sử dụng các
tài khoản khác với quyền hạn phù hợp.
password: guest:

Mật khẩu tương ứng với tên người dùng. Mật khẩu mặc định cho tài khoản guest là guest. Tương tự như tên người dùng, trong môi trường sản
xuất, bạn nên thay đổi mật khẩu này để bảo mật hơn.
connection-timeout: 10s:

Thời gian chờ tối đa để thiết lập kết nối đến RabbitMQ. Nếu không thể kết nối trong vòng 10 giây, ứng dụng sẽ ném ra một ngoại lệ. Thông
số này giúp đảm bảo rằng ứng dụng không bị treo lâu nếu RabbitMQ không phản hồi.
Tóm lại:
Đoạn mã cấu hình này thiết lập các thông số cần thiết để kết nối ứng dụng Spring với RabbitMQ, bao gồm địa chỉ máy chủ, cổng, thông tin
xác thực và thời gian chờ kết nối.









spring:
  kafka:
    binder:
      brokers:
        - localhost:9092
spring.kafka:

Đây là phần cấu hình cho Kafka trong ứng dụng Spring. Nó cho phép thiết lập các thông số cần thiết để kết nối đến Kafka.
binder:

binder là một khái niệm trong Spring Cloud Stream, dùng để kết nối ứng dụng với các message brokers như Kafka. Nó quản lý việc gửi và nhận thông điệp.
brokers:

Đây là danh sách các broker Kafka mà ứng dụng sẽ kết nối đến. Trong trường hợp này, ứng dụng sẽ kết nối đến một broker duy nhất.
- localhost:9092:

Đây là địa chỉ và cổng của broker Kafka. localhost có nghĩa là broker đang chạy trên cùng một máy với ứng dụng, và 9092 là cổng
mặc định mà Kafka lắng nghe các kết nối.
Tóm lại:
Đoạn mã cấu hình này thiết lập thông tin kết nối đến một broker Kafka trên địa chỉ localhost và cổng 9092, cho phép ứng dụng Spring
gửi và nhận thông điệp từ Kafka.












Trong kiến trúc microservices, việc giao tiếp giữa các service có thể thực hiện bằng nhiều cách khác nhau, bao gồm:

1. Service Discovery với OpenFeign:
OpenFeign: Là một công cụ giúp các microservice gọi lẫn nhau qua HTTP. Nó sử dụng service discovery để tìm địa chỉ của service cần gọi.
Ưu điểm:
Đơn giản hóa việc gọi API giữa các service.
Tự động hóa việc tạo các client HTTP.
Sử dụng: Thích hợp cho các trường hợp cần giao tiếp đồng bộ và có độ trễ thấp.
2. Giao tiếp qua Message Broker (RabbitMQ, Kafka):
Message Broker: Sử dụng RabbitMQ, Kafka hoặc các message broker khác để giao tiếp giữa các service.
Ưu điểm:
Hỗ trợ giao tiếp bất đồng bộ, giúp tăng tính linh hoạt và độ chịu lỗi của hệ thống.
Giúp giảm độ kết nối giữa các service, cho phép các service hoạt động độc lập hơn.
Sử dụng: Thích hợp cho các trường hợp xử lý thông điệp lớn hoặc cần độ tin cậy cao.
Kết hợp Cả Hai:
Trong nhiều ứng dụng microservices, bạn có thể sử dụng cả hai phương pháp:
Sử dụng OpenFeign cho các yêu cầu đồng bộ.
Sử dụng message broker cho các yêu cầu bất đồng bộ.
Tóm lại:
Việc sử dụng OpenFeign và message broker như RabbitMQ hoặc Kafka là hoàn toàn hợp lý trong kiến trúc microservices. Sự lựa
chọn giữa chúng phụ thuộc vào yêu cầu cụ thể của ứng dụng, bao gồm độ trễ, tính linh hoạt và khả năng mở rộng.

















Khi sử dụng Message Broker trong kiến trúc microservices, việc sử dụng Service Discovery không phải là bắt buộc, nhưng có thể là hữu
ích tùy vào tình huống cụ thể. Dưới đây là một số điểm cần xem xét:

1. Không Cần Service Discovery:
Giao tiếp Qua Message Broker: Khi các microservice gửi và nhận thông điệp qua một message broker (như RabbitMQ hoặc Kafka), chúng không
cần biết địa chỉ cụ thể của nhau. Chúng chỉ cần biết về topic hoặc queue mà chúng tương tác.
Tính Độc Lập: Các service có thể hoạt động độc lập, và việc thay đổi địa chỉ của một service không ảnh hưởng đến các service khác, miễn
là cấu hình của message broker được giữ nguyên.
2. Nên Sử Dụng Service Discovery:
Quản lý Nhiều Instance: Nếu bạn có nhiều instance của cùng một service, Service Discovery có thể giúp các service tìm thấy nhau một cách
nhanh chóng và dễ dàng.
Khả Năng Mở Rộng: Khi bạn mở rộng các service (như thêm instance mới), Service Discovery có thể giúp các instance mới tự động được phát hiện.
3. Tình Huống Cụ Thể:
Giao tiếp Bất Đồng Bộ: Nếu các service chủ yếu giao tiếp bằng message broker và không cần tương tác trực tiếp với nhau, thì không
cần Service Discovery.
Giao tiếp Đồng Bộ: Nếu có các yêu cầu cần giao tiếp đồng bộ (ví dụ: một service cần phản hồi ngay lập tức từ service khác), bạn có
thể cần Service Discovery để tìm địa chỉ của service đó.
Tóm lại:
Việc sử dụng Service Discovery khi có message broker là tùy thuộc vào cách thức giao tiếp giữa các service và yêu cầu cụ thể của
ứng dụng. Trong nhiều trường hợp, việc kết hợp cả hai sẽ mang lại lợi ích tối ưu.
















Trong kiến trúc microservices, có nhiều cách để các service giao tiếp với nhau. Dưới đây là những phương pháp phổ biến:

1. Giao tiếp Đồng Bộ:
HTTP REST:

Sử dụng giao thức HTTP để gọi các API RESTful giữa các service.
Ví dụ: OpenFeign, RestTemplate.
gRPC:

Sử dụng giao thức gRPC cho giao tiếp hiệu quả, hỗ trợ nhiều ngôn ngữ và có khả năng truyền tải dữ liệu nhanh chóng.
GraphQL:

Một phương thức để yêu cầu dữ liệu từ nhiều service thông qua một endpoint duy nhất.
2. Giao tiếp Bất Đồng Bộ:
Message Broker:
Sử dụng các message broker như RabbitMQ, Kafka để gửi và nhận thông điệp giữa các service.
Event Streaming:
Sử dụng các hệ thống như Kafka để xử lý luồng sự kiện và cho phép các service phản hồi các sự kiện.
3. Giao tiếp Qua Tệp (File-based communication):
Tệp tin:
Các service có thể giao tiếp thông qua việc ghi và đọc tệp tin từ một hệ thống tệp chung.
4. Giao tiếp qua Database:
Chia sẻ cơ sở dữ liệu:
Các service có thể giao tiếp bằng cách đọc và ghi dữ liệu vào cùng một cơ sở dữ liệu. Tuy nhiên, phương pháp này không được
khuyến khích do làm giảm tính độc lập của các service.
5. Service Discovery:
Phát hiện Dịch vụ:
Sử dụng các công cụ như Eureka, Consul để cho phép các service tìm kiếm địa chỉ của nhau, thường được kết hợp với các phương pháp trên.
Tóm lại:
Các phương pháp giao tiếp giữa các service trong microservices rất đa dạng và có thể được kết hợp tùy theo yêu cầu và kiến trúc
của hệ thống. Việc lựa chọn phương pháp phù hợp sẽ giúp cải thiện hiệu suất và khả năng mở rộng của ứng dụng.












apiVersion: apps/v1
kind: Deployment
metadata:
  name: eurekaserver-deployment
  labels:
    app: eurekaserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: eurekaserver
  template:
    metadata:
      labels:
        app: eurekaserver
    spec:
      containers:
      - name: eurekaserver
        image: eazybytes/eurekaserver:s12
        ports:
        - containerPort: 8070
        env:
        - name: SPRING_APPLICATION_NAME
          valueFrom:
            configMapKeyRef:
              name: eazybank-configmap
              key: EUREKA_APPLICATION_NAME
        - name: SPRING_CONFIG_IMPORT
          valueFrom: 
            configMapKeyRef:
              name: eazybank-configmap
              key: SPRING_CONFIG_IMPORT
apiVersion: apps/v1:

Xác định phiên bản API của Kubernetes được sử dụng cho Deployment.
kind: Deployment:

Loại đối tượng Kubernetes là Deployment, giúp quản lý việc triển khai và mở rộng ứng dụng.
metadata:

Chứa thông tin về Deployment, bao gồm tên và nhãn (labels).
spec:

Định nghĩa các thông số cho Deployment:
replicas: 1: Số lượng bản sao của ứng dụng cần chạy (1 instance).
selector: Xác định cách tìm các pod liên quan đến Deployment này.
template: Mô tả pod mà Deployment sẽ tạo:
metadata: Nhãn cho pod.
spec: Thông tin về các container trong pod:
containers: Danh sách các container. Trong trường hợp này chỉ có một container:
name: Tên của container.
image: Hình ảnh Docker được sử dụng cho container (eazybytes/eurekaserver:s12).
ports: Cổng mà container sẽ lắng nghe (8070).
env: Biến môi trường được cấu hình từ ConfigMap:
SPRING_APPLICATION_NAME: Tên ứng dụng.
SPRING_CONFIG_IMPORT: Cấu hình import.


apiVersion: v1:

Phiên bản API cho Service.
kind: Service:

Loại đối tượng Kubernetes là Service, dùng để định nghĩa cách truy cập vào một hoặc nhiều pod.
metadata:

Thông tin về Service, bao gồm tên.
spec:

Định nghĩa các thông số cho Service:
selector: Chọn các pod dựa trên nhãn app: eurekaserver.
type: LoadBalancer: Xác định loại Service là LoadBalancer, cho phép truy cập từ bên ngoài.
ports: Cấu hình cổng cho Service:
protocol: Giao thức (TCP).
port: Cổng mà Service sẽ lắng nghe (8070).
targetPort: Cổng mà pod sẽ lắng nghe (8070).


Đoạn mã này cấu hình một Deployment cho Eureka Server với một instance duy nhất và một Service để cung cấp
truy cập bên ngoài thông qua LoadBalancer. Biến môi trường được lấy từ một ConfigMap giúp cấu hình ứng dụng linh hoạt.

