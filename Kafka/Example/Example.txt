package com.poscodx.odc.ampro015.config.kafka.listeners;
import com.poscdx.odc.ampro015.domain.entity.payload.request.DeepFaceInfoRequest;
import com.poscdx.odc.ampro015.domain.lifecycle.ServiceLifecycle;
import org.apache.commons.lang3.time.DateUtils;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaHandler;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;
import java.util.Date;
import java.util.concurrent.TimeoutException;
@Component
@KafkaListener(id = "multiGroup1", topics = "amface", groupId = "deep-fake", autoStartup = "false")
public class MultiTypeKafkaListener {
	@Autowired
	ServiceLifecycle serviceLifecycle;
	@KafkaHandler
	public void handleUserFaceRequest(DeepFaceInfoRequest userFaceRequest) throws TimeoutException {
		try {
			System.out.println("User face received UserFaceRequest 1 (user-face): " + userFaceRequest);
		} catch (Exception e) {
			System.out.println("Handle kafka's message error: " + e.getMessage());
		}
	}
	@KafkaHandler
	public void handleUserFaceRequest2(String userFaceRequest) throws TimeoutException {
		try {
			System.out.println("User face received String 2 (user-face): " + userFaceRequest);
			DeepFaceInfoRequest receiver = DeepFaceInfoRequest.fromJson(userFaceRequest);
//            String dateInString = receiver.getAccessTime().replace("T", " ");
//            Date checkTime = DateUtils.parseDate(dateInString, new String[] { "yyyy-MM-dd HH:mm:ss", "dd/MM-yyyy" });
//            receiver.setAccessTimeToDate(checkTime);
			System.out.println("User face received String 2 (Oject): " + receiver);
			serviceLifecycle.requestLevel2WorkingTimeService().createOrUpdateWorkingTime(serviceLifecycle, receiver.getId(), receiver.getAccessTime());
			serviceLifecycle.requestLevel2Service().sendCheckFaceTimeNotification(receiver, "/topic/check-face-time");
		} catch (Exception e) {
			System.out.println("Handle kafka's message error: " + e.getMessage());
		}
	}
	@KafkaHandler
	public void unknown(ConsumerRecord<String, Object> record) throws TimeoutException {
		try {
			System.out.println("User face received Object 2 (user-face): " + record.value());
		} catch (Exception e) {
			System.out.println("Handle kafka's message error: " + e.getMessage());
		}
	}
}

package com.poscodx.odc.ampro015.config.kafka.producers;
import com.poscdx.odc.ampro015.domain.entity.payload.request.DeepFaceInfoRequest;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;
@Slf4j
@NoArgsConstructor
@Component
public class ProductProducer {
	final String topic = "deepfake";
	@Autowired
	private KafkaTemplate<String, DeepFaceInfoRequest> kafkaTemplate;
	@Autowired
	private KafkaTemplate<String, String> kafkaTemplate2;
//    @Autowired
//    private KafkaTemplate<String, Object> multiTypeKafkaTemplate;
	public void sendMessage(String message) {
		kafkaTemplate2.send(topic, message);
		log.info("Send message String: {}", message);
	}
	public void sendMessage(DeepFaceInfoRequest message) {
		kafkaTemplate.send(topic, message);
		log.info("Send message UserFaceRequest: {}", message);
	}
}

package com.poscodx.odc.ampro015.config.kafka;
import com.poscdx.odc.ampro015.domain.entity.payload.request.DeepFaceInfoRequest;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import java.util.HashMap;
import java.util.Map;
@Slf4j
@Configuration
public class KafkaConsumerConfig {
	@Value(value = "${spring.kafka.bootstrap-servers}")
	private String bootstrapAddress;
	public ConsumerFactory<String, String> consumerFactory(String groupId) {
		Map<String, Object> props = new HashMap<>();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
		props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
		props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, "20971520");
		props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, "20971520");
		return new DefaultKafkaConsumerFactory<>(props);
	}
	public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory(String groupId) {
		ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
		factory.setConsumerFactory(consumerFactory(groupId));
		return factory;
	}
	@Bean
	public ConcurrentKafkaListenerContainerFactory<String, String> deepFakeKafkaListenerContainerFactory() {
		return kafkaListenerContainerFactory("deep-fake");
	}
	public ConsumerFactory<String, DeepFaceInfoRequest> consumerFactory2(String groupId) {
		Map<String, Object> props = new HashMap<>();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
		props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
		return new DefaultKafkaConsumerFactory<>(props, new StringDeserializer(), new JsonDeserializer<>(DeepFaceInfoRequest.class));
	}
	@Bean
	public ConcurrentKafkaListenerContainerFactory<String, DeepFaceInfoRequest> deepFakeeKafkaListenerContainerFactory2() {
		ConcurrentKafkaListenerContainerFactory<String, DeepFaceInfoRequest> factory = new ConcurrentKafkaListenerContainerFactory<>();
		factory.setConsumerFactory(consumerFactory2("deep-fake"));
		return factory;
	}
}

package com.poscodx.odc.ampro015.config.kafka;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.ListTopicsOptions;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.actuate.health.Health;
import org.springframework.boot.actuate.health.HealthIndicator;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.stereotype.Component;
import java.util.concurrent.TimeUnit;
@Component("kafka_indicator")
@Slf4j
public class KafkaHealthIndicator implements HealthIndicator {
	@Autowired
	private KafkaAdmin kafkaAdmin;
	@Override
	public Health health() {
		try (AdminClient client = AdminClient.create(kafkaAdmin.getConfig())) {
			client.listTopics(new ListTopicsOptions().timeoutMs(1000)).listings().get(10, TimeUnit.SECONDS);
			return Health.up().build();
		} catch (Exception e) {
			return Health.down().withException(e).build();
		}
	}
}

package com.poscodx.odc.ampro015.config.kafka;
import com.poscdx.odc.ampro015.domain.entity.payload.request.DeepFaceInfoRequest;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;
import java.util.HashMap;
import java.util.Map;
@Configuration
public class
KafkaProducerConfig {
	@Value(value = "${spring.kafka.bootstrap-servers}")
	private String bootstrapAddress;
	@Bean
	public ProducerFactory<String, String> producerFactory() {
		Map<String, Object> configProps = new HashMap<>();
		configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
		configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		configProps.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, "20971520");
//        configProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,4000);
//        configProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG,6000);

		return new DefaultKafkaProducerFactory<>(configProps);
	}
	@Bean
	public KafkaTemplate<String, String> kafkaTemplate() {
		return new KafkaTemplate<>(producerFactory());
	}
	@Bean
	public ProducerFactory<String, DeepFaceInfoRequest> userFaceProducerFactory() {
		Map<String, Object> configProps = new HashMap<>();
		configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
		configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
		return new DefaultKafkaProducerFactory<>(configProps);
	}
	@Bean
	public KafkaTemplate<String, DeepFaceInfoRequest> userFaceKafkaTemplate() {
		return new KafkaTemplate<>(userFaceProducerFactory());
	}
}

package com.poscodx.odc.ampro015.config.kafka;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.NewTopic;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.KafkaAdmin;
import java.util.HashMap;
import java.util.Map;
@Configuration
public class KafkaTopicConfig {
	@Value(value = "${spring.kafka.bootstrap-servers}")
	private String bootstrapAddress;
	@Bean
	public KafkaAdmin kafkaAdmin() {
		Map<String, Object> configs = new HashMap<>();
		configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
		return new KafkaAdmin(configs);
	}
}			