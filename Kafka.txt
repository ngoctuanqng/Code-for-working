Nh·ªØng vi·ªác c·∫ßn l√†m:

	Apache Kafka Broker(s):
	
		C·∫ßn ch·∫°y kafka version 4.1 ƒë·ªÉ c√≥ th·ªÉ start topic v·ªõi replication-factor l√† 3

--- Apache Kafka Broker(s):


	Flow ch·∫°y v·ªõi version nh·ªè h∆°n 4.1:

		Ch·∫°y l·ªánh sau tr√™n cmd v·ªõi quy·ªÅn admin, wsl l√† window subsystem for linux:
		
			wsl --install
			
		M·ªü wsl
		
		Install Java:
		
			sudo apt-get update && sudo apt-get upgrade -y
			
			sudo apt install openjdk-17-jdk -y
			
		Download Kafka:
		
			wget https://dlcdn.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz
			
			tar -xzf kafka_2.13-3.5.0.tgz
			
			cd kafka_2.13-3.5.0
		
		Start the Kafka cluster:
		
			Run the kafka-storage.sh script to generate a cluster ID:
			
				KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
				
			Run the kafka-storage.sh script again to format the log directories:
			
				bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
				
			Run the kafka-server-start.sh script to start the Kafka server:
			
				bin/kafka-server-start.sh config/kraft/server.properties
				
		T·∫°o v√† ch·∫°y v·ªõi nhi·ªÅu broker:
		
			Th√™m 3 file sau v√†o link config/kraft:
			
				server-1.properties:
				
					# Licensed to the Apache Software Foundation (ASF) under one or more
					# contributor license agreements.  See the NOTICE file distributed with
					# this work for additional information regarding copyright ownership.
					# The ASF licenses this file to You under the Apache License, Version 2.0
					# (the "License"); you may not use this file except in compliance with
					# the License.  You may obtain a copy of the License at
					#
					#    http://www.apache.org/licenses/LICENSE-2.0
					#
					# Unless required by applicable law or agreed to in writing, software
					# distributed under the License is distributed on an "AS IS" BASIS,
					# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
					# See the License for the specific language governing permissions and
					# limitations under the License.

					#
					# This configuration file is intended for use in KRaft mode, where
					# Apache ZooKeeper is not present.
					#

					############################# Server Basics #############################

					# The role of this server. Setting this puts us in KRaft mode
					process.roles=broker,controller

					# The node id associated with this instance's roles
					node.id=1

					# The connect string for the controller quorum
					controller.quorum.voters=1@localhost:9093,2@localhost:9095,3@localhost:9097

					############################# Socket Server Settings #############################

					# The address the socket server listens on.
					# Combined nodes (i.e. those with `process.roles=broker,controller`) must list the controller listener here at a minimum.
					# If the broker listener is not defined, the default listener will use a host name that is equal to the value of java.net.InetAddress.getCanonicalHostName(),
					# with PLAINTEXT listener name, and port 9092.
					#   FORMAT:
					#     listeners = listener_name://host_name:port
					#   EXAMPLE:
					#     listeners = PLAINTEXT://your.host.name:9092
					listeners=PLAINTEXT://:9092,CONTROLLER://:9093

					# Name of listener used for communication between brokers.
					inter.broker.listener.name=PLAINTEXT

					# Listener name, hostname and port the broker will advertise to clients.
					# If not set, it uses the value for "listeners".
					advertised.listeners=PLAINTEXT://localhost:9092

					# A comma-separated list of the names of the listeners used by the controller.
					# If no explicit mapping set in `listener.security.protocol.map`, default will be using PLAINTEXT protocol
					# This is required if running in KRaft mode.
					controller.listener.names=CONTROLLER

					# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
					listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

					# The number of threads that the server uses for receiving requests from the network and sending responses to the network
					num.network.threads=3

					# The number of threads that the server uses for processing requests, which may include disk I/O
					num.io.threads=8

					# The send buffer (SO_SNDBUF) used by the socket server
					socket.send.buffer.bytes=102400

					# The receive buffer (SO_RCVBUF) used by the socket server
					socket.receive.buffer.bytes=102400

					# The maximum size of a request that the socket server will accept (protection against OOM)
					socket.request.max.bytes=104857600


					############################# Log Basics #############################

					# A comma separated list of directories under which to store log files
					log.dirs=/tmp/server-1/kraft-combined-logs

					# The default number of log partitions per topic. More partitions allow greater
					# parallelism for consumption, but this will also result in more files across
					# the brokers.
					num.partitions=1

					# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
					# This value is recommended to be increased for installations with data dirs located in RAID array.
					num.recovery.threads.per.data.dir=1

					############################# Internal Topic Settings  #############################
					# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
					# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
					offsets.topic.replication.factor=1
					transaction.state.log.replication.factor=1
					transaction.state.log.min.isr=1

					############################# Log Flush Policy #############################

					# Messages are immediately written to the filesystem but by default we only fsync() to sync
					# the OS cache lazily. The following configurations control the flush of data to disk.
					# There are a few important trade-offs here:
					#    1. Durability: Unflushed data may be lost if you are not using replication.
					#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
					#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
					# The settings below allow one to configure the flush policy to flush data after a period of time or
					# every N messages (or both). This can be done globally and overridden on a per-topic basis.

					# The number of messages to accept before forcing a flush of data to disk
					#log.flush.interval.messages=10000

					# The maximum amount of time a message can sit in a log before we force a flush
					#log.flush.interval.ms=1000

					############################# Log Retention Policy #############################

					# The following configurations control the disposal of log segments. The policy can
					# be set to delete segments after a period of time, or after a given size has accumulated.
					# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
					# from the end of the log.

					# The minimum age of a log file to be eligible for deletion due to age
					log.retention.hours=168

					# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
					# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
					#log.retention.bytes=1073741824

					# The maximum size of a log segment file. When this size is reached a new log segment will be created.
					log.segment.bytes=1073741824

					# The interval at which log segments are checked to see if they can be deleted according
					# to the retention policies
					log.retention.check.interval.ms=300000
					
				server-2.properties:
				
					# Licensed to the Apache Software Foundation (ASF) under one or more
					# contributor license agreements.  See the NOTICE file distributed with
					# this work for additional information regarding copyright ownership.
					# The ASF licenses this file to You under the Apache License, Version 2.0
					# (the "License"); you may not use this file except in compliance with
					# the License.  You may obtain a copy of the License at
					#
					#    http://www.apache.org/licenses/LICENSE-2.0
					#
					# Unless required by applicable law or agreed to in writing, software
					# distributed under the License is distributed on an "AS IS" BASIS,
					# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
					# See the License for the specific language governing permissions and
					# limitations under the License.

					#
					# This configuration file is intended for use in KRaft mode, where
					# Apache ZooKeeper is not present.
					#

					############################# Server Basics #############################

					# The role of this server. Setting this puts us in KRaft mode
					process.roles=broker,controller

					# The node id associated with this instance's roles
					node.id=2

					# The connect string for the controller quorum
					controller.quorum.voters=1@localhost:9093,2@localhost:9095,3@localhost:9097

					############################# Socket Server Settings #############################

					# The address the socket server listens on.
					# Combined nodes (i.e. those with `process.roles=broker,controller`) must list the controller listener here at a minimum.
					# If the broker listener is not defined, the default listener will use a host name that is equal to the value of java.net.InetAddress.getCanonicalHostName(),
					# with PLAINTEXT listener name, and port 9092.
					#   FORMAT:
					#     listeners = listener_name://host_name:port
					#   EXAMPLE:
					#     listeners = PLAINTEXT://your.host.name:9092
					listeners=PLAINTEXT://:9094,CONTROLLER://:9095

					# Name of listener used for communication between brokers.
					inter.broker.listener.name=PLAINTEXT

					# Listener name, hostname and port the broker will advertise to clients.
					# If not set, it uses the value for "listeners".
					advertised.listeners=PLAINTEXT://localhost:9094

					# A comma-separated list of the names of the listeners used by the controller.
					# If no explicit mapping set in `listener.security.protocol.map`, default will be using PLAINTEXT protocol
					# This is required if running in KRaft mode.
					controller.listener.names=CONTROLLER

					# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
					listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

					# The number of threads that the server uses for receiving requests from the network and sending responses to the network
					num.network.threads=3

					# The number of threads that the server uses for processing requests, which may include disk I/O
					num.io.threads=8

					# The send buffer (SO_SNDBUF) used by the socket server
					socket.send.buffer.bytes=102400

					# The receive buffer (SO_RCVBUF) used by the socket server
					socket.receive.buffer.bytes=102400

					# The maximum size of a request that the socket server will accept (protection against OOM)
					socket.request.max.bytes=104857600


					############################# Log Basics #############################

					# A comma separated list of directories under which to store log files
					log.dirs=/tmp/server-2/kraft-combined-logs

					# The default number of log partitions per topic. More partitions allow greater
					# parallelism for consumption, but this will also result in more files across
					# the brokers.
					num.partitions=1

					# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
					# This value is recommended to be increased for installations with data dirs located in RAID array.
					num.recovery.threads.per.data.dir=1

					############################# Internal Topic Settings  #############################
					# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
					# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
					offsets.topic.replication.factor=1
					transaction.state.log.replication.factor=1
					transaction.state.log.min.isr=1

					############################# Log Flush Policy #############################

					# Messages are immediately written to the filesystem but by default we only fsync() to sync
					# the OS cache lazily. The following configurations control the flush of data to disk.
					# There are a few important trade-offs here:
					#    1. Durability: Unflushed data may be lost if you are not using replication.
					#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
					#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
					# The settings below allow one to configure the flush policy to flush data after a period of time or
					# every N messages (or both). This can be done globally and overridden on a per-topic basis.

					# The number of messages to accept before forcing a flush of data to disk
					#log.flush.interval.messages=10000

					# The maximum amount of time a message can sit in a log before we force a flush
					#log.flush.interval.ms=1000

					############################# Log Retention Policy #############################

					# The following configurations control the disposal of log segments. The policy can
					# be set to delete segments after a period of time, or after a given size has accumulated.
					# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
					# from the end of the log.

					# The minimum age of a log file to be eligible for deletion due to age
					log.retention.hours=168

					# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
					# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
					#log.retention.bytes=1073741824

					# The maximum size of a log segment file. When this size is reached a new log segment will be created.
					log.segment.bytes=1073741824

					# The interval at which log segments are checked to see if they can be deleted according
					# to the retention policies
					log.retention.check.interval.ms=300000
					
			server-3.properties:

				# Licensed to the Apache Software Foundation (ASF) under one or more
				# contributor license agreements.  See the NOTICE file distributed with
				# this work for additional information regarding copyright ownership.
				# The ASF licenses this file to You under the Apache License, Version 2.0
				# (the "License"); you may not use this file except in compliance with
				# the License.  You may obtain a copy of the License at
				#
				#    http://www.apache.org/licenses/LICENSE-2.0
				#
				# Unless required by applicable law or agreed to in writing, software
				# distributed under the License is distributed on an "AS IS" BASIS,
				# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
				# See the License for the specific language governing permissions and
				# limitations under the License.

				#
				# This configuration file is intended for use in KRaft mode, where
				# Apache ZooKeeper is not present.
				#

				############################# Server Basics #############################

				# The role of this server. Setting this puts us in KRaft mode
				process.roles=broker,controller

				# The node id associated with this instance's roles
				node.id=3

				# The connect string for the controller quorum
				controller.quorum.voters=1@localhost:9093,2@localhost:9095,3@localhost:9097

				############################# Socket Server Settings #############################

				# The address the socket server listens on.
				# Combined nodes (i.e. those with `process.roles=broker,controller`) must list the controller listener here at a minimum.
				# If the broker listener is not defined, the default listener will use a host name that is equal to the value of java.net.InetAddress.getCanonicalHostName(),
				# with PLAINTEXT listener name, and port 9092.
				#   FORMAT:
				#     listeners = listener_name://host_name:port
				#   EXAMPLE:
				#     listeners = PLAINTEXT://your.host.name:9092
				listeners=PLAINTEXT://:9096,CONTROLLER://:9097

				# Name of listener used for communication between brokers.
				inter.broker.listener.name=PLAINTEXT

				# Listener name, hostname and port the broker will advertise to clients.
				# If not set, it uses the value for "listeners".
				advertised.listeners=PLAINTEXT://localhost:9096

				# A comma-separated list of the names of the listeners used by the controller.
				# If no explicit mapping set in `listener.security.protocol.map`, default will be using PLAINTEXT protocol
				# This is required if running in KRaft mode.
				controller.listener.names=CONTROLLER

				# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
				listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

				# The number of threads that the server uses for receiving requests from the network and sending responses to the network
				num.network.threads=3

				# The number of threads that the server uses for processing requests, which may include disk I/O
				num.io.threads=8

				# The send buffer (SO_SNDBUF) used by the socket server
				socket.send.buffer.bytes=102400

				# The receive buffer (SO_RCVBUF) used by the socket server
				socket.receive.buffer.bytes=102400

				# The maximum size of a request that the socket server will accept (protection against OOM)
				socket.request.max.bytes=104857600


				############################# Log Basics #############################

				# A comma separated list of directories under which to store log files
				log.dirs=/tmp/server-3/kraft-combined-logs

				# The default number of log partitions per topic. More partitions allow greater
				# parallelism for consumption, but this will also result in more files across
				# the brokers.
				num.partitions=1

				# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
				# This value is recommended to be increased for installations with data dirs located in RAID array.
				num.recovery.threads.per.data.dir=1

				############################# Internal Topic Settings  #############################
				# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
				# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
				offsets.topic.replication.factor=1
				transaction.state.log.replication.factor=1
				transaction.state.log.min.isr=1

				############################# Log Flush Policy #############################

				# Messages are immediately written to the filesystem but by default we only fsync() to sync
				# the OS cache lazily. The following configurations control the flush of data to disk.
				# There are a few important trade-offs here:
				#    1. Durability: Unflushed data may be lost if you are not using replication.
				#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
				#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
				# The settings below allow one to configure the flush policy to flush data after a period of time or
				# every N messages (or both). This can be done globally and overridden on a per-topic basis.

				# The number of messages to accept before forcing a flush of data to disk
				#log.flush.interval.messages=10000

				# The maximum amount of time a message can sit in a log before we force a flush
				#log.flush.interval.ms=1000

				############################# Log Retention Policy #############################

				# The following configurations control the disposal of log segments. The policy can
				# be set to delete segments after a period of time, or after a given size has accumulated.
				# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
				# from the end of the log.

				# The minimum age of a log file to be eligible for deletion due to age
				log.retention.hours=168

				# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
				# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
				#log.retention.bytes=1073741824

				# The maximum size of a log segment file. When this size is reached a new log segment will be created.
				log.segment.bytes=1073741824

				# The interval at which log segments are checked to see if they can be deleted according
				# to the retention policies
				log.retention.check.interval.ms=300000

		Ch·∫°y c√°c l·ªánh sau:
		
			- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ \./bin/kafka-storage.sh random-uuid
			- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ ./bin/kafka-storage.sh format -t 3oly753rSTyljoJIc6whCw -c config/kraft/server-1.properties
			- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ ./bin/kafka-server-start.sh config/kraft/server-1.properties
			- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --create --topic topic1 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092
			- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --create --topic topic2 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092,localhost:9094
			
			
			
			
			
		
		
	C√°ch c√°c team backend l√†m h·∫±ng ng√†y:
	
		B·∫¢N CH·∫§T K·∫æT N·ªêI

			Producer (Spring Boot) ch·∫°y tr√™n Windows

			Kafka Broker + Consumer ch·∫°y trong WSL (Ubuntu)

		Windows ‚Üî WSL kh√¥ng ph·∫£i 2 m√°y kh√°c nhau, m√†:

			WSL ch·∫°y trong Windows

			C√≥ network bridge ri√™ng

			C√≥ th·ªÉ giao ti·∫øp TCP b√¨nh th∆∞·ªùng

		S∆† ƒê·ªí ƒê∆†N GI·∫¢N
		
			Spring Boot Producer (Windows)
					|
					|  localhost:9092
					|
			Kafka Broker (WSL)
					|
			Kafka Consumer (WSL)


			Producer ch·ªâ c·∫ßn bi·∫øt ƒë√∫ng ƒë·ªãa ch·ªâ Kafka

		ƒêI·ªÄU QUAN TR·ªåNG NH·∫§T: advertised.listeners

			ƒê√¢y l√† ch·ªó 90% ng∆∞·ªùi m·ªõi b·ªã l·ªói.

			C·∫•u h√¨nh SAI (m·∫∑c ƒë·ªãnh)

				Kafka trong WSL th∆∞·ªùng c√≥:

				advertised.listeners=PLAINTEXT://localhost:9092


				‚Üí Producer tr√™n Windows s·∫Ω KH√îNG k·∫øt n·ªëi ƒë∆∞·ª£c

			C·∫§U H√åNH ƒê√öNG (KHI PRODUCER ·ªû WINDOWS)

				Trong server.properties (Kafka ch·∫°y WSL):

					listeners=PLAINTEXT://0.0.0.0:9092
					advertised.listeners=PLAINTEXT://localhost:9092


			V√¨ sao?

				0.0.0.0 ‚Üí Kafka l·∫Øng nghe m·ªçi interface

				localhost ‚Üí Windows truy c·∫≠p ƒë∆∞·ª£c WSL		

	kafka-storage.bat l√† batch file cho Command Prompt / cmd.exe, kh√¥ng ch·∫°y tr·ª±c ti·∫øp trong Bash ho·∫∑c Git Bash.
	
	G·∫∑p l·ªói The input line is too long. The syntax of the command is incorrect.
	
		Th√¨ ƒë∆∞a folder kafka ra ·ªï E, ch·ªß y·∫øu ƒë·ªÉ gi·∫£m chi·ªÅu d√†i c·ªßa link ƒë·∫øn th∆∞ m·ª•c
		
	T·∫†I SAO 3 FILE server-*.properties KH√îNG C√ì √ù NGHƒ®A KHI D√ôNG --standalone?

		Kafka KH√îNG quan t√¢m:

			b·∫°n c√≥ bao nhi√™u file config

			bao nhi√™u port

			bao nhi√™u process ƒëang ch·∫°y

		Kafka CH·ªà quan t√¢m:

			metadata c·ªßa controller quorum

		--standalone L√Ä ‚ÄúKH√ìA CH·∫æ ƒê·ªò CLUSTER‚Äù

			Khi b·∫°n ch·∫°y:

				kafka-storage.bat format --standalone


			Kafka s·∫Ω ghi c·ª©ng v√†o meta.properties:

				controller.quorum.voters=1@localhost:9093


		ƒêi·ªÅu n√†y c√≥ nghƒ©a:

			Cluster b·ªã kh√≥a = 1 controller

			Kh√¥ng node n√†o kh√°c ƒë∆∞·ª£c tham gia

		ƒêI·ªÄU G√å X·∫¢Y RA KHI B·∫†N L√ÄM 3 L·∫¶N --standalone?
		
		Server	B·∫°n nghƒ©	Kafka hi·ªÉu
		server-1	node 1	cluster A (1 node)
		server-2	node 2	cluster B (1 node)
		server-3	node 3	cluster C (1 node)

		Kh√¥ng c√≥ kh√°i ni·ªám ‚Äúg·ªôp‚Äù
		
	BA L·ª∞A CH·ªåN (HI·ªÇU ƒê·ªÇ KH√îNG NH·∫¶M)
	
		--standalone

			D√πng khi:

				1 node duy nh·∫•t

				v·ª´a broker + controller

			KH√îNG d√πng cho cluster 3 node

			Tr∆∞·ªùng h·ª£p c·ªßa b·∫°n ‚Üí KH√îNG PH√ô H·ª¢P

		--initial-controllers ‚úÖ (C√ÅI B·∫†N C·∫¶N)

			D√πng khi:

				Cluster nhi·ªÅu controller

				Ch∆∞a c√≥ metadata l·∫ßn ƒë·∫ßu

			CH√çNH X√ÅC cho setup 3 server

		--no-initial-controllers

			D√πng khi:

				Ch·ªâ format broker

				Controller ƒë√£ t·ªìn t·∫°i

			KH√îNG d√πng l√∫c kh·ªüi t·∫°o cluster

	- Commands:
	
		- cmd: tar -xzf kafka_2.13-4.1.1.tgz		
		- cmd: kafka-metadata-quorum.bat --bootstrap-server localhost:9092 describe --status
		- cmd: wget https://archive.apache.org/dist/kafka/3.5.0/kafka_2.13-3.5.0.tgz
		- cmd: ./bin/kafka-server-start.sh config/kraft/server-3.properties
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ ./bin/kafka-storage.sh random-uuid
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ ./bin/kafka-storage.sh format -t 3oly753rSTyljoJIc6whCw -c config/kraft/server-1.properties
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0$ ./bin/kafka-server-start.sh config/kraft/server-1.properties
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --create --topic topic1 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --create --topic topic2 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092,localhost:9094
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --list --bootstrap-server localhost:9092
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-topics.sh --describe --bootstrap-server localhost:9092
		
		
		
		
	- C√°c thay ƒë·ªïi c·∫ßn s·ª≠a trong file server.properties v·ªõi ch·∫°y mutiple server:
	
		node.id=2
		
		listeners=PLAINTEXT://localhost:9094,CONTROLLER://localhost:9095
		
		controller.quorum.bootstrap.servers=@localhost:9093,localhost:9095,localhost:9097
		
		advertised.listeners=PLAINTEXT://localhost:9094
		
		log.dirs=/tmp/server-2/kraft-combined-logs
		
--- Kafka CLI: Topics:

	- Commands:
	
		- cmd: E:\Kafka\kafka_2.13-4.1.1\bin\windows>kafka-topics.bat --create --topic tpoic1 --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092,localhost:9094
		
		- cmd: E:\Kafka\kafka_2.13-4.1.1\bin\windows>kafka-topics.bat --create --topic topic1 --bootstrap-server localhost:9092
		
--- Kafka CLI Producers:

	- Command:
	
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-console-producer.sh --bootstrap-server localhost:9092,localhost:9094 --topic my-topic
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-console-producer.sh --bootstrap-server localhost:9092,localhost:9094 --topic my-topic --property "parse.key=true" --property "key.separator=:"
		
--- Kafka CLI Consumers:

	- Command:
	
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092 --property  print.key=true
		- cmd: ngoctuanqng1@DESKTOP-GNVB183:~/kafka_2.13-3.5.0/bin$ ./kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092 --property  print.key=true --property print.value=true --from-beginning
		
--- Kafka Producer - Spring Boot Microservice:

		ƒê·ªÉ c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c tr√™n WSL c·∫ßn c·∫•u h√¨nh c√°c file nh∆∞ sau:
		
			D√π m·ªü 3 WSL th√¨ s·∫Ω ƒë·ªÅu c√≥ chung 1 ƒë·ªãa ch·ªâ IP, l·∫•y ƒë·ªãa ch·ªâ ƒë√≥ cho v√†o c·∫•u h√¨nh ph√≠a d∆∞·ªõi
			
			
			1. updating server.properties : 

				listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://127.0.0.1:9093
				advertised.listeners=PLAINTEXT://{IP ADDR OF WSL}:9092
				
					where "IP ADDR OF WSL" is the ip address of WSL.

			2. updating application.properties file -


				spring.kafka.bootstrap-servers={IP ADDR OF WSL}:9092,{IP ADDR OF WSL}:9094
				
		S·ª≠a file pom.xml:
		
			<dependency>
				<groupId>com.fasterxml.jackson.core</groupId>
				<artifactId>jackson-databind</artifactId>
				<version>2.17.1</version>
			</dependency>
			
			Kafka kh√¥ng b·∫Øt bu·ªôc Jackson

				Kafka ch·ªâ l√†m vi·ªác v·ªõi byte[].

				Kafka kh√¥ng bi·∫øt JSON l√† g√¨.

				Nh∆∞ng trong project c·ªßa b·∫°n, b·∫°n ƒëang d√πng:

				spring.kafka.producer.value-serializer= org.springframework.kafka.support.serializer.JsonSerializer

				JsonSerializer = Jackson

				N·∫øu kh√¥ng c√≥ Jackson ‚Üí Kafka kh√¥ng serialize ƒë∆∞·ª£c object ‚Üí n√©m l·ªói runtime.

			JsonSerializer c·ªßa Spring Kafka d√πng Jackson nh∆∞ th·∫ø n√†o?

				B√™n trong JsonSerializer c√≥ ƒëo·∫°n logic t∆∞∆°ng ƒë∆∞∆°ng:

					ObjectMapper mapper = new ObjectMapper();
					mapper.writeValueAsBytes(object);


				ObjectMapper n·∫±m trong jackson-databind

				N·∫øu thi·∫øu:

					jackson-databind

					ho·∫∑c version kh√¥ng t∆∞∆°ng th√≠ch

					üëâ JVM s·∫Ω b√°o l·ªói d·∫°ng:

					ClassNotFoundException

					NoClassDefFoundError

					Failed to construct Kafka producer

					TimeoutException (l·ªói gi·∫£, nguy√™n nh√¢n g·ªëc l√† serialize fail)
				
		S·ª≠a file application.properties:
		
			Case 1:
			
				server.port=0

				spring.kafka.bootstrap-servers=172.28.85.33:9092,172.28.85.33:9094

				spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
				spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer
				
		S·ª≠a file KafkaConfig.java:
		
			Case 1:

				@Configuration
				public class KafkaConfig {

					@Bean
					NewTopic createTopic() {
						return TopicBuilder.name("product-created-events-topic")
								.partitions(3)
								.replicas(3)
								.configs(Map.of("min.insync.replicas", "2"))
								.build();
					}

				}

		S·ª≠a file ProductServiceImpl.java:
		
			Case 1:
			
				package com.appsdeveloperblog.ws.products.service;

				import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.http.HttpStatus;
				import org.springframework.http.ResponseEntity;
				import org.springframework.kafka.core.KafkaTemplate;
				import org.springframework.kafka.support.SendResult;
				import org.springframework.stereotype.Service;

				import java.util.UUID;
				import java.util.concurrent.CompletableFuture;

				@Service
				public class ProductServiceImpl implements ProductService {

					KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;

					private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

					public ProductServiceImpl(KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate) {
						this.kafkaTemplate = kafkaTemplate;
					}

					@Override
					public String createProduct(CreateProductRestModel productRestModel) {
						String productId = UUID.randomUUID().toString();

						ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(productId,
								productRestModel.getTitle(), productRestModel.getPrice(),
								productRestModel.getQuantity());

						CompletableFuture<SendResult<String, ProductCreatedEvent>> future =
								kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent);

						future.whenComplete((result, exception) -> {
							if (exception != null) {
								LOGGER.error("Failed to send message: " + exception.getMessage());
							} else {
								LOGGER.info("Message sent successfully: " + result.getRecordMetadata());
							}
						});

						LOGGER.info("Returning product id");

						return productId;
					}
				}

			Case 2:
			
				package com.appsdeveloperblog.ws.products.service;

				import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.http.HttpStatus;
				import org.springframework.http.ResponseEntity;
				import org.springframework.kafka.core.KafkaTemplate;
				import org.springframework.kafka.support.SendResult;
				import org.springframework.stereotype.Service;

				import java.util.UUID;
				import java.util.concurrent.CompletableFuture;

				@Service
				public class ProductServiceImpl implements ProductService {

					KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;

					private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

					public ProductServiceImpl(KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate) {
						this.kafkaTemplate = kafkaTemplate;
					}

					@Override
					public String createProduct(CreateProductRestModel productRestModel) throws Exception {
						String productId = UUID.randomUUID().toString();

						ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(productId,
								productRestModel.getTitle(), productRestModel.getPrice(),
								productRestModel.getQuantity());

						LOGGER.info("Before publising a ProductCreatedEvent");

						SendResult<String, ProductCreatedEvent> result =
								kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent).get();

						LOGGER.info("Partition: " + result.getRecordMetadata().partition());
						LOGGER.info("Topic: " + result.getRecordMetadata().topic());
						LOGGER.info("Offset: " + result.getRecordMetadata().offset());

						LOGGER.info("Returning product id");

						return productId;
					}
				}
				
		S·ª≠a file ProductController.java:
		
			Case 1:
			
				package com.appsdeveloperblog.ws.products.rest;

				import com.appsdeveloperblog.ws.products.service.ProductService;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.http.HttpStatus;
				import org.springframework.http.ResponseEntity;
				import org.springframework.web.bind.annotation.PostMapping;
				import org.springframework.web.bind.annotation.RequestBody;
				import org.springframework.web.bind.annotation.RequestMapping;
				import org.springframework.web.bind.annotation.RestController;

				import java.util.Date;

				@RestController
				@RequestMapping("/products")
				public class ProductController {

					ProductService productService;

					private final Logger logger = LoggerFactory.getLogger(this.getClass());

					public ProductController(ProductService productService) {
						this.productService = productService;
					}

					@PostMapping
					public ResponseEntity<Object> createProduct(@RequestBody CreateProductRestModel product) {
						String productId;

						try {
							productId = productService.createProduct(product);
						} catch (Exception e) {
							logger.error(e.getMessage(), e);
							return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
									.body(new ErrorMessage(new Date(), e.getMessage(), "/products"));
						}

						return ResponseEntity.status(HttpStatus.CREATED).body(productId);
					}


				}

--- Kafka Consumer - Spring Boot Microservice:

		Trong file pom.xml ch√∫ √Ω th√™m ƒëo·∫°n code sau:
		
			<dependency>
				<groupId>com.fasterxml.jackson.core</groupId>
				<artifactId>jackson-databind</artifactId>
				<version>2.17.1</version>
			</dependency>

		C√°ch 1 class ƒë∆∞·ª£c d√πng chung ·ªü c√°c package kh√°c nhau:

			V√¨ project core ch·ª©a class d√πng chung v·ªõi c√°c project kh√°c th√¨ core ph·∫£i ƒë∆∞·ª£c build tr∆∞·ªõc
		
			Trong core x√≥a file application v√† file test application, trong pom.xml x√≥a maven-plugin
			
			Trong pom.xml c·ªßa c√°c project kh√°c s·∫Ω import link c·ªßa project core v√†o
			
			
		C√°ch 1 d√πng c·∫•u h√¨nh trong application.properties:
		
			core project:
			
				ProductCreatedEvent.java:
				
					package com.appsdeveloperblog.ws.core;

					import java.math.BigDecimal;

					public class ProductCreatedEvent {

						private  String productId;
						private String title;
						private BigDecimal price;
						private Integer quantity;

						public ProductCreatedEvent() {

						}

						public ProductCreatedEvent(String productId, String title, BigDecimal price, Integer quantity) {
							this.productId = productId;
							this.title = title;
							this.price = price;
							this.quantity = quantity;
						}

						public String getProductId() {
							return productId;
						}

						public String getTitle() {
							return title;
						}

						public BigDecimal getPrice() {
							return price;
						}

						public Integer getQuantity() {
							return quantity;
						}

						public void setProductId(String productId) {
							this.productId = productId;
						}

						public void setTitle(String title) {
							this.title = title;
						}

						public void setPrice(BigDecimal price) {
							this.price = price;
						}

					}

			
			EmailNotificationMicroservice project:
			
				ProductCreatedEventHandler.java:
				
					package com.appsdeveloperblog.ws.EmailNotificationMicroservice.handler;

					import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.kafka.annotation.KafkaHandler;
					import org.springframework.kafka.annotation.KafkaListener;
					import org.springframework.stereotype.Component;

					@Component
					@KafkaListener(topics = "product-created-events-topic")
					public class ProductCreatedEventHandler {

						private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

						@KafkaHandler
						public void handle(ProductCreatedEvent productCreatedEvent) {
							LOGGER.info("Received a new event: " + productCreatedEvent.getTitle());
						}

					}
					
				application.properties:
				
					server.port=0
					spring.kafka.consumer.bootstrap-servers=172.28.85.33:9092,172.28.85.33:9094
					spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
					spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
					spring.kafka.consumer.group-id=product-created-events
					spring.kafka.consumer.properties.spring.json.trusted.packages=*

			
			ProductMicroservice project:
			
				KafkaConfig.java:
				
				package com.appsdeveloperblog.ws.products;

				import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
				import org.apache.kafka.clients.admin.NewTopic;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.context.annotation.Bean;
				import org.springframework.context.annotation.Configuration;
				import org.springframework.kafka.annotation.KafkaHandler;
				import org.springframework.kafka.annotation.KafkaListener;
				import org.springframework.kafka.config.TopicBuilder;
				import org.springframework.stereotype.Component;

				import java.util.Map;

				@Configuration
				public class KafkaConfig {

					@Bean
					NewTopic createTopic() {
						return TopicBuilder.name("product-created-events-topic")
								.partitions(3)
								.replicas(3)
								.configs(Map.of("min.insync.replicas", "2"))
								.build();
					}

				}

			CreateProductRestModel.java:
			
				package com.appsdeveloperblog.ws.products.rest;

				import java.math.BigDecimal;

				public class CreateProductRestModel {

					private String title;
					private BigDecimal price;
					private Integer quantity;

					public String getTitle() {
						return title;
					}

					public BigDecimal getPrice() {
						return price;
					}

					public Integer getQuantity() {
						return quantity;
					}

					public void setTitle(String title) {
						this.title = title;
					}

					public void setPrice(BigDecimal price) {
						this.price = price;
					}

					public void setQuantity(Integer quantity) {
						this.quantity = quantity;
					}

				}
				
			ErrorMessage.java:
			
				package com.appsdeveloperblog.ws.products.rest;

				import java.util.Date;

				public class ErrorMessage {

					private Date timestamp;
					private String message;
					private String details;


					public ErrorMessage() {
					}

					public ErrorMessage(Date timestamp, String message, String details) {
						super();
						this.timestamp = timestamp;
						this.message = message;
						this.details = details;
					}

					public Date getTimestamp() {
						return timestamp;
					}

					public void setTimestamp(Date timestamp) {
						this.timestamp = timestamp;
					}

					public String getMessage() {
						return message;
					}

					public void setMessage(String message) {
						this.message = message;
					}

					public String getDetails() {
						return details;
					}

					public void setDetails(String details) {
						this.details = details;
					}
				}
				
			ProductController.java:
			
				package com.appsdeveloperblog.ws.products.rest;

				import com.appsdeveloperblog.ws.products.service.ProductService;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.http.HttpStatus;
				import org.springframework.http.ResponseEntity;
				import org.springframework.web.bind.annotation.PostMapping;
				import org.springframework.web.bind.annotation.RequestBody;
				import org.springframework.web.bind.annotation.RequestMapping;
				import org.springframework.web.bind.annotation.RestController;

				import java.util.Date;

				@RestController
				@RequestMapping("/products")
				public class ProductController {

					ProductService productService;

					private final Logger logger = LoggerFactory.getLogger(this.getClass());

					public ProductController(ProductService productService) {
						this.productService = productService;
					}

					@PostMapping
					public ResponseEntity<Object> createProduct(@RequestBody CreateProductRestModel product) {
						String productId;

						try {
							productId = productService.createProduct(product);
						} catch (Exception e) {
							logger.error(e.getMessage(), e);
							return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
									.body(new ErrorMessage(new Date(), e.getMessage(), "/products"));
						}

						return ResponseEntity.status(HttpStatus.CREATED).body(productId);
					}

				}
				
			ProductService.java:
			
				package com.appsdeveloperblog.ws.products.service;

				import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;

				public interface ProductService {

					String createProduct(CreateProductRestModel productRestModel) throws Exception;
				}
				
			ProductServiceImpl.java:
			
				package com.appsdeveloperblog.ws.products.service;

				import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
				import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;
				import org.slf4j.Logger;
				import org.slf4j.LoggerFactory;
				import org.springframework.http.HttpStatus;
				import org.springframework.http.ResponseEntity;
				import org.springframework.kafka.core.KafkaTemplate;
				import org.springframework.kafka.support.SendResult;
				import org.springframework.stereotype.Service;

				import java.util.UUID;
				import java.util.concurrent.CompletableFuture;

				@Service
				public class ProductServiceImpl implements ProductService {

					KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;

					private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

					public ProductServiceImpl(KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate) {
						this.kafkaTemplate = kafkaTemplate;
					}

					@Override
					public String createProduct(CreateProductRestModel productRestModel) throws Exception {
						String productId = UUID.randomUUID().toString();

						ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(productId,
								productRestModel.getTitle(), productRestModel.getPrice(),
								productRestModel.getQuantity());

						LOGGER.info("Before publising a ProductCreatedEvent");

						SendResult<String, ProductCreatedEvent> result =
								kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent).get();

						LOGGER.info("Partition: " + result.getRecordMetadata().partition());
						LOGGER.info("Topic: " + result.getRecordMetadata().topic());
						LOGGER.info("Offset: " + result.getRecordMetadata().offset());

						LOGGER.info("Returning product id");

						return productId;
					}
				}
				
				KafkaConfig.java:
				
					package com.appsdeveloperblog.ws.products;

					import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
					import org.apache.kafka.clients.admin.NewTopic;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.context.annotation.Bean;
					import org.springframework.context.annotation.Configuration;
					import org.springframework.kafka.annotation.KafkaHandler;
					import org.springframework.kafka.annotation.KafkaListener;
					import org.springframework.kafka.config.TopicBuilder;
					import org.springframework.stereotype.Component;

					import java.util.Map;

					@Configuration
					//@Component
					//@KafkaListener(topics = "product-created-events-topic")
					public class KafkaConfig {

						@Bean
						NewTopic createTopic() {
							return TopicBuilder.name("product-created-events-topic")
									.partitions(3)
									.replicas(3)
									.configs(Map.of("min.insync.replicas", "2"))
									.build();
						}

					//    private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());
					//
					//    @KafkaHandler
					//    public void handle(ProductCreatedEvent productCreatedEvent) {
					//        LOGGER.info("Recieved a new event: " + productCreatedEvent.getTitle());
					//    }

					}
				
			application.properties:
			
				server.port=0
				spring.kafka.bootstrap-servers=172.28.85.33:9092,172.28.85.33:9094
				spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
				spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer
				
		C√°ch 2 d√πng c·∫•u h√¨nh trong file java:
		
			core project:
			
				ProductCreatedEvent.java:
				
					package com.appsdeveloperblog.ws.core;

					import java.math.BigDecimal;

					public class ProductCreatedEvent {

						private  String productId;
						private String title;
						private BigDecimal price;
						private Integer quantity;

						public ProductCreatedEvent() {

						}

						public ProductCreatedEvent(String productId, String title, BigDecimal price, Integer quantity) {
							this.productId = productId;
							this.title = title;
							this.price = price;
							this.quantity = quantity;
						}

						public String getProductId() {
							return productId;
						}

						public String getTitle() {
							return title;
						}

						public BigDecimal getPrice() {
							return price;
						}

						public Integer getQuantity() {
							return quantity;
						}

						public void setProductId(String productId) {
							this.productId = productId;
						}

						public void setTitle(String title) {
							this.title = title;
						}

						public void setPrice(BigDecimal price) {
							this.price = price;
						}

					}


			
			EmailNotificationMicroservice project:
			
				ProductCreatedEventHandler.java:
				
					package com.appsdeveloperblog.ws.EmailNotificationMicroservice.handler;

					import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.kafka.annotation.KafkaHandler;
					import org.springframework.kafka.annotation.KafkaListener;
					import org.springframework.stereotype.Component;

					@Component
					@KafkaListener(topics = "product-created-events-topic")
					public class ProductCreatedEventHandler {

						private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

						@KafkaHandler
						public void handle(ProductCreatedEvent productCreatedEvent) {
							LOGGER.info("Received a new event: " + productCreatedEvent.getTitle());
						}

					}
				KafkaConsumerConfiguration.java:
				
					package com.appsdeveloperblog.ws.EmailNotificationMicroservice;

					import org.apache.kafka.clients.consumer.ConsumerConfig;
					import org.apache.kafka.common.serialization.StringDeserializer;
					import org.springframework.beans.factory.annotation.Autowired;
					import org.springframework.context.annotation.Bean;
					import org.springframework.context.annotation.Configuration;
					import org.springframework.core.env.Environment;
					import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
					import org.springframework.kafka.core.ConsumerFactory;
					import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
					import org.springframework.kafka.support.serializer.ErrorHandlingDeserializer;
					import org.springframework.kafka.support.serializer.JsonDeserializer;

					import java.util.HashMap;
					import java.util.Map;
					import java.util.function.Consumer;

					@Configuration
					public class KafkaConsumerConfiguration {

						@Autowired
						Environment environment;

						@Bean
						public ConsumerFactory<String, Object> consumerFactory() {
							Map<String, Object> config = new HashMap<>();
							config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
									environment.getProperty("spring.kafka.consumer.bootstrap-servers"));
							config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
									StringDeserializer.class);
					//        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
					//                JsonDeserializer.class);
					//        config.put(JsonDeserializer.TRUSTED_PACKAGES,
					//                environment.getProperty("spring.kafka.consumer.properties.spring.json.trusted.packages"));

							config.put(ConsumerConfig.GROUP_ID_CONFIG,
									environment.getProperty("spring.kafka.consumer.group-id"));

							// üî• CHU·∫®N M·ªöI ‚Äì KH√îNG DEPRECATED
							config.put(
									ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
									ErrorHandlingDeserializer.class
							);

							config.put(
									ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS,
									JsonDeserializer.class
							);

							// Khai b√°o type r√µ r√†ng (R·∫§T QUAN TR·ªåNG)
							config.put(
									JsonDeserializer.VALUE_DEFAULT_TYPE,
									"com.appsdeveloperblog.ws.core.ProductCreatedEvent"
							);

							config.put(
									JsonDeserializer.TRUSTED_PACKAGES,
									"com.appsdeveloperblog.ws.EmailNotificationMicroservice"
							);



							return new DefaultKafkaConsumerFactory<>(config);
						}

						@Bean
						ConcurrentKafkaListenerContainerFactory<String, Object> kafkaListenerContainerFactory(
								ConsumerFactory<String, Object> consumerFactory) {

							ConcurrentKafkaListenerContainerFactory<String, Object> factory =
									new ConcurrentKafkaListenerContainerFactory<>();
							factory.setConsumerFactory(consumerFactory);

							return factory;

						}
					}

				application.properties:
				
					server.port=0
					spring.kafka.consumer.bootstrap-servers=172.28.85.33:9092,172.28.85.33:9094
					#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
					#spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
					spring.kafka.consumer.group-id=product-created-events
					spring.kafka.consumer.properties.spring.json.trusted.packages=com.appsdeveloperblog.ws.core
			
			ProductMicroservice project:
			
				

				CreateProductRestModel.java:
				
					package com.appsdeveloperblog.ws.products.rest;

					import java.math.BigDecimal;

					public class CreateProductRestModel {

						private String title;
						private BigDecimal price;
						private Integer quantity;

						public String getTitle() {
							return title;
						}

						public BigDecimal getPrice() {
							return price;
						}

						public Integer getQuantity() {
							return quantity;
						}

						public void setTitle(String title) {
							this.title = title;
						}

						public void setPrice(BigDecimal price) {
							this.price = price;
						}

						public void setQuantity(Integer quantity) {
							this.quantity = quantity;
						}

					}
					
					
				ErrorMessage.java:
				
					package com.appsdeveloperblog.ws.products.rest;

					import java.util.Date;

					public class ErrorMessage {

						private Date timestamp;
						private String message;
						private String details;


						public ErrorMessage() {
						}

						public ErrorMessage(Date timestamp, String message, String details) {
							super();
							this.timestamp = timestamp;
							this.message = message;
							this.details = details;
						}

						public Date getTimestamp() {
							return timestamp;
						}

						public void setTimestamp(Date timestamp) {
							this.timestamp = timestamp;
						}

						public String getMessage() {
							return message;
						}

						public void setMessage(String message) {
							this.message = message;
						}

						public String getDetails() {
							return details;
						}

						public void setDetails(String details) {
							this.details = details;
						}
					}

					
				ProductController.java:
				
					package com.appsdeveloperblog.ws.products.rest;

					import com.appsdeveloperblog.ws.products.service.ProductService;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.http.HttpStatus;
					import org.springframework.http.ResponseEntity;
					import org.springframework.web.bind.annotation.PostMapping;
					import org.springframework.web.bind.annotation.RequestBody;
					import org.springframework.web.bind.annotation.RequestMapping;
					import org.springframework.web.bind.annotation.RestController;

					import java.util.Date;

					@RestController
					@RequestMapping("/products")
					public class ProductController {

						ProductService productService;

						private final Logger logger = LoggerFactory.getLogger(this.getClass());

						public ProductController(ProductService productService) {
							this.productService = productService;
						}

						@PostMapping
						public ResponseEntity<Object> createProduct(@RequestBody CreateProductRestModel product) {
							String productId;

							try {
								productId = productService.createProduct(product);
							} catch (Exception e) {
								logger.error(e.getMessage(), e);
								return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
										.body(new ErrorMessage(new Date(), e.getMessage(), "/products"));
							}

							return ResponseEntity.status(HttpStatus.CREATED).body(productId);
						}


					}
					
					
				ProductService.java:
				
					package com.appsdeveloperblog.ws.products.service;

					import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;

					public interface ProductService {

						String createProduct(CreateProductRestModel productRestModel) throws Exception;
					}

					
				ProductServiceImpl.java:
				
					package com.appsdeveloperblog.ws.products.service;

					import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
					import com.appsdeveloperblog.ws.products.rest.CreateProductRestModel;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.http.HttpStatus;
					import org.springframework.http.ResponseEntity;
					import org.springframework.kafka.core.KafkaTemplate;
					import org.springframework.kafka.support.SendResult;
					import org.springframework.stereotype.Service;

					import java.util.UUID;
					import java.util.concurrent.CompletableFuture;

					@Service
					public class ProductServiceImpl implements ProductService {

						KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;

						private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());

						public ProductServiceImpl(KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate) {
							this.kafkaTemplate = kafkaTemplate;
						}

						@Override
						public String createProduct(CreateProductRestModel productRestModel) throws Exception {
							String productId = UUID.randomUUID().toString();

							ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(productId,
									productRestModel.getTitle(), productRestModel.getPrice(),
									productRestModel.getQuantity());

							LOGGER.info("Before publising a ProductCreatedEvent");

							SendResult<String, ProductCreatedEvent> result =
									kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent).get();

							LOGGER.info("Partition: " + result.getRecordMetadata().partition());
							LOGGER.info("Topic: " + result.getRecordMetadata().topic());
							LOGGER.info("Offset: " + result.getRecordMetadata().offset());

							LOGGER.info("Returning product id");

							return productId;
						}
					}
					
				KafkaConfig.java:
				
					package com.appsdeveloperblog.ws.products;

					import com.appsdeveloperblog.ws.core.ProductCreatedEvent;
					import org.apache.kafka.clients.admin.NewTopic;
					import org.slf4j.Logger;
					import org.slf4j.LoggerFactory;
					import org.springframework.context.annotation.Bean;
					import org.springframework.context.annotation.Configuration;
					import org.springframework.kafka.annotation.KafkaHandler;
					import org.springframework.kafka.annotation.KafkaListener;
					import org.springframework.kafka.config.TopicBuilder;
					import org.springframework.stereotype.Component;

					import java.util.Map;

					@Configuration
					//@Component
					//@KafkaListener(topics = "product-created-events-topic")
					public class KafkaConfig {

						@Bean
						NewTopic createTopic() {
							return TopicBuilder.name("product-created-events-topic")
									.partitions(3)
									.replicas(3)
									.configs(Map.of("min.insync.replicas", "2"))
									.build();
						}

					//    private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());
					//
					//    @KafkaHandler
					//    public void handle(ProductCreatedEvent productCreatedEvent) {
					//        LOGGER.info("Recieved a new event: " + productCreatedEvent.getTitle());
					//    }

					}


					
				application.properties:
			
					server.port=0
					spring.kafka.bootstrap-servers=172.28.85.33:9092,172.28.85.33:9094
					spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
					spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer





